{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import qdrant_client\n",
    "from fastembed import SparseEmbedding, SparseTextEmbedding\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from qdrant_client.models import (\n",
    "    Distance,\n",
    "    NamedSparseVector,\n",
    "    PointStruct,\n",
    "    SearchRequest,\n",
    "    SparseIndexParams,\n",
    "    SparseVector,\n",
    "    SparseVectorParams,\n",
    "    VectorParams,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "import create_embeddings as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf\n",
      "/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf\n",
      "/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf\n",
      "/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf\n",
      "/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf\n",
      "/Users/carsten/Documents/Science/LLM/MemGPT Towards LLMs as Operating Systems.pdf\n",
      "/Users/carsten/Documents/Science/LLM/Learn Generative AI with PyTorch.pdf\n"
     ]
    }
   ],
   "source": [
    "rootdir = \"/Users/carsten/Documents/Science/LLM\"\n",
    "docs = ce.read_pdfs(rootdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 0}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 1}, page_content='1 \\n \\n“Andriy\\'s long-awaited sequel in his \"The Hundred-Page\" series of ma-\\nchine learning textbooks is a masterpiece of concision.”  \\n― Bob van Luijt, CEO and Co-Founder of Weaviate \\n“Andriy has this almost supernatural talent for shrinking epic AI con-\\ncepts down to bite-sized, ‘Ah, now I get it!’ moments.” \\n ― Jorge Torres, CEO at MindsDB \\n“Andriy paints for us, in 100 marvelous strokes, the journey from lin-\\near algebra basics to the implementation of transformers.” \\n ― Florian Douetteau, Co-founder and CEO at Dataiku \\n“Andriy\\'s book is an incredibly concise, clear, and accessible introduc-\\ntion to machine learning.” \\n ― Andre Zayarni, Co-founder and CEO at Qdrant \\n“This is one of the most comprehensive yet concise handbooks out there \\nfor truly understanding how LLMs work under the hood.” \\n ― Jerry Liu, Co-founder and CEO at LlamaIndex \\n \\n \\n \\nFeaturing a foreword by Tomáš Mikolov and back cover text by Vint Cerf'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 2}, page_content='2 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThe Hundred-Page Language Models Book \\n \\nAndriy Burkov'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 3}, page_content='3 \\n \\nCopyright © 2025 Andriy Burkov. All rights reserved. \\n \\n1. Read First, Buy Later: You are welcome to freely read and share this book \\nwith others by preserving this copyright notice. However, if you find the \\nbook valuable or continue to use it, you must purchase your own copy. This \\nensures fairness and supports the author. \\n2. No Unauthorized Use: No part of this work—its text, structure, or deriva-\\ntives—may be used to train artificial intelligence or machine learning mod-\\nels, nor to generate any content on websites, apps, or other services, without \\nthe author’s explicit written consent. This restriction applies to all forms of \\nautomated or algorithmic processing. \\n3. Permission Required If you operate any website, app, or service and wish \\nto use any portion of this work for the purposes mentioned above—or for \\nany other use beyond personal reading—you must first obtain the author’s \\nexplicit written permission. No exceptions or implied licenses are granted. \\n4. Enforcement: Any violation of these terms is copyright infringement. It \\nmay be pursued legally in any jurisdiction. By reading or distributing this \\nbook, you agree to abide by these conditions. \\nISBN 978-1-7780427-2-0  \\nPublisher: True Positive Inc.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 4}, page_content='4 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTo my family, with love'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 5}, page_content=\"5 \\n \\n“Language is the source of misunderstandings.”  \\n―Antoine de Saint-Exupéry, The Little Prince \\n \\n“In mathematics you don't understand things. You just get used to them.” \\n ―John von Neumann \\n \\n“Computers are useless. They can only give you answers.” \\n ― Pablo Picasso \\n \\n \\n \\n \\n \\nThe book is distributed on the “read first, buy later” principle\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 6}, page_content='6 \\n \\nContents \\nForeword \\n9 \\nPreface \\n11 \\nWho This Book Is For \\n11 \\nWhat This Book Is Not \\n12 \\nBook Structure \\n13 \\nShould You Buy This Book? \\n14 \\nAcknowledgements \\n15 \\nChapter 1. Machine Learning Basics \\n16 \\n1.1. AI and Machine Learning \\n16 \\n1.2. Model \\n16 \\n1.3. Four-Step Machine Learning Process \\n28 \\n1.4. Vector \\n28 \\n1.5. Neural Network \\n32 \\n1.6. Matrix \\n37 \\n1.7. Gradient Descent \\n40 \\n1.8. Automatic Differentiation \\n45 \\nChapter 2. Language Modeling Basics \\n50 \\n2.1. Bag of Words \\n50 \\n2.2. Word Embeddings \\n63 \\n2.3. Byte-Pair Encoding \\n70 \\n2.4. Language Model \\n75 \\n2.5. Count-Based Language Model \\n77 \\n2.6. Evaluating Language Models \\n84 \\nChapter 3. Recurrent Neural Network \\n98 \\n3.1. Elman RNN \\n98 \\n3.2. Mini-Batch Gradient Descent \\n100 \\n3.3. Programming an RNN \\n101 \\n3.4. RNN as a Language Model \\n104'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 7}, page_content='7 \\n3.5. Embedding Layer \\n105 \\n3.6. Training an RNN Language Model \\n107 \\n3.7. Dataset and DataLoader \\n111 \\n3.8. Training Data and Loss Computation \\n113 \\nChapter 4. Transformer \\n117 \\n4.1. Decoder Block \\n117 \\n4.2. Self-Attention \\n119 \\n4.3. Position-Wise Multilayer Perceptron \\n123 \\n4.4. Rotary Position Embedding \\n124 \\n4.5. Multi-Head Attention \\n131 \\n4.6. Residual Connection \\n133 \\n4.7. Root Mean Square Normalization \\n136 \\n4.8. Key-Value Caching \\n138 \\n4.9. Transformer in Python \\n139 \\nChapter 5. Large Language Model \\n147 \\n5.1. Why Larger Is Better \\n147 \\n5.2. Supervised Finetuning \\n154 \\n5.3. Finetuning a Pretrained Model \\n156 \\n5.4. Sampling From Language Models \\n171 \\n5.5. Low-Rank Adaptation (LoRA) \\n176 \\n5.6. LLM as a Classifier \\n180 \\n5.7. Prompt Engineering \\n182 \\n5.8. Hallucinations \\n188 \\n5.9. LLMs, Copyright, and Ethics \\n191 \\nChapter 6. Further Reading \\n195 \\n6.1. Mixture of Experts \\n195 \\n6.2. Model Merging \\n195 \\n6.3. Model Compression \\n196'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 8}, page_content='8 \\n \\n6.4. Preference-Based Alignment \\n196 \\n6.5. Advanced Reasoning \\n196 \\n6.6. Language Model Security \\n197 \\n6.7. Vision Language Model \\n197 \\n6.8. Preventing Overfitting \\n198 \\n6.9. Concluding Remarks \\n198 \\n6.10. More From the Author \\n199 \\nIndex \\n201'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 9}, page_content=\"9 \\nForeword \\nFirst time I got involved in language modeling was already two decades ago. I \\nwanted to improve some of my data compression algorithms and found out \\nabout the n-gram statistics. Very simple concept, but so hard to beat! Then I \\nquickly gained another motivation—since my childhood, I was interested in \\nartificial intelligence. I had a vision of machines that would understand pat-\\nterns in our world that are hidden from our limited minds. It would be so \\nexciting to talk with such super-intelligence. And I realized that language mod-\\neling could be a way towards such AI. \\nI started searching for others sharing this vision and did find the works of Sol-\\nomonoff, Schmidhuber and the Hutter prize competition organized by Matt \\nMahoney. They all did write about AI completeness of language modeling and \\nI knew I had to try to make it work. But the world was very different than it is \\ntoday. Language modeling was considered a dead research direction, and I've \\nheard countless times that I should give up as nothing will ever beat n-grams \\non large data. \\nI've completed my master's thesis on neural language models, as these models \\nwere quite like what I previously developed for data compression, and I did \\nbelieve the distributed representations that could be applied to any language \\nis the right way to go. This infuriated a local linguist who declared my ideas to \\nbe a total nonsense as language modeling has to be addressed from the lin-\\nguistics point of view, and each language had to be treated differently. \\nHowever, I did not give up and did continue working on my vision of AI-com-\\nplete language models. Just the summer before starting my PhD, I did come \\nup with the idea to generate text from these neural models. I was amazed by \\nhow much better this text was than text generated from n-grams models. That \\nwas summer 2007 and I quickly realized the only person excited about this at \\nthe Brno University of Technology was actually me. But I did not give up any-\\nways. \\nIn the following years, I did develop a number of algorithms to make neural \\nlanguage models more useful. To convince others about their qualities, I pub-\\nlished open-source toolkit RNNLM in 2010. It had the first implementations \\never of neural text generation, gradient clipping, dynamic evaluation, model \\nadaptation (nowadays called fine-tuning) and other tricks such as hierarchical \\nsoftmax or splitting infrequent words into subword units. However, the result\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 10}, page_content=\"10 \\n \\nI was the most proud of was when I could demonstrate in my PhD thesis that \\nneural language models not only beat n-grams on large datasets—something \\nwidely considered to be impossible at the time—but the improvements were \\nactually increasing with the amount of training data. This happened for the \\nfirst time after something like fifty years of language modeling research and I \\nstill remember the disbelief in faces of famous researchers when I showed them \\nmy work. \\nFast forward some fifteen years, and I'm amazed by how much the world has \\nchanged. The mindset completely flipped—what used to be some obscure tech-\\nnology in a dead research direction is now thriving and gets the attention of \\nCEOs of the largest companies in the world. Language models are everywhere \\ntoday. With all this hype, I think it is needed more than ever to actually under-\\nstand this technology. \\nYoung students who want to learn about language modeling are flooded with \\ninformation. Thus, I was delighted when I learned about Andriy's project to \\nwrite a short book with only one hundred pages that would cover some of the \\nmost important ideas. I think the book is a good start for anyone new to lan-\\nguage modeling who aspires to improve on state of the art—and if someone \\ntells you that everything that could have been invented in language modeling \\nhas already been discovered, don't believe it. \\nTomáš Mikolov, Senior Researcher at Czech Institute of Informatics, \\nRobotics and Cybernetics, the author of word2vec and FastText\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 11}, page_content='11 \\nPreface \\nMy interest in text began in the late 1990s during my teenage years, building \\ndynamic websites using Perl and HTML. This early experience with coding and \\norganizing text into structured formats sparked my fascination with how text \\ncould be processed and transformed. Over the years, I advanced to building \\nweb scrapers and text aggregators, developing systems to extract structured \\ndata from webpages. The challenge of processing and understanding text led \\nme to explore more complex applications, including designing chatbots that \\ncould understand and address user needs. \\nThe challenge of extracting meaning from words intrigued me. The complexity \\nof the task only fueled my determination to “crack” it, using every tool at my \\ndisposal—ranging from regular expressions and scripting languages to text \\nclassifiers and named entity recognition models. \\nThe rise of large language models (LLMs) transformed everything. For the first \\ntime, computers could converse with us fluently and follow verbal instructions \\nwith remarkable precision. However, like any tool, their immense power comes \\nwith limitations. Some are easy to spot, but others are more subtle, requiring \\ndeep expertise to handle properly. Attempting to build a skyscraper without \\nfully understanding your tools will only result in a pile of concrete and steel. \\nThe same holds true for language models. Approaching large-scale text pro-\\ncessing tasks or creating reliable products for paying users requires precision \\nand knowledge—guesswork simply isn’t an option. \\nWho This Book Is For \\nI wrote this book for those who, like me, are captivated by the challenge of \\nunderstanding language through machines. Language models are, at their \\ncore, just mathematical functions. However, their true potential isn’t fully ap-\\npreciated in theory—you need to implement them to see their power and how \\ntheir abilities grow as they scale. This is why I decided to make this book \\nhands-on. \\nThis book serves software developers, data scientists, machine learning engi-\\nneers, and anyone curious about language models. Whether your goal is to \\nintegrate existing models into applications or to train your own, you’ll find \\npractical guidance alongside theoretical foundations.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 12}, page_content='12 \\n \\nGiven its hundred-page format, the book makes certain assumptions about \\nreaders. You should have programming experience, as all hands-on examples \\nuse Python. \\nWhile familiarity with PyTorch and tensors—PyTorch’s fundamental data \\ntypes—is beneficial, it’s not mandatory. If you’re new to these tools, the book’s \\nwiki (thelmbook.com/wiki) provides a concise introduction with examples and \\nresource links for further learning. This wiki format ensures content remains \\ncurrent and addresses reader questions beyond publication. \\nCollege-level math knowledge helps, but you needn’t remember every detail \\nor have machine learning experience. The book introduces concepts systemat-\\nically, beginning with notations, definitions, and fundamental vector and ma-\\ntrix operations. From there, it progresses through simple neural networks to \\nmore advanced topics. Mathematical concepts are presented intuitively, with \\nclear diagrams and examples that facilitate understanding. \\nWhat This Book Is Not \\nThis book is focused on understanding and implementing language models. It \\nwill not cover: \\n• Large-scale training: This book won’t teach you how to train massive \\nmodels on distributed systems or how to manage training infrastruc-\\nture. \\n• Production deployment: Topics like model serving, API development, \\nscaling for high traffic, monitoring, and cost optimization are not cov-\\nered. The code examples focus on understanding the concepts rather \\nthan production readiness. \\n• Enterprise applications: This book won’t guide you through building \\ncommercial LLM applications, handling user data, or integrating with \\nexisting systems. \\nIf you’re interested in learning the mathematical foundations of language mod-\\nels, understanding how they work, implementing core components yourself, \\nor learning to work effectively with LLMs, this book is for you. But if you’re \\nprimarily looking to deploy models in production or build scalable applica-\\ntions, you may want to supplement this book with other resources.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 13}, page_content='13 \\nBook Structure \\nTo make this book engaging and to deepen the reader’s understanding, I de-\\ncided to discuss language modeling as a whole, including approaches that are \\noften overlooked in modern literature. While Transformer-based LLMs domi-\\nnate the spotlight, earlier approaches like count-based methods and recurrent \\nneural networks (RNNs) remain effective for some tasks. \\nLearning the math of the Transformer architecture from scratch may seem \\noverwhelming for someone starting from scratch. By revisiting these founda-\\ntional methods, my goal is to gradually build up the reader’s intuition and \\nmathematical understanding, making the transition to modern Transformer \\narchitectures feel like a natural progression rather than an intimidating leap. \\nThe book is divided into six chapters, progressing from fundamentals to ad-\\nvanced topics: \\n• Chapter 1 covers machine learning basics, including key concepts like \\nAI, models, neural networks, and gradient descent. Even if you’re famil-\\niar with these topics, the chapter provides important foundations for \\nunderstanding language models. \\n• Chapter 2 introduces language modeling fundamentals, exploring text \\nrepresentation methods like bag of words and word embeddings, as \\nwell as count-based language models and evaluation techniques. \\n• Chapter 3 focuses on recurrent neural networks, covering their imple-\\nmentation, training, and application as language models. \\n• Chapter 4 provides a detailed exploration of the Transformer architec-\\nture, including key components like self-attention, position embed-\\ndings, and practical implementation. \\n• Chapter 5 examines large language models (LLMs), discussing why \\nscale matters, finetuning techniques, practical applications, and im-\\nportant considerations around hallucinations, copyright, and ethics. \\n• Chapter 6 concludes with further reading on advanced topics like mix-\\nture of experts, model compression, preference-based alignment, and \\nvision language models, providing direction for continued learning. \\nMost chapters contain working code examples you can run and modify. While \\nonly essential code appears in the book, complete code is available as Jupyter \\nnotebooks on the book’s website, with notebooks referenced in relevant'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 14}, page_content=\"14 \\n \\nsections. All code in notebooks remains compatible with the latest stable ver-\\nsions of Python, PyTorch, and other libraries. \\nThe notebooks run on Google Colab, which at the time of writing offers free \\naccess to computing resources including GPUs and TPUs. These resources, \\nthough, aren’t guaranteed and have usage limits that may vary. Some exam-\\nples might require extended GPU access, potentially involving wait times for \\navailability. If the free tier proves limiting, Colab’s pay-as-you-go option lets \\nyou purchase compute credits for reliable GPU access. While these credits are \\nrelatively affordable by North American standards, costs may be significant \\ndepending on your location. \\nFor those familiar with the Linux command line, GPU cloud services provide \\nanother option through pay-per-time virtual machines with one or more GPUs. \\nThe book’s wiki maintains current information on free and paid notebook or \\nGPU rental services. \\nVerbatim terms and blocks indicate code, code fragments, or code execution \\noutputs. Bold terms link to the book’s term index, and occasionally highlight \\nalgorithm steps. \\nIn this book, we use pip3 to ensure the packages are installed for Python 3. \\nOn most modern systems, you can use pip instead if it's already set up for \\nPython 3. \\nShould You Buy This Book? \\nLike my previous two books, this one is distributed on the read first, buy later \\nprinciple. I firmly believe that paying for content before consuming it means \\nbuying a pig in a poke. At a dealership, you can see and try a car. In a depart-\\nment store, you can try on clothes. Similarly, you should be able to read a book \\nbefore paying for it. \\nThe read first, buy later principle means you can freely download the book, \\nread it, and share it with friends and colleagues. If you find the book helpful \\nor useful in your work, business, or studies—or if you simply enjoy reading \\nit—then buy it.\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 15}, page_content='15 \\n \\nAcknowledgements \\nThe high quality of this book would be impossible without volunteering edi-\\ntors. I especially thank Erman Sert, Viet Hoang Tran Duong, Alex Sherstinsky, \\nKelvin Sundli, and Mladen Korunoski for their systematic contributions. \\nI am also grateful to Alireza Bayat Makou, Taras Shalaiko, Domenico Siciliani, \\nPreethi Raju, Srikumar Sundareshwar, Mathieu Nayrolles, Abhijit Kumar, Gior-\\ngio Mantovani, Abhinav Jain, Steven Finkelstein, Ryan Gaughan, Ankita Guha, \\nHarmanan Kohli, Daniel Gross, Kea Kohv, Marcus Oliveira, Tracey Mercier, \\nPrabin Kumar Nayak, Saptarshi Datta, Gurgen R. Hayrapetyan, Sina Abdidi-\\nzaji, Federico Raimondi Cominesi, Santos Salinas, Anshul Kumar, Arash \\nMirbagheri, Roman Stanek, Jeremy Nguyen, Efim Shuf, Pablo Llopis, Marco \\nCeleri, Tiago Pedro, and Manoj Pillai for their help. \\nIf this is your first time exploring language models, I envy you a little—it’s truly \\nmagical to discover how machines learn to understand the world through nat-\\nural language. \\nI hope you enjoy reading this book as much as I enjoyed writing it. \\nNow grab your tea or coffee, and let’s begin!'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 16}, page_content='16 \\n \\nChapter 1. Machine Learning Basics \\nThis chapter starts with a brief overview of how artificial intelligence has \\nevolved, explains what a machine learning model is, and presents the four \\nsteps of the machine learning process. Then, it covers some math basics like \\nvectors and matrices, introduces neural networks, and wraps up with optimi-\\nzation methods like gradient descent and automatic differentiation. \\n1.1. AI and Machine Learning \\nThe term artificial intelligence (AI) was first introduced in 1955 during a \\nworkshop led by John McCarthy. Researchers at the workshop aimed to ex-\\nplore how machines could use language, form concepts, solve problems like \\nhumans, and improve over time. \\n1.1.1. Early Progress \\nThe field’s first major breakthrough came in 1956 with the Logic Theorist. \\nCreated by Allen Newell, Herbert Simon, and Cliff Shaw, it was the first pro-\\ngram engineered to perform automated reasoning, and has been later de-\\nscribed as “the first artificial intelligence program.” \\nFrank Rosenblatt’s Perceptron (1958) was an early neural network designed \\nto recognize patterns by adjusting its internal parameters based on examples. \\nPerceptron learned a decision boundary—a dividing line that separates exam-\\nples of different classes (e.g., spam versus not spam):'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 17}, page_content='17 \\nAround the same time, in 1959, Arthur Samuel coined the term machine \\nlearning. In his paper, “Some Studies in Machine Learning Using the Game of \\nCheckers,” he described machine learning as “programming computers to learn \\nfrom experience.” \\nAnother notable development of the mid-1960s was ELIZA. Developed in 1967 \\nby Joseph Weizenbaum and being the first chatbot in history, ELIZA gave the \\nillusion of understanding language by matching patterns in users’ text and gen-\\nerating preprogrammed responses. Despite its simplicity, it illustrated the lure \\nof building machines that could appear to think or understand. \\nOptimism about near-future breakthroughs ran high during this period. Her-\\nbert Simon, a future Turing Award recipient, exemplified this enthusiasm \\nwhen he predicted in 1965 that “machines will be capable, within twenty \\nyears, of doing any work a man can do.” Many experts shared this optimism, \\nforecasting that truly human-level AI—often called artificial general intelli-\\ngence (AGI)—was just a few decades away. Interestingly, these predictions \\nmaintained a consistent pattern: decade after decade, AGI remained roughly \\n25 years on the horizon:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 18}, page_content='18 \\n \\n1.1.2. AI Winters \\nAs researchers tried to deliver on early promises, they encountered unforeseen \\ncomplexity. Numerous high-profile projects failed to meet ambitious goals. As \\na consequence, funding and enthusiasm waned significantly between 1975 \\nand 1980, a period now known as the first AI winter. \\nDuring the first AI winter, even the term “AI” became somewhat taboo. \\nMany researchers rebranded their work as “informatics,” “knowledge-\\nbased systems,” or “pattern recognition” to avoid association with AI’s \\nperceived failures. \\nIn the 1980s, a resurgence of interest in expert systems—rule-based software \\ndesigned to replicate specialized human knowledge—promised to capture and \\nautomate domain expertise. These expert systems were part of a broader \\nbranch of AI research known as symbolic AI, often referred to as good old-\\nfashioned AI (GOFAI), which had been a dominant approach since AI’s earliest \\ndays. GOFAI methods relied on explicitly coded rules and symbols to represent \\nknowledge and logic, and while they worked well in narrowly defined areas, \\nthey struggled with scalability and adaptability. \\nFrom 1987 to 2000, AI entered its second winter, when the limitations of sym-\\nbolic methods caused funding to diminish, once again leading to numerous \\nresearch and development projects being put on hold or canceled. \\nDespite these setbacks, new techniques continued to evolve. In particular, de-\\ncision trees, first introduced in 1963 by John Sonquist and James Morgan and \\nthen advanced by Ross Quinlan’s ID3 algorithm in 1986, split data into subsets \\nthrough a tree-like structure. Each node in a tree represents a question about \\nthe data, each branch is an answer, and each leaf provides a prediction. While \\neasy to interpret, decision trees were prone to overfitting, where they adapted \\ntoo closely to training data, reducing their ability to perform well on new, un-\\nseen data. \\n1.1.3. The Modern Era \\nIn the late 1990s and early 2000s, incremental improvements in hardware and \\nthe availability of larger datasets (thanks to the widespread use of the Internet) \\nstarted to lift AI from its second winter. Leo Breiman’s random forest algo-\\nrithm (2001) addressed overfitting in decision trees by creating multiple trees \\non random subsets of the data and then combining their outputs—dramatically \\nimproving predictive accuracy.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 19}, page_content='19 \\nSupport vector machines (SVMs), introduced in 1992 by Vladimir Vapnik and \\nhis colleagues, were another significant step forward. SVMs identify the opti-\\nmal hyperplane that separates data points of different classes with the widest \\nmargin. The introduction of kernel methods allowed SVMs to manage com-\\nplex, non-linear patterns by mapping data into higher-dimensional spaces, \\nmaking it easier to find a suitable separating hyperplane. These innovations \\nplaced SVMs at the center of machine learning research in the early 2000s. \\nA turning point arrived around 2012, when more advanced versions of neural \\nnetworks called deep neural networks began outperforming other techniques \\nin fields like speech and image recognition. Unlike the simple Perceptron, \\nwhich used only a single “layer” of learnable parameters, this deep learning \\napproach stacked multiple layers to tackle much more complex problems. Surg-\\ning computational power, abundant data, and algorithmic advancements con-\\nverged to produce remarkable breakthroughs. As academic and commercial in-\\nterest soared, so did AI’s visibility and funding. \\nToday, AI and machine learning remain intimately entwined. Research and \\nindustry efforts continue to seek ever more capable models that learn complex \\ntasks from data. Although predictions of achieving human-level AI “in just 25 \\nyears” have consistently failed to materialize, AI’s impact on everyday applica-\\ntions is undeniable. \\nThroughout this book, AI refers broadly to techniques that enable machines to \\nsolve problems once considered solvable only by humans, with machine learn-\\ning being its key subfield focusing on creating algorithms learning from collec-\\ntions of examples. These examples can come from nature, be designed by hu-\\nmans, or be generated by other algorithms. The process involves gathering a \\ndataset and building a model from it, which is then used to solve a problem. \\nI will use “learning” and “machine learning” interchangeably to save \\nkeystrokes. \\nLet’s examine what exactly we mean by a model and how it forms the founda-\\ntion of machine learning. \\n1.2. Model \\nA model is typically represented by a mathematical equation: \\n𝑦= 𝑓(𝑥)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 20}, page_content='20 \\n \\nHere, 𝑥 is the input, 𝑦 is the output, and 𝑓 represents a function of 𝑥. A func-\\ntion is a named rule that describes how one set of values is related to another. \\nFormally, a function 𝑓 maps inputs from the domain to outputs in the codo-\\nmain, ensuring each input has exactly one output. The function uses a specific \\nrule or formula to transform the input into the output. \\nIn machine learning, the goal is to compile a dataset of examples and use \\nthem to build 𝑓, so when 𝑓 is applied to a new, unseen 𝑥, it produces a 𝑦 that \\ngives meaningful insight into 𝑥. \\nTo estimate a house’s price based on its area, the dataset might include (area, \\nprice) pairs such as {(150,200), (200,600), … }. Here, the area is measured in \\nm!, and the price is in thousands. \\nCurly brackets denote a set. A set containing 𝑁 elements, ranging from \\n𝑥\" to 𝑥#, is expressed as {𝑥$}$%\"\\n# . \\nImagine we own a house with an area of 250\\u2009m! (about 2691 square feet). To \\nfind a function 𝑓 that returns a reasonable price for this house, testing every \\npossible function is infeasible. Instead, we select a specific structure for 𝑓 and \\nfocus on functions that match this structure. \\nLet’s define the structure for 𝑓 as: \\n \\n𝑓(𝑥) =\\ndef 𝑤𝑥+ 𝑏, \\n(1.1) \\nwhich is a linear function of 𝑥. The formula 𝑤𝑥+ 𝑏 is a linear transformation \\nof 𝑥. \\nThe notation =\\ndef means “equals by definition” or “is defined as.” \\nFor linear functions, determining 𝑓 requires only two values: 𝑤 and 𝑏. These \\nare called the parameters or weights of the model. \\nIn other texts, 𝑤 might be referred to as the slope, coefficient, or weight term. \\nSimilarly, 𝑏 may be called the intercept, constant term, or bias. In this book, \\nwe’ll stick to “weight” for 𝑤 and “bias” for 𝑏, as these terms are widely used in \\nmachine learning. When the meaning is clear, “parameters” and “weights” will \\nbe used interchangeably. \\nFor instance, when 𝑤=\\n!\\n& and 𝑏= 1, the linear function is shown below:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 21}, page_content=\"21 \\n \\nHere, the bias shifts the graph vertically, so the line crosses the 𝑦-axis at 𝑦= 1. \\nThe weight determines the slope, meaning the line rises by 2 units for every 3 \\nunits it moves to the right. \\nMathematically, the function 𝑓(𝑥) = 𝑤𝑥+ 𝑏 is an affine transfor-\\nmation, not a linear one, since true linear transformations require 𝑏=\\n0. However, in machine learning, we often call such models “linear” \\nwhenever the parameters appear linearly in the equation—meaning 𝑤 \\nand 𝑏 are only multiplied by inputs or constants and added, without \\nmultiplying each other, being raised to powers, or appearing inside \\nfunctions like 𝑒'. \\nEven with a simple model like 𝑓(𝑥) = 𝑤𝑥+ 𝑏, the parameters 𝑤 and 𝑏 can take \\ninfinitely many values. To find the best ones, we need a way to measure opti-\\nmality. A natural choice is to minimize the average prediction error when esti-\\nmating house prices from area. Specifically, we want 𝑓(𝑥) = 𝑤𝑥+ 𝑏 to gener-\\nate predictions that match the actual prices as closely as possible.\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 22}, page_content='22 \\n \\nLet our dataset be {(𝑥$, 𝑦$)}$%\"\\n# , where 𝑁 is the size of the dataset and \\n{(𝑥\", 𝑦\"), (𝑥!, 𝑦!), … , (𝑥#, 𝑦#)} are individual examples, with each 𝑥$ being the \\ninput and corresponding 𝑦$ being the target. When examples contain both \\ninputs and targets, the learning process is called supervised. This book focuses \\non supervised machine learning. \\nOther machine learning types include unsupervised learning, where \\nmodels learn patterns from inputs alone, and reinforcement learning, \\nwhere models learn by interacting with environments and receiving \\nrewards or penalties for their actions. \\nWhen 𝑓(𝑥) is applied to 𝑥$, it generates a predicted value 𝑦9$. We can define \\nthe prediction error err(𝑦9$, 𝑦$) for a given example (𝑥$, 𝑦$) as: \\n \\nerr(𝑦9$, 𝑦$) =\\ndef (𝑦9$ −𝑦$)! \\n(1.2) \\nThis expression, called squared error, equals 0 when 𝑦9$ = 𝑦$. This makes \\nsense: no error if predicted price matches the actual price. The further 𝑦9$ devi-\\nates from 𝑦$, the larger the error becomes. Squaring ensures the error is always \\npositive, whether the prediction overshoots or undershoots. \\n \\nWe define 𝑤∗ and 𝑏∗ as the optimal parameter values for 𝑤 and 𝑏 in our func-\\ntion 𝑓, when they minimize the average price prediction error across our da-\\ntaset. This error is calculated using the following expression: \\nerr(𝑦9\", 𝑦\") + err(𝑦9!, 𝑦!) + ⋯+ err(𝑦9#, 𝑦#)\\n𝑁\\n \\nLet’s rewrite the above expression by expanding each err(⋅): \\n(𝑦9\" −𝑦\")! + (𝑦9! −𝑦!)! + ⋯+ (𝑦9# −𝑦#)!\\n𝑁\\n \\nLet’s assign the name 𝐽(𝑤, 𝑏) to our expression, turning it into a function: \\n \\n𝐽(𝑤, 𝑏) =\\ndef (𝑤𝑥\" + 𝑏−𝑦\")! + (𝑤𝑥! + 𝑏−𝑦!)! + ⋯+ (𝑤𝑥# + 𝑏\\n𝑁\\n(1.3) \\nIn the equation defining 𝐽(𝑤, 𝑏), which represents the average prediction error, \\nthe values of 𝑥$ and 𝑦$ for each 𝑖 from 1 to 𝑁 are known since they come from \\nthe dataset. The unknowns are 𝑤 and 𝑏. To determine the optimal 𝑤∗ and 𝑏∗,'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 23}, page_content='23 \\nwe need to minimize 𝐽(𝑤, 𝑏). As this function is quadratic in two variables, \\ncalculus guarantees it has a single minimum. \\nThe expression in Equation 1.3 is referred to as the loss function in the ma-\\nchine learning problem of linear regression. In this case, the loss function is \\nthe mean squared error or MSE. \\nTo find the optimum (minimum or maximum) of a function, we calculate its \\nfirst derivative. When we reach the optimum, the first derivative equals zero. \\nFor functions of two or more variables, like the loss function 𝐽(𝑤, 𝑏), we com-\\npute partial derivatives with respect to each variable. We denote these as \\n)*\\n)\\' \\nfor 𝑤 and \\n)*\\n)+ for 𝑏. \\nTo determine 𝑤∗ and 𝑏∗, we solve the following system of two equations: \\n?\\n∂𝐽\\n∂𝑤\\n= 0\\n∂𝐽\\n∂𝑏\\n= 0\\n \\nWe set the partial derivatives to zero because when this occurs, we are at an \\noptimum.  \\nFortunately, the MSE function’s structure and the model’s linearity allow us to \\nsolve this system of equations analytically. To illustrate, consider a dataset with \\nthree examples: (𝑥\", 𝑦\") = (150,200), (𝑥!, 𝑦!) = (200,600), and (𝑥&, 𝑦&) =\\n(260,500). For this dataset, the loss function is: \\n𝐽(𝑤, 𝑏) =\\ndef (150𝑤+ 𝑏−200)! + (200𝑤+ 𝑏−600)! + (260𝑤+ 𝑏−500)!\\n3\\n \\nLet’s plot it:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 24}, page_content='24 \\n \\n \\nNavigate to the book’s wiki, from the file thelmbook.com/py/1.1 re-\\ntrieve the code used to generate the above plot, run the code, and ro-\\ntate the graph to observe the minimum. \\nNow we need to derive the expressions for \\n)*\\n)\\' and \\n)*\\n)+. Notice that 𝐽(𝑤, 𝑏) is a \\ncomposition of the following functions: \\n• Functions 𝑑\" =\\ndef 150𝑤+ 𝑏−200, 𝑑! =\\ndef 200𝑤+ 𝑏−600, 𝑑& =\\ndef 260𝑤+\\n𝑏−500 are linear functions of 𝑤 and 𝑏; \\n• Functions err\" =\\ndef 𝑑\"\\n!, err! =\\ndef 𝑑!\\n!, err& =\\ndef 𝑑&\\n! are quadratic functions of \\n𝑑\", 𝑑!, and 𝑑&; \\n• Function 𝐽=\\ndef \"\\n& (err\" + err! + err&) is a linear function of err\", err!, and \\nerr&.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 25}, page_content='25 \\nA composition of functions means the output of one function be-\\ncomes the input to another. For example, with two functions 𝑓 and 𝑔, \\nyou first apply 𝑔 to 𝑥, then apply 𝑓 to the result. This is written as \\n𝑓C𝑔(𝑥)D, which means you calculate 𝑔(𝑥) first and then use that result \\nas the input for 𝑓. \\nIn our loss function 𝐽(𝑤, 𝑏), the process starts by computing the linear functions \\nfor 𝑑\", 𝑑!, and 𝑑& using the current values of 𝑤 and 𝑏. These outputs are then \\npassed into the quadratic functions err\", err!, and err&. The final step is aver-\\naging these results to compute 𝐽. \\nUsing the sum rule and the constant multiple rule of differentiation, \\n)*\\n)\\' is given by: \\n∂𝐽\\n∂𝑤= 1\\n3 E∂err\"\\n∂𝑤+ ∂err!\\n∂𝑤+ ∂err&\\n∂𝑤F, \\nwhere \\n)err!\\n)\\' , \\n)err\"\\n)\\' , and \\n)err#\\n)\\'  are the partial derivatives of err\", err!, and err& with \\nrespect to 𝑤. \\nThe sum rule of differentiation states that the derivative of the sum of \\ntwo functions equals the sum of their derivatives: \\n)\\n), [𝑓(𝑥) + 𝑔(𝑥)] =\\n)\\n), 𝑓(𝑥) +\\n)\\n), 𝑔(𝑥). \\nThe constant multiple rule of differentiation states that the derivative \\nof a constant multiplied by a function equals the constant times the \\nderivative of the function: \\n)\\n), [𝑐⋅𝑓(𝑥)] = 𝑐⋅\\n)\\n), 𝑓(𝑥). \\nBy applying the chain rule of differentiation, the partial derivatives of err\", \\nerr!, and err& with respect to 𝑤 are:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 26}, page_content='26 \\n \\nThe chain rule of differentiation states that the derivative of a com-\\nposite function 𝑓C𝑔(𝑥)D, written as \\n)\\n), J𝑓C𝑔(𝑥)DK, is the product of the \\nderivative of 𝑓 with respect to 𝑔 and the derivative of 𝑔 with respect \\nto 𝑥, or: \\n)\\n), J𝑓C𝑔(𝑥)DK =\\n)-\\n). ⋅\\n).\\n),. \\nThen, \\n \\nTherefore, \\n \\nSimilarly, we find \\n)*\\n)+: \\n∂𝐽\\n∂𝑏\\n= 1\\n3 C2 ⋅(150𝑤+ 𝑏−200) + 2 ⋅(200𝑤+ 𝑏−600) + 2 ⋅(260𝑤+ 𝑏−500)D\\n\\u2004\\n= 1\\n3 (1220𝑤+ 6𝑏−2600)\\n \\nSetting the partial derivatives to 0 results in the following system of equations: \\n?\\n1\\n3 (260200𝑤+ 1220𝑏−560000)\\n= 0\\n1\\n3 (1220𝑤+ 6𝑏−2600)\\n= 0\\n \\nSimplifying the system and using substitution to solve for the variables gives \\nthe optimal values: 𝑤∗= 2.58 and 𝑏∗= −91.76. \\nThe resulting model 𝑓(𝑥) = 2.58𝑥−91.76 is shown in the plot below. It in-\\ncludes the three examples (blue dots), the model itself (red solid line), and a \\nprediction for a new house with an area of 240 m! (dotted orange lines).'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 27}, page_content='27 \\n \\nA vertical blue dashed line shows the square root of the model’s prediction \\nerror compared to the actual price.1 Smaller errors mean the model fits the \\ndata better. The loss, which aggregates these errors, measures how well the \\nmodel aligns with the dataset. \\nWhen we calculate the loss using our model’s training dataset (called the train-\\ning set), we obtain the training loss. For our model, this training loss is de-\\nfined by Equation 1.3. Using our learned parameter values, we can now com-\\npute the loss for the training set: \\n𝐽(2.58, −91.76)\\n=\\n(2.58 ⋅150 −91.76 −200)!\\n3\\n+\\n(2.58 ⋅200 −91.76 −600)!\\n3\\n\\u2004\\n+\\n(2.58 ⋅260 −91.76 −500)!\\n3\\n\\u2004\\n= 15403.19.\\n \\n \\n1 It’s the square root of the error because our error, as defined in Equation 1.2, is the \\nsquare of the difference between the predicted price and the real price of the house. It’s \\ncommon practice to take the square root of the mean squared error because it expresses \\nthe error in the same units as the target variable (price in this case). This makes it easier \\nto interpret the error value.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 28}, page_content='28 \\n \\nThe square root of this value is approximately 124.1, indicating an average \\nprediction error of around $124,100. The interpretation of whether a loss \\nvalue is high or low depends on the specific business context and comparative \\nbenchmarks. Neural networks and other non-linear models, which we explore \\nlater in this chapter, typically achieve lower loss values. \\n1.3. Four-Step Machine Learning Process \\nAt this stage, you should clearly understand the four steps involved in super-\\nvised learning: \\n1. Collect a dataset: For example, (𝑥\", 𝑦\") = (150,200), (𝑥!, 𝑦!) =\\n(200,600), and (𝑥&, 𝑦&) = (260,500). \\n2. Define the model’s structure: For example, 𝑦= 𝑤𝑥+ 𝑏. \\n3. Define the loss function: Such as Equation 1.3. \\n4. Minimize the loss: Minimize the loss function on the dataset. \\nIn our example, we minimized the loss manually by solving a system of two \\nequations with two variables. This approach works for small systems. How-\\never, as models grow in complexity—such as large language models with bil-\\nlions of parameters—manual approach becomes infeasible. Let’s now intro-\\nduce new concepts that will help us address this challenge. \\n1.4. Vector \\nTo predict a house price, knowing its area alone isn’t enough. Factors like the \\nyear of construction or the number of bedrooms and bathrooms also matter. \\nSuppose we use two attributes: (1) area and (2) number of bedrooms. In this \\ncase, the input 𝐱 becomes a feature vector. This vector includes two features, \\nalso called dimensions or components: \\n𝐱=\\ndef R𝑥(\")\\n𝑥(!)S \\nIn this book, the vectors are represented with lowercase bold letters, such as 𝐱 \\nor 𝐰. For a given house 𝐱, 𝑥(\") represents its size in square meters, and 𝑥(!) is \\nthe number of bedrooms. \\nA vector is usually represented as a column of numbers, called a col-\\numn vector. However, in text, it is often written as its transpose, 𝐱1.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 29}, page_content='29 \\nTransposing a column vector converts it into a row vector. For exam-\\nple, 𝐱1 =\\ndef J𝑥(\"), 𝑥(!)K or 𝐱=\\ndef J𝑥(\"), 𝑥(!)K\\n1. \\nThe dimensionality of the vector, or its size, refers to the number of compo-\\nnents it contains. Here, 𝐱 has two components, so its dimensionality is 2. \\nWith two features, our linear model needs three parameters: the weights 𝑤(\") \\nand 𝑤(!), and the bias 𝑏. The weights can be grouped into a vector: \\n𝐰=\\ndef R𝑤(\")\\n𝑤(!)S \\nThe linear model can then be written compactly as: \\n \\n𝑦= 𝐰⋅𝐱+ 𝑏, \\n(1.4) \\nwhere 𝐰⋅𝐱 is a dot product of two vectors (also known as scalar product). \\nIt is defined as: \\n𝐰⋅𝐱=\\ndef U 𝑤(2)\\n3\\n2%\"\\n𝑥(2) \\nThe dot product combines two vectors of the same dimensionality to produce \\na scalar, a number like 22, 0.67, or −10.5. Scalars in this book are denoted by \\nitalic lowercase or uppercase letters, such as 𝑥 or 𝐷. The expression 𝐰⋅𝐱+ 𝑏 \\ngeneralizes the idea of a linear transformation to vectors. \\nThe equation above uses capital-sigma notation, where 𝐷 represents the di-\\nmensionality of the input, and 𝑗 runs from 1 to 𝐷. For example, in the 2-di-\\nmensional house scenario, ∑\\n𝑤(2)\\n!\\n2%\"\\n𝑥(2) =\\ndef 𝑤(\")𝑥(\") + 𝑤(!)𝑥(!). \\nAlthough the capital-sigma notation suggests the dot product might be \\nimplemented as a loop, modern computers handle it much more effi-\\nciently. Optimized linear algebra libraries like BLAS and cuBLAS \\ncompute the dot product using low-level, highly optimized methods. \\nThese libraries leverage hardware acceleration and parallel processing, \\nachieving speeds far beyond a simple loop. \\nThe sum of two vectors 𝐚 and 𝐛, both with the same dimensionality 𝐷, is \\ndefined as:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 30}, page_content='30 \\n \\n𝐚+ 𝐛=\\ndef J𝑎(\") + 𝑏(\"), 𝑎(!) + 𝑏(!), … , 𝑎(3) + 𝑏(3)K\\n1 \\nThe calculation for a sum of two 3-dimensional vectors is illustrated below: \\n \\nIn this chapter’s illustrations, the numbers in the cells indicate the po-\\nsition of an element within an input or output matrix, or a vector. They \\ndo not represent actual values. \\nThe element-wise product of two vectors 𝐚 and 𝐛 of dimensionality 𝐷, is de-\\nfined as: \\n𝐚⊙𝐛=\\ndef J𝑎(\") ⋅𝑏(\"), 𝑎(!) ⋅𝑏(!), … , 𝑎(3) ⋅𝑏(3)]1 \\nThe computation of the element-wise product for two 3-dimensional vectors is \\nshown below: \\n \\nThe norm of a vector 𝐱, denoted ∥𝐱∥, represents its length or magnitude. It \\nis defined as the square root of the sum of the squares of its components: \\n∥𝐱∥=\\ndef ^U(𝑥(2))!\\n3\\n2%\"\\n \\nFor a 2-dimensional vector 𝐱, the norm is: \\n∥𝐱∥= _(𝑥(\"))! + (𝑥(!))!'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 31}, page_content='31 \\nThe cosine of the angle 𝜃 between two vectors 𝐱 and 𝐲 is defined as: \\n \\ncos(𝜃) =\\n𝐱⋅𝐲\\n∥𝐱∥∥𝐲∥ \\n(1.5) \\nThe cosine of the angle between two vectors quantifies their similarity. For \\ninstance, two houses with similar areas and bedroom counts will have a cosine \\nsimilarity close to 1, otherwise the value will be lower. Cosine similarity is \\nwidely used to compare words or documents represented as embedding vec-\\ntors. This will be discussed further in Section 2.2. \\nA zero vector has all components equal to zero. A unit vector has a length of \\n1. To convert any non-zero vector 𝐱 into a unit vector 𝐱e, you divide the vector \\nby its norm: \\n𝐱e =\\n𝐱\\n∥𝐱∥ \\nDividing a vector by a number results in a new vector where each component \\nof the original vector is divided by that number. \\nA unit vector preserves the direction of the original vector but has a length of \\n1. The figure below demonstrates this with 2-dimensional examples. On the \\nleft, aligned vectors have cos(𝜃) = 0.78. On the right, nearly orthogonal vec-\\ntors have cos(𝜃) = −0.02.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 32}, page_content='32 \\n \\nUnit vectors are valuable because their dot product equals the cosine \\nof the angle between them, and computing dot products is efficient. \\nWhen documents are represented as unit vectors, finding similar ones \\nbecomes fast by calculating the dot product between the query vector \\nand document vectors. This is how vector search engines and libraries \\nlike Milvus, Qdrant, and Weaviate operate. \\nAs dimensions increase, the number of parameters in a linear model becomes \\ntoo large to solve manually. Furthermore, in high-dimensional spaces, we can-\\nnot visually verify if data follows a linear pattern. Even if we could visualize \\nbeyond three dimensions, we would still need more flexible models to handle \\ndata that linear models cannot fit. \\nThe next section covers non-linear models, focusing on neural networks. These \\nare key to understanding large language models, a specific type of neural net-\\nwork architecture. \\n1.5. Neural Network \\nA neural network differs from a linear model in two key ways: (1) it applies \\nfixed non-linear functions to the outputs of trainable linear functions, and (2) \\nits structure is deeper, combining multiple functions hierarchically through \\nlayers. Let’s illustrate these differences. \\nLinear models like 𝑤𝑥+ 𝑏 or 𝐰⋅𝐱+ 𝑏 cannot solve many machine learning \\nproblems effectively. Even if we combine them into a composite function \\n𝑓!C𝑓\"(𝑥)D, a composite function of linear functions remains linear. This is \\nstraightforward to verify. \\nLet’s define 𝑦\" = 𝑓\"(𝑥) =\\ndef 𝑎\"𝑥 and 𝑦! = 𝑓!(𝑦\") =\\ndef 𝑎!𝑦\". Here, 𝑓! depends on 𝑓\", \\nmaking it a composite function. We can rewrite 𝑓! as: \\n𝑦! = 𝑎!𝑦\" = 𝑎!(𝑎\"𝑥) = (𝑎!𝑎\")𝑥 \\nSince 𝑎\" and 𝑎! are constants, we can define 𝑎& =\\ndef 𝑎\"𝑎!, so 𝑦! = 𝑎&𝑥, which is \\nlinear. \\nA straight line often fails to capture patterns in one-dimensional data, as \\ndemonstrated when linear regression is applied to non-linear data:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 33}, page_content='33 \\n \\nTo address this, we add non-linearity. For a one-dimensional input, the model \\nbecomes: \\n𝑦= 𝜙(𝑤𝑥+ 𝑏) \\nThe function 𝜙 is a fixed non-linear function, known as an activation. Com-\\nmon choices are: \\n1) ReLU (rectified linear unit): ReLU(𝑧) =\\ndef max(0, 𝑧), which outputs \\nnon-negative values and is widely used in neural networks; \\n2) Sigmoid: 𝜎(𝑧) =\\ndef\\n\"\\n\"45$%, which outputs values between 0 and 1, making \\nit suitable for binary classification (e.g., classifying spam emails as 1 \\nand non-spam as 0); \\n3) Tanh (hyperbolic tangent): tanh(𝑧) =\\ndef 5%65$%\\n5%45$%; outputs values between \\n−1 and 1. \\nIn these equations, 𝑒 denotes Euler’s number, approximately 2.72. \\nThese functions are widely used due to their mathematical properties, simplic-\\nity, and effectiveness in diverse applications. This is what they look like:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 34}, page_content='34 \\n \\n \\nThe structure 𝜙(𝑤𝑥+ 𝑏) enables learning non-linear models but can’t capture \\nall non-linear curves. By nesting these functions, we build more expressive \\nmodels. For instance, let 𝑓\"(𝑥) =\\ndef 𝜙(𝑎𝑥+ 𝑏) and 𝑓!(𝑧) =\\ndef 𝜙(𝑐𝑧+ 𝑑). A compo-\\nsite model combining 𝑓\" and 𝑓! is: \\n𝑦= 𝑓!C𝑓\"(𝑥)D = 𝜙(𝑐𝜙(𝑎𝑥+ 𝑏) + 𝑑) \\nHere, the input 𝑥 is first transformed linearly using parameters 𝑎 and 𝑏, then \\npassed through the non-linear function 𝜙. The result is further transformed \\nlinearly with parameters 𝑐 and 𝑑, followed by another application of 𝜙. \\nBelow is the graph representation of the composite model 𝑦= 𝑓!C𝑓\"(𝑥)D:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 35}, page_content='35 \\nA computational graph represents the structure of a model. The computa-\\ntional graph above shows two non-linear units (blue rectangles), often re-\\nferred to as artificial neurons. Each unit contains two trainable parameters—\\na weight and a bias—represented by grey circles. The left arrow ← denotes \\nthat the value on the right is assigned to the variable on the left. This graph \\nillustrates a basic neural network with two layers, each containing one unit. \\nMost neural networks in practice are built with more layers and multiple units \\nper layer. \\nSuppose we have a two-dimensional input, an input layer with three units, \\nand an output layer with a single unit. The computational graph appears as \\nfollows: \\n \\nFigure 1.1: A neural network with two layers. \\nThis structure represents a feedforward neural network (FNN), where infor-\\nmation flows in one direction—left to right—without loops. When units in \\neach layer connect to all units in the subsequent layer, as shown above, we call \\nit a multilayer perceptron (MLP). A layer where each unit connects to all \\nunits in both adjacent layers is termed a fully connected layer, or dense layer. \\nIn Chapter 3, we will explore recurrent neural networks (RNNs). Unlike FNNs, \\nRNNs have loops, where outputs from a layer are used as inputs to the same \\nlayer.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 36}, page_content='36 \\n \\nConvolutional neural networks (CNN) are feedforward neural net-\\nworks with convolutional layers that are not fully connected. While \\ninitially designed for image processing, they are effective for tasks like \\ndocument classification in text data. To learn more about CNNs refer \\nto the additional materials in the book’s wiki. \\nTo simplify diagrams, individual neural units can be replaced with squares. Using \\nthis approach, the above network can be represented more compactly as follows: \\n \\nIf you think this simple model is too weak, look at the figure below. It contains \\nthree plots demonstrating how increasing model size improves performance. \\nThe left plot shows a model with 2 units: one input, one output, and ReLU \\nactivations. The middle plot is a model with 4 units: three inputs and one out-\\nput. The right plot shows a much larger model with 100 units: \\n \\nThe ReLU activation function, despite its simplicity, was a break-\\nthrough in machine learning. Neural networks before 2012 relied on \\nsmooth activations like tanh and sigmoid, which made training deep \\nmodels increasingly difficult. We will return to this subject in Chapter \\n4 on the Transformer neural network architecture.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 37}, page_content='37 \\nIncreasing the number of parameters helps the model approximate the data \\nmore accurately. Experiments consistently show that adding more units per \\nlayer or increasing the number of layers in a neural network improves its ca-\\npacity to fit high-dimensional datasets, such as natural language, voice, sound, \\nimage, and video data. \\n1.6. Matrix \\nNeural networks can handle high-dimensional datasets but require substantial \\nmemory and computation. Calculating a layer’s transformation naïvely would \\ninvolve iterating over thousands of parameters per unit across thousands of \\nunits and dozens of layers, which is both slow and resource-intensive. Using \\nmatrices makes the computations more efficient. \\nA matrix is a two-dimensional array of numbers arranged into rows and col-\\numns, which generalizes the concept of vectors to higher dimensionalities. For-\\nmally, a matrix 𝐀 with 𝑚 rows and 𝑛 columns is written as: \\n𝐀=\\ndef s\\n𝑎\",\"\\n𝑎\",!\\n⋯\\n𝑎\",8\\n𝑎!,\"\\n𝑎!,!\\n⋯\\n𝑎!,8\\n⋮\\n⋮\\n⋱\\n⋮\\n𝑎9,\"\\n𝑎9,!\\n⋯\\n𝑎9,8\\nv \\nHere, 𝑎$,2 represents the element in the 𝑖-th row and 𝑗-th column of the matrix. \\nThe dimensions of the matrix are expressed as 𝑚× 𝑛 (read as “m by n”). \\nMatrices are fundamental in machine learning. They compactly represent data \\nand weights and enable efficient computation through operations such as ad-\\ndition, multiplication, and transposition. In this book, matrices are represented \\nwith uppercase bold letters, such as 𝐗 or 𝐖. \\nThe sum of two matrices 𝐀 and 𝐁 of the same dimensionality is defined ele-\\nment-wise as: \\n(𝐀+ 𝐁)$,2 =\\ndef 𝑎$,2 + 𝑏$,2 \\nFor example, for two 2 × 3 matrices 𝐀 and 𝐁, the addition works like this:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 38}, page_content='38 \\n \\n \\nThe product of a matrices 𝐀 with dimensions 𝑚× 𝑛 and 𝐁 with dimensions \\n𝑛× 𝑝 is a matrix 𝐂 with dimensions 𝑚× 𝑝 such that the value in row 𝑖 and \\ncolumn 𝑘 is given by: \\n(𝐂)$,: = U 𝑎$,2\\n8\\n2%\"\\n𝑏2,: \\nFor example, for a 4 × 3 matrix 𝐀 and a 3 × 5 matrix 𝐁, the product is a 4 × 5 \\nmatrix: \\n \\nTransposing a matrix 𝐀 swaps its rows and columns, resulting in 𝐀1, where: \\n(𝐀1)$,2 = 𝑎2,$ \\nFor example, for a 2 × 3 matrix 𝐀, its transpose 𝐀1 look like this:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 39}, page_content='39 \\n \\nMatrix-vector multiplication is a special case of matrix multiplication. When \\nan 𝑚× 𝑛 matrix 𝐀 is multiplied by a vector 𝐱 of size 𝑛, the result is a vector \\n𝐲= 𝐀𝐱 with 𝑚 components. Each element 𝑦$ of the resulting vector 𝐲 is com-\\nputed as: \\n𝑦$ = U 𝑎$,2\\n8\\n2%\"\\n𝑥(2) \\nFor example, a 4 × 3 matrix 𝐀 multiplied by a 3D vector 𝐱 produces a 4-dimen-\\nsional vector: \\n \\nThe weights and biases in fully connected layers of neural networks can be \\ncompactly represented using matrices and vectors, enabling the use of highly \\noptimized linear algebra libraries. As a result, matrix operations form the back-\\nbone of neural network training and inference. \\nLet’s express the model in Figure 1.1 using matrix notation. Let 𝐱 be the 2D \\ninput feature vector. For the first layer, the weights and biases are represented \\nas a 3 × 2 matrix 𝐖\" and a 3D vector 𝐛\", respectively. The 3D output 𝐲\" of the \\nfirst layer is given by: \\n \\n𝐲\" = 𝜙(𝐖\"𝐱+ 𝐛\") \\n(1.6) \\nThe second layer also uses a weight matrix and a bias. The output 𝑦! of the \\nsecond layer is computed using the output 𝐲\" from the first layer. The weight \\nmatrix for the second layer is a 1 × 3 matrix 𝐖!. The bias for the second layer \\nis a scalar 𝑏!,\". The model output corresponds to the output of the second layer:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 40}, page_content='40 \\n \\n \\n𝑦! = 𝜙C𝐖!𝐲\" + 𝑏!,\"D \\n(1.7) \\nEquation 1.6 and Equation 1.7 capture the operations from input to output in \\nthe neural network, with each layer’s output serving as the input for the next. \\n1.7. Gradient Descent \\nNeural networks are typically large and composed of non-linear functions, \\nwhich makes solving for the minimum of the loss function analytically infeasi-\\nble. Instead, the gradient descent algorithm is widely used to minimize the \\nloss, including in large language models. \\nConsider a practical example: binary classification. This task assigns input \\ndata to one of two classes, like deciding if an email is spam or not, or detecting \\nwhether a website connection request is a DDoS attack. \\nOur training dataset 𝒟 is {(𝐱$, 𝑦$)}$%\"\\n# , where 𝐱$ are vectors of input features, \\nand 𝑦$ are the labels. Each 𝑦$, indexed from 1 to 𝑁, takes a value of 0 for “not \\nspam” or 1 for “spam.” A well-trained model should output 𝑦9 close to 1 for \\nspam inputs 𝐱 and close to 0 for non-spam inputs. We can define the model as \\nfollows: \\n \\n𝑦= 𝜎(𝐰⋅𝐱+ 𝑏), \\n(1.8) \\nwhere 𝐱= J𝑥(2)K2%\"\\n3\\n and 𝐰= J𝑤(2)K2%\"\\n3\\n are 𝐷-dimensional vectors, 𝑏 is a scalar, \\nand 𝜎 is the sigmoid defined in Section 1.5. \\nThis model, called logistic regression, is commonly used for binary classifica-\\ntion tasks. Unlike linear regression, which produces outputs ranging from −∞ \\nto ∞, logistic regression always outputs values between 0 and 1. It can serve \\neither as a standalone model or as the output layer in a larger neural network. \\nDespite being over 80 years old, logistic regression remains one of the \\nmost widely used algorithms in production machine learning systems. \\nA common choice for the loss function in this case is binary cross-entropy, \\nalso called logistic loss. For a single example 𝑖, the binary cross-entropy loss \\nis defined as: \\n \\nloss(𝑦9$, 𝑦$) =\\ndef−[𝑦$log(𝑦9$) + (1 −𝑦$)log(1 −𝑦9$)] \\n(1.9)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 41}, page_content='41 \\nIn this equation, 𝑦$ represents the actual label of the 𝑖-th example in the da-\\ntaset, and 𝑦9$ is the prediction score, a value between 0 and 1 that the model \\noutputs for input vector 𝐱$. The function log denotes the natural logarithm. \\nLoss functions are usually designed to penalize incorrect predictions while re-\\nwarding accurate ones. To see why logistic loss works for logistic regression, \\nconsider two extreme cases: \\n1. Perfect prediction, when 𝑦$ = 0 and 𝑦9$ = 0: \\nloss(0,0) = −[0 ⋅log(0) + (1 −0) ⋅log(1 −0)] = −log(1) = 0 \\n  Here, the loss is zero which is good because the prediction matches the \\nlabel. \\n2. Opposite prediction, when 𝑦$ = 0 and 𝑦9$ = 1: \\nloss(1,0) = −[0 ⋅log(1) + (1 −0) ⋅log(1 −1)] = −log(0) \\nThe logarithm of 0 is undefined, and as 𝑎 approaches 0, −log(𝑎) approaches \\ninfinity, representing a severe loss for completely wrong predictions. However, \\nsince 𝑦9$, the sigmoid’s output, always remains strictly between 0 and 1, without \\nreaching them, the loss stays finite. \\nFor an entire dataset 𝒟, the loss is given by the average loss for all examples \\nin the dataset: \\n \\nloss𝒟=\\ndef−1\\n𝑁U[𝑦$log(𝑦9$) + (1 −𝑦$)log(1 −𝑦9$)]\\n#\\n$%\"\\n \\n(1.10) \\nTo simplify the gradient descent derivation, we’ll stick to a single example, 𝑖, \\nand rewrite the equation by substituting the prediction score 𝑦9$ with the \\nmodel’s expression for it: \\nloss(𝑦9$, 𝑦$) = −J𝑦$logC𝜎(𝑧$)D + (1 −𝑦$)logC1 −𝜎(𝑧$)DK, where 𝑧$ = 𝐰⋅𝐱$ + 𝑏 \\nTo minimize loss(𝑦9$, 𝑦$), we calculate the partial derivatives with respect to \\neach weight 𝑤(2) and the bias 𝑏. We will use the chain rule because we have \\na composition of three functions: \\n• Function 1: 𝑧$ =\\ndef 𝐰⋅𝐱$ + 𝑏, a linear function with the weights 𝐰 and \\nthe bias 𝑏; \\n• Function 2: 𝑦9$ = 𝜎(𝑧$) =\\ndef\\n\"\\n\"45$%&, the sigmoid function applied to 𝑧$;'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 42}, page_content='42 \\n \\n• Function 3: loss(𝑦9$, 𝑦$), as defined in Equation 1.9, which depends on \\n𝑦9$. \\nNotice that 𝐱$ and 𝑦$ are given: 𝐱$ is the feature vector for example 𝑖, \\nand 𝑦$ ∈{0,1} is its label. The notation 𝑦$ ∈{0,1} means that 𝑦$ belongs \\nto the set {0,1} and, in this case, indicates that 𝑦$ can only be 0 or 1. \\nLet’s denote loss(𝑦9$, 𝑦$) as l$. For weights 𝑤(2), the application of the chain rule \\ngives us: \\n∂l$\\n∂𝑤(2) = ∂l$\\n∂𝑦9$\\n⋅∂𝑦9$\\n∂𝑧$\\n⋅∂𝑧$\\n∂𝑤(2) = (𝑦9$ −𝑦$) ⋅𝑥$\\n(2) \\nFor the bias 𝑏, we have: \\n∂l$\\n∂𝑏= ∂l$\\n∂𝑦9$\\n⋅∂𝑦9$\\n∂𝑧$\\n⋅∂𝑧$\\n∂𝑏= 𝑦9$ −𝑦$ \\nThis is where the beauty of machine learning math shines: the activa-\\ntion function—sigmoid—and loss function—cross-entropy—both arise \\nfrom 𝑒, Euler’s number. Their functional properties serve distinct pur-\\nposes: sigmoid ranges between 0 and 1, ideal for binary classification, \\nwhile cross-entropy spans from 0 to ∞, great as a penalty. When com-\\nbined, the exponential and logarithmic components elegantly cancel, \\nyielding a linear function—prized for its computational simplicity and \\nnumerical stability. The book’s wiki provides the full derivation. \\nThe partial derivatives with respect to 𝑤(2) and 𝑏 for a single example (𝐱$, 𝑦$) \\ncan be extended to the entire dataset {(𝐱$, 𝑦$)}$%\"\\n#  by averaging the contribu-\\ntions from all examples. This follows from the sum rule and the constant mul-\\ntiple rule of differentiation: \\n \\n∂loss\\n∂𝑤(2)\\n= 1\\n𝑁U ƒ(𝑦9$ −𝑦$) ⋅𝑥$\\n(2)„\\n#\\n$%\"\\n∂loss\\n∂𝑏\\n= 1\\n𝑁U[𝑦9$ −𝑦$]\\n#\\n$%\"\\n \\n(1.11)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 43}, page_content='43 \\nHere, loss denotes the average loss for the entire dataset. Averaging the losses \\nfor individual examples ensures that each example contributes equally to the \\noverall loss, regardless of the total number of examples. \\nThe gradient is a vector that contains all the partial derivatives. The gradient \\nof the loss function, denoted as ∇loss, is defined as follows: \\n∇loss =\\ndef †∂loss\\n∂𝑤(\") , ∂loss\\n∂𝑤(!) , … , ∂loss\\n∂𝑤(3) , ∂loss\\n∂𝑏‡ \\nIf a gradient’s component is positive, this means that increasing the corre-\\nsponding parameter will increase the loss. Therefore, to minimize the loss, we \\nshould decrease that parameter. \\nThe gradient descent algorithm uses the gradient of the loss function to iter-\\natively update the weights and bias, aiming to minimize the loss function. \\nHere’s how it operates: \\n0. Initialize parameters: Start with random values of parameters 𝑤(2) \\nand 𝑏. \\n1. Compute the predictions: For each training example (𝐱$, 𝑦$), compute \\nthe predicted value 𝑦9$ using the model: \\n𝑦9$ ←𝜎(𝐰⋅𝐱$ + 𝑏) \\n2. Compute the gradient: Calculate the partial derivatives of the loss \\nfunction with respect to each weight 𝑤(2) and the bias 𝑏 using Equa-\\ntion 1.11. \\n3. Update the weights and bias: Adjust the weights and bias in the direc-\\ntion that decreases the loss function. This adjustment involves taking a \\nsmall step in the opposite direction of the gradient. The step size is con-\\ntrolled by the learning rate 𝜂 (explained below): \\n𝑤(2) ←𝑤(2) −𝜂∂loss\\n∂𝑤(2) \\n𝑏←𝑏−𝜂∂loss\\n∂𝑏 \\n4. Calculate the loss: Calculate the logistic loss by substituting the up-\\ndated values of 𝑤(2) and 𝑏 into Equation 1.10. \\n5. Continue the iterative process: Repeat steps 1-4 for a set number of \\niterations (also called steps) or until the loss value converges to a min-\\nimum.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 44}, page_content='44 \\n \\nHere’s a bit more detail to clarify the steps: \\n• Gradients are subtracted from parameters because they point in the di-\\nrection of steepest ascent in the loss function. Since our goal is to mini-\\nmize loss, we move in the opposite direction—hence, the subtraction. \\n• The learning rate 𝜂 is a positive value close to 0 and serves as a hy-\\nperparameter—not learned by the model but set manually. It controls \\nthe step size of each update, and finding its optimal value requires ex-\\nperimentation. \\n• Convergence occurs when subsequent iterations yield minimal de-\\ncreases in loss. The learning rate 𝜂 is crucial here: too small, and pro-\\ngress crawls; too large, and we risk overshooting the minimum or even \\nseeing the loss increase. Choosing an appropriate 𝜂 is therefore essen-\\ntial for effective gradient descent. \\nLet’s illustrate the process with a simple dataset of 12 examples: \\n‰C(22,25), 0D, C(25,35), 0D, C(47,80), 1D, C(52,95), 1D, C(46,82), 1D, C(56,90), 1D,\\nC(23,27), 0D, C(30,50), 1D, C(40,60), 1D, C(39,57), 0D, C(53,95), 1D, C(48,88), 1D Š \\nIn this dataset, 𝐱$ contains two features: age (in years) and income (in thou-\\nsands of dollars). The objective is to predict whether a person will buy a prod-\\nuct, with label 𝑦$ being either 0 (will not buy) or 1 (will buy). \\nThe loss evolution across gradient descent steps and the resulting trained \\nmodel are shown in the figures below: \\nThe left plot shows the loss decreasing steadily during gradient descent opti-\\nmization. The right plot displays the trained model’s sigmoid function, with'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 45}, page_content='45 \\ntraining examples positioned by their z-values (𝑧$ = 𝐰∗⋅𝐱$ + 𝑏∗), where 𝐰∗ \\nand 𝑏∗ are the learned weights and bias. \\nThe 0.5 threshold was chosen based on the plot’s clear separation: all “will-\\nbuy” examples (blue dots) lie above it, while all “will-not-buy” examples (red \\ndots) fall below. For new inputs 𝐱, generate 𝑦9 = 𝜎(𝐰∗⋅𝐱+ 𝑏∗). If 𝑦9 < 0.5, pre-\\ndict “will not buy;” otherwise, predict “will buy.” \\n1.8. Automatic Differentiation \\nGradient descent optimizes model parameters but requires partial derivative \\nequations. Until now, we calculated these derivatives by hand for each model. \\nAs models grow more complex, particularly in neural networks with multiple \\nlayers, manual derivation becomes impractical. \\nThis is where automatic differentiation (or autograd) comes in. Built into \\nmachine learning frameworks like PyTorch and TensorFlow, this feature com-\\nputes partial derivatives directly from Python code defining the model. This \\neliminates manual derivation, even for very complex models. \\nModern automatic differentiation systems can handle derivatives for \\nmillions of variables efficiently. Manual computation of these deriva-\\ntives would be unfeasible—writing the equations alone could take \\nyears. \\nTo use gradient descent in PyTorch, first install it with pip3 like this: \\n$ pip3 install torch \\nNow that PyTorch is installed, let’s import the dependencies: \\nimport torch \\nimport torch.nn as nn \\nimport torch.optim as optim \\nThe torch.nn module contains building blocks for creating models. When you \\nuse these components, PyTorch automatically handles derivative calculations. \\nFor optimization algorithms like gradient descent, the torch.optim module \\nhas what you need. Here’s how to implement logistic regression in PyTorch: \\nmodel = nn.Sequential( \\n    nn.Linear(n_inputs, n_outputs), ➊'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 46}, page_content='46 \\n \\n    nn.Sigmoid() ➋ \\n) \\nOur model leverages PyTorch’s sequential API, which is well-suited for simple \\nfeedforward neural networks where data flows sequentially through layers. \\nEach layer’s output naturally becomes the input for the subsequent layer. The \\nmore versatile module API, which we’ll use in the next chapter, enables the \\ncreation of models with multiple inputs, outputs, or loops. \\nThe input layer, defined in line ➊ using nn.Linear, has input dimensionality \\nn_inputs matching the size of our feature vector 𝐱, while the output dimen-\\nsionality n_outputs determines the layer’s unit count. For our buy/no-buy \\nclassifier—a model assigning classes to inputs—we set n_inputs to 2 since \\n𝐱= J𝑥(\"), 𝑥(!)K\\n1. With the output 𝑧 being scalar, n_outputs becomes 1. Line \\n➋ transforms 𝑧 through the sigmoid function to produce the output score. \\nWe then proceed to define our dataset, create the model instance, establish the \\nbinary cross-entropy loss function, and set up the gradient descent algorithm: \\ninputs = torch.tensor([ \\n    [22, 25], [25, 35], [47, 80], [52, 95], [46, 82], [56, 90\\n], \\n    [23, 27], [30, 50], [40, 60], [39, 57], [53, 95], [48, 88\\n] \\n], dtype=torch.float32) ➊ \\n \\nlabels = torch.tensor([ \\n    [0], [0], [1], [1], [1], [1], [0], [1], [1], [0], [1], [1\\n] \\n], dtype=torch.float32) ➋ \\n \\nmodel = nn.Sequential( \\n    nn.Linear(inputs.shape[1], 1), \\n    nn.Sigmoid() \\n) \\noptimizer = optim.SGD(model.parameters(), lr=0.001) ➌ \\ncriterion = nn.BCELoss() # binary cross-entropy loss \\nIn the above code block, we defined inputs and labels. The inputs form a \\nmatrix with 12 rows and 2 columns, while the labels are a vector with 12'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 47}, page_content='47 \\ncomponents. The shape attribute of the inputs tensor return its dimensional-\\nity: \\n>>> inputs.shape \\ntorch.Size([12, 2]) \\nTensors are PyTorch’s core data structures—multi-dimensional arrays opti-\\nmized for computation on both CPU and GPU. Supporting automatic differen-\\ntiation and flexible data reshaping, tensors form the foundation for neural net-\\nwork operations. In our example, the inputs tensor contains 12 examples with \\n2 features each, while the labels tensor holds 12 examples with single labels. \\nFollowing standard convention, examples are arranged in rows and their fea-\\ntures in columns. \\nIf you’re not familiar with tensors, there’s an introductory chapter on \\ntensors available on the book’s wiki. \\nWhen creating tensors in PyTorch, specifying dtype=torch.float32 in line \\n➊ sets 32-bit floating-point precision explicitly. This precision setting is essen-\\ntial for neural network computations, including weight adjustments, activation \\nfunctions, and gradient calculations. \\nThe 32-bit floating-point precision is not the only option for neural \\nnetworks. Quantization, an advanced technique that uses lower-pre-\\ncision data types like 16-bit or 8-bit floats and integers, helps reduce \\nmodel size and improve computational efficiency. For more infor-\\nmation, refer to resources on model optimization and deployment \\navailable on the book’s wiki. \\nThe optim.SGD class in line ➌ implements gradient descent by taking a list of \\nmodel parameters and learning rate as inputs.2 Since our model inherits from \\nnn.Module, we can access all trainable parameters through its parameters \\nmethod. \\n \\n2 While 0.001 is a common default learning rate, optimal values vary by problem and da-\\ntaset. Finding the best rate involves systematically testing different values and comparing \\nmodel performance.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 48}, page_content='48 \\n \\nPyTorch provides the binary cross-entropy loss function through nn.BCEL-\\noss(). \\nNow, we have everything we need to start the training loop: \\nfor step in range(500): \\n    optimizer.zero_grad() ➊ \\n    loss = criterion(model(inputs), labels) ➋ \\n    loss.backward() ➌ \\n    optimizer.step() ➍ \\nLine ➋ calculates the binary cross-entropy loss (Equation 1.10) by evaluating \\nmodel predictions against training labels. Line ➌ then uses backpropagation to \\ncompute the gradient of this loss with respect to the model parameters. \\nBackpropagation applies differentiation rules, particularly the chain rule, to \\ncompute gradients through deep composite functions. This algorithm forms \\nthe backbone of neural network training. When PyTorch operates on tensors, \\nit builds a computational graph as shown in Figure 1.1 from Section 1.5. This \\ngraph tracks all operations performed on the tensors. The loss.backward() \\ncall prompts PyTorch to traverse this graph and compute gradients via the \\nchain rule, eliminating the need for manual gradient derivation and implemen-\\ntation. \\nThe flow of data from input to output through the computational graph con-\\nstitutes the forward pass, while the computation of gradients from output to \\ninput through backpropagation represents the backward pass. \\nPyTorch accumulates gradients in the .grad attribute of parameters \\nlike weights and biases. While this feature enables multiple gradient \\ncomputations before parameter updates—useful for recurrent neural \\nnetworks (covered in Section 3)—our implementation doesn’t require \\ngradient accumulation. Line ➊ therefore clears the gradients at each \\nstep’s beginning. \\nFinally, in line ➍, parameter values are updated by subtracting the product of \\nthe learning rate and the loss function’s partial derivatives, completing step 3 \\nof the gradient descent algorithm discussed earlier.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 49}, page_content='49 \\nThe reader might wonder why labels are floats and not integers in this \\nbinary classification problem. The reason lies in how PyTorch’s BCEL-\\noss function operates. Since the model’s output layer uses a sigmoid \\nactivation function that produces floating-point values between 0 and \\n1, BCELoss expects both predictions and target labels to be floating-\\npoint numbers in the same range. If we were to use integer types like \\ntorch.long, we would encounter an error because BCELoss isn’t de-\\nsigned to handle integer types and its internal calculations expect float-\\ning-point numbers. This is specific to BCELoss—other loss functions \\nlike CrossEntropyLoss that we will use later actually require integer \\nlabels instead. \\nOne of automatic differentiation’s key advantages is its flexibility with model \\nswitching—as long as you’re using PyTorch’s components, you can readily \\nswap between different architectures. For instance, you could replace logistic \\nregression with a basic two-layer FNN, defined through the sequential API: \\nmodel = nn.Sequential( \\n    nn.Linear(features.shape[1], 100), \\n    nn.Sigmoid(), \\n    nn.Linear(100, labels.shape[1]), \\n    nn.Sigmoid() \\n) \\nIn this setup, each of the 100 units in the first layer contains 2 weights and 1 \\nbias, while the output layer’s single unit has 100 weights and 1 bias. The au-\\ntomatic differentiation system handles gradient computation internally, so the \\nremaining code stays unchanged. \\nIn the next chapter, we examine representing and processing text data. We \\nstart with basic methods like bag-of-words and word embeddings for convert-\\ning documents into numerical formats, then introduce count-based language \\nmodeling.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 50}, page_content='50 \\n \\nChapter 2. Language Modeling Basics \\nLanguage modeling requires transforming text into numbers that computers \\ncan process. In this chapter, we\\'ll explore how to convert words and documents \\ninto numerical formats, introduce the fundamentals of language modeling, and \\nstudy count-based models as our first architecture. Finally, we\\'ll cover tech-\\nniques for measuring language model performance. \\nLet’s begin with one of the oldest yet effective techniques for converting text \\ninto usable data for machine learning: bag of words. \\n2.1. Bag of Words \\nSuppose you have a collection of documents and want to predict the main topic \\nof each one. When topics are defined in advance, this task is called classifica-\\ntion. With only two possible topics, it’s known as binary classification, as ex-\\nplained in Section 1.7. With more than two topics, we have multiclass classi-\\nfication. \\nIn multiclass classification, the dataset consists of pairs {(𝐱$, 𝑦$)}$%\"\\n# , where 𝑦$ ∈\\n{1, … , 𝐶}, 𝑁 represents the number of examples, and 𝐶 denotes the number of \\npossible classes. Each 𝐱$ could be a text document, with 𝑦$ being an integer \\nindicating its topic—for example, 1 for “music,” 2 for “science,” or 3 for “cin-\\nema.” \\nMachines don’t process text like humans. To use machine learning on text, we \\nfirst need to convert documents into numbers. Each document becomes a fea-\\nture vector, where each feature is a scalar. \\nA common and effective approach to convert a collection of documents into \\nfeature vectors is the bag of words (BoW). Here’s how it works for a collection \\nof 10 simple documents: \\nID Text \\n1 \\nMovies are fun for everyone. \\n2 \\nWatching movies is great fun. \\n3 \\nEnjoy a great movie today. \\n4 \\nResearch is interesting and important. \\n5 \\nLearning math is very important. \\n6 \\nScience discovery is interesting.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 51}, page_content='51 \\nID Text \\n7 \\nRock is great to listen to. \\n8 \\nListen to music for fun. \\n9 \\nMusic is fun for everyone. \\n10 Listen to folk music! \\nA collection of text documents used in machine learning is called a corpus. \\nThe bag of words method applied to a corpus involves two key steps: \\n1. Create a vocabulary: List all unique words in the corpus to create the \\nvocabulary. \\n2. Vectorize documents: Convert each document into a feature vector, \\nwhere each dimension represents a word from the vocabulary. The \\nvalue indicates the word’s presence, absence, or frequency in the docu-\\nment. \\nFor the 10-document corpus, the vocabulary is built by listing all unique words \\nin alphabetical order. This involves removing punctuation, converting words \\nto lowercase, and eliminating duplicates. After processing, we get: \\nvocabulary = [\"a\", \"and\", \"are\", \"discovery\", \"enjoy\", \"every\\none\", \"folk\", \"for\", \"fun\", \"great\", \"important\", \"interestin\\ng\", \"is\", \"learning\", \"listen\", \"math\", \"movie\", \"movies\", \"m\\nusic\", \"research\", \"rock\", \"science\", \"to\", \"today\", \"very\", \\n\"watching\"] \\nSplitting a document into small indivisible parts is called tokenization, and \\neach part is a token. There are different ways to tokenize. We tokenized our \\n10-document corpus by words. Sometimes, it’s useful to break words into \\nsmaller units, called subwords, to keep the vocabulary size manageable. For \\ninstance, instead of including “interesting” in the vocabulary, we might split it \\ninto “interest” and “-ing.” One method for subword tokenization, which we’ll \\ncover in this chapter, is byte-pair encoding. The choice of tokenization method \\ndepends on the language, dataset, and model, and the best one is found ex-\\nperimentally. \\nA count of all English word surface forms—like do, does, doing, and \\ndid—reveals several million possibilities. Languages with more'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 52}, page_content=\"52 \\n \\ncomplex morphology have even greater numbers. A Finnish noun \\nalone can take 2,000–3,000 different forms to express various case and \\nnumber combinations. Using subwords offers a practical solution, as \\nstoring every surface form in the vocabulary would consume excessive \\nmemory and computational resources. \\nWords are a type of token, so “token” and “word” are often used interchange-\\nably as the smallest indivisible units of a document. In this book, when a dis-\\ntinction is important, context will make it clear. While the bag-of-words ap-\\nproach can handle both words and subwords, it was originally designed for \\nwords—hence the name. \\nFeature vectors can be organized into a document-term matrix (DTM). Here, \\nrows represent documents, and columns represent tokens. Below is a partial \\ndocument-term matrix for a 10-document corpus. It includes only a subset of \\ntokens to fit within the page width: \\nDoc \\na \\nand \\n… \\nfun \\n… \\nlisten \\nmath \\n… \\nscience \\n… \\nwatching \\n1 \\n0 \\n0 \\n… \\n1 \\n… \\n0 \\n0 \\n… \\n0 \\n… \\n0 \\n2 \\n0 \\n0 \\n… \\n1 \\n… \\n0 \\n0 \\n… \\n0 \\n… \\n1 \\n3 \\n1 \\n0 \\n… \\n0 \\n… \\n0 \\n0 \\n… \\n0 \\n… \\n0 \\n4 \\n0 \\n1 \\n… \\n0 \\n… \\n0 \\n0 \\n… \\n0 \\n… \\n0 \\n5 \\n0 \\n0 \\n… \\n0 \\n… \\n0 \\n1 \\n… \\n0 \\n… \\n0 \\n6 \\n0 \\n0 \\n… \\n0 \\n… \\n0 \\n0 \\n… \\n1 \\n… \\n0 \\n7 \\n0 \\n0 \\n… \\n0 \\n… \\n1 \\n0 \\n… \\n0 \\n… \\n0 \\n8 \\n0 \\n0 \\n… \\n1 \\n… \\n1 \\n0 \\n… \\n0 \\n… \\n0 \\n9 \\n0 \\n0 \\n… \\n1 \\n… \\n0 \\n0 \\n… \\n0 \\n… \\n0 \\n10 \\n0 \\n0 \\n… \\n0 \\n… \\n1 \\n0 \\n… \\n0 \\n… \\n0 \\nIn the DTM above, 1 means the token appears in the document, while 0 means \\nit does not. For instance, the feature vector 𝐱! for document 2 (“Watching mov-\\nies is great fun.”) is: \\n𝐱! = [0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1]1. \\nIn natural languages, word frequencies follow Zipf’s Law, stating that \\na word's frequency is inversely proportional to its rank in the frequency \\ntable—for instance, the second most frequent word appears half as\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 53}, page_content='53 \\noften as the most frequent one. Consequently, document-term matri-\\nces are usually sparse, containing mostly zeros. \\nA neural network can be trained to predict a document’s topic using these fea-\\nture vectors. Let’s do that. The first step is to assign labels to the documents, a \\nprocess known as labeling. Labeling can be done manually or assisted by an \\nalgorithm. When algorithms are used, human validation is often needed to \\nconfirm accuracy. Here, we will manually label the documents by reading each \\none and choosing the most suitable topic from the three options. \\nDoc Text \\nClass ID Class Name \\n1 \\nMovies are fun for everyone. \\n1 \\nCinema \\n2 \\nWatching movies is great fun. \\n1 \\nCinema \\n3 \\nEnjoy a great movie today. \\n1 \\nCinema \\n4 \\nResearch is interesting and important. 3 \\nScience \\n5 \\nLearning math is very important. \\n3 \\nScience \\n6 \\nScience discovery is interesting. \\n3 \\nScience \\n7 \\nRock is great to listen to. \\n2 \\nMusic \\n8 \\nListen to music for fun. \\n2 \\nMusic \\n9 \\nMusic is fun for everyone. \\n2 \\nMusic \\n10 \\nListen to folk music! \\n2 \\nMusic \\nAdvanced chat language models enable highly accurate automated \\ndocument labeling through a panel of expert models. Using three \\nLLMs, when two or more assign the same label to a document, that \\nlabel is adopted. If all three disagree, either a human can decide, or a \\nfourth model can break the tie. In many business contexts, manual la-\\nbeling is becoming obsolete, as LLMs offer faster and often more reli-\\nable labeling.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 54}, page_content='54 \\n \\nWe have three classes: 1 for cinema, 2 for music, and 3 for science.3 While \\nbinary classifiers typically use the sigmoid activation function with the binary \\ncross-entropy loss, as discussed in Section 1.7, tasks involving three or more \\nclasses generally employ the softmax activation function paired with the cross-\\nentropy loss. \\nThe softmax function is defined as: \\nsoftmax(𝐳, 𝑘) =\\ndef\\n𝑒<(()\\n∑\\n𝑒<(*)\\n3\\n2%\"\\n \\nHere, 𝐳 is a 𝐷-dimensional vector of logits, 𝑘 is the index for which the softmax \\nis computed, and 𝑒 is Euler’s number. Logits are the raw outputs of a neural \\nnetwork, prior to applying an activation function, as shown below: \\n \\n3 Class labels in classification are arbitrary and unordered. You can assign numbers to the \\nclasses in any way, and the model’s performance won’t change as long as the mapping is \\nconsistent for all examples.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 55}, page_content='55 \\n \\nThe figure shows the output layer of a neural network, labeled as 𝑜. The logits \\n𝑧=\\n(:), for 𝑘∈{1,2,3}, are the values in light green. These represent the outputs \\nof the units before the activation function is applied. The vector 𝐳 is expressed \\nas 𝐳= = ƒ𝑧=\\n(\"), 𝑧=\\n(!), 𝑧=\\n(&)„\\n1\\n. \\nFor instance, the softmax for unit 𝑜, 2 in the figure is calculated as: \\nsoftmax(𝐳=, 2) =\\n𝑒<+\\n(\")\\n𝑒<+\\n(!) + 𝑒<+\\n(\") + 𝑒<+\\n(#)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 56}, page_content='56 \\n \\nSoftmax transforms a vector into a discrete probability distribution (DPD), \\nensuring that ∑\\nsoftmax\\n3\\n:%\"\\n(𝐳, 𝑘) = 1. A DPD assigns probabilities to values in \\na finite set, with their sum equaling 1. A finite set contains a countable number \\nof distinct elements. For instance, in a classification task with classes 1, 2, and \\n3, these classes constitute a finite set. The softmax function maps each class to \\na probability, with these probabilities summing to 1. \\nLet’s compute the probabilities step by step. Assume we have three logits, 𝐳=\\n[2.0,1.0,0.5]1, representing a document’s classification into cinema, music, or \\nscience. \\nFirst, calculate 𝑒<(() for each logit: \\n𝑒<(!) = 𝑒!.?\\n≈7.39,\\n𝑒<(\") = 𝑒\".?\\n≈2.72,\\n𝑒<(#) = 𝑒?.@\\n≈1.65\\n \\nNext, sum these values: ∑\\n𝑒<(*)\\n&\\n2%\"\\n= 7.39 + 2.72 + 1.65 ≈11.76. \\nNow use the softmax formula, softmax(𝐳, 𝑘) =\\n5%(()\\n∑\\n5%(*)\\n#\\n*,!\\n, to compute the proba-\\nbilities: \\nPr(cinema) = 7.39\\n11.76 ≈0.63, \\nPr(music) = 2.72\\n11.76 ≈0.23, \\nPr(science) = 1.65\\n11.76 ≈0.14 \\nNeural network softmax outputs are better characterized as “probabil-\\nity scores” rather than true statistical probabilities, despite summing to \\none and resembling class likelihoods. Unlike logistic regression or Na-\\nïve Bayes models, neural networks don\\'t generate genuine class prob-\\nabilities. For simplicity, though, I\\'ll refer to these probability scores as \\n“probabilities” throughout this book. \\nThe cross-entropy loss measures how well predicted probabilities match the \\ntrue distribution. The true distribution is typically a one-hot vector with a'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 57}, page_content='57 \\nsingle element equal to 1 (the correct class) and 0 elsewhere. For example, a \\none-hot encoding with 3 classes looks like: \\nClass One-hot vector \\n1 \\n[1,0,0]1 \\n2 \\n[0,1,0]1 \\n3 \\n[0,0,1]1 \\nThe cross-entropy loss for a single example is: \\nloss(𝐲9, 𝐲) = −U 𝑦(:)\\nB\\n:%\"\\nlogC𝑦9(:)D, \\nwhere 𝐶 is the number of classes, 𝐲 is the one-hot encoded true label, and 𝐲9 is \\nthe predicted probabilities. Here, 𝑦(:) and 𝑦9(:) represent the 𝑘th elements of 𝐲 \\nand 𝐲9, respectively. \\nSince 𝐲 is one-hot encoded, only the term corresponding to the correct class \\ncontributes to the summation. The summation thus simplifies by retaining only \\nthat single term. Let’s simplify it. Suppose the correct class is 𝑐, so 𝑦(C) = 1 and \\n𝑦(:) = 0 for all 𝑘≠𝑐. In the summation, only the term where 𝑘= 𝑐 will be \\nnon-zero. The equation simplifies to: \\n \\nloss(𝐲9, 𝐲) = −logC𝑦9(C)D \\n(2.1) \\nThis simplified form indicates that the loss corresponds to the negative loga-\\nrithm of the probability assigned to the correct class. For 𝑁 examples, the av-\\nerage loss is: \\nloss = −1\\n𝑁U log\\n#\\n$%\"\\n‘𝑦e$\\n(C&)’, \\nwhere 𝑐$ is the correct class index for the 𝑖th example. \\nWhen used with softmax in the output layer, cross-entropy loss guides the net-\\nwork to assign high probabilities to correct classes while reducing probabilities \\nfor incorrect ones. \\nFor a document classification example with three classes (cinema, music, and \\nscience), the network generates three logits. These logits are passed through \\nthe softmax function to convert them into probabilities for each class. The'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 58}, page_content='58 \\n \\ncross-entropy loss is then calculated between these scores and the true one-hot \\nencoded labels. \\nLet’s illustrate this by training a simple two-layer neural network to classify \\ndocuments into three classes. We first import dependencies, set a random seed, \\nand define the dataset: \\nimport re, torch, torch.nn as nn \\n \\ntorch.manual_seed(42) ➊ \\n \\ndocs = [ \\n    \"Movies are fun for everyone.\", \\n    \"Watching movies is great fun.\", \\n    ... \\n    \"Listen to folk music!\" \\n] \\n \\nlabels = [1, 1, 1, 3, 3, 3, 2, 2, 2, 2] \\nnum_classes = len(set(labels)) \\nSetting the random seed in line ➊ ensures consistent random number genera-\\ntion across PyTorch runs. This guarantees reproducibility, allowing you to at-\\ntribute performance changes to code or hyperparameter modifications rather \\nthan random variations. Reproducibility is also essential for teamwork, ena-\\nbling collaborators to examine issues under identical conditions. \\nNext, we convert documents into a bag of words using two methods: to-\\nkenize, which splits input text into lowercase words, and get_vocabulary, \\nwhich constructs the vocabulary: \\ndef tokenize(text): \\n    return re.findall(r\"\\\\w+\", text.lower()) ➊ \\n \\ndef get_vocabulary(texts): \\n    tokens = {token for text in texts for token in tokenize(t\\next)} ➋ \\n    return {word: idx for idx, word in enumerate(sorted(token\\ns))} ➌ \\n \\nvocabulary = get_vocabulary(docs)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 59}, page_content='59 \\nIn line ➊, the regular expression \\\\w+ extracts individual words from the text. \\nA regular expression is a sequence of characters used to define a search pat-\\ntern. The pattern \\\\w+ matches sequences of “word characters,” such as letters, \\ndigits, and underscores. \\nThe findall function from Python’s re module applies the regular expression \\nand returns a list of all matches in the input string. In this case, it extracts all \\nwords. \\nIn line ➋, the corpus is converted into a set of tokens by iterating through each \\ndocument and extracting words using the same regular expression. In line ➌, \\nthese tokens are sorted alphabetically and mapped to unique indices, forming \\na vocabulary. \\nOnce the vocabulary is built, the next step is to define the feature extraction \\nfunction that converts a document into a feature vector: \\ndef doc_to_bow(doc, vocabulary): \\n    tokens = set(tokenize(doc)) \\n    bow = [0] * len(vocabulary) \\n    for token in tokens: \\n        if token in vocabulary: \\n            bow[vocabulary[token]] = 1 \\n    return bow \\nThe doc_to_bow function takes a document string and a vocabulary and re-\\nturns the bag-of-words representation of the document. \\nNow, let’s transform our documents and labels into numbers: \\nvectors = torch.tensor( \\n    [doc_to_bow(doc, vocabulary) for doc in docs], \\n    dtype=torch.float32 \\n) \\nlabels = torch.tensor(labels, dtype=torch.long) - 1 ➊ \\nThe vectors tensor with shape (10, 26) represents 10 documents as rows \\nand 26 vocabulary tokens as columns, while the labels tensor of shape (10,) \\ncontains the class label for each document. The labels use integer indices ra-\\nther than one-hot encoding since PyTorch’s cross-entropy loss function \\n(nn.CrossEntropyLoss) expects this format.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 60}, page_content='60 \\n \\nLine ➊ uses torch.long to cast labels to 64-bit integers. The -1 adjustment \\nconverts our original classes 1, 2, 3 to indices 0, 1, 2, which aligns with \\nPyTorch’s expectation that class indices begin at 0 for models and loss func-\\ntions like CrossEntropyLoss. \\nPyTorch provides two APIs for model definition: the sequential API and the \\nmodule API. While we used the straightforward nn.Sequential API to define \\nour model in Section 1.8, we’ll now explore building a multilayer perceptron \\nusing the more versatile nn.Module API: \\ninput_dim = len(vocabulary) \\nhidden_dim = 50 \\noutput_dim = num_classes \\n \\nclass SimpleClassifier(nn.Module): \\n    def __init__(self, input_dim, hidden_dim, output_dim): \\n        super().__init__() \\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \\n        self.relu = nn.ReLU() \\n        self.fc2 = nn.Linear(hidden_dim, output_dim) \\n \\n    def forward(self, x): \\n        x = self.fc1(x) ➊ \\n        x = self.relu(x) ➋ \\n        x = self.fc2(x) ➌ \\n        return x \\n \\nmodel = SimpleClassifier(input_dim, hidden_dim, output_dim) \\nThe SimpleClassifier class implements a feedforward neural network \\nwith two layers. Its constructor defines the network components: \\n1. A fully connected layer, self.fc1, maps the input of size input_dim \\n(equal to the vocabulary size) to 50 (hidden_dim) outputs. \\n2. A ReLU activation function introduces non-linearity. \\n3. A second fully connected layer, self.fc2, reduces the 50 intermediate \\noutputs to output_dim, the number of unique labels. \\nThe forward method describes the forward pass, where inputs flow through \\nthe layers:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 61}, page_content='61 \\n• In line ➊, the input x of shape (10, 26) is passed to the first fully con-\\nnected layer, transforming it to shape (10, 50). \\n• In line ➋, output from this layer is fed through the ReLU activation \\nfunction, keeping the shape (10, 50). \\n• In line ➌, the result is sent to the second fully connected layer, reduc-\\ning it from shape (10, 50) to (10, 3), producing the model’s final \\noutput with logits. \\nThe forward method is called automatically when you pass input data to the \\nmodel instance, like this: model(input). \\nWhile SimpleClassifier omits a final softmax layer, this is inten-\\ntional—PyTorch\\'s CrossEntropyLoss combines softmax and cross-\\nentropy loss internally for stability. This design eliminates the need for \\nan explicit softmax in the model\\'s forward pass. \\nWith our model defined, the next steps, as outlined in Section 1.8, are to define \\nthe loss function, choose the gradient descent algorithm, and set up the train-\\ning loop: \\ncriterion = nn.CrossEntropyLoss() \\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001) \\n \\nfor step in range(3000): \\n    optimizer.zero_grad() \\n    loss = criterion(model(vectors), labels) \\n    loss.backward() \\n    optimizer.step() \\nAs you can see, the training loop is identical to the one in Section 1.8. Once \\nthe training is complete, we can test the model on a new document: \\nnew_docs = [ \\n    \"Listening to rock music is fun.\", \\n    \"I love science very much.\" \\n] \\nclass_names = [\"Cinema\", \"Music\", \"Science\"] \\n \\nnew_doc_vectors = torch.tensor('),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 62}, page_content=\"62 \\n \\n    [doc_to_bow(new_doc, vocabulary) for new_doc in new_docs]\\n, \\n    dtype=torch.float32 \\n) \\n \\nwith torch.no_grad(): ➊ \\n    outputs = model(new_doc_vectors) ➋ \\n    predicted_ids = torch.argmax(outputs, dim=1) + 1 ➌ \\n \\nfor i, new_doc in enumerate(new_docs): \\n    print(f'{new_doc}: {class_names[predicted_ids[i].item() - \\n1]}') \\nOutput: \\nListening to rock is fun.: Music \\nI love scientific research.: Science \\nThe torch.no_grad() statement in line ➊ disables the default gradient track-\\ning. While gradients are essential during training to update model parameters, \\nthey’re unnecessary during testing or inference. Since these phases don’t in-\\nvolve parameter updates, disabling gradient tracking conserves memory and \\nspeeds up computation. Note that the terms “testing,” “inference,” and “evalu-\\nation” are often used interchangeably when referring to generating predictions \\non unseen data. \\nIn line ➋, the model processes all inputs simultaneously during inference, just \\nas it does during training. This parallel processing approach leverages vector-\\nized operations, substantially reducing computation time compared to pro-\\ncessing inputs one by one. \\nWe only care about the final label, not the logits returned by the model. In line \\n➌, torch.argmax identifies the highest logit’s index, corresponding to the pre-\\ndicted class. Adding 1 compensates for the earlier shift from 1-based to 0-based \\nindexing. \\nWhile the bag-of-words approach offers simplicity and practicality, it has no-\\ntable limitations. Most significantly, it fails to capture token order or context. \\nConsider how “the cat chased the dog” and “the dog chased the cat” yield iden-\\ntical representations, despite conveying opposite meanings.\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 63}, page_content='63 \\nN-grams provide one solution to this challenge. An n-gram consists of 𝑛 con-\\nsecutive tokens from text. Consider the sentence “Movies are fun for every-\\none”—its bigrams (2-grams) include “Movies are,” “are fun,” “fun for,” and \\n“for everyone.” By preserving sequences of tokens, n-grams retain contextual \\ninformation that individual tokens cannot capture. \\nHowever, using n-grams comes at a cost. The vocabulary expands considera-\\nbly, increasing the computational cost of model training. Additionally, the \\nmodel requires larger datasets to effectively learn weights for the expanded set \\nof possible n-grams. \\nAnother limitation of bag-of-words is how it handles out-of-vocabulary words. \\nWhen a word appears during inference that wasn’t present during training—\\nand thus isn’t in the vocabulary—it can’t be represented in the feature vector. \\nSimilarly, the approach struggles with synonyms and near-synonyms. Words \\nlike “movie” and “film” are processed as completely distinct terms, forcing the \\nmodel to learn separate parameters for each. Since labeled data is often costly \\nto obtain, resulting in rather small labeled datasets, it would be more efficient \\nif the model could recognize and collectively process words with similar mean-\\nings. \\nWord embeddings address this by mapping semantically similar words to sim-\\nilar vectors. \\n2.2. Word Embeddings \\nConsider document 3 (“Enjoy a great movie today.”) from earlier. We can \\nbreak down this bag of words (BoW) into one-hot vectors representing indi-\\nvidual words: \\nBoW \\n1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 \\nenjoy \\n0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\na \\n1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\ngreat \\n0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\nmovie 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 \\ntoday \\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 64}, page_content='64 \\n \\nAs we see, a bag-of-word vector of a document is a sum of one-hot vectors of \\nits words. Now, let’s examine the one-hot vectors and the BoW vector for the \\ntext “Films are my passion.”: \\nBoW \\n0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\nfilms \\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\nare \\n0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\nmy \\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\npas-\\nsion \\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\nThere are two key problems here. First, even when a word exists in the training \\ndata and vocabulary, one-hot encoding reduces it to a single 1 in a vector of \\nzeros, giving the classifier almost no meaningful information to learn from. \\nSecond, in the above document, most one-hot encoded word vectors add no \\nvalue since three out of four become zero vectors—representing words miss-\\ning from the vocabulary. \\nA better approach would let the model understand that “films,” though unseen \\nin training, shares semantic meaning with “movies.” This would allow the fea-\\nture vector for “films” to be processed similarly to “movies.” Such an approach \\nrequires word representations that capture semantic relationships between \\nwords. \\nWord embeddings overcome the limitations of the bag-of-words model by \\nrepresenting words as dense vectors rather than sparse one-hot vectors. \\nThese lower-dimensional representations contain mostly non-zero values, with \\nsimilar words having embeddings that exhibit high cosine similarity. The em-\\nbeddings are learned from vast unlabeled datasets spanning millions to hun-\\ndreds of millions of documents. \\nWord2vec, a widely-used embedding learning algorithm, exists in two vari-\\nants. We’ll examine the skip-gram formulation. \\nSkip-grams are word sequences where one word is omitted. For example, in \\n“Professor Alan Turing’s * advanced computer science,” the missing word \\n(marked as *) might be “research,” “work,” or “theories”—words that fit con-\\ntextually despite not being exact synonyms. Training a model to predict these \\nskipped words from their surrounding context helps it learn semantic relation-\\nships between words. The process can also work in reverse: the skipped word'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 65}, page_content='65 \\ncan be used to predict its context words. This is the basis of the skip-gram \\nalgorithm. \\nThe skip-gram size specifies how many context words are included. For a size \\nof five, this means two words before and two after the skipped word. Here are \\nexamples of skip-grams of size five from our sentence, with different words \\nskipped (marked as *): \\nSkip-gram \\nSkipped word \\nprofessor alan * research advanced \\nturing’s \\nalan turing’s * advanced computer \\nresearch \\nturing’s research * computer science advanced \\nIf the corpus vocabulary contains 10,000 words, the skip-gram model with an \\nembedding layer of 300 units, is depicted below: \\n \\nThis is a skip-gram model with a skip-gram size of 5 and the embedding layer \\nof 300 units. As you can see, the model uses a one-hot encoded skipped word \\nto predict a context word, processing the input through two consecutive fully \\nconnected layers. It doesn’t predict all context words at once but makes sepa-\\nrate predictions for each. \\nHere’s how it works for the skip-gram `professor alan * research advanced” and \\nthe skipped word “turing’s”. We transform the skip-gram into 4 training pairs:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 66}, page_content='66 \\n \\nSkipped word (input) Context word (target) Position \\nturing’s \\nprofessor \\n−2 \\nturing’s \\nalan \\n−1 \\nturing’s \\nresearch \\n+1 \\nturing’s \\nadvanced \\n+2 \\nFor each pair of skipped and context words, say (turing’s, professor), the \\nmodel: \\n1. Takes “turing’s” as input, \\n2. Converts it to a one-hot vector, \\n3. Passes it through the embedding layer to get the word embedding, \\n4. Passes the word embedding through the output layer, and \\n5. Outputs probabilities for “professor.” \\nFor a given context word, the output layer produces a probability vector across \\nthe vocabulary. Each value represents how likely that vocabulary word is to be \\nthe context word. \\nA curious reader might notice: if the input for each training pair remains con-\\nstant—say, “turing’s”—why would the output differ? That’s a great observa-\\ntion! The output will indeed be identical for the same input. However, the loss \\ncalculation varies depending on each context word. \\nWhen using chat language models, you may notice that the same \\nquestion often yields different answers. While this might suggest the \\nmodel is non-deterministic, that’s not accurate. An LLM is fundamen-\\ntally a neural network, similar to a skip-gram model but with far more \\nparameters. The apparent randomness comes from the way these mod-\\nels are used to generate text. During generation, words are sampled \\nbased on their predicted probabilities. Though higher-probability \\nwords are more likely to be chosen, lower-probability ones may still be \\nselected. This sampling process creates the variations we observe in \\nresponses. We will talk about sampling in Chapter 5. \\nThe skip-gram model uses cross-entropy as its loss function, just as in the \\nthree-class text classifier discussed earlier but handles 10,000 classes—one for \\neach word in the vocabulary. For each skip-gram in the training set, the model'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 67}, page_content='67 \\ncomputes losses separately for each context word, such as the four words sur-\\nrounding “turing’s,” then averages these losses to receive feedback on all con-\\ntext word predictions simultaneously. \\nThis training approach enables the model to capture meaningful word rela-\\ntionships, even when working with the same input across different training \\npairs. \\nHere’s an example. For the input word “turing’s,” suppose the model assigns \\nthese probabilities to different vocabulary words: professor (0.1), alan (0.15), \\nresearch (0.2), advanced (0.05). When training the model, each input-target \\nword pair contributes to the loss function. For example, when “turing’s” ap-\\npears with “professor” in the training data, the loss works to increase the score \\nof 0.1. Similarly, when paired with “alan,” the loss works to increase 0.15, with \\n“research” to increase 0.2, and with “advanced” to increase 0.05. \\nDuring backpropagation, the model adjusts its weights to make these scores \\nhigher for the given context words. For instance, the updated scores might be: \\nprofessor: 0.11, alan: 0.17, research: 0.22, advanced: 0.07, while the scores for \\nother vocabulary words decrease slightly. \\nOnce training is complete, the output layer is discarded. The embedding layer \\nthen serves as the new output layer. When given a one-hot encoded input \\nword, the model produces a 300-dimensional vector—this is the word embed-\\nding. \\nWord2vec is just one method for learning word embeddings from large text \\ncorpora. Other methods, such as GloVe and FastText, offer alternative ap-\\nproaches, focusing on capturing global co-occurrence statistics or subword in-\\nformation to create more robust embeddings. \\nUsing word embeddings to represent texts offers clear advantages over bag of \\nwords. One advantage is dimensionality reduction, which compresses the \\nword representation from the size of the vocabulary (as in one-hot encoding) \\nto a small vector, typically between 100 and 1000 dimensions. This makes it \\nfeasible to process very large corpora in machine learning tasks. \\nSemantic similarity is another advantage of word embeddings. Words with \\nsimilar meanings are mapped to vectors that are close to each other in the \\nembedding space. For example, consider word2vec embeddings trained by'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 68}, page_content='68 \\n \\nGoogle on a news corpus containing about 100 billion words.4 In the graph \\nbelow, “Moscow” and “Beijing,” or “Russia” and “China,” are represented by \\npoints located near one another. This reflects their semantic relationships: \\n \\nThe graph shows a 2D projection of 300-dimensional embedding vectors for \\ncountries and their capitals. Words with related meanings cluster together, \\nwhile nearly parallel lines connect cities to their respective countries, revealing \\ntheir semantic relationships. \\nThe skip-gram model captures semantic similarity when words occur in similar \\ncontexts, even without direct co-occurrence. For instance, if the model pro-\\nduces different probabilities for “films” and “movies,” the loss function drives \\nit to predict similar ones, since context words frequently overlap. Through \\nbackpropagation, the embedding layer outputs for these words converge. \\n \\n4 These embeddings can be found online by using the “GoogleNews-vectors-nega-\\ntive300.bin.gz” query. A backup is available on the book’s wiki at thelm-\\nbook.com/data/word-vectors.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 69}, page_content='69 \\nBefore word embeddings, WordNet (created at Princeton in 1985) at-\\ntempted to capture word relationships by organizing words into sets \\nof synonyms and recording semantic links between them. While effec-\\ntive, these hand-crafted mappings couldn’t scale to large vocabularies \\nor capture the subtle patterns in word usage that naturally emerge \\nfrom embedding-based approaches. \\nBecause directly visualizing 300-dimensional vectors isn’t possible, we used a \\ndimensionality reduction technique called principal component analysis \\n(PCA) to project them onto two dimensions, known as first and second prin-\\ncipal components. \\nDimensionality reduction algorithms compress high-dimensional vectors while \\nmaintaining their relationships. The first and second principal components in \\nthe above graph preserved the semantic connections between words, revealing \\ntheir relationships. \\nFor resources on PCA and other dimensionality reduction methods, \\ncheck the recommended material listed on the book’s wiki. \\nWord embeddings capture the meaning of words and their relationships to \\nother words. They are fundamental to many natural language processing \\n(NLP) tasks. Neural language models, for example, encode documents as ma-\\ntrices of word embeddings. Each row corresponds to a word’s embedding vec-\\ntor, and its position in the matrix reflects the word’s position in the document. \\nThe discovery that word2vec embeddings support meaningful arith-\\nmetic operations (like “king − man + woman ≈ queen”) was a pivotal \\nmoment, revealing that neural networks could encode semantic rela-\\ntionships in a space where vector operations produced changes in word \\nmeaning. This made the invention of neural networks capable of doing \\ncomplex math on words, like large language models do, only a matter \\nof time. \\nModern language models, though, often use subwords—tokens smaller than \\ncomplete words. Before moving on to language models—the main topic of this'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 70}, page_content='70 \\n \\nbook—let’s first examine byte-pair encoding, a widely used subword tokeniza-\\ntion method. \\n2.3. Byte-Pair Encoding \\nByte-pair encoding (BPE) is a tokenization algorithm that addresses the chal-\\nlenges of handling out-of-vocabulary words by breaking words into smaller \\nunits called subwords. \\nInitially a data compression technique, BPE was adapted for NLP by treating \\nwords as sequences of characters. It merges the most frequent symbol pairs—\\ncharacters or subwords—into new subword units. This continues until the vo-\\ncabulary reaches the target size. \\nBelow is the basic BPE algorithm: \\n1. Initialization: Use a text corpus. Split each word in the corpus into in-\\ndividual characters. For example, the word “hello” becomes “h e l l o”. \\nThe initial vocabulary consists of all unique characters in the corpus. \\n2. Iterative merging: \\no Count adjacent symbol pairs: Treat each character as a symbol. \\nGo through the corpus and count every pair of adjacent symbols. \\nFor example, in “h e l l o”, the pairs are “h e”, “e l”, “l l”, “l o”. \\no Select the most frequent symbol pair: Identify the pair with the \\nhighest count in the entire corpus. For instance, if “l l” occurs \\nmost frequently, select it. \\no Merge the selected pair: Replace all occurrences of the most fre-\\nquent symbol pair with a new single merged symbol. For exam-\\nple, “l l” would be replaced with a new merged symbol “ll”. The \\nword “h e l l o” now becomes “h e ll o”. \\no Update the vocabulary: Add the new merged symbol to the vo-\\ncabulary. The vocabulary now includes the original characters \\nand the new symbol “ll”. \\n3. Repeat: Continue the iterative merging until the vocabulary reaches \\nthe desired size. \\nThe algorithm is simple, but implementing it directly on large corpora is inef-\\nficient. Recomputing symbol pairs or updating the entire corpus after each \\nmerge is computationally expensive.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 71}, page_content=\"71 \\nA more efficient approach initializes the vocabulary with all unique words in \\nthe corpus and their counts. Pair counts are calculated using these word \\ncounts, and the vocabulary is updated iteratively by merging the most popular \\npairs. Let’s write the code: \\nfrom collections import defaultdict \\n \\ndef initialize_vocabulary(corpus): \\n    vocabulary = defaultdict(int) \\n    charset = set() \\n    for word in corpus: \\n        word_with_marker = '_' + word ➊ \\n        characters = list(word_with_marker) ➋ \\n        charset.update(characters) ➌ \\n        tokenized_word = ' '.join(characters) ➍ \\n        vocabulary[tokenized_word] += 1 ➎ \\n    return vocabulary, charset \\nThe function generates a vocabulary that represents words as sequences of \\ncharacters and tracks their counts. Given a corpus (a list of words), it returns \\ntwo outputs: vocabulary, a dictionary mapping each word—tokenized with \\nspaces between characters—to its count, and charset, a set of all unique char-\\nacters present in the corpus. \\nHere’s how it works: \\n• Line ➊ adds a word boundary marker “_” to the start of each word to \\ndifferentiate subwords at the beginning from those in the middle. For \\nexample, “_re” in “restart” is distinct from “re” in “agree.” This helps re-\\nbuild sentences from tokens generated using the model. When a token \\nstarts with “_”, it marks the beginning of a new word, requiring a space \\nto be added before it. \\n• Line ➋ splits each word into individual characters. \\n• Line ➌ updates charset with any new characters encountered in the \\nword. \\n• Line ➍ joins characters with spaces to create a tokenized version of \\nthe word. For example, the word “hello” becomes _ h e l l o.\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 72}, page_content='72 \\n \\n• Line ➎ adds tokenized_word to vocabulary with its count incre-\\nmented. \\nAfter the initialization, BPE iteratively merges the most frequent pairs of tokens \\n(bigrams) in the vocabulary. By removing spaces between these pairs, it \\nforms progressively longer tokens. \\ndef get_pair_counts(vocabulary): \\n    pair_counts = defaultdict(int) \\n    for tokenized_word, count in vocabulary.items(): \\n        tokens = tokenized_word.split() ➊ \\n        for i in range(len(tokens) - 1): \\n            pair = (tokens[i], tokens[i + 1]) ➋ \\n            pair_counts[pair] += count ➌ \\n    return pair_counts \\nThe function counts how often adjacent token pairs appear in the tokenized \\nvocabulary words. The input vocabulary maps tokenized words to their \\ncounts, and the output is a dictionary of token pairs and their total counts. \\nFor each tokenized_word in vocabulary, we split it into tokens in line ➊. A \\nnested loop forms adjacent token pairs in line ➋ and increments their count by \\nthe word’s count in line ➌. \\ndef merge_pair(vocabulary, pair): \\n    new_vocabulary = {} \\n    bigram = re.escape(\\' \\'.join(pair)) ➊ \\n    pattern = re.compile(r\"(?<!\\\\S)\" + bigram + r\"(?!\\\\S)\") ➋ \\n    for tokenized_word, count in vocabulary.items(): \\n        new_tokenized_word = pattern.sub(\"\".join(pair), token\\nized_word) ➌ \\n        new_vocabulary[new_tokenized_word] = count \\n    return new_vocabulary \\nThe function merges the input token pair in all tokenized words from the vo-\\ncabulary. It returns a new vocabulary where every occurrence of the pair is \\nmerged into a single token. For example, if the pair is (\\'e\\', \\'l\\') and a to-\\nkenized word is \"_ h e l l o\", merging \\'e\\' and \\'l\\' removes the space \\nbetween them, resulting in \"_ h el l o\".'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 73}, page_content='73 \\nIn line ➊, the re.escape function automatically adds backslashes to special \\ncharacters in a string (like ., *, or ?), so they are interpreted as literal charac-\\nters rather than having their special meaning in regular expressions. \\nThe regular expression in line ➋ matches only whole token pairs. It ensures \\nthe bigram is not part of a larger word by checking for the absence of non-\\nwhitespace characters immediately before and after the match. For instance \\n\"good morning\" matches in \"this is good morning\", but not in \"thisis-\\ngood morning\", where \"good\" is part of \"thisisgood\". \\nThe expressions (?<!\\\\S) and (?!\\\\S) are regex negative lookbehind \\nand negative lookahead assertions that ensure a bigram stands alone. \\nThe lookbehind checks that no non-whitespace character precedes the \\nbigram, meaning it follows whitespace or the start of text. The \\nlookahead similarly ensures no non-whitespace follows the bigram, \\nmeaning it precedes whitespace or the end of text. Together, these pre-\\nvent the bigram from being part of longer words. \\nFinally, in line ➌, the function uses pattern.sub() to replace all occurrences \\nof the matched pattern with the joined pair, creating the new tokenized word. \\nThe function below implements the BPE algorithm, merging the most frequent \\ntoken pairs iteratively until no merges remain or the target vocabulary size is \\nreached: \\ndef byte_pair_encoding(corpus, vocab_size): \\n    vocabulary, charset = initialize_vocabulary(corpus) \\n    merges = [] \\n    tokens = set(charset) \\n    while len(tokens) < vocab_size: ➊ \\n        pair_counts = get_pair_counts(vocabulary) \\n        if not pair_counts: ➋ \\n            break \\n        most_frequent_pair = max(pair_counts, key=pair_counts\\n.get) ➌ \\n        merges.append(most_frequent_pair) \\n        vocabulary = merge_pair(vocabulary, most_frequent_pai\\nr) ➍'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 74}, page_content='74 \\n \\n        new_token = \\'\\'.join(most_frequent_pair) ➎ \\n        tokens.add(new_token) ➏ \\n         \\n    return vocabulary, merges, charset, tokens \\nThis function processes a corpus to produce the components needed for a to-\\nkenizer. It initializes the vocabulary and character set, creates an empty \\nmerges list for storing merge operations, and sets tokens to the initial char-\\nacter set. Over time, tokens grows to include all unique tokens the tokenizer \\nwill be able to generate. \\nThe loop in line ➊ continues until the number of tokens supported by the to-\\nkenizer reaches vocab_size or no pairs remain to merge. Line ➋ checks if \\nthere are no more valid pairs, in which case the loop exits. Line ➌ finds the \\nmost frequent token pair, which is merged throughout the vocabulary in line \\n➍ to create a new token in line ➎. This new token is added to the tokens set \\nin line ➏, and the merge is recorded in merges. \\nThe function returns four outputs: the updated vocabulary, the list of merge \\noperations, the original character set, and the final set of unique tokens. \\nThe function below tokenizes a word using a trained tokenizer: \\ndef tokenize_word(word, merges, vocabulary, charset, unk_toke\\nn=\"<UNK>\"): \\n    word = \\'_\\' + word \\n    if word in vocabulary: \\n        return [word] \\n    tokens = [char if char in charset else unk_token for char \\nin word] \\n     \\n    for left, right in merges: \\n        i = 0 \\n        while i < len(tokens) - 1: \\n            if tokens[i:i+2] == [left, right]: \\n                tokens[i:i+2] = [left + right] \\n            else: \\n                i += 1 \\n    return tokens'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 75}, page_content='75 \\nThis function tokenizes a word using merges, vocabulary, and charset from \\nbyte_pair_encoding. The word is first prefixed. If the prefixed word exists \\nin the vocabulary, it returns it as the only token. Otherwise, the word is split \\ninto characters, with any not in charset replaced by unk_token. These char-\\nacters are then iteratively merged using the order of rules in merges. \\nTo tokenize a text, we first split it into words based on spaces and then tokenize \\neach word individually. The thelmbook.com/nb/2.1 notebook contains code \\nfor training a BPE tokenizer by using a news corpus. The tokenized version of \\nthe sentence “Let’s proceed to the language modeling chapter.” using the to-\\nkenizer trained in the notebook, is: \\n[\"_Let\", \"\\'\", \"s\", \"_proceed\", \"_to\", \"_the\", \"_language\", \"_\\nmodel\", \"ing\", \"_part\", \".\"] \\nHere, “let’s,” and “modeling,” were broken into subwords. This indicates their \\nrelative rarity in the training data and a small target vocabulary size (I set 5000 \\ntokens). \\nThe tokenize_word algorithm is inefficient due to nested loops: it iterates \\nover all merges in line ➍ while checking every token pair in line ➎. However, \\nsince modern language models have vocabularies exceeding 100,000 tokens, \\nmost input words exist in the vocabulary, bypassing subword tokenization. The \\nnotebook’s optimized version uses caching and precomputed data structures \\nto eliminate these nested loops, reducing tokenization time from 0.0549 to \\n0.0037 seconds. While actual performance varies by system, the optimized ap-\\nproach consistently delivers better speed. \\nFor languages without spaces, like Chinese, or for multilingual models, the \\ninitial space-based tokenization is typically skipped. Instead, the text is split \\ninto individual characters. From there, BPE proceeds as usual, merging the \\nmost frequent character or token pairs to form subwords. \\nWe\\'re now ready to examine the core ideas of language modeling. We\\'ll begin \\nwith traditional count-based methods and cover neural network-based tech-\\nniques in later chapters. \\n2.4. Language Model \\nA language model predicts the next token in a sequence by estimating its con-\\nditional probability based on previous tokens. It assigns a probability to all'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 76}, page_content='76 \\n \\npossible next tokens, enabling the selection of the most likely one. This capa-\\nbility supports tasks like text generation, machine translation, and speech \\nrecognition. Trained on large unlabeled text corpora, language models learn \\nstatistical patterns in language, allowing them to be used to generate human-\\nlike text. \\nFormally, for a sequence 𝐬 of 𝐿 tokens (𝑡\", 𝑡!, … , 𝑡D), a language model com-\\nputes: \\n \\nPrC𝑡= 𝑡D4\"|𝐬= (𝑡\", 𝑡!, … , 𝑡D)D \\n(2.2) \\nHere, Pr represents the conditional probability distribution over the vocabulary \\nfor the next token. A conditional probability quantifies the likelihood of one \\nevent occurring given that another has already occurred. In language models, \\nit reflects the probability of a specific token being the next one, given the pre-\\nceding sequence of tokens. This sequence is often referred to as the input se-\\nquence, context, or prompt. \\nThe following notations are equivalent to Equation 2.2: \\n \\nPr(𝑡D4\"|𝑡\", 𝑡!, … , 𝑡D) or Pr(𝑡D4\"|𝐬) \\n(2.3) \\nWe will select different notations, ranging from concise to detailed, based on \\nthe context. \\nFor any token 𝑡 and sequence 𝐬, the conditional probability satisfies Pr(𝑡|𝐬) ≥\\n0, meaning probabilities are always non-negative. Furthermore, the probabili-\\nties for all possible next tokens in the vocabulary 𝒱 must sum to 1: \\n∑\\n𝑃\\nE∈𝒱\\n(𝑡|𝐬) = 1. This ensures the model outputs a valid discrete probability \\ndistribution over the vocabulary. \\nTo illustrate, let’s consider an example with a vocabulary 𝒱 containing 5 \\nwords: “are,” “cool,” “language,” “models,” and “useless.” For the sequence 𝐬=\\n(language, models, are), a language model could output the following proba-\\nbilities for each possible next word in 𝒱: \\nPrC𝑡= are|𝐬= (language, models, are)D\\n= 0.01\\nPrC𝑡= cool|𝐬= (language, models, are)D\\n= 0.77\\nPrC𝑡= language|𝐬= (language, models, are)D\\n= 0.02\\nPrC𝑡= models|𝐬= (language, models, are)D\\n= 0.15\\nPrC𝑡= useless|𝐬= (language, models, are)D\\n= 0.05'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 77}, page_content='77 \\nThe illustration demonstrates how the language model assigns probabilities \\nacross its vocabulary for each potential next word, with “cool” receiving the \\nhighest probability. These probabilities sum to 1, forming a valid discrete prob-\\nability distribution. \\nThis type of model is an autoregressive language model, also known as a \\ncausal language model. Autoregression involves predicting an element in a \\nsequence using only its predecessors. Such models excel at text generation and \\ninclude Transformer-based chat language models (chat LMs) and all lan-\\nguage models discussed in this book. \\nIn contrast, masked language models, such as BERT—a pioneering Trans-\\nformer-based model—use a different approach. These models predict inten-\\ntionally masked tokens within sequences, utilizing both preceding and follow-\\ning context. This bidirectional approach particularly suits tasks like text classi-\\nfication and named entity recognition. \\nBefore neural networks became standard for language modeling, traditional \\nmethods relied on statistical techniques. These count-based models, still used \\nin smartphone autocomplete, estimate the probability of word sequences based \\non word or n-gram frequency counts learned from a corpus. To understand \\nthese methods better, let\\'s implement a simple count-based language model. \\n2.5. Count-Based Language Model \\nWe’ll focus on a trigram model (𝑛= 3) to illustrate how this works. In a tri-\\ngram model, the probability of a token is calculated based on the two preced-\\ning tokens: \\n \\nPr(𝑡$|𝑡$6!, 𝑡$6\") = 𝐶(𝑡$6!, 𝑡$6\", 𝑡$)\\n𝐶(𝑡$6!, 𝑡$6\") , \\n(2.4) \\nwhere 𝐶(⋅) denotes the count of occurrences of an n-gram in the training data. \\nFor instance, if the trigram “language models rock” appears 50 times in the \\ncorpus and “language models” appears 200 times overall, then: \\nPr(rock|language, models) = 50\\n200 = 0.25 \\nThis means that “rock” follows “language models” 25% of the time in our train-\\ning data.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 78}, page_content='78 \\n \\nEquation 2.4 is the maximum likelihood estimate (MLE) of a token’s \\nprobability given its context. It measures the relative frequency of a \\ntrigram compared to all trigrams sharing the same two-token history. \\nWith a larger training corpus, the MLE becomes more reliable for n-\\ngrams that occur frequently. This aligns with a basic statistical princi-\\nple: larger datasets yield more accurate estimates. \\nHowever, a limited-size corpus poses a problem: some n-grams we may en-\\ncounter in practice might not appear in the training data. For instance, if the \\ntrigram “language models sing” never appears in our corpus, its probability \\nwould be zero according to the MLE: \\nPr(sing|language, models) =\\n0\\n200 = 0 \\nThis is problematic because it assigns a zero probability to any sequence con-\\ntaining an unseen n-gram, even if it’s a valid phrase. To solve this, several \\ntechniques exist, one of which is backoff. The idea is simple: if a higher-order \\nn-gram (e.g., trigram) is not observed, we “back off” to a lower-order n-gram \\n(e.g., bigram). The probability Pr(𝑡$|𝑡$6!, 𝑡$6\") is given by one of the following \\nexpressions, depending on whether the condition is true: \\n \\nHere, 𝐶(𝑡$6!, 𝑡$6\", 𝑡$) is the count of the trigram (𝑡$6!, 𝑡$6\", 𝑡$), 𝐶(𝑡$6!, 𝑡$6\") and \\n𝐶(𝑡$6\", 𝑡$) are the counts of the bigrams (𝑡$6!, 𝑡$6\") and (𝑡$6\", 𝑡$) respectively. \\nThe bigram probability and unigram probability are computed as: \\nPr(𝑡$|𝑡$6\") = 𝐶(𝑡$6\", 𝑡$)\\n𝐶(𝑡$6\") , \\u2001Pr(𝑡$) = 𝐶(𝑡$) + 1\\n𝑊+ 𝑉,'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 79}, page_content=\"79 \\nwhere 𝐶(𝑡$) is the count of the token 𝑡$, 𝑊 is the total number of tokens in the \\ncorpus, and 𝑉 is the vocabulary size. \\nAdding 1 to 𝐶(𝑡$), known as add-one smoothing or Laplace smoothing, ad-\\ndresses zero probabilities for tokens not present in the corpus. If we used the \\nactual frequency Pr(𝑡$) =\\nB(E&)\\nH , any token not found in the corpus would have \\na zero probability, creating problems when the model encounters valid but \\nunseen tokens. Laplace smoothing solves this by adding 1 to each token count, \\nensuring all tokens, including unseen ones, receive a small, non-zero probabil-\\nity. The denominator is adjusted by adding 𝑉 to account for the extra counts \\nintroduced in the numerator. \\nNow, let’s implement a language model with backoff in the CountLanguage-\\nModel class (we'll implement Laplace smoothing in the next section): \\nclass CountLanguageModel: \\n    def __init__(self, n): ➊ \\n        self.n = n \\n        self.ngram_counts = [{} for _ in range(n)] ➋ \\n        self.total_unigrams = 0 \\n \\n    def predict_next_token(self, context): ➌ \\n        for n in range(self.n, 1, -1): ➍ \\n            if len(context) >= n - 1: ➎ \\n                context_n = tuple(context[-(n - 1):]) ➏ \\n                counts = self.ngram_counts[n - 1].get(context\\n_n) \\n                if counts: \\n                    return max(counts.items(), key=lambda x: \\nx[1])[0] \\n        unigram_counts = self.ngram_counts[0].get(()) \\n        if unigram_counts: \\n            return max(unigram_counts.items(), key=lambda x: \\nx[1])[0] \\n        return None \\nIn line ➊, the model is initialized with an n parameter, defining the maximum \\nn-gram order (e.g., n=3 for trigrams). The ngram_counts list in line ➋ stores\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 80}, page_content='80 \\n \\nn-gram frequency dictionaries for unigrams, bigrams, trigrams, etc., populated \\nduring training. For n=3, given the corpus “Language models are powerful. Lan-\\nguage models are useful.” in lowercase with punctuation removed, \\nself.ngram_counts would contain: \\nngram_counts[0] = {(): {\"language\": 2, \"models\": 2, \"are\": 2, \\n\"powerful\": 1, \"useful\": 1}} \\n \\nngram_counts[1] = {(\"language\",): {\"models\": 2}, (\"models\",): \\n{\"are\": 2}, (\"are\",): {\"powerful\": 1, \"useful\": 1}, (\"powerfu\\nl\",): {\"language\": 1}} \\n \\nngram_counts[2] = {(\"language\", \"models\"): {\"are\": 2}, (\"mode\\nls\", \"are\"): {\"powerful\": 1, \"useful\": 1}, (\"are\", \"powerful\"\\n): {\"language\": 1}, (\"powerful\", \"language\"): {\"models\": 1}} \\nThe predict_next_token method uses backoff to predict the next token. \\nStarting from the highest n-gram order in line ➍, it checks if the context con-\\ntains enough tokens for this n-gram order in line ➎. If so, it extracts the context \\nin line ➏ and attempts to find a match in ngram_counts. If no match is found, \\nit backs off to lower-order n-grams or defaults to unigram counts. For instance, \\ngiven context=[\"language\", \"models\", \"are\"] and n=3: \\n• First iteration: context_n = (\"models\", \"are\") \\n• Second iteration (if needed): context_n = (\"are\",) \\n• Last resort: unigram counts with empty tuple key () \\nIf a matching context is found, the method returns the token with the highest \\ncount for that context. For input [\"language\", \"models\"] it will return \\n\"are\", the token with highest count among values for the key (\"language\", \\n\"models\") in ngram_counts[2]. However, for the input [\"english\", \\n\"language\"] it will not find the key (\"english\", \"language\") in \\nngram_counts[2], so it will backoff to ngram_counts[1] and return \"mod-\\nels\", the token with highest count among values for the key (\"language\",). \\nNow, let’s define the method that trains our model: \\ndef train(model, tokens): \\n    model.total_unigrams = len(tokens) \\n    for n in range(1, model.n + 1): ➊ \\n        counts = model.ngram_counts[n - 1]'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 81}, page_content='81 \\n        for i in range(len(tokens) - n + 1): \\n            context = tuple(tokens[i:i + n - 1]) ➋ \\n            next_token = tokens[i + n - 1] ➌ \\n            if context not in counts: \\n                counts[context] = defaultdict(int) \\n            counts[context][next_token] = counts[context][nex\\nt_token] + 1 \\nThe train method takes a model (an instance of CountLanguageModel) and \\na list of tokens (the training corpus) as input. It updates the n-gram counts in \\nthe model using these tokens. \\nIn line ➊, the method iterates over n-gram orders from 1 to model.n (inclu-\\nsive). For each n, it generates n-grams of that order from the token sequence \\nand counts their occurrences. \\nLines ➋ and ➌ extract contexts and their subsequent tokens to build a nested \\ndictionary where each context maps to a dictionary of following tokens and \\ntheir counts. These counts are stored in model.ngram_counts, which the \\npredict_next_token method later uses to make predictions based on con-\\ntext. \\nNow, let’s train the model: \\nset_seed(42) \\nn = set_hyperparameters() \\ndata_url = \"https://www.thelmbook.com/data/brown\" \\ntrain_corpus, test_corpus = download_and_prepare_data(data_ur\\nl) \\n \\nmodel = CountLanguageModel(n) \\ntrain(model, train_corpus) \\n \\nperplexity = compute_perplexity(model, test_corpus) \\nprint(f\"\\\\nPerplexity on test corpus: {perplexity:.2f}\") \\n \\ncontexts = [ \\n    \"i will build a\", \\n    \"the best place to\", \\n    \"she was riding a\"'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 82}, page_content='82 \\n \\n] \\n \\nfor context in contexts: \\n    words = tokenize(context) \\n    next_word = model.predict_next_token(words) \\n    print(f\"\\\\nContext: {context}\") \\n    print(f\"Next token: {next_word}\") \\nThe full implementation of this model, including the methods to retrieve and \\nprocess the training data, can be found in the thelmbook.com/nb/2.2 note-\\nbook. Within the download_and_prepare_data method, the corpus is down-\\nloaded, converted to lowercase, tokenized into words, and divided into train-\\ning and test partitions with a 90/10 split. Let’s take a moment to understand \\nwhy this last step is critical. \\nIn machine learning, using the entire dataset for training leaves no way to \\nevaluate whether the model generalizes well. A frequent issue is overfitting, \\nwhere the model excels on training data but struggles to make accurate pre-\\ndictions on unseen, new data. \\nPartitioning the dataset into training and test sets is a standard practice to \\ncontrol overfitting. It involves two steps: (1) shuffling the data and (2) splitting \\nit into two subsets. The larger subset, called the training data, is used for train-\\ning the model, while the smaller subset, called the test data, is used to evaluate \\nthe model’s performance on unseen examples. \\nThe test set requires sufficient size to reliably estimate model perfor-\\nmance. A test ratio of 0.1 to 0.3 (10% to 30% of the entire dataset) is \\ncommon, though this varies with dataset size. For very large datasets, \\neven a smaller test set ratio results in enough examples to provide re-\\nliable performance estimates. \\nThe training data comes from the Brown Corpus, a collection of over 1 million \\nwords from American English texts published in 1961. This corpus is frequently \\nused in linguistic studies. \\nWhen you run the code, you will see the following output: \\nPerplexity on test corpus: 299.06 \\n \\nContext: i will build a'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 83}, page_content=\"83 \\nNext word: wall \\n \\nContext: the best place to \\nNext word: live \\n \\nContext: she was riding a \\nNext word: horse \\nIgnore the perplexity number for now; we'll discuss it shortly. Count-based \\nlanguage models can produce reasonable immediate continuations, making \\nthem good for autocomplete systems. However, they have notable limitations. \\nThese models generally work with word-tokenized corpora, as their n-gram \\nsize is typically small (up to 𝑛= 5). Extending beyond this would require too \\nmuch memory and lead to slower processing. Subword tokenization, while \\nmore efficient, results in many n-grams that represent only fragments of words, \\ndegrading the quality of next-word predictions. \\nWord-level tokenization creates another significant drawback: count-based \\nmodels cannot handle out-of-vocabulary (OOV) words. This is similar to the \\nissue seen in the bag-of-words approach discussed in Section 2.1. For exam-\\nple, consider the context: “according to WHO, COVID-19 is a.” If “COVID-19” \\nwasn’t in the training data, the model would back off repeatedly until it relies \\nonly on “is a,” severely limiting the context for meaningful predictions. \\nCount-based models are also unable to capture long-range dependencies in \\nlanguage. While modern Transformer models can handle thousands of tokens, \\ntraining a count-based model with a context of 1000 tokens would require \\nstoring counts for all n-grams from 𝑛= 1 to 𝑛= 1000, requiring prohibitive \\namounts of memory. \\nAdditionally, these models cannot be adapted for downstream tasks after train-\\ning, as their n-gram counts are fixed, and any adjustment requires retraining \\non new data. \\nThese limitations have led to the development of advanced methods, particu-\\nlarly neural network-based language models, which have largely replaced \\ncount-based models in modern natural language processing. Approaches like \\nrecurrent neural networks and transformers, which we'll discuss in the next \\ntwo chapters, handle longer contexts effectively, producing coherent and\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 84}, page_content='84 \\n \\ncontext-aware text. Before exploring these methods, let\\'s look at how to eval-\\nuate a language model\\'s quality. \\n2.6. Evaluating Language Models \\nEvaluating language models measures their performance and allows compar-\\ning models. Several metrics and techniques are commonly used. Let’s look at \\nthe main ones. \\n2.6.1. Perplexity \\nPerplexity is a widely used metric for evaluating language models. It measures \\nhow well a model predicts a text. Lower perplexity values indicate a better \\nmodel—one that is more confident in its predictions. Perplexity is defined as \\nthe exponential of the average negative log-likelihood per token in the test \\nset: \\n \\nPerplexity(𝒟, 𝑘) = exp  −1\\n𝐷U log\\n3\\n$%\"\\nPrC𝑡$|𝑡IJK(\",$6:), … , 𝑡$6\"D¡ \\n(2.5) \\nHere, 𝒟 represents the test set, 𝐷 is the total number of tokens in it, 𝑡$ is the \\n𝑖th token, and PrC𝑡$|𝑡IJK(\",$6:), … , 𝑡$6\"D is the probability the model assigns to \\n𝑡$ given its preceding context window of size 𝑘, where max(1, 𝑖−𝑘) ensures \\nwe start from the first token when there aren’t enough preceding tokens to fill \\nthe context window. The notations exp(𝑥) and 𝑒,, where 𝑒 is Euler’s number, \\nare equivalent. \\nThe negative log-likelihood (NLL) in Equation 2.5 is the negative logarithm of \\nthe probabilities our language model assigns. When a model processes text like \\n“language models are” and assigns a probability of 0.77 to the next word “cool”, \\nthe NLL would be −log(0.77). It’s called “negative” log-likelihood because we \\ntake the negative of the logarithm, and “likelihood” refers to these conditional \\nprobabilities the model computes. In language modeling, NLL serves two pur-\\nposes: it acts as a loss function during training to help models learn better \\nprobability distributions (which we’ll see in the next chapter when training a \\nrecurrent neural network language model), and as shown in the perplexity \\nformula, it helps us evaluate how well models predict text. \\nPerplexity can be understood more intuitively through its geometric mean for-\\nmulation. The geometric mean of a set of numbers is the 𝐷th root of their'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 85}, page_content='85 \\nproduct (where 𝐷 is the number of values), and perplexity is the geometric \\nmean of the inverse probabilities: \\nPerplexity(𝒟, 𝑘) = ¢£\\n1\\nPrC𝑡$|𝑡IJK(\",$6:), … , 𝑡$6\"D\\n3\\n$%\"\\n¤\\n\"\\n3\\n \\nThis form shows that perplexity represents the weighted average factor by \\nwhich the model is “perplexed” when predicting each token. A perplexity of 10 \\nmeans that, on average, the model is as uncertain as if it had to choose uni-\\nformly between 10 possibilities at each step. \\nIf a language model assigns equal probability to every token in a vo-\\ncabulary of size 𝑉, its perplexity equals 𝑉. This provides an intuitive \\nupper bound for perplexity—a model cannot be more uncertain than \\nwhen it assigns equal likelihood to all possible tokens.  \\nWhile both formulations of perplexity shown above are mathematically equiv-\\nalent (proof available on the book’s wiki), the exponential form is computa-\\ntionally more convenient as it transforms products into sums through the log-\\narithm, making calculations more numerically stable. \\nLet’s calculate perplexity using the example text with word-level tokenization \\nand ignoring punctuation: “We are evaluating a language model for English.” To \\nkeep things simple, we assume a context of up to three words. We begin by \\ndetermining the probability of each word based on its preceding context of \\nthree words as provided by the model. Here are the probabilities: \\nPr(We)\\n= 0.10\\nPr(are ∣We)\\n= 0.20\\nPr(evaluating ∣We, are)\\n= 0.05\\nPr(a ∣We, are, evaluating)\\n= 0.50\\nPr(language ∣are, evaluating, a)\\n= 0.30\\nPr(model ∣evaluating, a, language)\\n= 0.40\\nPr(for ∣a, language, model)\\n= 0.15\\nPr(English ∣language, model, for)\\n= 0.25\\n \\nUsing the probabilities, we compute the negative log-likelihood for each word:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 86}, page_content='86 \\n \\n−logC𝑃(We)D\\n= −log(0.10) ≈2.30\\n−logC𝑃(are ∣We)D\\n= −log(0.20) ≈1.61\\n−logC𝑃(evaluating ∣We, are)D\\n= −log(0.05) ≈3.00\\n−logC𝑃(a ∣We, are, evaluating)D\\n= −log(0.50) ≈0.69\\n−logC𝑃(language ∣are, evaluating, a)D\\n= −log(0.30) ≈1.20\\n−logC𝑃(model ∣evaluating, a, language)D\\n= −log(0.40) ≈0.92\\n−logC𝑃(for ∣a, language, model)D\\n= −log(0.15) ≈1.90\\n−logC𝑃(English ∣language, model, for)D\\n= −log(0.25) ≈1.39\\n \\nNext, we sum these values and divide the sum by the number of words (8) to \\nget the average: \\n(2.30 + 1.61 + 3.00 + 0.69 + 1.20 + 0.92 + 1.90 + 1.39)/8 ≈1.63 \\nFinally, we exponentiate the average negative log-likelihood to obtain the per-\\nplexity: \\n𝑒\".L& ≈5.10 \\nSo, the model’s perplexity on this text, using a 3-word context, is about 5.10. \\nThis means that, on average, the model acts as if it selects from roughly 5 \\nequally likely options for each prediction. \\nNow, let’s calculate the perplexity of the count-based model from the previous \\nsection. To do this, the model must be updated to return the probability of a \\ntoken given a specific context. Add this function to the CountLanguageModel \\nwe implemented earlier: \\ndef get_probability(self, token, context): \\n    for n in range(self.n, 1, -1): ➊ \\n        if len(context) >= n - 1: \\n            context_n = tuple(context[-(n - 1):]) \\n            counts = self.ngram_counts[n - 1].get(context_n) \\n            if counts: ➋ \\n                total = sum(counts.values()) ➌ \\n                count = counts.get(token, 0) \\n                if count > 0: \\n                    return count / total ➍ \\n    unigram_counts = self.ngram_counts[0].get(()) ➎ \\n    count = unigram_counts.get(token, 0)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 87}, page_content=\"87 \\n    V = len(unigram_counts) \\n    return (count + 1) / (self.total_unigrams + V) ➏ \\nThe get_probability function is similar to predict_next_token. Both loop \\nthrough n-gram orders in reverse (line ➊) and extract the relevant context \\n(context_n). If context_n matches in the n-gram counts (line ➋), the func-\\ntion retrieves the token counts. If no match exists, it backs off to lower-order \\nn-grams and, finally, unigrams (line ➎). \\nUnlike predict_next_token, which returns the most probable token directly, \\nget_probability calculates a token’s probability. In line ➌, total is the sum \\nof counts for tokens following the context, acting as the denominator. Line ➍ \\ndivides the token count by total to compute its probability. If no higher-order \\nmatch exists, line ➏ uses add-one smoothing with unigram counts. \\nThe compute_perplexity method computes a language model’s perplexity \\nfor a token sequence. It takes three arguments: the model, the token sequence, \\nand the context size: \\ndef compute_perplexity(model, tokens, context_size): \\n    if not tokens: \\n        return float('inf') \\n    total_log_likelihood = 0 \\n    num_tokens = len(tokens) \\n    for i in range(num_tokens): ➊ \\n        context_start = max(0, i - context_size) \\n        context = tuple(tokens[context_start:i]) ➋ \\n        word = tokens[i] \\n        probability = model.get_probability(word, context) \\n        total_log_likelihood += math.log(probability) ➌ \\n    average_log_likelihood = total_log_likelihood / num_token\\ns ➍ \\n    perplexity = math.exp(-average_log_likelihood) ➎ \\n    return perplexity \\nIn line ➊, the function iterates through each token in the sequence. For every \\ntoken:\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 88}, page_content='88 \\n \\n• Line ➋ extracts its context, using up to context_size tokens before it. \\nThe expression max(0, i - context_size) ensures indices stay \\nwithin bounds like in Equation 2.5. \\n• In line ➌, the log of the token’s probability is added to the cumulative \\nlog-likelihood. The get_probability method from the model handles \\nthe probability calculation. \\nOnce all tokens are processed, line ➍ computes the average log-likelihood by \\ndividing the total log-likelihood by the number of tokens. \\nFinally, in line ➎, the perplexity is computed as the exponential of the negative \\naverage log-likelihood, as described in Equation 2.5. \\nBy applying this method to the test_corpus sequence from the thelm-\\nbook.com/nb/2.2 notebook, we observe the following output: \\nPerplexity on test corpus: 299.06 \\nThis perplexity is very high. For example, GPT-2 has a perplexity of about 20, \\nwhile modern LLMs achieve values below 5. Later, we’ll compute perplexities \\nfor RNN and Transformer-based models and compare them with the perplexity \\nof this count-based model. \\n2.6.2. ROUGE \\nPerplexity is a standard metric used to evaluate language models trained on \\nlarge, unlabeled datasets by measuring how well they predict the next token \\nin context. These models are referred to as pretrained models or base mod-\\nels. As we’ll discuss in the chapter on large language models, their ability to \\nperform specific tasks or answer questions comes from supervised finetuning. \\nThis additional training uses a labeled dataset where input contexts are \\nmatched with target outputs, such as answers or task-specific results. This en-\\nables problem-solving capabilities. \\nPerplexity is not an ideal metric for evaluating a finetuned model. Instead, \\nmetrics are needed that compare the model’s output to reference texts, often \\ncalled ground truth. A common choice is ROUGE (Recall-Oriented Under-\\nstudy for Gisting Evaluation). ROUGE is widely used for tasks like summariza-\\ntion and machine translation. It evaluates text quality by measuring overlaps, \\nsuch as tokens or n-grams, between the generated text and the reference.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 89}, page_content='89 \\nROUGE has several variants, each focusing on different aspects of text similar-\\nity. Here, we’ll discuss three widely used ones: ROUGE-1, ROUGE-N, and \\nROUGE-L. \\nROUGE-N evaluates the overlap of n-grams between the generated and refer-\\nence texts, with N indicating the length of the n-gram. One of the most com-\\nmonly used versions is ROUGE-1. \\nROUGE-1 measures the overlap of unigrams (single tokens) between the gen-\\nerated and reference texts. As a recall-focused metric (hence the “R” in \\nROUGE), it assesses how much of the reference text is captured in the gener-\\nated output. \\nRecall is the ratio of matching tokens to the total number of tokens in the ref-\\nerence text: \\nrecall =\\ndef\\nNumber of matching tokens\\nTotal number of tokens in reference texts \\nFormally, ROUGE-1 is defined as: \\nROUGE-1 =\\ndef ∑\\n∑\\ncount\\nE∈M\\n(.,M)∈𝒟\\n(𝑡, 𝑔)\\n∑\\nlength\\n(.,M)∈𝒟\\n(𝑟)\\n \\nHere, 𝒟 is the dataset of (generated text, reference text) pairs, count(𝑡, 𝑔) \\ncounts how often a token 𝑡 from the reference text 𝑟 appears in the generated \\ntext 𝑔, and the denominator is the total token count across all reference texts. \\nTo understand this calculation, consider a simple example: \\nReference text \\nGenerated text \\nLarge language models are very im-\\nportant for text processing. \\nLarge language models are useful in \\nprocessing text. \\nLet’s use word-level tokenization and calculate: \\n• Matching words: large, language, models, are, processing, text (6 \\nwords) \\n• Total words in the reference text: 9 \\n• ROUGE-1: \\nL\\nN ≈0.67 \\nA ROUGE-1 score of 0.67 means roughly two-thirds of the words from the ref-\\nerence text appear in the generated text. However, this number alone has little'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 90}, page_content='90 \\n \\nvalue. ROUGE scores are only useful for comparing how different language \\nmodels perform on the same test set, as they indicate which model more effec-\\ntively captures the content of the reference texts. \\nROUGE-N extends the ROUGE metric from unigrams to n-grams while using \\nthe same formula. \\nROUGE-L relies on the longest common subsequence (LCS). This is the long-\\nest sequence of tokens appearing in both generated and reference texts in the \\nsame order, without being adjacent. \\nLet 𝑔 and 𝑟 be the generated and reference texts with lengths 𝐿. and 𝐿M. Then: \\nrecallLCS =\\ndef LCS(𝑔, 𝑟)\\n𝐿M\\n,\\u2001precisionLCS =\\ndef LCS(𝑔, 𝑟)\\n𝐿.\\n \\nHere, LCSC𝐿., 𝐿MD represents the number of tokens in the LCS between the \\ngenerated text 𝑔 and the reference text 𝑟. The recall measures the proportion \\nof the reference text captured by the LCS, while the precision measures the \\nproportion of the generated text that matches the reference. Recall and preci-\\nsion are combined into a single metric as follows: \\nROUGE-L =\\ndef (1 + 𝛽!) ×\\nrecallLCS × precisionLCS\\nrecallLCS + 𝛽! × precisionLCS\\n \\nHere, 𝛽 controls the trade-off between precision and recall in the ROUGE-L \\nscore. Since ROUGE favors recall, 𝛽 is usually set high, such as 8. \\nLet’s revisit the two texts used to illustrate ROUGE-L. For these sentences, there \\nare two valid longest common subsequences, each with a length of 5 words: \\nLCS 1 \\nLCS 2 \\nLarge, language, models, are, text Large, language, models, are, pro-\\ncessing \\nBoth subsequences are the longest sequences of tokens that appear in the same \\norder in both sentences, but not necessarily consecutively. When multiple LCS \\noptions exist, ROUGE-L can use any of them since their lengths are identical. \\nHere’s how the calculations work here. The length of the LCS is 5 words. The \\nreference text is 9 words long, and the generated text is 8 words long. Thus, \\nrecall and precision are:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 91}, page_content='91 \\nrecallLCS = 5\\n9 ≈0.56,\\u2001precisionLCS = 5\\n8 ≈0.63 \\nWith 𝛽= 8, ROUGE-L is then given by: \\nROUGE-L =\\n(1 + 8!) ⋅0.56 ⋅0.63\\n0.56 + 8! ⋅0.63\\n≈0.56 \\nROUGE scores range from 0 to 1, where 1 means a perfect match between \\ngenerated and reference texts. However, even excellent summaries or transla-\\ntions rarely approach 1 in practice. \\nChoosing the right ROUGE metric depends on the task: \\n• ROUGE-1 and ROUGE-2 are standard starting points. ROUGE-1 checks \\noverall content similarity using unigram overlap, while ROUGE-2 eval-\\nuates local fluency and phrase accuracy using bigram matches. \\n• ROUGE-L is preferred over ROUGE-1 or ROUGE-2 when evaluating text \\nquality in terms of sentence structure and flow, particularly in summa-\\nrization and translation tasks, since it captures the longest sequence of \\nmatching words that appear in the same relative order, better reflecting \\ngrammatical coherence. \\n• In cases where preserving longer patterns is key—such as maintaining \\ntechnical terms or idioms—higher-order metrics like ROUGE-3 or \\nROUGE-4 might be more relevant. \\nA combination of metrics, such as ROUGE-1, ROUGE-2, and ROUGE-L, often \\ngives a more balanced evaluation, covering both content overlap and structural \\nflexibility. \\nKeep in mind, though, that ROUGE has limitations. It measures lexical overlap \\nbut not semantic similarity or factual correctness. To address these gaps, \\nROUGE is often paired with human evaluations or other methods for a fuller \\nassessment of text quality. \\n2.6.3. Human Evaluation \\nAutomated metrics are useful, but human evaluation is still necessary to assess \\nlanguage models. Humans can evaluate qualities that automated metrics often \\nmiss, like fluency and accuracy. Two common approaches for human evalua-\\ntion are Likert scale ratings and Elo ratings.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 92}, page_content='92 \\n \\nLikert scale ratings involve assigning scores to outputs using a fixed, typically \\nsymmetric scale. Raters judge the quality by selecting a score, often from −2 \\nto 2, where each scale point corresponds to a descriptive label. For instance, \\n−2 could mean “Strongly Disagree” or “Poor,” while 2 might mean “Strongly \\nAgree” or “Excellent.” The scale is symmetric because it includes equal levels \\nof agreement and disagreement around a neutral midpoint, making positive \\nand negative responses easier to interpret. \\nLikert scales are flexible for evaluating different aspects of language model \\noutputs, such as fluency, coherence, relevance, and accuracy. For example, a \\nrater could separately rate a sentence’s grammatical correctness and its rele-\\nvance to a prompt, both on a scale from −2 to 2. \\nHowever, the method has limitations. One issue is central tendency bias, \\nwhere some raters avoid extreme scores and stick to the middle of the scale. \\nAnother challenge is inconsistency in how raters interpret the scale—some may \\nreserve a 2 for exceptional outputs, while others may assign it to any high-\\nquality response. \\nTo mitigate these issues, researchers often involve multiple raters, phrase sim-\\nilar questions in different ways for the same rater, and use detailed rubrics that \\nclearly define each scale point. \\nLet’s illustrate Likert scale evaluation using a scenario where machine-gener-\\nated summaries of news articles are assessed. \\nHuman raters compare a model-generated summary to the original article. \\nThey rate it on three aspects using 5-point Likert scales: coherence, informa-\\ntiveness, and factual accuracy. \\nFor example, consider the news article on the left and the generated summary \\non the right:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 93}, page_content='93 \\n \\nRaters assess the summary based on how effectively it meets these three crite-\\nria. \\nThe scale for assessing coherence—that is, how well-organized, readable, and \\nlogically connected the summary is—might look like this: \\nVery poor Poor Acceptable Good Excellent \\n-2 \\n-1 \\n0 \\n1 \\n2 \\nThe scale for assessing informativeness, that is, how well the summary cap-\\ntures the essence and main points of the original article, might look this way: \\nNot informative Slightly Moderately Very Extremely informative \\n-2 \\n-1 \\n0 \\n1 \\n2 \\nThe scale for assessing factual accuracy—that is, how precisely the summary \\nrepresents facts and data from the original article—might look like this: \\nVery low \\nSome inaccura-\\ncies \\nMostly Accurate \\nVery accurate \\nPerfect \\n-2 \\n-1 \\n0 \\n1 \\n2 \\nIn this example, raters would select one option for each of the three aspects. \\nThe use of descriptive anchors at each point on the scale helps standardize \\nunderstanding among raters. \\nAfter collecting ratings from multiple raters across various summaries, re-\\nsearchers analyze the data through several approaches:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 94}, page_content='94 \\n \\n• Calculate average scores for each aspect across all summaries and \\nraters to get an overall performance measure. \\n• Compare scores across different versions of the model to track improve-\\nments. \\n• Analyze the correlation between different aspects (e.g., is high coher-\\nence associated with high factual accuracy?). \\nAlthough Likert scale ratings were originally intended for humans, the \\nrise of advanced chat LMs means raters can now be either humans or \\nlanguage models. \\nPairwise comparison is a method where two outputs are evaluated side-by-\\nside, and the better one is chosen based on specific criteria. This simplifies \\ndecision-making, especially when outputs are of similar quality or changes are \\nminor. \\nThe method builds on the principle that relative judgments are easier than \\nabsolute ones. Binary choices often produce more consistent and reliable re-\\nsults than absolute ratings. \\nIn practice, raters compare pairs of outputs, such as translations, summaries, \\nor answers, and decide which is better based on criteria like coherence, in-\\nformativeness, or factual accuracy. \\nFor example, in machine translation evaluation, raters compare pairs of trans-\\nlations for each source sentence, selecting which one better preserves the orig-\\ninal meaning in the target language. By repeating this process across many \\npairs, evaluators can compare different models or versions. \\nPairwise comparison helps rank models or model versions by having each rater \\nevaluate multiple pairs, with each model compared against others several \\ntimes. This repetition minimizes individual biases, resulting in more reliable \\nevaluations. A related approach is ranking, where a rater orders multiple re-\\nsponses by quality. Ranking requires less effort than pairwise comparisons \\nwhile still capturing relative quality. \\nResults from pairwise comparisons are typically analyzed statistically to deter-\\nmine significant differences between models. A common method for this anal-\\nysis is the Elo rating system.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 95}, page_content='95 \\nElo ratings, originally created by Arpad Elo in 1960 for ranking chess players, \\ncan be adapted for language model evaluation. The system assigns ratings \\nbased on “wins” and “losses” in direct comparisons, quantifying relative model \\nperformance. \\nIn language model evaluation, all models typically start with an initial rating, \\noften set to 1500. When two models are compared, the probability of one \\nmodel “winning” is calculated using their current ratings. After each compari-\\nson, their ratings are updated based on the actual result versus the expected \\nresult. \\nThe probability of model 𝐴 with rating Elo(𝐴) winning against model 𝐵 with \\nrating Elo(𝐵) is: \\nPr(𝐴 wins) =\\n1\\n1 + 10OElo(P)6Elo(Q)R/T?? \\nThe value 400 in the Elo formula acts as a scaling factor, creating a \\nlogarithmic relationship between rating differences and winning prob-\\nabilities. Arpad Elo chose this number ensuring that a 400-point rating \\ndifference reflects 10: 1 odds in favor of the higher-rated chess player. \\nWhile originally designed for chess, this scaling factor has proven ef-\\nfective in other contexts, including language model evaluation. \\nAfter a match, ratings are updated using the formula: \\nElo(𝐴) ←Elo(𝐴) + 𝑘× Cscore(𝐴) −Pr(𝐴 wins)D, \\nwhere 𝑘 (typically between 4 and 32) controls the maximum rating change, \\nand score(𝐴) reflects the outcome: 1 for a win, 0 for a loss, and 0.5 for a draw. \\nConsider an example with three models: LM\", LM!, and LM&. We’ll evaluate \\nthem based on their ability to generate coherent text continuations. Assume \\ntheir initial ratings are: \\nElo(LM\")\\n= 1500\\nElo(LM!)\\n= 1500\\nElo(LM&)\\n= 1500\\n \\nWe’ll use 𝑘= 32 for this example. \\nConsider this prompt: “The scientists were shocked when they discovered…”'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 96}, page_content='96 \\n \\nContinuation by LM\": “…a new species of butterfly in the Amazon rainforest. Its \\nwings were unlike anything they had ever seen before.” \\nContinuation by LM!: “…that the ancient artifact they unearthed was emitting \\na faint, pulsating light. They couldn’t explain its source.” \\nContinuation by LM&: “…the results of their experiment contradicted everything \\nthey thought they knew about quantum mechanics.” \\nLet’s say we conduct pairwise comparisons and get the following results: \\n1. LM\" vs LM!: LM\" wins \\no Pr(LM\" wins) = 1/C1 + 10O(\"@??6\"@??)/T??RD = 0.5 \\no New rating for LM\" ←1500 + 32(1 −0.5) = 1516 \\no New rating for LM! ←1500 + 32(0 −0.5) = 1484 \\n2. LM\" vs LM&: LM& wins \\no Pr(LM\" wins) = 1/C1 + 10O(\"@??6\"@\"L)/T??RD ≈0.523 \\no New rating for LM\" ←1516 + 32(0 −0.523) ≈1499 \\no New rating for LM& ←1500 + 32(1 −0.477) ≈1517 \\n3. LM! vs LM&: LM& wins \\no Pr(LM! wins) = 1/C1 + 10O(\"@\"U6\"TVT)/T??RD ≈0.453 \\no New rating for LM! ←1484 + 32(0 −0.453) ≈1470 \\no New rating for LM& ←1517 + 32(1 −0.547) ≈1531 \\nFinal ratings after these comparisons: \\nElo(LM\")\\n= 1499\\nElo(LM!)\\n= 1470\\nElo(LM&)\\n= 1531\\n \\nElo ratings quantify how models perform relative to each other. In this case, \\nLM& is the strongest, followed by LM\", with LM! ranking last. \\nPerformance isn’t judged from a single match. Instead, multiple pairwise \\nmatches are used. This limits the effects of random fluctuations or biases in \\nindividual comparisons, giving a better estimate of each model’s performance. \\nA variety of prompts or inputs ensures evaluation across different contexts and \\ntasks. When human raters are involved, several raters assess each comparison \\nto reduce individual bias. \\nTo avoid order effects, both the sequence of comparisons and the presentation \\nof outputs are randomized. Elo ratings are updated after every comparison.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 97}, page_content='97 \\nHow many matches are needed until the results are reliable? There’s no uni-\\nversal number that applies to all cases. As a general guideline, some research-\\ners suggest that each model should participate in at least 100–200 comparisons \\nbefore considering the Elo ratings stable and ideally 500+ comparisons for \\nhigh confidence. However, for high-stakes evaluations or when comparing very \\nsimilar models, thousands of comparisons may be necessary. \\nStatistical methods can be used to calculate a confidence interval for \\na model’s Elo rating. Explaining these techniques is beyond the scope \\nof this book. For those interested, the Bradley–Terry model and boot-\\nstrap resampling are good starting points. Both are well-documented, \\nwith resources linked on the book’s wiki. \\nElo ratings provide a continuous scale for ranking models, making it easier to \\ntrack incremental improvements. The system rewards wins against strong op-\\nponents more than wins against weaker ones, and it can handle incomplete \\ncomparison data, meaning not every model needs to be compared against \\nevery other model. However, the choice of 𝑘 significantly affects rating vola-\\ntility; a poorly chosen 𝑘 can undermine the evaluation’s stability. \\nTo address these limitations, Elo ratings are often used alongside other evalu-\\nation methods. For instance, researchers might use Elo ratings for ranking \\nmodels in pairwise comparisons, while collecting Likert scale ratings to assess \\nabsolute quality. This combined approach yields a more comprehensive view \\nof a language model’s performance. \\nNow that we’ve covered language modeling and evaluation methods, let’s ex-\\nplore a more advanced model architecture: recurrent neural networks (RNNs). \\nRNNs made significant progress in processing text. They introduced the ability \\nto maintain context over long sequences, allowing for the creation of more \\npowerful language models.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 98}, page_content=\"98 \\n \\nChapter 3. Recurrent Neural Network \\nIn this chapter, we explore recurrent neural networks, a fundamental architec-\\nture that revolutionized sequential data processing. While transformers have \\nbecome dominant in many applications, understanding RNNs first provides an \\nideal stepping stone—their elegant design introduces key sequential processing \\nconcepts that make Transformer mathematics more intuitive. We'll examine \\nRNNs' structure and applications in language modeling, building essential \\nfoundations for more advanced architectures. \\n3.1. Elman RNN \\nA recurrent neural network, or RNN, is a neural network designed for se-\\nquential data. Unlike feedforward neural networks, RNNs include loops in \\ntheir connections, enabling information to carry over from one step in the se-\\nquence to the next. This makes them well-suited for tasks like time series anal-\\nysis, natural language processing, and other sequential data problems. \\nTo illustrate the sequential nature of RNNs, let’s consider a neural network \\nwith a single unit. Consider the input document “Learning from text is cool.” \\nIgnoring case and punctuation, the matrix representing this document would \\nbe as follows: \\nWord \\nEmbedding vector \\nlearning \\n[0.1,0.2,0.6]1 \\nfrom \\n[0.2,0.1,0.4]1 \\ntext \\n[0.1,0.3,0.3]1 \\nis \\n[0.0,0.7,0.1]1 \\ncool \\n[0.5,0.2,0.7]1 \\nPAD \\n[0.0,0.0,0.0]1 \\nEach row of the matrix represents a word’s embedding learned during neural \\nnetwork training. The order of words is preserved. The matrix dimensions are \\n(sequence length, embedding dimensionality). Sequence length specifies the \\nmaximum number of words in a document. Shorter documents are padded \\nwith padding tokens (like PAD in this example), while longer ones are trun-\\ncated. Padding uses dummy embeddings, usually zero vectors. \\nMore formally, the matrix would look like this:\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 99}, page_content='99 \\n𝐗=\\n⎣\\n⎢\\n⎢\\n⎢\\n⎢\\n⎡0.1\\n0.2\\n0.6\\n0.2\\n0.1\\n0.4\\n0.1\\n0.3\\n0.3\\n0.0\\n0.7\\n0.1\\n0.5\\n0.2\\n0.7\\n0.0\\n0.0\\n0.0⎦\\n⎥\\n⎥\\n⎥\\n⎥\\n⎤\\n \\nHere, we have five 3D embedding vectors, 𝐱\", … , 𝐱@, representing each word \\nin the document. For instance, 𝐱\" = [0.1,0.2,0.6]1, 𝐱! = [0.2,0.1,0.4]1, and so \\non. The sixth vector is a padding vector.  \\nThe Elman RNN, introduced by Jeffrey Locke Elman in 1990 as the simple \\nrecurrent neural network, processes a sequence of embedding vectors one at \\na time, as illustrated below: \\n \\nAt each time step 𝑡, the current input embedding 𝐱E and the previous hidden \\nstate 𝐡E6\" are combined by multiplying them with trainable weight matrices \\n𝐖W and 𝐔W, adding a bias vector 𝐛W, and producing the updated hidden state \\n𝐡E. Unlike MLP units, which output scalars, an RNN unit outputs vectors and \\nacts as an entire layer. The initial hidden state 𝐡? is usually a zero vector. \\nA hidden state is a memory vector that captures information from previous \\nsteps in a sequence. Updated at each step using current input and past state, it \\nhelps neural networks use context from earlier words to predict the next word \\nin a sentence. \\nTo deepen the network, we add a second RNN layer. The first layer’s outputs, \\n𝐡E, become inputs to the second, whose outputs are the network’s final out-\\nputs:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 100}, page_content='100 \\n \\nFigure 3.1: A two-layer Elman RNN. The first layer’s outputs serve as inputs to the second \\nlayer. \\n3.2. Mini-Batch Gradient Descent \\nBefore coding the RNN model, we need to discuss the shape of the input data. \\nIn Section 1.7, we used the entire dataset for each gradient descent step. Here, \\nand for training all future models, we’ll adopt mini-batch gradient descent, \\na widely used method for large models and datasets. Mini-batch gradient de-\\nscent calculates derivatives over smaller data subsets, which speeds up learn-\\ning and reduces memory usage. \\nWith mini-batch gradient descent, the data shape is organized as (batch size, \\nsequence length, embedding dimensionality). This structure divides the train-\\ning set into fixed-size mini-batches, each containing sequences of embeddings \\nwith consistent lengths. (From this point on, “batch” and “mini-batch” will be \\nused interchangeably.) \\nFor example, if the batch size is 2, the sequence length is 4, and the embedding \\ndimensionality is 3, the mini-batch can be represented as: \\nbatch\" = R\\nseq\",\"\\nseq\",!\\nseq\",&\\nseq\",T\\nseq!,\"\\nseq!,!\\nseq!,&\\nseq!,TS \\nHere, seq$,2, for 𝑖∈{1,2} and 𝑗∈{1, … ,4} is an embedding vector. \\nLet’s have the following embeddings for each sequence:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 101}, page_content='101 \\nseq\": s\\n[0.1,0.2,0.3]\\n[0.4,0.5,0.6]\\n[0.7,0.8,0.9]\\n[1.0,1.1,1.2]\\nv \\nseq!: s\\n[1.3,1.4,1.5]\\n[1.6,1.7,1.8]\\n[1.9,2.0,2.1]\\n[2.2,2.3,2.4]\\nv \\nThe mini-batch will look like this: \\nbatch\" = R[0.1,0.2,0.3]\\n[0.4,0.5,0.6]\\n[0.7,0.8,0.9]\\n[1.0,1.1,1.2]\\n[1.3,1.4,1.5]\\n[1.6,1.7,1.8]\\n[1.9,2.0,2.1]\\n[2.2,2.3,2.4]S \\nDuring each step of gradient descent, we: \\n1. Select a mini-batch from the training set, \\n2. Pass it through the neural network, \\n3. Compute the loss, \\n4. Calculate gradients, \\n5. Update model parameters, \\n6. Repeat from step 1. \\nMini-batch gradient descent often achieves faster convergence compared to \\nusing the entire training set per step. It efficiently handles large models and \\ndatasets by using modern hardware’s parallel processing capabilities. In \\nPyTorch, models require the first dimension of the input data to be the batch \\ndimension, even if there’s only one example in the batch. \\n3.3. Programming an RNN \\nLet’s implement an Elman RNN unit: \\nimport torch \\nimport torch.nn as nn \\n \\nclass ElmanRNNUnit(nn.Module): \\n    def __init__(self, emb_dim): \\n        super().__init__() \\n        self.Uh = nn.Parameter(torch.randn(emb_dim, emb_dim)) \\n➊'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 102}, page_content='102 \\n \\n        self.Wh = nn.Parameter(torch.randn(emb_dim, emb_dim)) \\n➋ \\n        self.b = nn.Parameter(torch.zeros(emb_dim)) ➌ \\n         \\n    def forward(self, x, h): \\n        return torch.tanh(x @ self.Wh + h @ self.Uh + self.b) \\n➍ \\nIn the constructor: \\n• Lines ➊ and ➋ initialize self.Uh and self.Wh, the weight matrices for \\nthe hidden state and input vector, with random values. \\n• Line ➌ sets self.b, the bias vector, to zero. \\nIn the forward method, line ➍ handles the computation for each time step. It \\nprocesses the current input x and the previous hidden state h, both shaped \\n(batch_size, emb_dim), combines them with the weight matrices and bias, \\nand applies the tanh activation. The output is the new hidden state, also of \\nshape (batch_size, emb_dim). \\nThe @ character is the matrix multiplication operator in PyTorch. We use x @ \\nself.Wh rather than self.Wh @ x because of the way PyTorch handles batch \\ndimensions in matrix multiplication. When working with batched inputs, x has \\na shape of (batch_size, emb_dim), while self.Wh has a shape of (emb_dim, \\nemb_dim). Remember from Section 1.6 that for two matrices to be multiplia-\\nble, the number of columns in the left matrix must be the same as the number \\nof rows in the right matrix. This is satisfied in x @ self.Wh. \\nNow, let’s define the class ElmanRNN, which implements a two-layer Elman \\nRNN using ElmanRNNUnit as its core building block: \\nclass ElmanRNN(nn.Module): \\n    def __init__(self, emb_dim, num_layers): \\n        super().__init__() \\n        self.emb_dim = emb_dim \\n        self.num_layers = num_layers \\n        self.rnn_units = nn.ModuleList( \\n            [ElmanRNNUnit(emb_dim) for _ in range(num_layers)\\n] \\n        ) ➊'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 103}, page_content='103 \\n    def forward(self, x): \\n        batch_size, seq_len, emb_dim = x.shape ➋ \\n        h_prev = [ \\n            torch.zeros(batch_size, emb_dim, device=x.device) \\n➌ \\n            for _ in range(self.num_layers) \\n        ] \\n        outputs = [] \\n        for t in range(seq_len): ➍ \\n            input_t = x[:, t] \\n            for l, rnn_unit in enumerate(self.rnn_units): \\n                h_new = rnn_unit(input_t, h_prev[l]) \\n                h_prev[l] = h_new    # Update hidden state \\n                input_t = h_new      # Input for next layer \\n            outputs.append(input_t)  # Collect outputs \\n        return torch.stack(outputs, dim=1) ➎ \\nIn line ➊ of the constructor, we initialize the RNN layers by creating a Mod-\\nuleList containing ElmanRNNUnit instances—one per layer. Using Mod-\\nuleList instead of a regular Python list ensures the parent module (Elman-\\nRNN) properly registers all RNN unit parameters. This guarantees that calling \\n.parameters() or .to(device) on the parent module includes parameters \\nfrom all modules in the ModuleList. \\nIn the forward method: \\n• Line ➋ extracts batch_size, seq_len, and emb_dim from the input \\ntensor x. \\n• Line ➌ initializes the hidden states h_prev for all layers with zero ten-\\nsors. Each hidden state in the list has the shape (batch_size, \\nemb_dim). \\nWe store hidden states for each layer in a list instead of a multidimen-\\nsional tensor because we need to modify them during processing. In-\\nplace modifications of tensors can disrupt PyTorch’s automatic differ-\\nentiation system, which might result in incorrect gradient calculations. \\n• Line ➍ iterates over time steps t in the input sequence. For each t:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 104}, page_content='104 \\n \\no Extract the input at time t: input_t = x[:, t]. \\no For each layer l: \\n§ Compute the new hidden state h_new from input_t and \\nh_prev[l]. \\n§ Update the hidden state: h_prev[l] = h_new (updates in \\nplace). \\n§ Set input_t = h_new to pass to the next layer. \\no Append the output of the last layer: outputs.append(input_t). \\n• Once all time steps are processed, line ➎ converts the outputs list into \\na tensor by stacking it along the time dimension. The resulting tensor \\nhas the shape (batch_size, seq_len, emb_dim). \\n3.4. RNN as a Language Model \\nAn RNN-based language model uses ElmanRNN as its building block: \\nclass RecurrentLanguageModel(nn.Module): \\n    def __init__(self, vocab_size, emb_dim, num_layers, pad_i\\ndx): \\n        super().__init__() \\n        self.embedding = nn.Embedding( \\n            vocab_size,  \\n            emb_dim,  \\n            padding_idx=pad_idx \\n        ) ➊ \\n        self.rnn = ElmanRNN(emb_dim, num_layers) \\n        self.fc = nn.Linear(emb_dim, vocab_size) \\n     \\n    def forward(self, x): \\n        embeddings = self.embedding(x) \\n        rnn_output = self.rnn(embeddings) \\n        logits = self.fc(rnn_output) \\n        return logits \\nThe RecurrentLanguageModel class integrates three components: an embed-\\nding layer, the ElmanRNN defined earlier, and a final linear layer. \\nIn the constructor, line ➊ defines the embedding layer. This layer transforms \\ninput token indices into dense vectors. The padding_idx parameter ensures'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 105}, page_content='105 \\nthat padding tokens are represented by zero vectors. (We’ll cover the embed-\\nding layer in the next section.) \\nNext, we initialize the custom ElmanRNN, specifying the embedding dimension-\\nality and the number of layers. Finally, we add a fully connected layer, which \\nconverts the RNN’s output into vocabulary-sized logits for each token in the \\nsequence. \\nIn the forward method: \\n• We pass the input x through the embedding layer. Input x has shape \\n(batch_size, seq_len), and the output embeddings have shape \\n(batch_size, seq_len, emb_dim). \\n• We then pass the embedded input through our ElmanRNN, obtaining \\nrnn_output with shape (batch_size, seq_len, emb_dim). \\n• Finally, we apply the fully connected layer to the RNN output, produc-\\ning logits for each token in the vocabulary at each position in the se-\\nquence. The output logits have shape (batch_size, seq_len, vo-\\ncab_size). \\n3.5. Embedding Layer \\nAn embedding layer, implemented as nn.Embedding in PyTorch, maps token \\nindices from a vocabulary to dense, fixed-size vectors. It acts as a learnable \\nlookup table, where each token is assigned a unique embedding vector. During \\ntraining, these vectors are adjusted to capture meaningful numerical represen-\\ntations of the tokens. \\nLet’s see how an embedding layer works. Imagine a vocabulary with five to-\\nkens, indexed from 0 to 4. We want each token to have a 3D embedding vector. \\nTo begin, we create an embedding layer: \\nimport torch \\nimport torch.nn as nn \\n \\nvocab_size = 5  # Number of unique tokens \\nemb_dim = 3     # Size of each embedding vector \\nemb_layer = nn.Embedding(vocab_size, emb_dim)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 106}, page_content='106 \\n \\nThe embedding layer initializes the embedding matrix 𝐄 with random values. \\nIn this case, the matrix has 5 rows (one for each token) and 3 columns (the \\nembedding dimensionality): \\n𝐄=\\n⎣\\n⎢\\n⎢\\n⎢\\n⎡0.2\\n−0.4\\n0.1\\n−0.3\\n0.8\\n−0.5\\n0.7\\n0.1\\n−0.2\\n−0.6\\n0.5\\n0.4\\n0.9\\n−0.7\\n0.3⎦\\n⎥\\n⎥\\n⎥\\n⎤\\n \\nEach row in 𝐄 represents the embedding vector for a specific token in the vo-\\ncabulary. \\nNow, let’s input a sequence of token indices: \\ntoken_indices = torch.tensor([0, 2, 4]) \\nThe embedding layer retrieves the rows of 𝐄 corresponding to the input indi-\\nces: \\nEmbeddings = ¸\\n0.2\\n−0.4\\n0.1\\n0.7\\n0.1\\n−0.2\\n0.9\\n−0.7\\n0.3\\n¹ \\nThis output is a matrix whose number of rows equals the input sequence length \\nand whose number of columns equals the embedding dimensionality: \\nembeddings = embedding_layer(token_indices) \\nprint(embeddings) \\nThe output might look like this: \\ntensor([[ 0.2, -0.4,  0.1], \\n        [ 0.7,  0.1, -0.2], \\n        [ 0.9, -0.7,  0.3]]) \\nThe embedding layer can manage padding tokens as well. Padding ensures \\nsequences in a mini-batch have the same length. To prevent the model from \\nupdating embeddings for padding tokens during training, the layer maps them \\nto a zero vector that remains unchanged. For example, we can define the pad-\\nding index as follows: \\nemb_layer = nn.Embedding(vocab_size, emb_dim, padding_idx=0) \\nWith this configuration, the embedding for token 0 (padding token) is always \\n[0,0,0]1.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 107}, page_content='107 \\nGiven the input: \\ntoken_indices = torch.tensor([0, 2, 4]) \\nembeddings = emb_layer(token_indices) \\nprint(embeddings) \\nThe result would be: \\ntensor([[ 0.0,  0.0,  0.0],  # Padding token \\n        [ 0.7,  0.1, -0.2],  # Token 2 embedding \\n        [ 0.9, -0.7,  0.3]]) # Token 4 embedding \\nWith modern language models, vocabularies often include hundreds of thou-\\nsands of tokens, and embedding dimensions are typically several thousands. \\nThis makes the embedding matrix a significant part of the model, sometimes \\ncontaining up to 2 billion parameters. \\n3.6. Training an RNN Language Model \\nStart by importing libraries and defining utility functions: \\nimport torch, torch.nn as nn \\n \\ndef set_seed(seed): \\n    random.seed(seed) \\n    torch.manual_seed(seed) \\n    torch.cuda.manual_seed_all(seed) ➊ \\n    torch.backends.cudnn.deterministic = True ➋ \\n    torch.backends.cudnn.benchmark = False ➌ \\nThe set_seed function enforces reproducibility by setting the Python random \\nseed, the PyTorch CPU seed, and, in line ➊, the CUDA seed for all GPUs \\n(Graphics Processing Units). CUDA is NVIDIA’s parallel computing platform \\nand API that enables significant performance improvements in computing by \\nleveraging the power of GPUs. Using torch.cuda.manual_seed_all ensures \\nconsistent GPU-based random behavior, while lines ➋ and ➌ disable CUDA’s \\nauto-tuner and enforce deterministic algorithms, guaranteeing identical re-\\nsults across different GPU models. \\nWith the model class ready, we’ll train our neural language model. First, we \\ninstall the transformers package—an open-source library providing APIs and'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 108}, page_content='108 \\n \\ntools to easily download, train and use pretrained models from the Hugging \\nFace Hub: \\n$ pip3 install transformers \\nThe package offers a Python API for training that works with both PyTorch \\nand TensorFlow. For now, we only need it to get a tokenizer. \\nNow we import transformers, set the tokenizer, define the hyperparameter \\nvalues, prepare the data, and instantiate the model, loss function, and opti-\\nmizer objects: \\nfrom transformers import AutoTokenizer \\n \\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() els\\ne \"cpu\") ➊ \\ntokenizer = AutoTokenizer.from_pretrained( \\n    \"microsoft/Phi-3.5-mini-instruct\" \\n) ➋ \\nvocab_size = len(tokenizer) ➌ \\n \\nemb_dim, num_layers, batch_size, learning_rate, num_epochs = \\nget_hyperparameters() \\n \\ndata_url = \"https://www.thelmbook.com/data/news\" \\ntrain_loader, test_loader = download_and_prepare_data( \\n    data_url, batch_size, tokenizer) ➍ \\n \\nmodel = RecurrentLanguageModel( \\n    vocab_size, emb_dim, num_layers, tokenizer.pad_token_id \\n) \\ninitialize_weights(model) ➎ \\nmodel.to(device) \\n \\ncriterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_to\\nken_id) ➏ \\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning\\n_rate) \\nLine ➊ detects a CUDA device if it’s available. Otherwise, it defaults to CPU.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 109}, page_content=\"109 \\nCUDA is not the only GPU acceleration framework available for train-\\ning neural networks—PyTorch also provides native support for check-\\ning availability of MPS (Apple Metal) through its is_available() \\nmethod. In this book, though, we will use CUDA as it remains the most \\nwidely used platform for machine learning acceleration. \\nMost models on the Hugging Face Hub include the tokenizer that was used to \\ntrain them. Line ➋ initializes the Phi 3.5 mini tokenizer. It was trained on a \\nlarge text corpus using the byte-pair encoding algorithm and has a vocabulary \\nsize of 32,064. \\nLine ➌ retrieves the tokenizer’s vocabulary size. Line ➍ downloads and pre-\\npares the dataset—a collection of news sentences from online articles—to-\\nkenizing them and creating DataLoader objects. We'll explore DataLoader \\nshortly. For now, think of them as iterators over batches. \\nLine ➎ initializes the model parameters. Initial parameter values can greatly \\ninfluence the training process. They can affect how quickly training progresses \\nand the final loss value. Certain initialization techniques, like Xavier initiali-\\nzation, have shown good results in practice. The initialize_weights func-\\ntion, implementing this method, is defined in the notebook. \\nLine ➏ creates the loss function with the ignore_index parameter. This en-\\nsures the loss is not calculated for padding tokens. \\nNow, let’s look at the training loop: \\nfor epoch in range(num_epochs): ➊ \\n    model.train() ➋ \\n    for batch in train_loader: ➌ \\n        input_seq, target_seq = batch \\n        input_seq = input_seq.to(device) ➍ \\n        target_seq = target_seq.to(device) ➎ \\n        batch_size_current, seq_len = input_seq.shape ➏ \\n        optimizer.zero_grad() \\n        output = model(input_seq) \\n        output = output.reshape(batch_size_current * seq_len, \\nvocab_size) ➐\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 110}, page_content='110 \\n \\n        target = target_seq.reshape(batch_size_current * seq_\\nlen) ➑ \\n        loss = criterion(output, target) ➒ \\n        loss.backward() \\n        optimizer.step() \\nLine ➊ iterates over epochs. An epoch is a single pass through the entire da-\\ntaset. Training for multiple epochs can improve the model, especially with lim-\\nited training data. The number of epochs is a hyperparameter that you adjust \\nbased on the model’s performance on the test set. \\nLine ➋ calls model.train() at the start of each epoch to set the model in \\ntraining mode. This is important for models that have layers behaving differ-\\nently during training vs. evaluation. \\nAlthough our RNN model doesn’t use such layers, calling \\nmodel.train() ensures the model is properly configured for training. \\nThis avoids unexpected behavior and keeps consistency, especially if \\nfuture changes add layers dependent on the mode. \\nLine ➌ iterates over batches. Each batch is a tuple: one tensor contains input \\nsequences, and the other contains target sequences. Lines ➍ and ➎ move these \\ntensors to the same device as the model. If the model and data are on different \\ndevices, PyTorch raises an error. \\nLine ➏ retrieves the batch size and sequence length from input_seq (tar-\\nget_seq has the same shape). These dimensions are needed to reshape the \\nmodel’s output tensor (batch_size_current, seq_len, vocab_size) and \\ntarget tensor (batch_size_current, seq_len) into compatible shapes for the \\ncross-entropy loss function. In line ➐, the output is reshaped to \\n(batch_size_current * seq_len, vocab_size), and in line ➑, the target \\nis flattened to batch_size_current * seq_len, allowing the loss calculation \\nin line ➒ to process all tokens in the batch simultaneously and return the av-\\nerage loss per token. \\nThis concludes the training loop implementation. The full RNN language \\nmodel training implementation is in the thelmbook.com/nb/3.1 notebook. \\nNow, let’s examine the DataLoader and Dataset classes that make this batch \\nprocessing possible.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 111}, page_content='111 \\n3.7. Dataset and DataLoader \\nAs mentioned earlier, the download_and_prepare_data function returns two \\nloader objects: train_loader and test_loader. I asked you to think of them \\nas iterators over batches of data. But what are they, exactly? \\nThese classes were designed to manage data efficiently during training. While \\nthis book doesn’t focus on data loading and manipulation, a brief explanation \\nis important for clarity. \\nThe Dataset class serves as an interface to your actual data source. By imple-\\nmenting its __len__ method, you can get the size of the dataset. By defining \\n__getitem__, you can access individual examples. These examples can come \\nfrom many “physical” sources: files, databases, or even data generated on the \\nfly. \\nLet’s look at an example. Assume we have a JSONL file called data.jsonl, \\nwhere each line is a JSON object containing two input features and a label. \\nHere’s how a couple of lines might look: \\n{\"feature1\": 1.0, \"feature2\": 2.0, \"label\": 3.0} \\n{\"feature1\": 4.0, \"feature2\": 5.0, \"label\": 9.0} \\n... \\nHere’s how you can create a custom Dataset to read this file: \\nimport json \\nimport torch \\nfrom torch.utils.data import Dataset \\n \\nclass JSONDataset(Dataset): \\n    def __init__(self, file_path): \\n        self.data = [] \\n        with open(file_path, \\'r\\') as f: \\n            for line in f: \\n                item = json.loads(line) \\n                features = [item[\\'feature1\\'], item[\\'feature2\\'\\n]] \\n                label = item[\\'label\\'] \\n                self.data.append((features, label)) \\n \\n    def __len__(self):'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 112}, page_content='112 \\n \\n        return len(self.data) \\n \\n    def __getitem__(self, idx): \\n        features, label = self.data[idx] \\n        features = torch.tensor(features, dtype=torch.float32\\n) \\n        label = torch.tensor(label, dtype=torch.long) \\n        return features, label \\nIn this example: \\n• __init__ reads the file and stores the data in memory, \\n• __len__ returns the total number of examples, \\n• __getitem__ retrieves a single example and converts it to tensors. \\nWe can access individual examples like this: \\ndataset = JSONDataset(\\'data.jsonl\\') \\nfeatures, label = dataset[0] \\nA DataLoader is used with a Dataset to manage tasks like batching, shuffling, \\nand loading data in parallel. For example: \\nfrom torch.utils.data import DataLoader \\n \\ndataset = JSONLDataset(\\'data.jsonl\\') ➊ \\n \\ndata_loader = DataLoader( \\n    dataset, \\n    batch_size=32, # Number of examples per batch \\n    shuffle=True,  # Shuffle data at every epoch \\n    num_workers=0  # Number of subprocesses for data loading \\n) ➋ \\n \\nnum_epochs = 5 \\nfor epoch in range(num_epochs): \\n    for batch_features, batch_labels in data_loader: ➌ \\n        print(f\"Batch features shape: {batch_features.shape}\") \\n        print(f\"Batch labels shape: {batch_labels.shape}\") \\n        # Feed batch_features and batch_labels into your mode\\nl'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 113}, page_content='113 \\nLine ➊ creates a Dataset instance. Line ➋ then wraps the dataset in a Data-\\nLoader. Finally, line ➌ iterates over the DataLoader for five epochs. With \\nshuffle=True, the data is shuffled before batching in each epoch. This pre-\\nvents the model from learning the order of the training data. \\nWith num_workers=0, data loading happens in the main process. This simple \\nsetup may not be the most efficient, especially for large datasets. Using a pos-\\nitive value for num_workers makes PyTorch spawn that many worker pro-\\ncesses, enabling parallel data loading. This can significantly speed up training \\nby preventing data loading from becoming a bottleneck. \\nOutput: \\nBatch features shape: torch.Size([32, 2]) \\nBatch labels shape: torch.Size([32]) \\nBy using a well-designed Dataset with a DataLoader, you can scale your \\ntraining pipeline to handle large datasets, optimize data loading with parallel \\nworkers, and experiment with different batching strategies. This approach \\nstreamlines the training process, letting you concentrate on model design and \\noptimization. \\n3.8. Training Data and Loss Computation \\nWhen studying neural language models, a key aspect is understanding the \\nstructure of a training example. The text corpus is split into overlapping input \\nand target sequences. Each input sequence aligns with a target sequence \\nshifted by one token. This setup trains the model to predict the next word at \\neach position in the sequence. \\nFor instance, take the sentence “We train a recurrent neural network as a lan-\\nguage model.” After tokenizing it with the Phi 3.5 mini tokenizer, we get: \\n[\"_We\", \"_train\", \"_a\", \"_rec\", \"urrent\", \"_neural\", \"_networ\\nk\", \"_as\", \"_a\", \"_language\", \"_model\", \".\"] \\nTo create one training example, we convert the sentence into input and target \\nsequences by shifting tokens forward by one position: \\nInput: [\"_We\", \"_train\", \"_a\", \"_rec\", \"urrent\", \"_neural\", \"\\n_network\", \"_as\", \"_a\", \"_language\", \"_model\"]'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 114}, page_content='114 \\n \\nTarget: [\"_train\", \"_a\", \"_rec\", \"urrent\", \"_neural\", \"_netwo\\nrk\", \"_as\", \"_a\", \"_language\", \"_model\", \".\"] \\nA training example doesn\\'t need to be a complete sentence. Modern language \\nmodels process sequences up to their context window length—the maximum \\ntokens (like 8192) they can handle at once. The window limits how far apart \\nthe model can connect relationships in text. Training splits text into window-\\nsized chunks, each target sequence shifted one token forward from its input. \\nDuring training, the RNN processes one token at a time, updating its hidden \\nstates layer by layer. At each step, it generates logits aimed at predicting the \\nnext token in the sequence. Each logit corresponds to a vocabulary token and \\nis converted into probabilities using softmax. These probabilities are then used \\nto compute the loss. \\nEach training example results in multiple predictions and losses. For example, \\nthe model first processes “_We” and tries to predict “_train” by assigning prob-\\nabilities to all vocabulary tokens. The loss is computed using the probability of \\n“_train,” as defined in Equation 2.1. Next, the model processes “_train” to pre-\\ndict “_a,” generating another loss. This continues for every token in the input \\nsequence. For the above example, the model makes 11 predictions and calcu-\\nlates 11 losses. \\nThe losses are averaged across the tokens in a training example and all exam-\\nples in the batch. The average loss expression is then used in backpropagation \\nto update the model’s parameters. \\nLet’s break down the loss calculation for each position with some made-up \\nnumbers: \\n• Position 1: \\no Target token: “_train” \\no Logit for “_train”: −0.5 \\no After applying softmax to the logits, suppose the probability of \\n“_train” is 0.1 \\no Contribution to the total loss by Equation 2.1 is −log(0.1) = 2.30 \\n• Position 2: \\no Target token: “_a” \\no Logit for “_a”: 3.2 \\no After softmax, the probability for “_a”: 0.05'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 115}, page_content=\"115 \\no Contribution to loss: −log(0.05) = 2.99 \\n• Position 3: \\no The probability for “_rec”: 0.02 \\no Contribution to loss: −log(0.02) = 3.91 \\n• Position 4: \\no The probability for “urrent”: 0.34 \\no Contribution to loss: −log(0.34) = 1.08 \\nWe continue until calculating the loss contribution for the final token, the pe-\\nriod: \\n• Position 11: \\no Target token: “.” \\no Logit for “.”: −1.2 \\no After softmax, the probability for “.”: 0.11 \\no Contribution to loss: −log(0.11) = 2.21 \\nThe final loss is calculated by taking the average of these values: \\n(2.30 + 2.99 + 3.91 + 1.08 + ⋯+ 2.21)\\n11\\n= 2.11 (hypothetically) \\nDuring training, the objective is to minimize this loss. This involves improving \\nthe model so that it assigns higher probabilities to the correct target tokens at \\neach position. \\nThe full code for training the RNN-based language model can be found in \\nthelmbook.com/nb/3.1. I used the following hyperparameter values: emb_dim \\n= 128, num_layers = 2, batch_size = 128, learning_rate = 0.001, \\nand num_epochs = 1. \\nHere are three continuations for the prompt “The President” generated at later \\ntraining steps: \\nThe President refused to comment on the best news in the five \\non BBC . \\nThe President has been a `` very serious '' and `` unacceptab\\nle '' . \\nThe President 's office is not the first time to be able to t\\nake the lead .\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 116}, page_content=\"116 \\n \\nWhen Elman introduced RNNs in 1990, his experiments used se-\\nquences averaging 3.92 words, limited by the hardware of the time. By \\n2014, advances in computing and improved activation functions made \\nit possible to train RNNs on sequences hundreds of words long, turning \\nthem from an academic idea into a practical tool. \\nAt training start, our model produced nearly random tokens, but gradually im-\\nproved, reaching a perplexity of 72.41—better than the count-based model’s \\n299.06 but far behind GPT-2’s 20 and modern LLMs’ sub-5 scores. \\nThree key factors explain this performance gap: \\n1. The model is small, with just 8,292,619 parameters, mostly in the em-\\nbedding layer. \\n2. The context size we used was relatively short—30 tokens. \\n3. The Elman RNN’s hidden state gradually “forgets” information from \\nearlier tokens. \\nLong short-term memory (LSTM) networks improved upon RNNs but still \\nstruggled with very long sequences. Transformers later superseded both archi-\\ntectures, becoming dominant in natural language processing by 2023 through \\nbetter handling of long contexts and improved parallel computation enabling \\nlarger models. \\nInterest in RNNs was reignited in 2024 with the invention of the \\nminLSTM and xLSTM architectures, which achieve performance com-\\nparable to Transformer-based models. This resurgence reflects a \\nbroader trend in AI research: no model type is ever permanently obso-\\nlete. Researchers often revisit and refine older ideas, adapting them to \\naddress modern challenges and leverage current hardware capabili-\\nties. \\nWith this, we've completed our study of recurrent neural networks and their \\napplications in language modeling. In the remainder of the book, we'll examine \\ntransformer neural networks and language modeling based on them. We'll in-\\nvestigate their approach to tasks like question answering, document classifica-\\ntion, and other practical applications.\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 117}, page_content='117 \\nChapter 4. Transformer \\nTransformer models have greatly advanced NLP. They overcome RNNs’ limi-\\ntations in managing long-range dependencies and enable parallel processing \\nof input sequences. There are three main Transformer architectures: encoder-\\ndecoder, initially formulated for machine translation; encoder-only, typically \\nused for classification; and decoder-only, commonly found in chat LMs. \\nIn this chapter, we’ll explore the decoder-only Transformer architecture in de-\\ntail, as it is the most widely used approach for training autoregressive lan-\\nguage models. \\nThe transformer architecture introduces two key innovations: self-attention \\nand positional encoding. Self-attention enables the model to assess how each \\nword relates to all others during prediction, while positional encoding captures \\nword order and sequential patterns. Unlike RNNs, transformers process all to-\\nkens simultaneously, using positional encod-\\ning to maintain sequential context despite \\nparallel processing of each token. This chap-\\nter explores these fundamental elements in \\ndetail. \\nA decoder-only Transformer (referred to \\nsimply as “decoder” from here on)  is made up \\nof multiple identical5 layers, known as de-\\ncoder blocks, stacked vertically as shown on \\nthe right. \\nAs you can see, training a decoder involves \\npairing each input sequence with a target se-\\nquence that is shifted forward by one token —\\nthe same method used for RNN-based lan-\\nguage models. \\n4.1. Decoder Block \\nEach decoder block has two sub-layers: self-attention and a position-wise mul-\\ntilayer perceptron (MLP) as shown below: \\n \\n5 Decoder blocks share the same architecture but have distinct trainable parameters \\nunique to each block.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 118}, page_content='118 \\n \\n \\nThe illustration simplifies certain aspects to avoid introducing too many new \\nconcepts at once. We’ll introduce the missing details step by step. \\nLet’s take a closer look at what happens in a decoder block, starting with the \\nfirst one: \\n \\nThe first decoder block processes input token embeddings. For this example, \\nwe use 6-dimensional input and output embeddings, though in practice these \\ndimensions grow larger with parameter count and token vocabulary. The self-\\nattention layer, transforms each input embedding vector 𝐱E into a new vector \\n𝐠E for every token 𝑡, from 1 to 𝐿, where 𝐿 represents the input length. \\nHere, we simplified each unit as a square, following the same approach \\nwe used for the four-unit network in Section 1.5. While our earlier \\nchapters showed information in a neural network flowing from left to \\nright, we’ve now shifted to a bottom-to-top orientation—the standard'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 119}, page_content='119 \\nconvention for high-level language model diagrams in the literature. \\nWe’ll maintain this vertical orientation from now on. \\nAfter self-attention, the position-wise MLP independently processes each vec-\\ntor 𝐠E one at a time. Each decoder block has its own MLP with unique param-\\neters, and within a block, this same MLP is applied independently to each po-\\nsition’s vector, taking one 𝐠E as input and producing one 𝐳E as output. When \\nthe MLP finishes processing each position sequentially, the number of output \\nvectors 𝐳E equals the number of input tokens 𝐱E. \\nThe output vectors 𝐳E then serve as inputs to the next decoder block. This pro-\\ncess repeats through each decoder block, preserving a number of output vec-\\ntors equal to the number of input tokens 𝐱E. \\n4.2. Self-Attention \\nTo see how self-attention works, let’s start with an intuitive comparison. \\nTransforming 𝐠E into 𝐳E is straightforward: a position-wise MLP takes an input \\nvector and outputs a new vector by applying a learned transformation. This is \\nwhat feedforward networks are designed to do. However, self-attention can \\nseem more complex. \\nConsider a 5-token example: [“we,” “train,” “a,” “transformer,” “model”], and \\nassume a decoder with a maximum input sequence length of 4. \\nIn each decoder block, the self-attention function relies on three tensors of \\ntrainable parameters: 𝐖X, 𝐖Y, and 𝐖Z. Here, 𝑄 stands for “query,” 𝐾 for \\n“key,” and 𝑉 for “value.” \\nLet’s assume these tensors are 6 × 6. This means each of the four 6-dimensional \\ninput vectors will be transformed into four 6-dimensional output vectors. Let’s \\nuse the second token, 𝐱!, representing the word “train,” as our illustrative ex-\\nample. To compute the output 𝐠! for 𝐱!, the self-attention layer works in six \\nsteps. \\n4.2.1. Step 1 of Self-Attention \\nCompute matrices 𝐐, 𝐊, and 𝐕 as shown below:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 120}, page_content='120 \\n \\n \\nFigure 4.1: Matrix multiplication in the self-attention layer. \\nIn the illustration, we combined the four input embeddings 𝐱\", 𝐱!, 𝐱&, and 𝐱T \\ninto a matrix 𝐗. Then, we multiplied 𝐗 by the weight matrices 𝐖X, 𝐖Y, and \\n𝐖Z to create matrices 𝐐, 𝐊, and 𝐕. These matrices hold 6-dimensional query, \\nkey, and value vectors, respectively. Since the process generates the same num-\\nber of query, key, and value vectors as input embeddings, each input embed-\\nding 𝐱E corresponds to a query vector 𝐪E, a key vector 𝐤E, and a value vector \\n𝐯E. \\n4.2.2. Step 2 of Self-Attention \\nTaking the second token 𝐱! as our example, we compute attention scores by \\ntaking the dot product of its query vector 𝐪! with each key vector 𝐤E. Let’s \\nassume the resulting scores are: \\n𝐪! ⋅𝐤\" = 4.90,\\u2001𝐪! ⋅𝐤! = 17.15,\\u2001𝐪! ⋅𝐤& = 9.80,\\u2001𝐪! ⋅𝐤T = 12.25 \\nIn vector format: \\n𝐬𝐜𝐨𝐫𝐞𝐬! = [4.90,17.15,9.80,12.25]1'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 121}, page_content=\"121 \\n4.2.3. Step 3 of Self-Attention \\nTo obtain the scaled scores, we divide each attention score by the square root \\nof the key vector's dimensionality. In our example, since the key vector has a \\ndimensionality of 6, we divide all scores by √6 ≈2.45, yielding: \\n𝐬𝐜𝐚𝐥𝐞𝐝_𝐬𝐜𝐨𝐫𝐞𝐬! = R 4.9\\n2.45 , 17.15\\n2.45 , 9.8\\n2.45 , 12.25\\n2.45 S\\n1\\n= [2,7,4,5]1 \\n4.2.4. Step 4 of Self-Attention \\nWe then apply the causal mask to the scaled scores. (If the reason for using \\nthe causal mask isn’t clear yet, it will be explained in detail soon.) For the \\nsecond input position, the causal mask is: \\n𝐜𝐚𝐮𝐬𝐚𝐥_𝐦𝐚𝐬𝐤! =\\ndef [0,0, −∞, −∞]1 \\nWe add the scaled scores to the causal mask, resulting in the masked scores: \\n𝐦𝐚𝐬𝐤𝐞𝐝_𝐬𝐜𝐨𝐫𝐞𝐬! = 𝐬𝐜𝐚𝐥𝐞𝐝_𝐬𝐜𝐨𝐫𝐞𝐬! + 𝐜𝐚𝐮𝐬𝐚𝐥_𝐦𝐚𝐬𝐤! = [2,7, −∞, −∞]1 \\n4.2.5. Step 5 of Self-Attention \\nWe apply the softmax function to the masked scores to produce the attention \\nweights: \\n𝐚𝐭𝐭𝐞𝐧𝐭𝐢𝐨𝐧_𝐰𝐞𝐢𝐠𝐡𝐭𝐬! = softmax([2,7, −∞, −∞]1) \\nSince scores of −∞ become zero after applying the exponential function, the \\nattention weights for the third and fourth positions will be zero. The remaining \\ntwo weights are calculated as: \\n𝐚𝐭𝐭𝐞𝐧𝐭𝐢𝐨𝐧_𝐰𝐞𝐢𝐠𝐡𝐭𝐬!\\n= Ð\\n𝑒!\\n𝑒! + 𝑒U ,\\n𝑒U\\n𝑒! + 𝑒U , 0,0Ñ\\n1\\n \\n≈[0.0067,0.9933,0,0]1 \\nDividing attention scores by the square root of the key dimensionality \\nhelps prevent the dot products from growing too large in magnitude \\nas the dimensionality increases, which could lead to extremely small \\ngradients after applying softmax (due to very large negative or posi-\\ntive values pushing the softmax outputs to 0 or 1).\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 122}, page_content='122 \\n \\n4.2.6. Step 6 of Self-Attention \\nWe compute the output vector 𝐠! for the input embedding 𝐱! by taking a \\nweighted sum of the value vectors 𝐯\", 𝐯!, 𝐯&, and 𝐯T using the attention weights \\nfrom the previous step: \\n𝐠! ≈0.0067 ⋅𝐯\" + 0.9933 ⋅𝐯! + 0 ⋅𝐯& + 0 ⋅𝐯T \\nAs you can see, the decoder’s output for position 2 depends only on (or, we \\ncan say “attends only to”) the inputs at positions 1 and 2, with position 2 hav-\\ning a much stronger influence. This effect comes from the causal mask, which \\nrestricts the model from attending to future positions when generating an out-\\nput for a given position. This property is essential for maintaining the auto-\\nregressive nature of language models, ensuring that predictions for each po-\\nsition rely solely on previous and current inputs, not future ones. \\nWhile this token primarily attends to itself in our example, attention \\npatterns vary across different contexts. A token may attend strongly to \\nother tokens providing relevant semantic or syntactic information, de-\\npending on sentence structure. \\nThe vectors 𝐪E, 𝐤E, and 𝐯E can be interpreted as follows: each input position \\n(token or embedding) seeks information about other positions. For example, a \\ntoken like “I” might look for a name in another position, allowing the model \\nto process “I” and the name in a similar way. To enable this, each position 𝑡 is \\nassigned a query 𝐪E. \\nThe self-attention mechanism calculates a dot product between 𝐪E and every \\nkey 𝐤[ across all positions 𝑝. A larger dot-product indicates greater similarity \\nbetween the vectors. If position 𝑝’s key 𝐤[ aligns closely with position 𝑡’s query \\n𝐪E, then position 𝑝’s value 𝐯[ contributes more significantly to the final result. \\nThe concept of attention emerged before the Transformer. In 2014, \\nDzmitry Bahdanau, while studying under Yoshua Bengio, addressed a \\nfundamental challenge in machine translation: enabling an RNN to fo-\\ncus on the most relevant parts of a sentence. Drawing from his own \\nexperience learning English—where he moved his focus between dif-\\nferent parts of the text—Bahdanau developed a mechanism for the \\nRNN to “decide” which input words were most important at each'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 123}, page_content='123 \\ntranslation step. This mechanism, which Bengio then termed attention, \\nbecame a cornerstone of modern neural networks. \\nThe process used to calculate 𝐠! is repeated for each position in the input se-\\nquence, resulting in a set of output vectors: 𝐠\", 𝐠!, 𝐠&, and 𝐠T. Each position \\nhas its own causal mask, so when calculating 𝐠\", 𝐠&, and 𝐠T, a different causal \\nmask is applied for each position. The full causal mask for all positions is shown \\nbelow: \\n𝐌=\\ndef s\\n0\\n−∞\\n−∞\\n−∞\\n0\\n0\\n−∞\\n−∞\\n0\\n0\\n0\\n−∞\\n0\\n0\\n0\\n0\\nv \\nAs you can see, the first token attends only to itself, the second to itself and \\nthe first, the third to itself and the first two, and the last to itself and all pre-\\nceding tokens. \\nThe general formula for computing attention for all positions is: \\n𝐆= attention(𝐐, 𝐊, 𝐕) =\\ndef softmax †𝐐𝐊1\\nÔ𝑑:\\n+ 𝐌‡ 𝐕 \\nHere, 𝐐 and 𝐕 are 𝐿× 𝑑: query and value matrices. 𝐊1 is the 𝑑: × 𝐿 transposed \\nkey matrix. 𝑑: is the dimensionality of the key, query, and value vectors, and \\n𝐿 is the sequence length. \\nWhile we computed the attention scores explicitly for 𝐱! earlier, the matrix \\nmultiplication 𝐐𝐊1 calculates the scores for all positions at once. This method \\nmakes the process much faster. \\nThis completes the definition of self-attention. \\n4.3. Position-Wise Multilayer Perceptron \\nAfter the masked self-attention layer, each output vector 𝐠E is individually pro-\\ncessed by a multilayer perceptron (MLP). The MLP applies a sequence of ad-\\nditional transformations: \\n𝐳E = 𝐖!CReLU(𝐖\"𝐠E + 𝐛\")D + 𝐛! \\nHere, 𝐖\", 𝐖!, 𝐛\", and 𝐛! are learned parameters. The resulting vector 𝐳E is \\nthen either passed to the next decoder block or, if it’s the final decoder block, \\nused to generate the output vector.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 124}, page_content='124 \\n \\nThis component is a position-wise multilayer perceptron, which is why \\nI use that term. The literature may refer to it as a feedforward network, \\ndense layer, or fully connected layer, but these names can be mislead-\\ning. The entire Transformer is a feedforward neural network. Addition-\\nally, dense or fully connected layers typically incorporate one weight \\nmatrix, one bias vector, and an output non-linearity. The position-wise \\nMLP in a Transformer, however, utilizes two weight matrices, two bias \\nvectors, and omits an output non-linearity. \\n4.4. Rotary Position Embedding \\nThe Transformer architecture, as described so far, does not inherently account \\nfor word order. The causal mask ensures that each token cannot attend to \\ntokens on its right, but rearranging tokens on the left does not affect the atten-\\ntion weights of a given token. This is unlike RNNs, where hidden states are \\ncomputed sequentially, each depending on the previous one. Changing word \\norder in RNNs alters the hidden states and, consequently, the output. In con-\\ntrast, Transformers calculate attention across all tokens at once, without se-\\nquential dependency. \\nTo handle word order, Transformers need to incorporate positional infor-\\nmation. A widely used method for this is rotary position embedding (RoPE), \\nwhich applies position-dependent rotations to the query and key vectors in the \\nattention mechanism. One key benefit of RoPE is its ability to generalize effec-\\ntively to sequences longer than those seen during training. This allows models \\nto be trained on shorter sequences—saving time and computational re-\\nsources—while still supporting much longer contexts at inference. \\nRoPE encodes positional information by rotating the query and key vectors. \\nThis rotation occurs before the attention computation. The illustration on the \\nnext page shows how it works in 2D. The black arrow labeled “Original” shows \\na position-less key or query vector in self-attention. RoPE embeds positional \\ninformation by rotating this vector according to the token’s position.6 The col-\\nored arrows show the resulting rotated vectors for positions 1, 3, 5, and 7. \\n \\n6 In practice, RoPE operates by rotating pairs of adjacent dimensions within query and key \\nvectors, rather than rotating the entire vectors themselves, as we will explore shortly.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 125}, page_content='125 \\n \\nA key property of RoPE is that the angle between any two rotated vectors en-\\ncodes the distance between their positions in the sequence. For example, the \\nangle between positions 1 and 3 is the same as the angle between positions 5 \\nand 7, since both pairs are two positions apart. \\nSo, how do we rotate vectors? We use matrix multiplication! Rotation matri-\\nces are widely used in fields like computer graphics to rotate 3D scenes—one \\nof the original purposes of GPUs (the “G” in GPU stands for graphical) before \\nthey were applied to neural network training. \\nIn two dimensions, the rotation matrix for an angle 𝜃 is: \\n𝐑\\\\ = Rcos(𝜃)\\n−sin(𝜃)\\nsin(𝜃)\\ncos(𝜃)S \\nLet’s rotate the two-dimensional vector 𝐪= [2,1]1. To do this, we multiply 𝐪 \\nby the rotation matrix 𝐑\\\\. The result is a new vector, representing 𝐪 rotated \\ncounterclockwise by an angle 𝜃. \\nFor a 45∘ rotation (𝜃= 𝜋/4 radians), we can use the special values cos(𝜃) =\\nsin(𝜃) = √!\\n! . This gives us the rotation matrix:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 126}, page_content='126 \\n \\n𝐑T@∘=\\n⎣\\n⎢\\n⎢\\n⎢\\n⎡√2\\n2\\n−√2\\n2\\n√2\\n2\\n√2\\n2 ⎦\\n⎥\\n⎥\\n⎥\\n⎤\\n \\nTo find the rotated vector, we multiply 𝐑T@∘ by 𝐪: \\n𝐪rotated = 𝐑T@∘⋅𝐪=\\n⎣\\n⎢\\n⎢\\n⎢\\n⎡√2\\n2\\n−√2\\n2\\n√2\\n2\\n√2\\n2 ⎦\\n⎥\\n⎥\\n⎥\\n⎤\\nƒ2\\n1„ \\nComputing this multiplication step by step: \\n𝐪rotated =\\n⎣\\n⎢\\n⎢\\n⎢\\n⎡√2\\n2 ⋅2 −√2\\n2 ⋅1\\n√2\\n2 ⋅2 + √2\\n2 ⋅1⎦\\n⎥\\n⎥\\n⎥\\n⎤\\n=\\n⎣\\n⎢\\n⎢\\n⎢\\n⎡√2\\n2 (2 −1)\\n√2\\n2 (2 + 1)⎦\\n⎥\\n⎥\\n⎥\\n⎤\\n=\\n⎣\\n⎢\\n⎢\\n⎢\\n⎡√2\\n2 ⋅1\\n√2\\n2 ⋅3⎦\\n⎥\\n⎥\\n⎥\\n⎤\\n=\\n⎣\\n⎢\\n⎢\\n⎢\\n⎡√2\\n2\\n3√2\\n2 ⎦\\n⎥\\n⎥\\n⎥\\n⎤\\n \\nThe figure below illustrates 𝐪 and its rotated version for 𝜃= 45∘: \\n \\nFor a position 𝑡, RoPE rotates each pair of dimensions in the query and key \\nvectors defined as: \\n𝐪E\\n= ƒ𝑞E\\n(\"), 𝑞E\\n(!), … , 𝑞E\\nO_.6\"R, 𝑞E\\nO_.R„\\n1\\n𝐤E\\n= ƒ𝑘E\\n(\"), 𝑘E\\n(!), … , 𝑘E\\n(_(6\"), 𝑘E\\n(_()„\\n1'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 127}, page_content='127 \\nHere, 𝑑` and 𝑑: are the (even) dimensionality of the query and key vectors. \\nRoPE rotates pairs of dimensions indexed as (2𝑝−1, \\u20042𝑝), where each pair’s \\nindex 𝑝 ranges from 1 to 𝑑`/2. \\nTo split the dimensions of 𝐪E into 𝑑`/2 pairs, we group them like this: \\nƒ𝑞E\\n(\"), 𝑞E\\n(!)„\\n1\\n, ƒ𝑞E\\n(&), 𝑞E\\n(T)„\\n1\\n, … , ƒ𝑞E\\nO_.6\"R, 𝑞E\\nO_.R„\\n1\\n \\nWhen we write 𝐪E(𝑝), it represents the pair ƒ𝑞E\\n(![6\"), 𝑞E\\n(![)„. For example, 𝐪E(3) \\ncorresponds to: \\nƒ𝑞E\\n(!⋅&6\"), 𝑞E\\n(!⋅&)„ = ƒ𝑞E\\n(@), 𝑞E\\n(L)„ \\nEach pair 𝑝 undergoes a rotation based on the token position 𝑡 and a rotation \\nfrequency 𝜃[: \\nRoPEC𝐪E(𝑝)D =\\ndef ÐcosC𝜃[𝑡D\\n−sinC𝜃[𝑡D\\nsinC𝜃[𝑡D\\ncosC𝜃[𝑡DÑ ¸𝑞E\\n(![6\")\\n𝑞E\\n(![) ¹ \\nApplying the matrix-vector multiplication rule, the rotation results in the fol-\\nlowing 2D vector: \\nRoPEC𝐪E(𝑝)D\\n= ƒ𝑞E\\n(![6\")cosC𝜃[𝑡D −𝑞E\\n(![)sinC𝜃[𝑡D,\\u2004𝑞E\\n(![6\")sinC𝜃[𝑡D\\n+ 𝑞E\\n(![)cosC𝜃[𝑡D„\\n1\\n, \\nwhere 𝜃[ is the rotation frequency for the 𝑝th pair. It is defined as: \\n𝜃[ =\\ndef\\n1\\n𝛩!([6\")/_. \\nHere, 𝛩 is a constant. Initially set to 10,000, later experiments demonstrated \\nthat higher values of 𝛩—such as 500,000 (used in Llama 2 and 3 series of \\nmodels) or 1,000,000 (in Qwen 2 and 2.5 series)—enable support for larger \\ncontext sizes (hundreds of thousands of tokens). \\nThe full rotated embedding RoPE(𝐪E) is constructed by concatenating all the \\nrotated pairs: \\nRoPE(𝐪E) =\\ndef concat ƒRoPEC𝐪E(1)D, \\u2004RoPEC𝐪E(2)D, … , \\u2004RoPE ‘𝐪EC𝑑`/2D’„'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 128}, page_content='128 \\n \\nNote how the rotation frequency 𝜃[ decreases quickly for each subsequent pair \\nbecause of the exponential term in the denominator. This enables RoPE to cap-\\nture fine-grained local position information in the early dimensions, where ro-\\ntations are more frequent, and coarse-grained global position information in \\nthe later dimensions, where rotations slow down. This combination creates \\nricher positional encoding, allowing the model to differentiate token positions \\nin a sequence more effectively than using a single rotation frequency across all \\ndimensions. \\nTo illustrate the process, consider a 6-dimensional query vector at position 𝑡 \\nand 𝛩= 10,000: \\n𝐪E = ƒ𝑞E\\n(\"), 𝑞E\\n(!), 𝑞E\\n(&), 𝑞E\\n(T), 𝑞E\\n(@), 𝑞E\\n(L)„\\n1\\n=\\ndef [0.8,0.6,0.7,0.3,0.5,0.4]1 \\nFirst, we split it into three pairs (𝑑`/2 = 3): \\n𝐪E(1) = ƒ𝑞E\\n(\"), 𝑞E\\n(!)„ = [0.8,0.6]1\\n𝐪E(2) = ƒ𝑞E\\n(&), 𝑞E\\n(T)„ = [0.7,0.3]1\\n𝐪E(3) = ƒ𝑞E\\n(@), 𝑞E\\n(L)„ = [0.5,0.4]1\\n \\nEach pair 𝑝 undergoes a rotation by angle 𝜃[𝑡, where: \\n𝜃[ =\\n1\\n10000!([6\")/_. \\nLet the position 𝑡 be 100. First, we calculate the rotation angles for each pair \\n(in radians): \\n𝜃\"\\n=\\n1\\n10000!(\"6\")/L =\\n1\\n10000?/L = 1.0000,\\ntherefore: 𝜃\"𝑡\\n= 100.00\\n𝜃!\\n=\\n1\\n10000!(!6\")/L =\\n1\\n10000!/L ≈0.0464,\\ntherefore: 𝜃!𝑡\\n= 4.64\\n𝜃&\\n=\\n1\\n10000!(&6\")/L =\\n1\\n10000T/L ≈0.0022,\\ntherefore: 𝜃&𝑡\\n= 0.22\\n \\nThe rotated pair 1 is: \\nRoPEC𝐪\"??(1)D = Rcos(100)\\n−sin(100)\\nsin(100)\\ncos(100)S ƒ0.8\\n0.6„ ≈ƒ 0.86\\n0.51\\n−0.51\\n0.86„ ƒ0.8\\n0.6„\\n= [0.99,0.11]1 \\nThe rotated pair 2 is:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 129}, page_content='129 \\nRoPEC𝐪\"??(2)D = Rcos(4.64)\\n−sin(4.64)\\nsin(4.64)\\ncos(4.64)S ƒ0.7\\n0.3„ ≈ƒ−0.07\\n1.00\\n−1.00\\n−0.07„ ƒ0.7\\n0.3„\\n= [0.25, −0.72]1 \\nThe rotated pair 3 is: \\nRoPEC𝐪\"??(3)D = Rcos(0.22)\\n−sin(0.22)\\nsin(0.22)\\ncos(0.22)S ƒ0.5\\n0.4„ ≈ƒ0.98\\n−0.21\\n0.21\\n0.98„ ƒ0.5\\n0.4„\\n= [0.40,0.50]1 \\nThese is what the original and rotated pairs look like when plotted: \\n \\nThe final RoPE-encoded vector is the concatenation of these pairs: \\nRoPE(𝐪\"??) ≈[0.99,0.11,0.25, −0.72,0.40,0.50]1 \\nThe math for RoPE(𝐤E) is the same as for RoPE(𝐪E). In each decoder block, \\nRoPE is applied to each row of the query (𝐐) and key (𝐊) matrices within the \\nself-attention mechanism. \\nValue vectors only provide the information that is selected and com-\\nbined after the attention weights are determined. Since the positional \\nrelationships are already captured in the query-key alignment, value \\nvectors don’t need their own rotary embeddings. In other words, the \\nvalue vectors simply “deliver” the content once the positional-aware \\nattention has identified where to look. \\nRecall that 𝐐 and 𝐊 are generated by multiplying the decoder block inputs by \\nweight matrices 𝐖X and 𝐖Y, as illustrated in Figure 4.1. RoPE is applied im-\\nmediately after obtaining 𝐐 and 𝐊, and before the attention scores are calcu-\\nlated.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 130}, page_content=\"130 \\n \\nRoPE is applied across all decoder blocks, ensuring positional information \\nflows consistently throughout the network's depth. The illustration below \\nshows its implementation in two sequential decoder blocks. \\n \\nIn this graph, the outputs of the second decoder block are used to compute \\nlogits for each position. This is achieved by multiplying the outputs of the final \\ndecoder block by a matrix of shape (embedding dimensionality, vocabulary \\nsize) shared across all positions. We'll explore this part in more detail when we \\nimplement the decoder model in Python. \\nThe self-attention mechanism we’ve described would work as is. However, \\ntransformers typically employ an enhanced version called multi-head atten-\\ntion. This allows the model to focus on multiple aspects of information simul-\\ntaneously. For example, one attention head might capture syntactic relation-\\nships, another might emphasize semantic similarities, and a third could detect \\nlong-range dependencies between tokens.\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 131}, page_content='131 \\n4.5. Multi-Head Attention \\nOnce you understand self-attention, understanding multi-head attention is rel-\\natively straightforward. For each head ℎ, from 1 to 𝐻, there is a separate triplet \\nof attention matrices: \\nÜC𝐖W\\nX, 𝐖W\\nY, 𝐖W\\nZDÝW∈\",…,c \\nEach triplet is applied to the input vectors 𝐱\", … , 𝐱T, producing 𝐻 matrices 𝐆W. \\nFor each head, this gives four vectors 𝐠W,\", … , 𝐠W,T, as shown in Figure 4.2 for \\nthree heads (𝐻= 3). As you can see, the multi-head self-attention mechanism \\nprocesses an input sequence through multiple self-attention “heads.” For in-\\nstance, with 3 heads, each head calculates self-attention scores for the input \\ntokens independently. RoPE is applied separately in each head. \\nAll input tokens 𝐱\", … , 𝐱T are processed by all three heads, producing output \\nmatrices 𝐆\", 𝐆!, and 𝐆&. Each matrix 𝐆W has as many rows as there are input \\ntokens, meaning each head generates an embedding for every token. The em-\\nbedding dimensionality of each 𝐆W is reduced to one-third of the total embed-\\nding dimensionality. As a result, each head outputs lower-dimensional embed-\\ndings compared to the original embedding size.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 132}, page_content='132 \\n \\n \\nFigure 4.2: 3-head self-attention. \\nThe outputs from the three heads are concatenated along the embedding di-\\nmension in the concatenation and projection layer, creating a single matrix \\nthat integrates information from all heads. This matrix is then transformed by \\nthe projection matrix 𝐖d, resulting in the final output matrix 𝐆. This output \\nis passed to the position-wise MLP: \\n \\nConcatenating the matrices 𝐆\", 𝐆!, and 𝐆& restores the original embedding \\ndimensionality (e.g., 6 in this case). However, applying the trainable parame-\\nter matrix 𝐖d enables the model to combine the heads’ information more ef-\\nfectively than mere concatenation.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 133}, page_content='133 \\nModern large language models often use up to 128 heads. \\nAt this stage, the reader understands the Transformer model architecture at a \\nhigh level. Two key technical details remain to explore: layer normalization \\nand residual connections, both essential components that enable the Trans-\\nformer’s effectiveness. Let’s begin with residual connections. \\n4.6. Residual Connection \\nResidual connections (or skip connections) are essential to the Transformer \\narchitecture. They solve the vanishing gradient problem in deep neural net-\\nworks, enabling the training of much deeper models. \\nA network containing more than two layers is called a deep neural network. \\nTraining them is called deep learning. Before ReLU and residual connections, \\nthe vanishing gradient problem severely limited network depth. Remember \\nthat during gradient descent, partial derivatives update all parameters by tak-\\ning small steps in the opposite direction of the gradient. In deeper networks, \\nthese updates become very small in earlier layers (those closer to the input), \\neffectively halting parameter adjustment. Residual connections strengthen \\nthese updates by creating pathways for the gradient to “bypass” certain layers, \\nhence the term skip connections. \\nTo better understand the vanishing gradient problem, let’s analyze a 3-layer \\nneural network expressed as a composite function: \\n𝑓(𝑥) = 𝑓& ‘𝑓!C𝑓\"(𝑥)D’, \\nwhere 𝑓\" represents the first layer, 𝑓! represents the second layer, and 𝑓& rep-\\nresents the third (output) layer. Let these functions be defined as follows: \\n𝑧= 𝑓\"(𝑥) =\\ndef 𝑤\"𝑥+ 𝑏\"\\n𝑟= 𝑓!(𝑧) =\\ndef 𝑤!𝑧+ 𝑏!\\n𝑦= 𝑓&(𝑟) =\\ndef 𝑤&𝑟+ 𝑏&\\n \\nHere, 𝑤e and 𝑏e are scalar weights and biases for each layer 𝑙∈{1,2,3}. \\nLet’s define the loss function 𝐿 in terms of the network output 𝑓(𝑥) and the \\ntrue label 𝑦 as 𝐿(𝑓(𝑥), 𝑦). The gradient of the loss 𝐿 with respect to 𝑤\", denoted \\nas \\n)D\\n)\\'!, is given by:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 134}, page_content='134 \\n \\n∂𝐿\\n∂𝑤\"\\n= ∂𝐿\\n∂𝑓⋅∂𝑓\\n∂𝑤\"\\n= ∂𝐿\\n∂𝑓&\\n⋅∂𝑓&\\n∂𝑓!\\n⋅∂𝑓!\\n∂𝑓\"\\n⋅∂𝑓\"\\n∂𝑤\"\\n, \\nwhere: \\n∂𝑓&\\n∂𝑓!\\n= 𝑤&,\\u2001∂𝑓!\\n∂𝑓\"\\n= 𝑤!,\\u2001 ∂𝑓\"\\n∂𝑤\"\\n= 𝑥 \\nSo, we can write: \\n∂𝐿\\n∂𝑤\"\\n= ∂𝐿\\n∂𝑓&\\n⋅𝑤& ⋅𝑤! ⋅𝑥 \\nThe vanishing gradient problem occurs when weights like 𝑤! and 𝑤& are small \\n(less than 1). When multiplied together, they produce even smaller values, \\ncausing the gradient for earlier weights such as 𝑤\" to approach zero. This issue \\nbecomes particularly severe in networks with many layers. \\nTake large language models as an example. These networks often include 32 \\nor more decoder blocks. To simplify, assume all blocks are fully connected lay-\\ners. If the average weight value is around 0.5, the gradient for the input layer \\nparameters becomes 0.5&! ≈0.0000000002. This is extremely small. After \\nmultiplying by the learning rate, updates to the early layers become negligible. \\nAs a result, the network stops learning effectively. \\nResidual connections offer a solution to the vanishing gradient problem by cre-\\nating shortcuts in the gradient computation path. The basic idea is simple: in-\\nstead of passing only the output of a layer to the next one, the layer’s input is \\nadded to its output. Mathematically, this is written as: \\n𝑦= 𝑓(𝑥) + 𝑥, \\nwhere 𝑥 is the input, 𝑓(𝑥) is the layer’s computed function, and 𝑦 is the output. \\nThis addition forms the residual connec-\\ntion. Graphically, it is shown in the pic-\\nture on the right. In this illustration, the \\ninput 𝑥 is processed both through the \\nlayer (represented as 𝑓(𝑥)) and added directly to the layer’s output. \\nNow let’s introduce residual connections into our 3-layer network. We’ll see \\nhow this changes gradient computation and mitigates the vanishing gradient \\nissue. Starting with the original network 𝑓(𝑥) = 𝑓& ‘𝑓!C𝑓\"(𝑥)D’, let’s add resid-\\nual connections to layers 2 and 3:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 135}, page_content='135 \\n𝑧\\n←𝑓\"(𝑥) =\\ndef 𝑤\"𝑥+ 𝑏\"\\n𝑟\\n←𝑓!(𝑧) =\\ndef 𝑤!𝑧+ 𝑏! + 𝑧\\n𝑦\\n←𝑓&(𝑟) =\\ndef 𝑤&𝑟+ 𝑏& + 𝑟\\n \\nOur composite function becomes: \\n𝑓(𝑥) = 𝑤&[𝑤!(𝑤\"𝑥+ 𝑏\") + 𝑏! + 𝑤\"𝑥+ 𝑏\"] + 𝑏& + 𝑤!(𝑤\"𝑥+ 𝑏\") + 𝑏! + 𝑤\"𝑥\\n+ 𝑏\" \\nNow, let’s calculate the gradient of the loss 𝐿 with respect to 𝑤\": \\n∂𝐿\\n∂𝑤\"\\n= ∂𝐿\\n∂𝑓⋅∂𝑓\\n∂𝑤\"\\n \\nExpanding \\n)-\\n)\\'!: \\n∂𝑓\\n∂𝑤\"\\n=\\n∂\\n∂𝑤\"\\nÐC𝑤&C𝑤!(𝑤\"𝑥+ 𝑏\") + 𝑏! + (𝑤\"𝑥+ 𝑏\")D + 𝑏&D +\\nC𝑤!(𝑤\"𝑥+ 𝑏\") + 𝑏! + (𝑤\"𝑥+ 𝑏\")D\\nÑ\\n= (𝑤&𝑤! + 𝑤& + 𝑤! + 1) ⋅𝑥\\n \\nTherefore, the full gradient is: \\n∂𝐿\\n∂𝑤\"\\n= ∂𝐿\\n∂𝑓⋅(𝑤&𝑤! + 𝑤& + 𝑤! + 1) ⋅𝑥 \\nComparing this to our original gradient without residual connections: \\n∂𝐿\\n∂𝑤\"\\n= ∂𝐿\\n∂𝑓⋅𝑤& ⋅𝑤! ⋅𝑥 \\nWe observe that residual connections introduce three additional terms: 𝑤&, 𝑤!, \\nand 1. This guarantees that the gradient will not vanish completely, even when \\n𝑤! and 𝑤& are small, due to the added constant term 1. \\nFor example, if 𝑤! = 𝑤& = 0.5 as in the previous case: \\n• Without residual connections: 0.5 ⋅0.5 = 0.25 \\n• With residual connections: 0.5 ⋅0.5 + 0.5 + 0.5 + 1 = 2.25 \\nThe illustration below depicts an encoding block with residual connections:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 136}, page_content='136 \\n \\n \\nAs shown, each decoder block includes two residual connections. The layers \\nare now named like Python objects, which we will implement shortly. Addi-\\ntionally, two RMSNorm layers have been added. Let’s discuss their purpose. \\n4.7. Root Mean Square Normalization \\nThe RMSNorm layer applies root mean square normalization to the input \\nvector. This operation takes place just before the vector enters the self-atten-\\ntion layer and the position-wise MLP. Let’s illustrate this with a three-dimen-\\nsional vector. \\nSuppose we have a vector 𝐱= [𝑥(\"), 𝑥(!), 𝑥(&)]1. To apply RMS normalization, \\nwe first calculate the root mean square (RMS) of the vector:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 137}, page_content='137 \\nRMS(𝐱) = ^1\\n3 U(𝑥($))!\\n&\\n$%\"\\n= ß1\\n3 [(𝑥(\"))! + (𝑥(!))! + (𝑥(&))!] \\nThen, we normalize the vector by dividing each component by the RMS value \\nto obtain 𝐱9: \\n𝐱9 =\\n𝐱\\nRMS(𝐱) = Ð\\n𝑥(\")\\nRMS(𝐱) ,\\n𝑥(!)\\nRMS(𝐱) ,\\n𝑥(&)\\nRMS(𝐱)Ñ\\n1\\n \\nFinally, we apply the scale factor 𝛾 to each dimension of 𝐱9: \\n𝐱‾ = RMSNorm(𝐱) =\\ndef 𝛄⊙𝐱9 = J𝛾(\")𝑥9(\"), 𝛾(!)𝑥9(!), 𝛾(&)𝑥9(&)K\\n1, \\nwhere ⊙ denotes the element-wise product. The vector 𝛄 is a trainable pa-\\nrameter, and each RMSNorm layer has its own independent 𝛄. \\nThe primary purpose of RMSNorm is to stabilize training by keeping the scale \\nof the input to each layer consistent. This improves numerical stability, helping \\nto prevent gradient updates that are excessively large or small. \\nNow that we’ve covered the key components of the Transformer architecture, \\nlet’s summarize how a decoder block processes its input: \\n1. The input embeddings 𝐱E first go through RMS normalization. \\n2. The normalized embeddings 𝐱‾E are processed by the multi-head self-at-\\ntention mechanism, with RoPE applied to key and query vectors. \\n3. The self-attention output 𝐠E is added to the original input 𝐱E (residual \\nconnection). \\n4. This sum, 𝐠eE, undergoes RMS normalization again. \\n5. The normalized sum 𝐠‾ E is passed through the multilayer perceptron. \\n6. The perceptron output 𝐳E is added to the pre-RMS-normalization vector \\n𝐠eE (another residual connection). \\n7. The result, 𝐳eE, is the output of the decoder block, serving as input for \\nthe next block (or the final output layer if it’s the last block). \\nThis sequence is repeated for each decoder block in the Transformer.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 138}, page_content='138 \\n \\n4.8. Key-Value Caching \\nDuring training,  the decoder can process all positions in parallel because at \\neach block it computes the query, key, and value matrices, 𝐐= 𝐗𝐖X, 𝐊=\\n𝐗𝐖Y, and 𝐕= 𝐗𝐖Z, for the entire sequence 𝐗. However, during an autoregres-\\nsive (left-to-right) inference, tokens must be generated one at a time. Nor-\\nmally, each time we generate a new token, we would have to: \\n1. Calculate the key, query, and value vectors for the new token. \\n2. Recalculate the key and value matrices for all previous tokens. \\n3. Merge these with the new token’s key and value vectors to compute \\nself-attention for the new token. \\nKey-value caching skips step 2 by saving the key and value matrices from \\nearlier tokens, avoiding repeated calculations. Since 𝐖Y and 𝐖Z are fixed after \\ntraining, the key and value vectors of earlier tokens stay constant during infer-\\nence. These vectors can be stored (“cached”) after being computed once. For \\nevery new token: \\n• Its key and value vectors are computed using 𝐖Y and 𝐖Z. \\n• These vectors are appended to the cached key-value pairs for self-atten-\\ntion. \\nQuery vectors, however, are not cached because they depend on the current \\ntoken being processed. Every time a new token is added, its query vector must \\nbe computed on-the-fly to attend to all cached keys and values. \\nThis approach eliminates reprocessing the rest of the sequence, cutting com-\\nputation significantly for long sequences. In each decoder block, cached keys \\nand values are stored per attention head with shapes (𝐿× 𝑑W) for both matri-\\nces, where 𝐿 grows by one with each new token, and 𝑑W is the dimensionality \\nof the query, key, and value vectors for that head. For a model with 𝐻 attention \\nheads, the combined key and value caches in each decoder block have shapes \\n(𝐻× 𝐿× 𝑑W). \\nRoPE applies position-dependent rotations to vectors, but this doesn’t \\ninterfere with caching. When a new token arrives, it simply takes the \\nnext available position index (if the sequence has 𝐿 tokens, the new \\none becomes position 𝐿+ 1), while previously processed tokens retain \\ntheir original positions from 1 through 𝐿. This means the cached keys \\nand values, already rotated according to their respective positions,'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 139}, page_content='139 \\nremain unchanged. The rotation is only applied to the new token at \\nposition 𝐿+ 1. \\nNow that we understand how the Transformer operates, we’re ready to start \\ncoding. \\n4.9. Transformer in Python \\nLet’s begin implementing the decoder in Python by defining the Attention-\\nHead class: \\nclass AttentionHead(nn.Module): \\n    def __init__(self, emb_dim, d_h): \\n        super().__init__() \\n        self.W_Q = nn.Parameter(torch.empty(emb_dim, d_h)) \\n        self.W_K = nn.Parameter(torch.empty(emb_dim, d_h)) \\n        self.W_V = nn.Parameter(torch.empty(emb_dim, d_h)) \\n        self.d_h = d_h \\n \\n    def forward(self, x, mask): \\n        Q = x @ self.W_Q ➊ \\n        K = x @ self.W_K  \\n        V = x @ self.W_V ➋ \\n \\n        Q, K = rope(Q), rope(K) ➌ \\n \\n        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_h\\n) ➍ \\n        masked_scores = scores.masked_fill(mask == 0, float(\"\\n-inf\")) ➎ \\n        attention_weights = torch.softmax(masked_scores, dim=\\n-1) ➏ \\n        return attention_weights @ V ➐ \\nThis class implements a single attention head in the multi-head attention \\nmechanism. In the constructor, we initialize three trainable weight matrices: \\nthe query matrix W_Q, the key matrix W_K, and the value matrix W_V. Each of \\nthese is a Parameter tensor of shape (emb_dim, d_h), where emb_dim is the'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 140}, page_content='140 \\n \\ninput embedding dimension and d_h is the dimensionality of the query, key, \\nand value vectors for this attention head. \\nIn the forward method: \\n• Lines ➊ and ➋ compute the query, key, and value matrices by multiply-\\ning the input vector x with the respective weight matrices. Given that x \\nhas shape (batch_size, seq_len, emb_dim), Q, K, and V each have \\nshape (batch_size, seq_len, d_h). \\n• Line ➌ applies the rotary positional encoding to Q and K. After the \\nquery and key vectors are rotated, line ➍ computes the attention \\nscores. Here’s a breakdown: \\no K.transpose(-2, -1) swaps the last two dimensions of K. If K \\nhas shape (batch_size, seq_len, d_h), transposing it results in \\n(batch_size, d_h, seq_len). This prepares K for matrix multi-\\nplication with Q. \\no Q @ K.transpose(-2, -1) performs batch matrix multiplica-\\ntion, resulting in a tensor of attention scores of shape \\n(batch_size, seq_len, seq_len). \\no As mentioned in Section 4.2, we divide by sqrt(d_h) for numer-\\nical stability. \\nWhen the matrix multiplication operator @ is applied to tensors with \\nmore than two dimensions, PyTorch uses broadcasting. This tech-\\nnique handles dimensions that aren’t directly compatible with the @ \\noperator, which is normally defined only for two-dimensional tensors \\n(matrices). In this case, PyTorch treats the first dimension as the batch \\ndimension, performing the matrix multiplication separately for each \\nexample in the batch. This process is known as batch matrix multipli-\\ncation. \\n• Line ➎ applies the causal mask. The mask tensor has the shape \\n(seq_len, seq_len) and contains 0s and 1s. The masked_fill func-\\ntion replaces all cells in the input matrix where mask == 0 with nega-\\ntive infinity. This prevents attention to future tokens. Since the mask \\nlacks the batch dimension while scores includes it, PyTorch uses'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 141}, page_content='141 \\nbroadcasting to apply the mask to the scores of each sequence in the \\nbatch. \\n• Line ➏ applies softmax to the scores along the last dimension, turning \\nthem into attention weights. Then, line ➐ computes the output by mul-\\ntiplying these attention weights with V. The resulting output has the \\nshape (batch_size, seq_len, d_h). \\nGiven the attention head class, we can now define the MultiHeadAttention \\nclass: \\nclass MultiHeadAttention(nn.Module): \\n    def __init__(self, emb_dim, num_heads): \\n        super().__init__() \\n        d_h = emb_dim // num_heads ➊ \\n        self.heads = nn.ModuleList([ \\n            AttentionHead(emb_dim, d_h) \\n            for _ in range(num_heads) \\n        ]) ➋ \\n        self.W_O = nn.Parameter(torch.empty(emb_dim, emb_dim)\\n) ➌ \\n \\n    def forward(self, x, mask): \\n        head_outputs = [head(x, mask) for head in self.heads] \\n➍ \\n        x = torch.cat(head_outputs, dim=-1) ➎ \\n        return x @ self.W_O ➏ \\nIn the constructor: \\n• Line ➊ calculates d_h, the dimensionality of each attention head, by di-\\nviding the model’s embedding dimensionality emb_dim by the number \\nof heads. \\n• Line ➋ creates a ModuleList containing num_heads instances of At-\\ntentionHead. Each head takes the input dimensionality emb_dim and \\noutputs a vector of size d_h. \\n• Line ➌ initializes W_O, a learnable projection matrix with shape \\n(emb_dim, emb_dim) to combine the outputs from all attention heads.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 142}, page_content='142 \\n \\nIn the forward method: \\n• Line ➍ applies each attention head to the input x of shape \\n(batch_size, seq_len, emb_dim). Each head’s output has shape \\n(batch_size, seq_len, d_h). \\n• Line ➎ concatenates all heads’ outputs along the last dimension. The \\nresulting x has shape (batch_size, seq_len, emb_dim) since \\nnum_heads * d_h = emb_dim. \\n• Line ➏ multiplies the concatenated output by the projection matrix \\nW_O. The output has the same shape as input. \\nNow that we have multi-head attention, the last piece needed for the decoder \\nblock is the position-wise multilayer perceptron. Let’s define it: \\nclass MLP(nn.Module): \\n    def __init__(self, emb_dim): \\n        super().__init__() \\n        self.W_1 = nn.Parameter(torch.empty(emb_dim, emb_dim \\n* 4)) \\n        self.B_1 = nn.Parameter(torch.empty(emb_dim * 4)) \\n        self.W_2 = nn.Parameter(torch.empty(emb_dim * 4, emb_\\ndim)) \\n        self.B_2 = nn.Parameter(torch.empty(emb_dim)) \\n \\n    def forward(self, x): \\n        x = x @ self.W_1 + self.B_1 ➊ \\n        x = torch.relu(x) ➋ \\n        x = x @ self.W_2 + self.B_2 ➌ \\n        return x \\nIn the constructor, we initialize learnable weights and biases. \\nIn the forward method: \\n• Line ➊ multiplies the input x by the weight matrix W_1 and adds the \\nbias vector B_1. The input has shape (batch_size, seq_len, \\nemb_dim), so the result has shape (batch_size, seq_len, emb_dim * \\n4). \\n• Line ➋ applies the ReLU activation function element-wise, adding non-\\nlinearity.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 143}, page_content='143 \\n• Line ➌ multiplies the result by the second weight matrix W_2 and adds \\nthe bias vector B_2, reducing the dimensionality back to (batch_size, \\nseq_len, emb_dim). \\nThe first linear transformation expands to 4 times the embedding dimension-\\nality (emb_dim * 4) to provide the network with greater capacity for learning \\ncomplex patterns and relationships between variables. The 4x factor balances \\nexpressiveness and efficiency. After expanding the dimensionality, it’s com-\\npressed back to the original embedding dimensionality (emb_dim). This en-\\nsures compatibility with residual connections, which require matching dimen-\\nsionalities. Empirical results support this expand-and-compress approach as an \\neffective trade-off between computational cost and performance. \\nWith all components defined, we’re ready to set up the complete decoder \\nblock: \\nclass DecoderBlock(nn.Module): \\n    def __init__(self, emb_dim, num_heads): \\n        super().__init__() \\n        self.norm1 = RMSNorm(emb_dim) \\n        self.attn = MultiHeadAttention(emb_dim, num_heads) \\n        self.norm2 = RMSNorm(emb_dim) \\n        self.mlp = MLP(emb_dim) \\n \\n    def forward(self, x, mask): \\n        attn_out = self.attn(self.norm1(x), mask) ➊ \\n        x = x + attn_out ➋ \\n        mlp_out = self.mlp(self.norm2(x)) ➌ \\n        x = x + mlp_out ➍ \\n        return x \\nThe DecoderBlock class represents a single decoder block in a Transformer \\nmodel. In the constructor, we set up the necessary layers: two RMSNorm layers, \\na MultiHeadAttention instance (configured with the embedding dimension-\\nality and number of heads), and an MLP layer. \\nIn the forward method: \\n• Line ➊ applies RMSNorm to the input x, which has shape (batch_size, \\nseq_len, emb_dim). The output of RMSNorm keeps this shape. This'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 144}, page_content='144 \\n \\nnormalized tensor is then passed to the multi-head attention layer, \\nwhich outputs a tensor of the same shape. \\n• Line ➋ adds a residual connection by combining the attention output \\nattn_out with the original input x. The shape doesn’t change. \\n• Line ➌ applies the second RMSNorm to the result from the residual con-\\nnection, retaining the same shape. This normalized tensor is then \\npassed through the MLP, which outputs another tensor with shape \\n(batch_size, seq_len, emb_dim). \\n• Line ➍ adds a second residual connection, combining mlp_out with its \\nunnormalized input. The decoder block’s final output shape is \\n(batch_size, seq_len, emb_dim), ready for the next decoder block or \\nthe final output layer. \\nWith the decoder block defined, we can now build the decoder transformer \\nlanguage model by stacking multiple decoder blocks sequentially: \\nclass DecoderLanguageModel(nn.Module): \\n    def __init__( \\n        self, vocab_size, emb_dim,  \\n        num_heads, num_blocks, pad_idx \\n    ): \\n        super().__init__() \\n        self.embedding = nn.Embedding( \\n            vocab_size, emb_dim,  \\n            padding_idx=pad_idx \\n        ) ➊ \\n        self.layers = nn.ModuleList([ \\n            DecoderBlock(emb_dim, num_heads) for _ in range(n\\num_blocks) \\n        ]) ➋ \\n        self.output = nn.Parameter(torch.rand(emb_dim, vocab_\\nsize)) ➌ \\n \\n    def forward(self, x): \\n        x = self.embedding(x) ➍ \\n        _, seq_len, _ = x.shape \\n        mask = torch.tril(torch.ones(seq_len, seq_len, device\\n=x.device)) ➎'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 145}, page_content='145 \\n        for layer in self.layers: ➏ \\n            x = layer(x, mask) \\n        return x @ self.output ➐ \\nIn the constructor of the DecoderLanguageModel class: \\n• Line ➊ creates an embedding layer that converts input token indices to \\ndense vectors. The padding_idx specifies the ID of the padding token, \\nensuring that padding tokens are mapped to zero vectors. \\n• Line ➋ creates a ModuleList with num_blocks DecoderBlock in-\\nstances, forming the stack of decoder layers. \\n• Line ➌ defines a matrix to project the last decoder block’s output to \\nlogits over the vocabulary, enabling next token prediction. \\nIn the forward method: \\n• Line ➍ converts the input token indices to embeddings. The input ten-\\nsor x has shape (batch_size, seq_len); the output has shape \\n(batch_size, seq_len, emb_dim). \\n• Line ➎ creates the causal mask. \\n• Line ➏ applies each decoder block to the input tensor x with shape \\n(batch_size, seq_len, emb_dim), producing an output tensor of the \\nsame shape. Each block refines the sequence and passes it to the next \\nuntil the final block. \\n• Line ➐ projects the output of the final decoder block to vocabulary-\\nsized logits by multiplying it with the self.output matrix, which has \\nshape (emb_dim, vocab_size). After this batched matrix multiplica-\\ntion, the final output has shape (batch_size, seq_len, vocab_size), \\nproviding scores for each token in the vocabulary at each position in \\nthe input sequence. This output can then be used to generate the \\nmodel’s predictions as we will discuss in the next chapter. \\nThe training loop for DecoderLanguageModel is the same as for the RNN (Sec-\\ntion 3.6), so it is not repeated here for brevity. Implementations of RMSNorm \\nand RoPE are also skipped. Training data is prepared just like for the RNN: the \\ntarget sequence is offset by one position relative to the input sequence, as de-\\nscribed in Section 3.7. The complete code for training the decoder language \\nmodel is available in the thelmbook.com/nb/4.1 notebook.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 146}, page_content=\"146 \\n \\nIn the notebook, I used these hyperparameter values: emb_dim = 128, \\nnum_heads = 8, num_blocks = 2, batch_size = 128, learning_rate = \\n0.001, num_epochs = 1, and context_size = 30. With these settings, the \\nmodel achieved a perplexity of 55.19, improving on the RNN’s 72.23. This is a \\ngood result given the comparable number of trainable parameters (8,621,963 \\nfor the Transformer vs. 8,292,619 for the RNN). The real strengths of trans-\\nformers, however, become apparent at larger scales of model size, context \\nlength, and training data. Reproducing experiments at such scales in this book \\nis, of course, impractical. \\nLet’s look at some continuations of the prompt “The President” generated by \\nthe decoder model at later training steps: \\nThe President has been in the process of a new deal to make a \\ndecision on the issue . \\n \\nThe President 's office said the government had `` no intenti\\non of making any mistakes '' . \\n \\nThe President of the United States has been a key figure for \\nthe first time in the past ## years . \\nThe “#” characters in the training data represent individual digits. For exam-\\nple, “##” likely represents the number of years. \\n*** \\nIf you’ve made it this far, well done! You now understand the mechanics of \\nlanguage models. But understanding the mechanics alone won’t let you fully \\nappreciate what modern language models are capable of. To truly understand, \\nyou need to work with one. \\nIn the next chapter, we’ll explore large language models (LLMs). We’ll discuss \\nwhy they’re called large and what’s so special about the size. Then, we’ll cover \\nhow to finetune an existing LLM for practical tasks like question answering \\nand document classification, as well as how to use LLMs to address a variety \\nof real-world problems.\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 147}, page_content=\"147 \\nChapter 5. Large Language Model \\nLarge language models have transformed NLP through their remarkable capa-\\nbilities in text generation, translation, and question-answering. But how can a \\nmodel trained solely to predict the next word achieve these results? The an-\\nswer lies in two factors: scale and supervised finetuning. \\n5.1. Why Larger Is Better \\nLLMs are built with a large number of parameters, large context windows, and \\ntrained on large corpora backed by substantial computational resources. This \\nscale enables them to learn complex language patterns and even memorize \\ninformation. \\nCreating a chat LM, capable of handling dialogue and following complex in-\\nstructions, involves two stages. The first stage is pretraining on a massive da-\\ntaset, often containing trillions of tokens. In this phase, the model learns to \\npredict the next token based on context—similar to what we did with the RNN \\nand decoder models, but at a vastly larger scale. \\nWith more parameters and extended context windows, the model aims to “un-\\nderstand” the context as deeply as possible to improve the next token predic-\\ntion and minimize the cross-entropy loss. For example, consider this context: \\nThe CRISPR-Cas9 technique has revolutionized genetic engineer\\ning by enabling precise modifications to DNA sequences. The p\\nrocess uses a guide RNA to direct the Cas9 enzyme to a specif\\nic location in the genome. Once positioned, Cas9 acts like mo\\nlecular scissors, cutting the DNA strand. This cut activates \\nthe cell's natural repair mechanisms, which scientists can ex\\nploit to \\nTo accurately predict the next token, the model must know: \\n1) about CRISPR-Cas9 and its components, such as guide RNA and Cas9 \\nenzyme, \\n2) how CRISPR-Cas9 works—locating specific DNA sequences and cutting \\nDNA, \\n3) about cellular repair mechanisms, and \\n4) how these mechanisms enable gene editing.\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 148}, page_content='148 \\n \\nA well-trained LLM might suggest continuations like “insert new genetic mate-\\nrial” or “delete unwanted genes.” Choosing “insert” or “delete” over vague \\nterms like “change” or “fix” requires encoding the context into embedding vec-\\ntors that reflect a deeper understanding of the gene-editing process, rather \\nthan relying on surface-level patterns as count-based models do. \\nIt’s intuitive to think that if words and paragraphs can be represented by dense \\nembedding vectors, then entire documents or complex explanations could the-\\noretically be represented this way too. However, before LLMs were discovered, \\nNLP researchers believed embeddings could only represent basic concepts like \\n“animal,” “building,” “economy,” “technology,” “verb,” or “noun.” This belief \\nis evident in the conclusion of one of the most influential papers of the 2010s, \\nwhich detailed the training of a state-of-the-art language model at that time: \\n“As with all text generated by language models, the sample does not make \\nsense beyond the level of short phrases. The realism could perhaps be im-\\nproved with a larger network and/or more data. However, it seems futile \\nto expect meaningful language from a machine that has never been ex-\\nposed to the sensory world to which language refers.” (Alex Graves, “Gen-\\nerating Sequences With RNNs,” 2014) \\nGPT-3 showed some ability to continue relatively complex patterns. But only \\nwith GPT-3.5—able to handle multi-stage dialogue and follow elaborate in-\\nstructions—it became clear that something unexpected happens when a lan-\\nguage model surpasses a certain parameter scale and is pretrained on a suffi-\\nciently large corpus. \\nScale is fundamental to building a capable LLM. Let’s look at the core features \\nthat make LLMs “large” and how these features contribute to their capabilities. \\n5.1.1. Large Parameter Count \\nOne of the most striking features of LLMs is the sheer number of parameters \\nthey contain. While our decoder model has around 8 million parameters, state-\\nof-the-art LLMs can reach hundreds of billions or even trillions of parameters. \\nIn a transformer model, the number of parameters is largely determined by the \\nembedding dimensionality (emb_dim) and the number of decoder blocks \\n(num_blocks). As these values increase, the parameter count grows quadrati-\\ncally with embedding dimensionality in the self-attention and MLP layers, and'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 149}, page_content='149 \\nlinearly with the number of decoder blocks. Doubling the embedding dimen-\\nsionality roughly quadruples the number of parameters in the attention and \\nMLP components of each decoder block. \\nOpen-weight models are models with publicly accessible trained pa-\\nrameters. These can be downloaded and used for tasks like text gener-\\nation or finetuned for specific applications. However, while the \\nweights are open, the model’s license governs its permitted uses, in-\\ncluding whether commercial use is allowed. Licenses like Apache 2.0 \\nand MIT permit unrestricted commercial use, but you should always \\nreview the license to confirm your intended use aligns with the crea-\\ntors’ terms. \\nThe table below shows key features of several open-weight LLMs compared to \\nour tiny model: \\n \\nnum_blocks emb_dim num_heads vocab_size \\nOur model \\n2 \\n128 \\n8 \\n32,011 \\nLlama 3.1 8B \\n32 \\n4,096 \\n32 \\n128,000 \\nGemma 2 9B \\n42 \\n3,584 \\n16 \\n256,128 \\nGemma 2 27B \\n46 \\n4,608 \\n32 \\n256,128 \\nLlama 3.1 70B \\n80 \\n8,192 \\n64 \\n128,000 \\nLlama 3.1 405B \\n126 \\n16,384 \\n128 \\n128,000 \\nBy convention, the number before “B” in the name of an open-weight model \\nindicates its total number of parameters in billions. \\nIf you were to store each parameter of a 70B model as a 32-bit float \\nnumber, it would require about 280GB of RAM—more storage than \\nthe Apollo 11 guidance computer had by a factor of over 30 million \\ntimes. \\nThis massive number of parameters allows LLMs to learn and represent a vast \\namount of information about grammar, semantics, world knowledge, and ex-\\nhibit reasoning capabilities.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 150}, page_content=\"150 \\n \\n5.1.2. Large Context Size \\nAnother crucial aspect of LLMs is their ability to process and maintain much \\nlarger contexts than earlier models. While our decoder model used a context \\nof only 30 tokens, modern LLMs can handle contexts of thousands—and some-\\ntimes even millions—of tokens. \\nGPT-3’s 2,048-token context could accommodate roughly 4 pages of \\ntext. In contrast, Llama 3.1’s 128,000-token context is large enough to \\nfit the entire text of “Harry Potter and the Sorcerer’s Stone” with room \\nto spare. \\nThe key challenge with processing long texts in transformer models lies in the \\nself-attention mechanism’s computational complexity. For a sequence of length \\nn, self-attention requires computing attention scores between every pair of to-\\nkens, resulting in quadratic 𝑂(𝑛!) time and space complexity. This means that \\ndoubling the input length quadruples both the memory requirements and com-\\nputational cost. This quadratic scaling becomes particularly problematic for \\nlong documents—for instance, a 10,000-token input would require computing \\nand storing 100 million attention scores for each attention layer. \\nThe increased context size is made possible through architectural improve-\\nments and optimizations in attention computation. Techniques like grouped-\\nquery attention and FlashAttention (which are beyond the scope of this \\nbook) enable efficient memory management, allowing LLMs to handle much \\nlarger contexts without excessive computational costs. \\nLLMs typically undergo pretraining on shorter contexts around 4K-8K tokens, \\nas the attention mechanism’s quadratic complexity makes training on long se-\\nquences computationally intensive. Additionally, most training data naturally \\nconsists of shorter sequences. \\nLong-context capabilities emerge through long-context pretraining, a special-\\nized stage following initial training. This process involves: \\n1. Incremental training for longer contexts: The model's context win-\\ndow gradually expands from 4,000-8,000 tokens to 128,000-256,000 \\ntokens through a series of incremental stages. Each stage increases the \\ncontext length and continues training until the model meets two key \\ncriteria: restoring its performance on short-context tasks while\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 151}, page_content='151 \\nsuccessfully handling longer-context challenges like “needle in a hay-\\nstack” evaluations. \\n \\nA needle in a haystack test evaluates a model’s ability to identify and \\nutilize relevant information buried within a very long context, typically \\nby placing a crucial piece of information early in the sequence and ask-\\ning a question that requires retrieving that specific detail from among \\nthousands of tokens of unrelated text. \\n2. Efficient scaling for self-attention: To handle the computational de-\\nmands of self-attention’s quadratic scaling with sequence length, the \\napproach implements context parallelism. This method splits input se-\\nquences into manageable chunks and uses an all-gather mechanism for \\nmemory-efficient processing. \\nAll-gather is a collective communication operation in distributed com-\\nputing where each GPU shares its local data with all other GPUs, ag-\\ngregating the data so that every GPU ends up with a complete, concat-\\nenated dataset. \\n5.1.3. Large Training Dataset \\nThe third factor behind LLMs’ capabilities is the size of the corpus used for \\ntraining. While our decoder was trained on a small corpus of news sentences \\nwith about 25 million tokens, modern LLMs use datasets with trillions of to-\\nkens. These datasets often include: \\n1) books and literature from different genres and eras, \\n2) web pages and online articles on diverse topics, \\n3) academic papers and scientific studies, \\n4) social media posts and discussions, and \\n5) code repositories and technical documents. \\nThe diversity and scale of these datasets allow LLMs to learn a broad vocabu-\\nlary, understand multiple languages, acquire knowledge on a wide array of \\ntopics—from history and science to current events and pop culture—adapt to'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 152}, page_content='152 \\n \\nvarious writing styles and formats, and acquire basic reasoning and problem-\\nsolving skills. \\n \\nThe illustration above depicts the composition of LLM training datasets, using \\nthe open Dolma dataset as an example. Segments represent different docu-\\nment types, with sizes scaled logarithmically to prevent web pages—the largest \\ncategory—from overwhelming the visualization. Each segment shows both to-\\nken count (in billions) and percentage of the corpus. While Dolma’s 3 trillion \\ntokens are substantial, they fall short of more recent datasets like Qwen 2.5’s \\n18 trillion tokens, a number likely to grow in future iterations. \\nIt would take approximately 51,000 years for a human to read the en-\\ntire Dolma dataset, reading 8 hours every day at 250 words per minute. \\nSince neural language models train on such vast corpora, they typically process \\nthe data just once. This single-epoch training approach prevents overfitting \\nwhile reducing computational demands. Processing these enormous datasets'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 153}, page_content='153 \\nmultiple times would be extremely time-consuming and may not yield signifi-\\ncant additional benefits. \\n5.1.4. Large Amount of Compute \\nIf you tried to process 3 trillion tokens of the Dolma dataset on a single modern \\nGPU, it would take over 100 years—which helps explain why major language \\nmodels require massive computing clusters. Training an LLM demands signifi-\\ncant computing power, often measured in FLOPs (floating-point operations) \\nor GPU-hours. For context, while training our decoder model might take a few \\nhours on a single GPU, modern LLMs can require thousands of GPUs running \\nfor months. \\nThe computational demands grow with three main factors: \\n1) the number of parameters in the model, \\n2) the size of the training corpus, and \\n3) the context length used during training. \\nFor example, training the Llama 3.1 series of models consumed approximately \\n40 million GPU-hours—equivalent to running a single GPU continuously for \\nalmost 4600 years. Llama 3.1’s training process uses an advanced system called \\n4D parallelism, which integrates four different parallel processing methods to \\nefficiently distribute the model across thousands of GPUs. \\nThe four dimensions of parallelism are: tensor parallelism, which partitions \\nweight matrices (𝐖X, 𝐖Y, 𝐖Z, 𝐖d, 𝐖\", 𝐖!) across devices; pipeline paral-\\nlelism, which assigns specific transformer layers to different GPUs; context \\nparallelism, which segments input sequences for processing long sequences; \\nand data parallelism, which enables simultaneous batch processing across \\nGPUs with post-step synchronization. \\nEach of these four parallelism dimensions could merit its own chapter, \\nand thus a full exploration of them lies beyond this book’s scope. \\nTraining large language models can cost tens to hundreds of millions of dol-\\nlars. These expenses include hardware, electricity, cooling, and engineering \\nexpertise. Such costs limit the development of state-of-the-art LLMs to large \\ntech companies and well-funded research labs. However, open-weight models'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 154}, page_content='154 \\n \\nhave lowered the barrier, enabling smaller organizations to leverage existing \\nmodels through methods like supervised finetuning and prompt engineering. \\n5.2. Supervised Finetuning \\nDuring pretraining, the model learns most of its capabilities. However, since it \\nis trained only to predict the next word, its default behavior is to continue the \\ninput. For instance, if you input “Explain how machine learning works,” the \\npretrained model might respond with something like “and also name three \\nmost popular algorithms.” This is not what users would expect. The model’s \\nability to follow instructions, answer questions, and hold conversations is de-\\nveloped through a process called supervised finetuning. \\nLet’s compare the behavior of a pretrained model and the same model fine-\\ntuned to follow instructions and answer questions. \\nWe’ll use two models: google/gemma-2-2b, pretrained for next-token predic-\\ntion, and google/gemma-2-2b-it, a finetuned version for instruction follow-\\ning. \\nModels on the Hugging Face Hub follow this naming convention: \\n“creator/model” with no spaces. The “model” part typically includes \\ninformation about the model’s version, number of parameters, and \\nwhether it was finetuned for conversation or instruction-following. In \\nthe name google/gemma-2-2b-it, we see that the creator is Google, \\nthe model has version 2, 2 billion parameters, and it was finetuned to \\nfollow instructions (with “it” standing for “instruction-tuned”). \\nThe prompt we’ll use is the following: \\nContinue the list: apple, orange, tomato. \\nThis is the output of the pretrained-only google/gemma-2-2b given the above \\nprompt: \\nThe list of fruits and vegetables that are good for you is lo\\nng. But there are some that are better than others. \\n \\nThe best fruits and vegetables are those that are high in fib\\ner, low in sugar, and high in vitamins and minerals. \\n \\nThe best fruits and vegetables are those that are high in fib'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 155}, page_content=\"155 \\ner, low in sugar, and high in vitamins and minerals. \\n... \\nThe output isn’t complete—the model keeps repeating the same sentence end-\\nlessly. As you can see, the output is quite similar to what we observed with our \\ndecoder model. While google/gemma-2-2b, being larger, produces more co-\\nherent sentence structures, the text still fails to align with the context, which \\nclearly requests a list of fruits. \\nNow, let’s apply the finetuned google/gemma-2-2b-it to the same input. The \\noutput is: \\nHere are a few more fruits to continue the list: \\n \\n* **Banana** \\n* **Grapefruit** \\n* **Strawberry** \\n* **Pineapple** \\n* **Blueberry**  \\n \\nLet me know if you'd like more! \\nAs you can see, the model with the same number of parameters now follows \\nthe instruction. This change is achieved through supervised finetuning. \\nSupervised finetuning, or simply finetuning, modifies a pretrained model’s \\nparameters to specialize it for specific tasks. The goal isn’t to train the model \\nto answer every question or follow every instruction. Instead, finetuning “un-\\nlocks” the knowledge and skills the model already learned during pretraining. \\nWithout finetuning, this knowledge remains “hidden” and is used mainly for \\npredicting the next token, not for problem-solving. \\nDuring finetuning, while the model is still trained to predict next tokens, it \\nlearns from examples of quality conversations and problem-solving rather than \\ngeneral text. This targeted training enables the model to better leverage its \\nexisting knowledge, producing relevant information in response to prompts \\ninstead of generating arbitrary continuations.\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 156}, page_content='156 \\n \\n5.3. Finetuning a Pretrained Model \\nAs discussed, training an LLM from scratch is a complex and expensive under-\\ntaking that requires significant computational resources, vast amounts of high-\\nquality training data, as well as deep expertise in machine learning research \\nand engineering. \\nThe good news is that open-weight models often come with permissive li-\\ncenses, allowing you to use or finetune them for business tasks. While models \\nup to 8 billion parameters can be finetuned in a Colab notebook (in the paid \\nversion that supports more powerful GPUs), the process is time-consuming, \\nand single-GPU memory constraints may limit model size and prompt length. \\nTo speed up finetuning and process longer contexts, organizations often use \\nservers with multiple high-end GPUs running in parallel. Each GPU has sub-\\nstantial VRAM (video random access memory), which stores models and data \\nduring computation. By distributing the model’s weights across the GPUs’ com-\\nbined memory, finetuning becomes significantly faster than relying on a single \\nGPU. This approach is called model parallelism. \\nPyTorch supports model parallelism with methods like Fully Sharded \\nData Parallel (FSDP). FSDP enables efficient distribution of model pa-\\nrameters across GPUs by sharding the model—splitting it into smaller \\nparts. This way, each GPU processes only a portion of the model. \\nRenting multi-GPU servers for large language model finetuning can be prohib-\\nitively expensive for smaller organizations or individuals. The computational \\ndemands can result in significant costs, with training runs potentially lasting \\nanywhere from several hours to multiple weeks depending on the model size \\nand training dataset. \\nCommercial LLM service providers offer a more cost-effective finetuning op-\\ntion. They charge based on the number of tokens in the training data and use \\nvarious techniques to lower costs. Though these methods aren’t covered in this \\nbook, you can find an up-to-date list of LLM finetuning services with pay-per-\\ntoken pricing on the book’s wiki. \\nLet’s finetune a pretrained LLM to generate an emotion. Our dataset has the \\nfollowing structure:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 157}, page_content='157 \\n{\"text\": \"i slammed the door and screamed in rage\", \"label\": \\n\"anger\"} \\n{\"text\": \"i danced and laughed under the bright sun\", \"label\"\\n: \"joy\"} \\n{\"text\": \"tears rolled down my face in silence today\", \"label\\n\": \"sadness\"} \\n... \\nIt’s a JSONL file, where each row is a labeled example formatted as a JSON \\nobject. The text key contains a text expressing one of six emotions; the label \\nkey is the corresponding emotion. The label can be one of six values: sadness, \\njoy, love, anger, fear, and surprise. Thus, we have a document classification \\nproblem with six classes. \\nWe\\'ll finetune GPT-2, a pretrained model licensed under the MIT license, \\nwhich permits unrestricted commercial use. This language model, with its \\nmodest 124M parameters, is often classified as an SLM (small language \\nmodel). Despite these constraints, it demonstrates impressive capabilities on \\ncertain tasks and remains accessible for finetuning even within free-tier Colab \\nnotebooks. \\nBefore training a complex model, it’s wise to establish baseline performance. \\nA baseline is a simple, easy-to-implement solution that sets the minimum ac-\\nceptable performance level. Without it, we can’t determine if a complex \\nmodel’s performance justifies its added complexity. \\nWe\\'ll use logistic regression with bag of words as our baseline. This pairing \\nhas proven effective for document classification. Implementation will use \\nscikit-learn, an open-source library that streamlines the training and evalua-\\ntion of traditional “shallow” machine learning models. \\n5.3.1. Baseline Emotion Classifier \\nFirst, we install scikit-learn: \\n$ pip3 install scikit-learn \\nNow, let’s load the data and prepare it for machine learning:7 \\n \\n7 We will load the data from the book’s website to ensure it remains accessible. The da-\\ntaset’s original source is https://huggingface.co/datasets/dair-ai/emotion. It was first used \\nin Saravia et al., “CARER: Contextualized Affect Representations for Emotion'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 158}, page_content='158 \\n \\nrandom.seed(42) ➊ \\n \\ndata_url = \"https://www.thelmbook.com/data/emotions\" \\nX_train_text, y_train, X_test_text, y_test = download_and_spl\\nit_data( \\n    data_url, test_ratio=0.1 \\n) ➋ \\nThe \\nfunction \\ndownload_and_split_data \\n(defined \\nin \\nthe \\nthelm-\\nbook.com/nb/5.1 notebook) downloads a compressed dataset from a specified \\nURL, extracts the training examples, and splits the dataset into training and \\ntest partitions. The test_ratio parameter in line ➋ specifies the fraction of \\nthe dataset to reserve for testing. Setting a seed in ➊ ensures that the random \\nshuffle in line ➋ produces the same result on every execution for reproducibil-\\nity. \\nWith the data loaded and split into training and test sets, we transform it into \\na bag-of-words: \\nfrom sklearn.feature_extraction.text import CountVectorizer \\n \\nvectorizer = CountVectorizer(max_features=10_000, binary=True\\n) \\nX_train = vectorizer.fit_transform(X_train_text) \\nX_test = vectorizer.transform(X_test_text) \\nCountVectorizer’s fit_transform method converts training data into the \\nbag-of-words format. max_features limits vocabulary size, and binary de-\\ntermines whether features represent a word’s presence (True) or count \\n(False). The subsequent transform converts the test data into a bag-of-words \\nrepresentation using the vocabulary built using training data. This approach \\nprevents data leakage—where information from the test set inadvertently in-\\nfluences the machine learning process. Maintaining this separation between \\ntraining and test data is crucial, as any leakage would compromise the model’s \\nability to generalize to truly unseen examples. \\n \\nRecognition,” Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-\\nguage Processing, 2018.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 159}, page_content='159 \\nThe logistic regression implementation in scikit-learn accepts labels as strings, \\nso there is no need to convert them to numbers. The library handles the con-\\nversion automatically. \\nNow, let’s train a logistic regression model: \\nfrom sklearn.linear_model import LogisticRegression \\nfrom sklearn.metrics import accuracy_score \\n \\nmodel = LogisticRegression(random_state=42, max_iter=1000) \\nmodel.fit(X_train, y_train) # Model is trained here \\n \\ny_train_pred = model.predict(X_train) \\ny_test_pred = model.predict(X_test) \\n \\ntrain_accuracy = accuracy_score(y_train, y_train_pred) \\ntest_accuracy = accuracy_score(y_test, y_test_pred) \\n \\nprint(f\"Training accuracy: {train_accuracy * 100:.2f}%\") \\nprint(f\"Test accuracy: {test_accuracy * 100:.2f}%\") \\nOutput: \\nTraining accuracy: 0.9854 \\nTest accuracy: 0.8855 \\nThe LogisticRegression object is first created. Its fit method, called next, \\ntrains the model8 on the training data. Afterward, the model predicts outcomes \\nfor both the training and test sets, and the accuracy for each is calculated. \\nThe random_state parameter in LogisticRegression sets the seed for the \\nrandom number generator. The max_iter parameter limits the solver to a \\nmaximum of 1000 iterations. \\n \\n8 In reality, scikit-learn trains a model slightly different from classical logistic regression; it \\nuses softmax with cross-entropy loss instead of using the sigmoid function and binary \\ncross-entropy. This approach generalizes logistic regression to multiclass classification \\nproblems.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 160}, page_content='160 \\n \\nA solver is the algorithm that optimizes a model’s parameters. It works \\nlike gradient descent but might use different techniques to improve \\nefficiency, handle constraints, or ensure numerical stability. In \\nLogisticRegression, the default solver is lbfgs (Limited-memory \\nBroyden–Fletcher–Goldfarb–Shanno). This algorithm performs well \\nwith small to medium datasets and suits loss functions such as logistic \\nloss. Setting max_iter = 1000 ensures the solver has enough itera-\\ntions to converge. \\nThe accuracy metric calculates the proportion of correct predictions out of all \\npredictions: \\nAccuracy = Number of correct predictions\\nTotal number of predictions  \\nAs you can see, the model overfits: it performs almost perfectly on the training \\ndata but significantly worse on the test data. To address this, we can adjust the \\nhyperparameters of our algorithm. Let’s try incorporating bigrams and in-\\ncrease the vocabulary size to 20,000: \\nvectorizer = CountVectorizer(max_features=20_000, \\nngram_range=(1, 2)) \\nThis adjustment leads to slight improvement on the test set, but it still falls \\nshort compared to the training set performance: \\nTraining accuracy: 0.9962 \\nTest accuracy: 0.8910 \\nNow that we see a simple approach achieves a test accuracy of 0.8910, any \\nmore complex solution must outperform this baseline. If it performs worse, we \\nwill know that our implementation likely contains an error. \\nLet’s finetune GPT-2 to generate emotion labels as text. This approach is easy \\nto implement since no additional classification output layer is needed. Instead, \\nthe model is trained to output labels as regular words, which, depending on \\nthe tokenizer, may span multiple tokens. \\n5.3.2. Emotion Generation \\nFirst, we get the data, model, and tokenizer:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 161}, page_content='161 \\nfrom transformers import AutoTokenizer, AutoModelForCausalLM \\n \\nset_seed(42) \\ndata_url = \"https://www.thelmbook.com/data/emotions\" \\nmodel_name = \"openai-community/gpt2\" \\n \\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() els\\ne \"cpu\") \\n \\ntokenizer = AutoTokenizer.from_pretrained(model_name) ➊ \\ntokenizer.pad_token = tokenizer.eos_token ➋ \\n \\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(d\\nevice) ➌ \\n \\nnum_epochs, batch_size, learning_rate = get_hyperparameters() \\n \\ntrain_loader, test_loader = download_and_prepare_data( \\n    data_url, tokenizer, batch_size \\n) \\nThe AutoModelForCausalLM class from the transformers library, used in \\nline ➌, automatically loads a pretrained autoregressive language model. Line \\n➊ loads the pretrained tokenizer. The tokenizer used in GPT-2 does not include \\na padding token. Therefore, in line ➋, we set the padding token by reusing the \\nend-of-sequence token. \\nNow, we set up the training loop: \\nfor epoch in range(num_epochs): \\n    for input_ids, attention_mask, labels in train_loader: \\n        input_ids = input_ids.to(device) \\n        attention_mask = attention_mask.to(device) ➊ \\n        labels = labels.to(device) \\n        outputs = model( \\n            input_ids=input_ids, \\n            labels=labels, \\n            attention_mask=attention_mask \\n        )'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 162}, page_content='162 \\n \\n        outputs.loss.backward() \\n        optimizer.step() \\n        optimizer.zero_grad() \\nThe attention_mask  in line ➊ is a binary tensor showing which tokens in \\nthe input are actual data and which are padding. It has 1s for real tokens and \\n0s for padding tokens. This mask is different from the causal mask, which \\nblocks positions from attending to future tokens. \\nLet’s illustrate input_ids, labels, and attention_mask for a batch of two \\nsimple examples: \\nText \\nEmotion \\nI feel very happy joy \\nSo sad today \\nsadness \\nWe convert these examples into text completion tasks by adding a task defini-\\ntion and solution: \\nTable 5.1: Text completion template. \\nTask \\nSolution \\nPredict emotion: I feel very happy\\\\nEmotion: joy \\nPredict emotion: So sad today\\\\nEmotion: \\nsadness \\nIn the table above, “\\\\n” denotes a new line character, while “\\\\nEmotion:” \\nmarks the boundary between the task description and the solution. This for-\\nmat, while optional, helps the model use its pretrained understanding of text. \\nThe sole new ability to be learned during finetuning is generating one of six \\noutputs: sadness, joy, love, anger, fear, or surprise—no other outputs. \\nLLMs gained emotion classification skills during pretraining partly be-\\ncause of the widespread use of emojis online. Emojis acted as labels \\nfor the text around them. \\nAssuming a simple tokenizer that splits strings by spaces and assigns unique \\nIDs to each token, here’s a hypothetical token-to-ID mapping: \\nToken \\nID Token \\nID \\nPredict \\n1 So \\n8'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 163}, page_content='163 \\nToken \\nID Token \\nID \\nemotion: \\n2 sad \\n9 \\nI \\n3 today \\n10 \\nfeel \\n4 joy \\n11 \\nvery \\n5 sadness \\n12 \\nhappy \\n6 [EOS] \\n0 \\n\\\\nEmo-\\ntion: \\n7 [PAD] \\n−1 \\nThe special [EOS] token indicates the end of generation, while [PAD] serves \\nas a padding token. The following examples show how texts are converted to \\ntoken IDs: \\nText \\nToken IDs \\nPredict emotion: I feel very happy\\\\nEmotion: [1, 2, 3, 4, 5, 6, 7] \\njoy \\n[11] \\nPredict emotion: So sad today\\\\nEmotion: \\n[1, 2, 8, 9, 10, 7] \\nsadness \\n[12] \\nWe then concatenate the input tokens with the completion tokens and append \\nthe [EOS] token so the model learns to stop generating once the emotion label \\ngeneration is completed. The input_ids tensor contains these concatenated \\ntoken IDs. The labels tensor is made by replacing all input text tokens with \\n−100 (a special masking value), while keeping the actual token IDs for the \\ncompletion and [EOS] tokens. This ensures the model only computes loss on \\npredicting the completion tokens, not on reproducing the input text. \\nThe value −100 is a special token ID in PyTorch (and similar frame-\\nworks) used to exclude specific positions during loss computation. \\nWhen finetuning language models, this ensures the model concen-\\ntrates on predicting tokens for the desired output (the “solution”) ra-\\nther than the tokens in the input (the “task”). \\nHere’s the resulting table:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 164}, page_content='164 \\n \\nText \\ninput_ids \\nlabels \\nPredict emotion: I feel \\nvery happy\\\\nEmotion: \\njoy \\n[1, 2, 3, 4, \\n5, 6, 7, 11, 0] \\n[-100, -100, -100, -100, \\n-100, -100, -100, 11, 0] \\nPredict emotion: So \\nsad today\\\\nEmotion: \\nsadness \\n[1, 2, 8, 9, \\n10, 7, 12, 0] \\n[-100, -100, -100, -100, \\n-100, -100, 12, 0] \\nTo form a batch, all sequences must have the same length. The longest se-\\nquence has 9 tokens (from the first example), so we pad the shorter sequences \\nto match that length. Here’s the final table showing how the input_ids, la-\\nbels, and attention_mask are adjusted after padding: \\ninput_ids \\nlabels \\nattention_mask \\n[1, 2, 3, 4, 5, \\n6, 7, 11, 0] \\n[-100, -100, -100, -100, -100, \\n-100, -100, 11, 0] \\n[1, 1, 1, 1, 1, \\n1, 1, 1, 1] \\n[1, 2, 8, 9, 10, \\n7, 12, 0, -1] \\n[-100, -100, -100, -100, -100, \\n-100, 12, 0, -100] \\n[1, 1, 1, 1, 1, \\n1, 1, 1, 0] \\nIn input_ids, all sequences have a length of 9 tokens. The second example is \\npadded with the [PAD] token (ID −1). In the attention_mask, real tokens \\nare marked as 1, while padding tokens are marked as 0. \\nThis padded batch is now ready for the model to handle. \\nAfter finetuning the model with num_epochs = 2, batch_size = 16, and \\nlearning_rate = 0.00005, it achieves a test accuracy of 0.9415. This is more \\nthan 5 percentage points higher than the baseline result of 0.8910 obtained \\nwith logistic regression. \\nWhen finetuning, a smaller learning rate is often used to avoid large \\nchanges to the pretrained weights. This helps retain the general \\nknowledge from pretraining while adjusting to the new task. A com-\\nmon choice is 0.00005 (5 × 106@), as it often works well in practice. \\nHowever, the best value depends on the specific task and model. \\nThe full code for supervised finetuning of an LLM is available in the thelm-\\nbook.com/nb/5.2 notebook. You can adapt this code for any text generation \\ntask by updating the data files (while keeping the same JSON format) and'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 165}, page_content='165 \\nadjusting Task and Solution in Table 5.1 with text relevant to the specific busi-\\nness problem. \\nLet’s see how this code can be adapted for finetuning for a general instruction-\\nfollowing task. \\n5.3.3. Finetuning to Follow Instructions \\nWhile similar to the emotion generation task, let’s quickly review the specifics \\nof finetuning a large language model to follow arbitrary instructions. \\nWhen finetuning a language model for instruction-following, the first step is \\nchoosing a prompting format or prompting style. For emotion generation, \\nwe used this format: \\nPredict emotion: {text} \\nEmotion: {emotion} \\nThis format allows the LLM to see where the Task part ends (“\\\\nEmotion:”) \\nand the Solution starts. When we finetune for a general-purpose instruction \\nfollowing, we cannot use “\\\\nEmotion:” as a separator. We need a more general \\nformat. Since first open-weight models were introduced, many prompting for-\\nmats were used by various people and organizations. Below, there are only \\ntwo of them, named after famous LLMs using these formats: \\nVicuna: \\nUSER: {instruction} \\nASSISTANT: {solution} \\nAlpaca: \\n### Instruction: \\n{instruction} \\n \\n### Response: \\n{solution} \\nChatML (chat markup language) is a prompting format used in many popular \\nfinetuned LLMs. It provides a standardized way to encode chat messages, in-\\ncluding the role of the speaker and the content of the message.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 166}, page_content=\"166 \\n \\nThe format uses two tags: <|im_start|> to indicate the start of a message \\nand <|im_end|> to mark its end. A basic ChatML message structure looks like \\nthis: \\n<|im_start|>{role} \\n{message} \\n<|im_end|> \\nThe message is either an instruction (question) or a solution (answer). The \\nrole is usually one of the following: system, user, and assistant. For ex-\\nample: \\n<|im_start|>system \\nYou are a helpful assistant. \\n<|im_end|> \\n<|im_start|>user \\nWhat is the capital of France? \\n<|im_end|> \\n<|im_start|>assistant \\nThe capital of France is Paris. \\n<|im_end|> \\nThe user role is the person who asks questions or gives instructions. The as-\\nsistant role is the chat LM providing responses. The system role specifies \\ninstructions or context for the model’s behavior. The system message, known \\nas the system prompt, can include private details about the user, like their \\nname, age, or other information useful for the LLM-based application. \\nThe prompting format has little impact on the quality of a finetuned model \\nitself. However, when working with a model finetuned by someone else, you \\nneed to know the format used during finetuning. Using the wrong format could \\naffect the quality of the model’s outputs. \\nAfter transforming the training data into the chosen prompting format, the \\ntraining process uses the same code as the emotion generation model. You can \\nfind the complete code for instruction finetuning an LLM in the thelm-\\nbook.com/nb/5.3 notebook. \\nThe dataset I used has about 500 examples, generated by a state-of-the-art \\nLLM. While this may not be enough for high-quality instruction following, \\nthere's no standard approach for building an ideal instruction finetuning da-\\ntaset. Online datasets vary widely, from thousands to millions of examples of\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 167}, page_content='167 \\nvarying quality. Still, some experiments suggest that a carefully selected set of \\ndiverse examples, even as small as 1,000, can enable strong instruction-follow-\\ning in a sufficiently large pretrained language model, as Meta’s LIMA model \\ndemonstrated. \\nA consensus among the practitioners is that the quality, not quantity, of exam-\\nples is crucial for achieving state-of-the-art results in instruction finetuning. \\nThe training examples can be found in this file: \\ndata_url = \"https://www.thelmbook.com/data/instruct\" \\nIt has the following structure: \\n... \\n{\"instruction\": \"Translate \\'Good night\\' into Spanish.\", \"solu\\ntion\": \"Buenas noches\"} \\n{\"instruction\": \"Name primary colors.\", \"solution\": \"Red, blu\\ne, yellow\"} \\n... \\nThe instructions and examples used during finetuning fundamentally \\nshape a model\\'s behavior. Models exposed to polite or cautious re-\\nsponses tend to mirror those traits. Through finetuning, models can \\neven be trained to consistently generate falsehoods. Users of third-\\nparty finetuned models should watch for biases introduced in the pro-\\ncess. “Unbiased” models often simply have biases that serve certain \\ninterests. \\nTo understand the impact of instruction finetuning, let’s first see how a pre-\\ntrained model handles instructions without any special training. Let’s first use \\na pretrained GPT-2: \\nfrom transformers import AutoTokenizer, AutoModelForCausalLM \\nimport torch \\n \\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() els\\ne \"cpu\") \\n \\ntokenizer = AutoTokenizer.from_pretrained(\"openai-community/g\\npt2\")'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 168}, page_content='168 \\n \\ntokenizer.pad_token = tokenizer.eos_token \\n \\nmodel = AutoModelForCausalLM.from_pretrained(\"openai-communit\\ny/gpt2\").to(device) \\n \\ninstruction = \"Who is the President of the United States?\" \\ninputs = tokenizer(instruction, return_tensors=\"pt\").to(devic\\ne) \\n \\noutputs = model.generate( \\n    input_ids=inputs[\"input_ids\"], \\n    attention_mask=inputs[\"attention_mask\"], \\n    max_new_tokens=32, \\n    pad_token_id=tokenizer.pad_token_id \\n) \\n \\ngenerated_text = tokenizer.decode(outputs[0], skip_special_to\\nkens=True) \\nprint(generated_text) \\nOutput: \\nWho is the President of the United States? \\n \\nThe President of the United States is the President of the Un\\nited States. \\n \\nThe President of the United States is the President of the Un\\nited States. \\nAgain, like google/gemma-2-2b, the model exhibits sentence repetition. Now, \\nlet’s look at the output after finetuning on our instruction dataset. The infer-\\nence code for an instruction-finetuned model must follow the prompting for-\\nmat used during finetuning. The build_prompt method applies the ChatML \\nprompting format to our instruction: \\ndef build_prompt(instruction, solution = None): \\n    wrapped_solution = \"\" \\n    if solution: \\n        wrapped_solution = f\"\\\\n{solution}\\\\n<|im_end|>\" \\n    return f\"\"\"<|im_start|>system'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 169}, page_content='169 \\nYou are a helpful assistant. \\n<|im_end|> \\n<|im_start|>user \\n{instruction} \\n<|im_end|> \\n<|im_start|>assistant\"\"\" + wrapped_solution \\nThe same build_prompt function is used for both training and testing. During \\ntraining, it takes both instruction and solution as input. During testing, it \\nonly receives instruction. \\nNow, let’s define the function that generates text: \\ndef generate_text(model, tokenizer, prompt, max_new_tokens=10\\n0): \\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(mod\\nel.device) \\n     \\n    end_tokens = tokenizer.encode(\"<|im_end|>\", add_special_t\\nokens=False) ➊ \\n     \\n    stopping = [EndTokenStoppingCriteria(end_tokens, model.de\\nvice)] ➋ \\n     \\n    output_ids = model.generate( \\n        input_ids=input_ids[\"input_ids\"], \\n        attention_mask=input_ids[\"attention_mask\"], \\n        max_new_tokens=max_new_tokens, \\n        pad_token_id=tokenizer.pad_token_id, \\n        stopping_criteria=stopping \\n    )[0] \\n \\n    generated_ids = output_ids[input_ids[\"input_ids\"].shape[1\\n]:] ➌ \\n    generated_text = tokenizer.decode(generated_ids).strip() \\n    return generated_text \\nLine ➊ encodes the <|im_end|> tag into token IDs which will be used to indi-\\ncate the end of generation. Line ➋ sets up a stopping criterion using the'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 170}, page_content='170 \\n \\nEndTokenStoppingCriteria class (defined below), ensuring the generation \\nhalts when end_tokens appear. Line ➌ slices the generated tokens to remove \\nthe input prompt, leaving only the newly generated text. \\nThe EndTokenStoppingCriteria class defines the signal to stop generating \\ntokens: \\nfrom transformers import StoppingCriteria \\nclass EndTokenStoppingCriteria(StoppingCriteria): \\n    def __init__(self, end_tokens, device): \\n        self.end_tokens = torch.tensor(end_tokens).to(device) \\n➊ \\n         \\n    def __call__(self, input_ids, scores): \\n        do_stop = [] \\n        for sequence in input_ids: ➋ \\n            if len(sequence) >= len(self.end_tokens): \\n                last_tokens = sequence[-len(self.end_tokens):\\n] ➌ \\n                do_stop.append(torch.all(last_tokens == self.\\nend_tokens)) ➍ \\n            else: \\n                do_stop.append(False) \\n        return torch.tensor(do_stop, device=input_ids.device) \\nIn the constructor: \\n• Line ➊ converts the end_tokens list into a PyTorch tensor and moves \\nit to the specified device. This ensures the tensor is on the same device \\nas the model. \\nIn the __call__ method, line ➋ loops through the generated sequences in the \\nbatch. For each: \\n• Line ➌ takes the last len(end_tokens) tokens and stores them in \\nlast_tokens. \\n• Line ➍ checks if last_tokens match end_tokens. If they do, True is \\nadded to the do_stop list, which tracks whether to stop generation for \\neach sequence in the batch.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 171}, page_content='171 \\nThis is how we call the inference for a new instruction: \\ninput_text = \"Who is the President of the United States?\" \\nprompt = build_prompt(input_text) \\ngenerated_text = generate_text(model, tokenizer, prompt) \\nprint(generated_text.replace(\"<|im_end|>\", \"\").strip()) \\nOutput: \\nGeorge W. Bush \\nSince GPT-2 is a relatively small language model and wasn’t finetuned on re-\\ncent facts, this confusion about presidents isn’t surprising. What matters here \\nis that the finetuned model now interprets the instruction as a question and \\nresponds accordingly. \\n5.4. Sampling From Language Models \\nTo generate text with a language model, we convert the output logits into to-\\nkens. Greedy decoding, which selects the highest probability token at each \\nstep, is effective for tasks like math or factual questions that demand precision. \\nHowever, many tasks benefit from randomness. Brainstorming story ideas, for \\ninstance, improves with diverse outputs. Debugging code can gain from alter-\\nnative suggestions when the first attempt fails. Even in summarization or trans-\\nlation, sampling helps explore equally valid phrasings when the model is un-\\ncertain. \\nTo address this, we sample from the probability distribution instead of always \\nchoosing the most likely token. Different techniques allow us to control how \\nmuch randomness to introduce. \\nLet’s explore some of these techniques. \\n5.4.1. Basic Sampling with Temperature \\nThe simplest approach converts logits to probabilities using the softmax func-\\ntion with a temperature parameter 𝑇: \\nPr(𝑗) =\\nexpC𝑜(2)/𝑇D\\n∑\\nexp\\nZ\\n:%\"\\n(𝑜(:)/𝑇) \\nwhere 𝑜(2) represents the logit for token 𝑗, Pr(𝑗) gives its resulting probability, \\nand 𝑉 denotes the vocabulary size. The temperature 𝑇 determines the sharp-\\nness of the probability distribution:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 172}, page_content='172 \\n \\n• At 𝑇= 1, we obtain standard softmax probabilities. \\n• As 𝑇→0, the distribution focuses on the highest probability tokens. \\n• As 𝑇→∞, the distribution approaches uniformity. \\nFor example, if we have logits [4,2,0]1 for tokens “cat”, “dog”, and “bird” (as-\\nsuming only three words in the vocabulary), here’s how different temperatures \\naffect the probabilities: \\n𝑇 \\nProbabilities \\nComment \\n0.5 [0.98,0.02,0.00]1 More focused on “cat” \\n1.0 [0.87,0.12,0.02]1 Standard softmax \\n2.0 [0.67,0.24,0.09]1 More evenly distributed \\nTemperature controls the balance between creativity and determinism. Low \\nvalues (0.1–0.3) produce focused, precise outputs, suitable for tasks like fac-\\ntual responses, coding, or math. Moderate values (around 0.7–0.8) offer a mix \\nof creativity and coherence, ideal for conversation or content writing. High \\nvalues (1.5–2.0) add randomness, useful for brainstorming or story genera-\\ntion, though coherence may drop. Extreme values (near 0 or above 2) are \\nrarely used. \\nThese ranges are guidelines; the optimal temperature depends on the model \\nand task and should be determined through experimentation. \\nGiven the vocabulary and probabilities, this Python function returns the sam-\\npled token: \\nimport numpy as np \\n \\ndef sample_token(probabilities, vocabulary): \\n    if len(probabilities) != len(vocabulary): ➊ \\n        raise ValueError(\"Mismatch between the two inputs\\' sizes\\n.\") \\n \\n    if not np.isclose(sum(probabilities), 1.0, rtol=1e-5): ➋ \\n        raise ValueError(\"Probabilities must sum to 1.\") \\n \\n    return np.random.choice(vocabulary, p=probabilities) ➌'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 173}, page_content='173 \\nThe function performs two checks before sampling. Line ➊ ensures there is one \\nprobability for each token in the vocabulary. Line ➋ confirms the probabilities \\nsum to 1, allowing for a small tolerance due to floating-point precision. Once \\nthese validations pass, line ➌ handles the sampling. It selects a token from the \\nvocabulary based on the probabilities, so a token with a 0.7 probability is cho-\\nsen roughly 70% of the time when the function is run repeatedly. \\n5.4.2. Top-𝒌 Sampling \\nWhile temperature helps control randomness, it allows sampling from the en-\\ntire vocabulary, including very unlikely tokens that the model assigns ex-\\ntremely low probabilities to. Top-k sampling addresses this by limiting the \\nsampling pool to the 𝑘 most likely tokens as follows: \\n1) Sort tokens by probability, \\n2) Keep only the top 𝑘 tokens, \\n3) Renormalize their probabilities to sum to 1, \\n4) Sample from this reduced distribution. \\nWe can update sample_token to support both temperature and top-𝑘 sam-\\npling: \\ndef sample_token(logits, vocabulary, temperature=0.7, \\ntop_k=50): \\n    if len(logits) != len(vocabulary): \\n        raise ValueError(\"Mismatch between logits and vocabulary \\nsizes.\") \\n    if temperature <= 0: \\n        raise ValueError(\"Temperature must be positive.\") \\n    if top_k < 1: \\n        raise ValueError(\"top_k must be at least 1.\") \\n    if top_k > len(logits): \\n        raise ValueError(\"top_k must be at most len(logits).\") \\n \\n    logits = logits / temperature ➊ \\n    cutoff = np.sort(logits)[-top_k] ➋ \\n    logits[logits < cutoff] = float(\"-inf\") ➌ \\n \\n    probabilities = np.exp(logits - np.max(logits)) ➍'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 174}, page_content='174 \\n \\n    probabilities /= probabilities.sum() ➎ \\n \\n    return np.random.choice(vocabulary, p=probabilities) \\nThe function begins by validating inputs: ensuring logits match the vocabulary \\nsize, temperature is positive, top-k is at least 1, and top-𝑘 does not exceed the \\nvocabulary size. Line ➊ scales the logits by the temperature. Line ➋ determines \\nthe top-𝑘 cutoff by sorting the logits and selecting the 𝑘th largest value. Line ➌ \\ndiscards less likely tokens by setting logits below the cutoff to negative infinity. \\nLine ➍ converts the remaining logits into probabilities using a numerically sta-\\nble softmax. Line ➎ ensures the probabilities sum to 1. \\nSubtracting np.max(logits) before exponentiating avoids numerical \\noverflow. Large logits can produce excessively large exponentials. \\nShifting the largest logit to 0 keeps values stable while preserving their \\nrelative proportions. \\nThe value of 𝑘 depends on the task. Low values (5–10) focus on the most likely \\ntokens, improving accuracy and consistency, which suits factual responses and \\nstructured tasks. Mid-range values (20–50) balance variation and coherence, \\nmaking them good defaults for general writing and dialogue. High values \\n(100–500) allow more diversity, useful for creative tasks. These ranges are \\npractical guidelines, but the best 𝑘 depends on the model, vocabulary size, and \\napplication. Very low values (below 5) can be too limiting, while extremely \\nhigh values (over 500) rarely improve quality. Experimentation is necessary to \\nfind the best setting. \\n5.4.3. Nucleus (Top-p) Sampling \\nNucleus sampling, or top-p sampling, takes a different approach to token \\nselection. Instead of using a fixed number of tokens, it selects the smallest \\ngroup of tokens whose cumulative probability exceeds a threshold 𝑝. \\nHere’s how it works for 𝑝= 0.9: \\n1) Rank tokens by probability, \\n2) Add tokens to the subset until their cumulative probability surpasses \\n0.9, \\n3) Renormalize the probabilities of this subset, \\n4) Sample from the adjusted distribution.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 175}, page_content='175 \\nThis method adapts to the context. It might select just a few tokens for highly \\nfocused distributions or many tokens when the model is less certain. \\nIn practice, these three methods are often used together in the following se-\\nquence: \\n1. Temperature scaling (e.g., 𝑇 = 0.7) adjusts the randomness by sharp-\\nening or softening the probabilities of tokens. \\n2. Top-k filtering (e.g., 𝑘 = 50) limits the sampling pool to the 𝑘 most \\nprobable tokens, ensuring computational efficiency and preventing ex-\\ntremely low-probability tokens from being considered. \\n3. Top-p filtering (e.g., 𝑝 = 0.9) further refines the sampling pool by se-\\nlecting the smallest set of tokens whose cumulative probability meets \\nthe threshold 𝑝. \\n5.4.4. Penalties \\nModern language models use penalty parameters alongside temperature and \\nfiltering methods to manage text diversity and quality. These penalties help \\navoid issues such as repeated words, overused tokens, and generation loops. \\nThe frequency penalty adjusts token probabilities based on how often they’ve \\nappeared in the generated text so far. When a token appears multiple times, \\nits probability is reduced proportionally to its appearance count. The penalty \\nis applied by subtracting a scaled version of the token’s count from its logits \\nbefore the softmax: \\n𝑜(2) ←𝑜(2) −𝛼⋅count(𝑗), \\nwhere 𝛼 is the frequency penalty parameter. Higher values (0.8-1.0) decrease \\nthe model’s likelihood to repeat the same line verbatim or getting stuck in a \\nloop. \\nThe presence penalty modifies token probabilities based on whether they ap-\\npear anywhere in the generated text, regardless of count: \\n𝑜(2) ←‰𝑜(2) −𝛾,\\nif token 𝑗 is in generated text,\\n𝑜(2),\\notherwise\\n \\nHere, 𝛾 is the presence penalty parameter. Higher values of 𝛾 (0.7-1.0) in-\\ncrease the model’s likelihood to talk about new topics.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 176}, page_content='176 \\n \\nThe optimal values depend on the specific task. For creative writing, higher \\npenalties encourage novelty. For technical documentation, lower penalties \\nmaintain precision and consistency. \\nThe complete implementation of sample_token that combines temperature, \\ntop-𝑘, top-𝑝, and the two penalties can be found in the thelmbook.com/nb/5.4 \\nnotebook. \\n5.5. Low-Rank Adaptation (LoRA) \\nFinetuning LLMs through adjustment of their billions of parameters requires \\nextensive computational resources and memory, creating barriers for those \\nwith limited infrastructure. \\nLoRA (low-rank adaptation) offers a solution by updating only a small por-\\ntion of parameters. It adds to the model small matrices to capture adjustments \\ninstead of altering the full model. This approach achieves similar performance \\nwith a fraction of the training effort. \\n5.5.1. The Core Idea \\nIn the Transformer, most parameters are found in the weight matrices of self-\\nattention and position-wise MLP layers. Rather than modifying the large \\nweight matrices directly, LoRA introduces two smaller matrices for each. Dur-\\ning finetuning, these smaller matrices are trained to capture the required ad-\\njustments, while the original weight matrices stay “frozen.” \\nConsider a 𝑑× 𝑘 weight matrix 𝐖? in a pretrained model. Instead of updating \\n𝐖? directly during finetuning, we modify the process like this: \\n1. Freeze the original weights: The matrix 𝐖? remains unchanged dur-\\ning finetuning. \\n2. Add two small matrices: Introduce an 𝑑× 𝑟 matrix 𝐀 and an 𝑟× 𝑘 \\nmatrix 𝐁, where 𝑟—referred to as the rank—is an integer much smaller \\nthan both 𝑑 and 𝑘 (e.g., 𝑟= 8). \\n3. Adjust the weights: Compute the adapted weight matrix 𝐖 during \\nfinetuning as: \\n𝐖= 𝐖? + 𝛼\\n𝑟𝛥𝐖= 𝐖? + 𝛼\\n𝑟𝐀𝐁 \\n  Here, 𝛥𝐖= 𝐀𝐁 represents the adjustment to 𝐖?, scaled by the scaling \\nfactor \\nf\\nM.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 177}, page_content='177 \\nThe matrices 𝐀 and 𝐁, together, are called a LoRA adapter. Their product, \\n𝛥𝐖, acts as an update matrix that adjusts the original weights 𝐖? to enhance \\nperformance on a new task. Since 𝐀 and 𝐁 are much smaller than 𝐖?, this \\nmethod significantly reduces the number of trainable parameters. \\nFor example, if 𝐖? has dimensions 1024 × 1024, it would contain over a mil-\\nlion parameters to finetune directly (1,048,576 parameters). With LoRA, we \\nintroduce 𝐀 with dimensions 1024 × 8 (8,192 parameters) and 𝐁 with dimen-\\nsions 8 × 1024 (8,192 parameters). This setup requires only 8,192 + 8,192 =\\n16,384 parameters to be trained. \\nThe adapted weight matrix 𝐖 is used in the layers of the finetuned trans-\\nformer, replacing the original matrix 𝐖? to alter the token embeddings as they \\npass through the transformer blocks. The creation of 𝐖 is illustrated below: \\n \\nThe scaling factor \\nf\\nM controls the size of the weight updates introduced by LoRA \\nduring finetuning. Both 𝑟 and 𝛼 are hyperparameters, with 𝛼 typically set as a \\nmultiple of 𝑟. For example, if 𝑟= 8, 𝛼 might be 16, resulting in a scaling factor \\nof 2. The optimal values for 𝑟 and 𝛼 are found experimentally by assessing the \\nfinetuned LLM’s performance on the test set.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 178}, page_content='178 \\n \\nLoRA is usually applied to the weight matrices in the self-attention layers—\\nspecifically the query, key, and value weight matrices 𝐖X, 𝐖Y, 𝐖Z, and the \\nprojection matrix 𝐖d. It can also be applied to the weight matrices 𝐖\" and \\n𝐖! in the position-wise MLP layers. \\nFinetuning LLMs with LoRA is faster than a full model finetune and uses less \\nmemory for gradients, enabling the finetuning of very large models on limited \\nhardware. \\n5.5.2. Parameter-Efficient Finetuning (PEFT) \\nThe Hugging Face Parameter-Efficient Finetuning (PEFT) library provides a \\nsimple way to implement LoRA in transformer models. Let’s install it first: \\n$ pip3 install peft \\nWe can modify our previous code by incorporating the PEFT library to apply \\nLoRA: \\nfrom peft import get_peft_model, LoraConfig, TaskType \\n \\npeft_config = LoraConfig( \\n    task_type=TaskType.CAUSAL_LM,  # Specify the task type \\n    inference_mode=False,          # Set to False for trainin\\ng \\n    r=8,                           # Set the rank r \\n    lora_alpha=16                  # LoRA alpha \\n) \\n \\nmodel = get_peft_model(model, peft_config) \\nThe LoraConfig object defines the parameters for LoRA finetuning: \\n• task_type specifies the task, which in this case is causal language \\nmodeling, \\n• r is the LoRA adapter rank, \\n• lora_alpha is the scaling factor 𝛼. \\nThe function get_peft_model wraps the original model and integrates LoRA \\nadapters. How does it decide which matrices to augment? PEFT is designed to \\ndetect standard LLM architectures. When finetuning models such as Llama, \\nGemma, Mistral, or Qwen, it automatically applies LoRA to the appropriate \\nlayers. For custom transformers—like the decoder from Chapter 4—you can'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 179}, page_content='179 \\nadd the target_modules parameter to specify which matrices should use \\nLoRA: \\npeft_config = LoraConfig( \\n    #same as above \\n    target_modules=[\"W_Q\",\"W_K\",\"W_V\",\"W_O\"] \\n) \\nNext, we set up the optimizer as usual: \\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning\\n_rate) \\nIn PyTorch, the requires_grad attribute controls whether a tensor tracks op-\\nerations for automatic differentiation. When requires_grad=True, PyTorch \\nkeeps track of all operations on the tensor, enabling gradient computation dur-\\ning the backward pass. To freeze a model parameter (preventing updates dur-\\ning training), set its requires_grad to False: \\nimport torch.nn as nn \\n \\nmodel = nn.Linear(2, 1)  # Linear layer: y = WX + b \\n \\nprint(model.weight.requires_grad) \\nprint(model.bias.requires_grad) \\n \\nmodel.bias.requires_grad = False \\nprint(model.bias.requires_grad) \\nOutput: \\nTrue \\nTrue \\nFalse \\nThe PEFT library ensures that only the LoRA adapter parameters have re-\\nquires_grad=True, keeping all other model parameters frozen. \\nAfter wrapping the model with get_peft_model, the training loop stays the \\nsame. For instance, finetuning GPT-2 on an emotion generation task using \\nLoRA with r=16 and lora_alpha=32 achieves a test accuracy of 0.9420. This \\nis marginally better than the 0.9415 from full finetuning. Generally, LoRA'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 180}, page_content='180 \\n \\ntends to perform slightly worse than full finetuning. However, the outcome \\ndepends on the choice of hyperparameters, dataset size, base model, and task. \\nThe full code for GPT-2 finetuning with LoRA is available in the thelm-\\nbook.com/nb/5.5 notebook. You can customize it for your own tasks by mod-\\nifying the dataset and LoRA settings. \\n5.6. LLM as a Classifier \\nWhen finetuning GPT-2 for emotion prediction, we didn’t turn it into a classi-\\nfier. Instead, it generated the class name as text. While this method works, it’s \\nnot always optimal for classification tasks. A different approach is to train the \\nmodel to produce logits for each emotion class. \\nWe can attach a classification head to a pretrained LLM. This is a fully con-\\nnected layer with a softmax activation mapping logits to class probabilities. \\nIn transformers, there is a class designed to make this easier. Instead of load-\\ning the model with AutoModelForCausalLM, we use AutoModelForSe-\\nquenceClassification: \\nfrom transformers import AutoModelForSequenceClassification \\n \\nmodel = AutoModelForSequenceClassification.from_pretrained( \\n    model_path, num_labels=6 \\n) \\nFor pretrained autoregressive language models, the class maps the embedding \\nof the final (right-most) non-padding token from the last decoder block to a \\nvector with dimensionality matching the number of classes (6 in this case). \\nThe structure of this modification is as follows:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 181}, page_content='181 \\n \\nAs you can see, once the final decoder block processes the input (the second \\nblock in our example), the output embedding 𝐳T,! of the last token is passed \\nthrough the classification head’s weight matrix, 𝐖B. This projection converts \\nthe embedding into logits, one per class. \\nThe parameter tensor 𝐖B is initialized with random values and trained on the \\nlabeled emotions dataset. Training relies on cross-entropy to measure the loss \\nbetween the predicted probability distribution and the one-hot encoded true \\nclass label. This error is backpropagated, updating the weights in both the clas-\\nsification head and the rest of the model. This can be combined with LoRA. \\nAfter finetuning with num_epochs = 8, batch_size = 16, and learn-\\ning_rate = 0.00005, the model reaches a test accuracy of 0.9460. This is \\nslightly better than the 0.9415 accuracy from finetuning the unmodified model \\nto generate class labels as text. The improvement might be more noticeable \\nwith a different base model or dataset. \\nThe code for finetuning GPT-2 as an emotion classifier is available on the wiki \\nin the thelmbook.com/nb/5.6 notebook. It can be easily adapted for any text \\nclassification task by replacing the data in the file while keeping the same JSON \\nformat.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 182}, page_content=\"182 \\n \\n5.7. Prompt Engineering \\nChat language models, or chat LMs, are language models finetuned on dia-\\nlogue examples. This finetuning resembles instruction finetuning but uses \\nmulti-turn conversation inputs, such as those in the ChatML format, with the \\ntargets being the assistant’s responses. \\nDespite its simplicity, the conversational interface allows solving various prac-\\ntical problems. This section explores best practices for using chat LMs to ad-\\ndress such problems known as prompt engineering techniques. \\n5.7.1. Features of a Good Prompt \\nTo get the best results from a chat LM, you need a well-crafted prompt. The \\nkey components of a strong prompt include: \\n1. Situation: Describe why you’re asking for help. \\n2. Role: Define the expert persona the model should emulate. \\n3. Task: Give clear, specific instructions about what the model must do. \\n4. Output format: Explain how you expect the response to be structured, \\nsuch as bullet points, JSON, or code. \\n5. Constraints: Mention any limitations, preferences, or requirements. \\n6. Quality criteria: Define what makes a response satisfactory. \\n7. Examples: Provide few-shot examples of inputs with expected outputs. \\n8. Call to action: Restate the task simply and ask the model to perform it. \\nPutting input-output examples in the prompt is called few-shot prompting or \\nin-context learning. These examples include both positive cases showing de-\\nsired outputs and negative ones demonstrating incorrect responses. Adding \\nexplanations that connect incorrect responses to specific constraints helps \\nmodel understand why they are wrong. \\nHere’s an example of a prompt that includes some of the above elements: \\nSituation: I'm creating a system to analyze insurance claims. \\nIt processes adjuster reports to extract key details for disp\\nlay in a SaaS platform. \\n \\nYour role: Act as a seasoned insurance claims analyst familia\\nr with industry-standard classifications. \\n \\nTask: Identify the type of incident, the primary cause, and t\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 183}, page_content='183 \\nhe significant damages described in the report. \\n \\nOutput format: Return a JSON object with this structure: \\n{ \\n    \"type\": \"string\",      // Incident type \\n    \"cause\": \"string\",     // Primary cause \\n    \"damage\": [\"string\"]   // Major damages \\n}   \\n \\n<examples> \\n    <example> \\n        <input> \\n        Observed two-vehicle accident at an intersection. Ins\\nured\\'s car was hit after the other driver ran a red light. Wi\\ntnesses confirm. The vehicle has severe front-end damage, air\\nbags deployed, and was towed from the scene. \\n        </input> \\n        <output> \\n        { \\n            \"type\": \"collision\", \\n            \"cause\": \"failure to stop at signal\", \\n            \"damage\": [\"front-end damage\", \"airbag deployment\\n\"] \\n        } \\n        </output>   \\n    </example> \\n    <example> \\n        ... \\n    </example> \\n</examples> \\n \\nCall to action: Extract the details from this report: \\n \\n\"Arrived at the scene of a fire at a residential building. Ex\\ntensive damage to the kitchen and smoke damage throughout. Fi\\nre caused by unattended cooking. Neighbors evacuated; no inju\\nries reported.\"'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 184}, page_content='184 \\n \\nSection names such as “Situation,” “Your role,” or “Task” are optional. \\nWhen working on a prompt, keep in mind that the attention mechanism in \\nLLMs has limitations. It might concentrate on certain parts of a prompt while \\noverlooking others. A good prompt strikes a balance between detail and brev-\\nity. Excessive detail can overwhelm the model, while insufficient detail risks \\nleaving gaps that the model may fill with incorrect assumptions. \\nI used XML tags for few-shot examples because they clearly define ex-\\nample boundaries and are familiar to LLMs from pretraining on struc-\\ntured data. Furthermore, chat LM models are often finetuned using \\nconversational examples with XML structures. Using XML isn’t manda-\\ntory though, but could be helpful. \\n5.7.2. Followup Actions \\nThe first solution from a model is often imperfect. User analysis and follow-up \\nare key to getting the most out of a chat LM. Common follow-up actions in-\\nclude: \\n1. Asking the LLM whether its solution contains errors or can be simpli-\\nfied without breaking the constraints. \\n2. Copying the solution and starting a new conversation from scratch with \\nthe same LLM. In this new conversation, the user can ask the model to \\nvalidate the solution as if it were “provided by an expert,” without re-\\nvealing it was generated by the same model. \\n3. Using a different LLM to review or enhance the solution. \\n4. For code outputs, running the code in the execution environment, ana-\\nlyzing the results, and giving feedback to the model. If code fails, the \\nfull error message and stack traceback can be shared with the model. \\nWhen working with the same chat LM for follow-ups, especially in tasks like \\ncoding or handling complex structured outputs, it’s generally a good idea to \\nstart fresh after three-five exchanges. This recommendation comes from two \\nkey observations: \\n1. Chat LMs are typically finetuned using examples of short conversations. \\nCreating long, high-quality conversations for finetuning is both difficult \\nand costly, so the training data often lacks examples of long'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 185}, page_content='185 \\ninteractions focused on problem solving. As a result, the model per-\\nforms better with shorter exchanges. \\n2. Long contexts can cause errors to accumulate. In the self-attention \\nmechanism, the softmax is applied over many positions to compute \\nweights for combining value vectors. As the context length increases, \\ninaccuracies build up, and the model’s “focus” may shift to irrelevant \\ndetails or earlier mistakes. \\nWhen starting fresh, it’s important to update the initial prompt with key details \\nfrom earlier follow-ups. This helps the model avoid repeating previous mis-\\ntakes. By consolidating the relevant information into a clear, concise starting \\npoint, you ensure the model has the context it needs without relying on the \\nlong and noisy history of the prior conversation. \\n5.7.3. Code Generation \\nOne valuable use of chat LMs is generating code. The user describes the desired \\ncode, and the model tries to generate it. As we know, modern LLMs are pre-\\ntrained on vast collections of open-source code across many programming lan-\\nguages. This pretraining allows them to learn syntax and many standard or \\nwidely used libraries. Seeing the same algorithms implemented in different \\nlanguages also enables LLMs to form shared internal representations (like syn-\\nonyms in word2vec), making them generally indifferent to the programming \\nlanguage when reading or creating code. \\nMoreover, much of this code includes comments and annotations, which help \\nthe model understand the code’s purpose—what it is designed to achieve. \\nSources like StackOverflow and similar forums add further value by providing \\nexamples of problems paired with their solutions. The exposure to such data \\ngave LLMs an ability to respond with relevant code. Supervised finetuning im-\\nproved their skill in interpreting user requests and turning them into code. \\nAs a result, LLMs can generate code in nearly any language. For high-quality \\nresults, users must specify in detail what code should do. For example, provid-\\ning a detailed docstring like this: \\nWrite Python code that implements a method with the following \\nspecifications: \\n \\ndef find_target_sum(numbers: list[int], target: int) -> tuple'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 186}, page_content='186 \\n \\n: \\n    \"\"\"Find pairs of indices in a list whose values sum to a \\ntarget. \\n     \\n    Args: \\n        numbers: List of integers to search through. Can be e\\nmpty. \\n        target: Integer sum to find. \\n     \\n    Returns: \\n        Tuple of two distinct indices whose values sum to tar\\nget, \\n        or None if no solution exists. \\n     \\n    Examples: \\n        >>> find_target_sum([2, 7, 11, 15], 9) \\n        (0, 1) \\n        >>> find_target_sum([3, 3], 6) \\n        (0, 1) \\n        >>> find_target_sum([1], 5) \\n        None \\n        >>> find_target_sum([], 0) \\n        None \\n     \\n    Requirements: \\n        - Time complexity: O(n) \\n        - Space complexity: O(n) \\n        - Each index can only be used once \\n        - If multiple solutions exist, return any valid solut\\nion \\n        - All numbers and target can be any valid integer \\n        - Return None if no solution exists \\n    \"\"\" \\nProviding a highly detailed docstring can sometimes feel as time-consuming as \\ncoding the function itself. A less detailed description might seem more practi-\\ncal, but this increases the likelihood of the generated code not fully meeting \\nuser needs. In such cases, users can review the output and refine their instruc-\\ntions with additional requests or constraints.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 187}, page_content='187 \\nBy the way, the book’s official website, thelmbook.com, was created \\nentirely through collaboration with an LLM. While it wasn’t generated \\nperfectly on the first try, through iterative feedback, multiple conver-\\nsation restarts, and switching between different chat LLMs when \\nneeded, I refined every element you see—from the graphics to the an-\\nimations—until they met my vision. \\nLanguage models can generate functions, classes, or even entire applications. \\nHowever, the chance of success decreases as the level of abstraction increases. \\nIf the problem resembles model’s training data, the model performs well with \\nminimal input. However, for novel or unique business or engineering prob-\\nlems, detailed instructions are crucial for good results. \\nIf you decide to use a brief prompt to save time, ask the model to pose \\nclarifying questions. You can also request it to describe the code it \\nplans to generate first. This allows you to adjust or add details to the \\ninstructions before code is created. \\n5.7.4. Documentation Synchronization \\nA common challenge in software development is keeping documentation syn-\\nchronized with code changes. As codebases evolve, documentation often be-\\ncomes outdated, leading to confusion and reduced maintainability. LLMs offer \\nan automated solution to this problem through integration with version con-\\ntrol systems. \\nThe process involves creating a documentation synchronization pipeline that \\nleverages the LLM’s ability to understand both code and natural language. \\nWhen developers stage changes for commit, the pipeline: \\n1. Uses an LLM to analyze the staged differences and identify affected \\ndocumentation files in the project’s documentation directory. The \\nmodel examines code changes and determines which documentation \\nfiles might need updates. \\n2. Both the existing documentation content and staged code changes are \\nthen passed to another LLM call. This second step generates updated \\ndocumentation that reflects the code modifications while maintaining \\nthe existing documentation’s style and structure.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 188}, page_content='188 \\n \\n3. Places the updated documentation in the staging area alongside code \\nchanges. This allows developers to review both code and documenta-\\ntion updates together before committing, ensuring accuracy and main-\\ntaining a single source of truth. \\nThis approach treats documentation as a first-class citizen in the development \\nprocess, ensuring it evolves alongside the code. \\nWhile LLMs can help maintain documentation alignment, they should \\nnot operate autonomously. Human review remains crucial to verify the \\naccuracy of generated documentation updates and ensure they align \\nwith the team’s communication standards. \\nThis pipeline is especially useful for keeping API documentation, architectural \\ndescriptions, and implementation guides up to date. However, like other LLM-\\nbased systems, it must include safeguards against hallucinations. We discuss \\nthis next. \\n5.8. Hallucinations \\nA major challenge with LLMs is their tendency to produce content that seems \\nplausible but is factually incorrect. These inaccuracies, called hallucinations, \\ncreate problems for using LLMs in production systems where reliability and \\naccuracy are required. \\n5.8.1. Reasons for Hallucinations \\nHallucinations in LLMs are by design. As we know, these models are optimized \\nto predict the next token that fits the context, not to ensure factual accuracy. \\nDuring pretraining, they learn to generate coherent text by following language \\npatterns. However, no training dataset can cover every fact. When the model \\nfaces knowledge gaps, it tries to fill them with plausible content based on pat-\\nterns it recognizes. This often results in fabricated details. Here’s an example \\nof hallucinated information from a widely used chat LM:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 189}, page_content=\"189 \\n \\nAs you can imagine, “Blockchain Quantum Neural Network (BQNN)” \\nis not a real concept. The LLM’s two-page explanation, including de-\\ntailed descriptions of how it works, is entirely fabricated. \\nLow quality of training data also contributes to hallucinations. During pretrain-\\ning on large volumes of internet text, models are exposed to both accurate and \\ninaccurate information. They learn these inaccuracies but lack the ability to \\ndifferentiate between truth and falsehood. \\nFinally, LLMs generate text one token at a time. This approach means that \\nerrors in earlier tokens can cascade, leading to increasingly incoherent outputs. \\n5.8.2. Preventing Hallucinations \\nHallucinations cannot be completely avoided, but they can be minimized. A \\npractical way to reduce hallucinations is by grounding the model’s responses \\nin verified information. This is done by including relevant factual context di-\\nrectly in the prompt. For instance, rather than posing an open-ended question, \\nwe can provide specific documents or data for the model to reference and in-\\nstruct the model to only answer based on the provided documents. \\nThis method, called retrieval-augmented generation (RAG), anchors the \\nmodel’s output to verifiable facts. The model still generates text but does so—\\nmost of the time—within the limits of the provided context, which significantly \\nreduces hallucinations. \\nHere's how RAG works: a user submits a query, and the system searches a \\nknowledge base—like a document repository or database—for relevant infor-\\nmation. It uses keyword matching and embedding-based search, where the \\nquery is converted into an embedding vector. Documents with similar\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 190}, page_content='190 \\n \\nembeddings are retrieved using cosine similarity. To handle long documents, \\nthey are split into smaller chunks before embedding. \\nThe retrieved content is added to the prompt alongside the user’s question. \\nThis approach merges the strengths of traditional information retrieval with \\nthe language generation capabilities of LLMs. For example, if a user asks about \\na company’s latest quarterly results, the RAG system would first retrieve the \\nmost recent financial reports and use them to produce the response, avoiding \\nreliance on potentially outdated training data. \\nAnother way to reduce hallucinations is by finetuning the model on reliable, \\ndomain-specific knowledge using unlabeled documents. For instance, a ques-\\ntion-answering system for law firms could be finetuned on legal documents, \\ncase law, and statutes to improve accuracy within the legal domain. This ap-\\nproach is often referred to as domain-specific pretraining. \\nFor critical applications, implementing a multi-step verification workflow can \\nprovide additional protection against hallucinations. This might involve using \\nmultiple models with different architectures or training data to cross-validate \\nresponses and having domain experts review generated content before it’s used \\nin production. \\nHowever, it’s important to recognize that hallucinations cannot be completely \\neliminated with current LLM technology. While we can implement various \\nsafeguards and detection mechanisms, the most robust approach is to design \\nsystems that account for this limitation. \\nFor instance, in a customer service application, an LLM could draft responses, \\nbut human review would be necessary before sending messages containing \\nspecific product details or policy information. Similarly, in a code generation \\nsystem, the model might generate code, but automated tests and human re-\\nview should always occur before deployment. \\nThe potential for hallucinations was notably demonstrated when Air \\nCanada’s customer service chatbot provided incorrect information \\nabout bereavement travel rates to a passenger. The chatbot falsely \\nclaimed that customers could book full-price tickets and later apply for \\nreduced fares, contradicting the airline’s actual policy. When the pas-\\nsenger tried to claim the fare reduction, Air Canada’s denial led to a \\nsmall claims court case, resulting in an $812 CAD (near $565 USD) \\ncompensation order. This case highlights the tangible business'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 191}, page_content='191 \\nconsequences of AI inaccuracies, including financial losses, customer \\nfrustration, and reputational damage. \\nSuccess with LLMs lies in recognizing that hallucinations are an inherent limi-\\ntation of the technology. However, this issue can be managed through thought-\\nful system design, safeguards, and a clear understanding of when and where \\nthese models should be applied. \\n5.9. LLMs, Copyright, and Ethics \\nThe widespread deployment of LLMs has introduced novel challenges in cop-\\nyright law, particularly regarding training data usage and the legal status of \\nAI-generated content. These issues affect both the companies developing LLMs \\nand the businesses building applications with them. \\n5.9.1. Training Data \\nThe first major copyright consideration involves training data. LLMs are \\ntrained on large text datasets that include copyrighted material such as books, \\narticles, and software code. While some claim that this might qualify as fair \\nuse,9 this has not been tested in court. The issue is further complicated by the \\nmodels’ capacity to output protected content. This legal uncertainty has al-\\nready sparked high-profile lawsuits from authors and publishers against AI \\ncompanies, posing risks for businesses using LLM applications. \\nMeta’s decision to withhold its multimodal Llama model from the Eu-\\nropean Union in July 2024 exemplifies the growing tension between \\nAI development and regulatory compliance. Citing concerns over the \\nregion’s “unpredictable” regulatory environment, particularly regard-\\ning the use of copyrighted and personal data for training, Meta joined \\nother tech giants like Apple in limiting AI deployments in European \\n \\n9 Fair use is a U.S. legal doctrine. Other regions handle copyright exceptions differently. \\nThe EU relies on “fair dealing” and specific statutory exceptions, Japan has distinct copy-\\nright limitations, and other countries apply unique rules for permitted uses. This variation \\ncomplicates global LLM deployment, as training data allowed under U.S. fair use might \\nviolate copyright laws elsewhere.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 192}, page_content=\"192 \\n \\nmarkets. This restriction highlights the challenges companies face in \\nbalancing innovation with regional regulations. \\nWhen selecting models for commercial use, companies should review the train-\\ning documentation and license terms. Models trained primarily on public do-\\nmain or properly licensed materials involve lower legal risks. However, the \\nmassive datasets required for effective LLMs make it nearly impossible to avoid \\ncopyrighted material entirely. Businesses need to understand these risks and \\nfactor them into their development strategies. \\nBeyond legal issues, training LLMs on copyrighted material raises ethical con-\\ncerns. Even when legally permissible, using copyrighted works without consent \\nmay appear exploitative, especially if the model outputs compete with the cre-\\nators’ work. Transparency about training data sources and proactive engage-\\nment with creators can help address these concerns. Ethical practices should \\nalso involve compensating creators whose contributions significantly improve \\nthe model, fostering a more equitable system. \\n5.9.2. Generated Content \\nThe copyright status of content generated by LLMs presents challenges that \\ntraditional copyright law cannot easily resolve. Copyright law is built around \\nthe assumption of human authorship, leaving it unclear whether AI-generated \\nworks qualify for protection or who the rightful owner might be. Another issue \\nis that LLMs can sometimes reproduce portions of their training data verbatim, \\nincluding copyrighted material. This ability to generate exact reproductions—\\nbeyond learning abstract patterns—raises serious legal questions. \\nSome businesses address these challenges by using LLMs as assistive tools ra-\\nther than independent creators. For example, a marketing team might use an \\nLLM to draft text, leaving human writers to edit and finalize it. This approach \\nmaintains clearer copyright ownership while leveraging AI's efficiency. Simi-\\nlarly, software developers use LLMs to generate code snippets, which they re-\\nview and integrate into larger systems. By 2024, this practice had grown sig-\\nnificantly—at Google, over 25% of all code was generated by LLMs and then \\nrefined by developers. \\nTo minimize copyright risks in LLM applications, companies often implement \\ntechnical safeguards.\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 193}, page_content='193 \\nOne method involves comparing model outputs against a database of copy-\\nrighted materials to detect verbatim copies. For example, a company may \\nmaintain a repository of copyrighted texts and employ similarity detection \\nmethods—such as cosine similarity or edit distance—to flag outputs that sur-\\npass a defined similarity threshold. \\nHowever, these methods are not foolproof. Paraphrased content can make the \\noutput formally distinct while remaining substantively similar, which auto-\\nmated systems may fail to detect. To handle this, businesses often supplement \\nthese tools with human review to ensure compliance. \\n5.9.3. Open-Weight Models \\nThe copyright status of model weights poses legal questions separate from \\nthose concerning training data or generated outputs. Model weights encode \\npatterns learned during training and could be viewed as derivative works of \\nthe training data. This leads to the question: does sharing weights amount to \\nindirectly redistributing the original copyrighted materials, even in their trans-\\nformed form? Some argue that weights are an abstract transformation and \\nconstitute new intellectual property. Others contend that if weights can repro-\\nduce fragments of the training data, they inherently include copyrighted con-\\ntent and should be treated similarly under copyright law. \\nThis debate carries serious implications for open-source AI development. If \\nmodel weights are classified as derivative works, sharing and distributing mod-\\nels trained on copyrighted data could become legally restricted, even if the \\ntraining process qualifies as fair use. As a result, some organizations have \\nshifted to training models solely on public domain or explicitly licensed con-\\ntent. However, this strategy often limits the effectiveness of the models, as the \\nsmaller, restricted datasets typically lead to reduced performance. \\nAs laws around LLMs evolve, businesses must stay flexible. They may need to \\nadjust workflows as courts define legal boundaries or revise policies as AI-spe-\\ncific legislation appears. Consulting intellectual property lawyers with AI ex-\\npertise can help manage these risks. \\n5.9.4. Broader Ethical Considerations \\nBeyond copyright concerns, LLMs raise significant ethical challenges that affect \\nsociety at large. One fundamental issue is explainability. While LLMs can'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 194}, page_content='194 \\n \\narticulate reasoning for their outputs and provide detailed explanations when \\nasked, this verbal explanation capability differs from true algorithmic trans-\\nparency. The model’s explanations are post-hoc rationalizations—generated \\ntext that sounds plausible but may not reflect the actual computational process \\nthat produced the original output. This creates a unique challenge where the \\nmodel appears transparent while its underlying decision-making process re-\\nmains opaque. This limitation becomes particularly significant in high-stakes \\napplications like healthcare or legal services. \\nThe question of bias presents another challenge. LLMs trained on internet data \\ninevitably absorb societal biases present in their training data. These models \\ncan perpetuate or amplify discriminatory patterns in areas such as gender, \\nrace, age, and cultural background. For instance, they might generate different \\nresponses to equivalent prompts that only differ in demographic details, or \\nproduce content that reinforces stereotypes. \\nOrganizations deploying LLMs must implement structured evaluation proto-\\ncols, including automated bias detection across demographic groups and au-\\ndits using standardized test sets. This should include deploying concrete safe-\\nguards like toxic language filters, mandatory human review for high-stakes \\ndecisions, and clear user notifications about AI involvement.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 195}, page_content='195 \\nChapter 6. Further Reading \\nYou’ve learned the core concepts of language modeling throughout this book. \\nThere are many advanced topics to explore on your own, and this final chapter \\nprovides pointers for further study. I’ve chosen topics that represent important \\ncurrent developments in the field, from architectural innovations to security \\nconsiderations. \\n6.1. Mixture of Experts \\nMixture of experts (MoE) is an architectural pattern designed to increase \\nmodel capacity without a proportional rise in cost. Instead of a single position-\\nwise MLP processing all tokens in a decoder block, MoE uses multiple special-\\nized sub-networks called experts. A router network (or gate network) de-\\ncides which tokens are processed by which experts. \\nThe core idea is activating only a subset of experts for each token. This sparse \\nactivation reduces active computations while enabling larger overall parame-\\nter counts. Sparse MoE layers replace traditional MLP layers, using techniques \\nlike top-k routing and load balancing to efficiently assign tokens to experts. \\nThis concept gained attention with the Switch Transformer and has been ap-\\nplied in models such as Mixtral 8x7B, which has 47B total parameters but only \\nactivates about 13B during inference. \\n6.2. Model Merging \\nModel merging combines multiple pretrained models to make use of their \\ncomplementary strengths. Techniques include model soups, SLERP (spherical \\ninterpolation that maintains parameter norms), and task vector algorithms \\nsuch as TIES-Merging and DARE. \\nThese methods generally rely on some architectural similarity or compatibility \\nbetween models. The passthrough method stands out by concatenating layers \\nfrom different LLMs. This approach can create models with unconventional \\nparameter counts (e.g., 13B by merging two 7B models). Such models are of-\\nten called frankenmerges. \\nmergekit is a popular open-source tool for merging and combining language \\nmodels that implements many of these techniques. It provides a flexible con-\\nfiguration system for experimenting with different merging strategies and ar-\\nchitectures.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 196}, page_content='196 \\n \\n6.3. Model Compression \\nModel compression addresses deploying LLMs in resource-limited environ-\\nments by reducing size and computation needs without greatly sacrificing per-\\nformance. Neural networks are often over-parameterized, containing redun-\\ndant units that can be optimized. \\nKey methods include post-training quantization, which lowers parameter \\nprecision (e.g., 32-bit floats to 8-bit integers), quantization-aware training, \\ntraining models at lower precision, such as QLoRA (quantized low-rank adap-\\ntation), unstructured pruning, removing individual weights by importance, \\nstructured pruning, removing components like layers or attention heads, and \\nknowledge distillation, where a smaller “student” model learns from a larger \\n“teacher” model. \\n6.4. Preference-Based Alignment \\nPreference-based alignment methods help align LLMs with user values and \\nintent, so they produce helpful and safe outputs. A widely used approach is \\nreinforcement learning from human feedback (RLHF), where humans rank \\nmodel responses, a reward model is trained on these rankings, and then the \\nLLM is finetuned to optimize for higher reward. \\nAnother approach is constitutional AI (CAI), which uses a set of guiding prin-\\nciples or a “constitution” that the model refers to when producing its output; \\nthe model can self-critique and revise its responses based on these principles. \\nBoth strategies address the problem that LLMs, when trained on vast internet \\ntext, may generate harmful or misaligned responses, but they differ in how \\nthey incorporate human oversight and explicit guidelines. \\n6.5. Advanced Reasoning \\nAdvanced reasoning techniques enable large language models to handle com-\\nplex tasks by (1) training them to generate an explicit chain of thought (CoT) \\nfor step-by-step reasoning and (2) equipping them with function calling ca-\\npabilities to invoke external APIs or tools, thereby addressing limitations of \\nsimple prompt-response patterns. Chain-of-thought reasoning can significantly \\nimprove performance on tasks such as multi-step mathematics and logical in-\\nference, while function calling allows offloading specialized computations to \\nexternal frameworks.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 197}, page_content='197 \\nAdditionally, tree of thought (ToT) extends CoT by exploring multiple reason-\\ning paths in a tree-like structure. Self-consistency further refines reasoning by \\naggregating multiple CoT outputs for the most consistent answer. ReAct (rea-\\nsoning+act) integrates reasoning with action-taking, allowing models to in-\\nteract with environments dynamically. Program-aided language models \\n(PAL) leverage interpreters (e.g., Python) to execute code for precise calcula-\\ntions. \\n6.6. Language Model Security \\nJailbreak attacks and prompt injection are major security vulnerabilities in \\nLLMs. Jailbreaks bypass the model’s safety controls by crafting specific inputs \\nthat trick the model into producing restricted content, often using techniques \\nlike roleplaying as a different character or setting up hypothetical scenarios. \\nFor example, an attacker might prompt the model to act as a pirate to obtain \\ninstructions on illegal activities. \\nIn contrast, prompt injection attacks manipulate how LLM applications com-\\nbine system prompts with user input, allowing attackers to alter the applica-\\ntion’s behavior. For instance, an attacker could insert commands that make the \\napplication execute unauthorized actions. While jailbreaks primarily risk ex-\\nposing harmful or restricted content, prompt injection presents more severe \\nsecurity implications for applications with privileged access, such as those that \\nread emails or execute system commands. \\n6.7. Vision Language Model \\nVision language models (VLMs) integrate an LLM with a vision encoder to \\nhandle both text and images. Unlike traditional models that process modalities \\nin isolation, VLMs excel at multimodal reasoning, enabling them to perform \\na variety of vision tasks by following natural language instructions without \\ntask-specific retraining. The architecture includes three main components: a \\nCLIP-based (contrastive language-image pretraining) vision encoder trained \\non millions of image-text pairs to understand visual content, a cross-attention \\nmechanism that allows the VLM to integrate and reason about visual and tex-\\ntual information, and the language model itself that generates and interprets \\ntext. VLMs are developed through multiple training stages, starting with pre-\\ntraining to align the visual and language components, followed by supervised \\nfinetuning to improve their ability to understand and respond to user prompts.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 198}, page_content='198 \\n \\n6.8. Preventing Overfitting \\nTechniques for preventing overfitting are essential for achieving model gen-\\neralization, ensuring that models perform well not just on training data but \\nalso on new, unseen examples. The primary defense against overfitting is reg-\\nularization, which includes methods like L1 and L2. These techniques add \\nspecific penalty terms—such as the sum of absolute or squared weights—to \\nthe loss function, limiting the size of model parameters and encouraging sim-\\npler models. \\nDropout is a regularization method for neural networks. It works by randomly \\ndeactivating some units during each training step. This encourages the net-\\nwork to develop multiple independent pathways, reducing reliance on specific \\nfeatures. Early stopping prevents overfitting by monitoring validation perfor-\\nmance. Training stops when validation accuracy stops improving or starts to \\ndecline, avoiding the memorization of random noise happening at later \\nepochs. \\nA validation set is similar to the test set in that it is used to evaluate the \\nmodel’s performance on unseen data; however, the key difference is that the \\nvalidation set is used during the training process to tune hyperparameters and \\nmake decisions such as early stopping, while the test set is reserved for final \\nevaluation to measure the model’s performance after training is complete. \\n6.9. Concluding Remarks \\nYou’ve come a long way in understanding language models, from the basic \\nbuilding blocks of machine learning to the inner workings of transformers and \\nthe practical aspects of working with large language models. You now have a \\nsolid technical foundation that lets you not only understand how these models \\nwork but also implement and adapt them for your own purposes. \\nNew architectures, training methods, and applications of language models are \\nemerging. You now have the tools to read research papers, follow technical \\ndiscussions, and evaluate new developments critically. Whether you aim to \\ntrain models or build systems using them, you have the core concepts to pro-\\nceed confidently. \\nI encourage you to stay curious and hands-on—implement the concepts you’ve \\nlearned, experiment with different approaches, and keep up with the latest \\ndevelopments. Consider starting with some of the advanced topics covered in'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 199}, page_content=\"199 \\nthis chapter, but remember that the fundamentals you’ve learned here will \\nserve as your compass in navigating future innovations. \\nA good way of keeping up with the latest developments is to subscribe to the \\nbook’s newsletter. \\n \\nThe book ends here. Remember to check the companion wiki from time to time \\nfor updates on developments in various language modeling areas. Please don’t \\nforget that the book is shared under the read first, buy later principle. So, if \\nyou're reading this as a PDF and don't recall paying for it, you are probably the \\nright person to purchase the book. \\n6.10. More From the Author \\nIf you’re still reading, it likely means you enjoyed the book and are wondering \\nwhat else you can read from this author. I have two more books that will def-\\ninitely enhance your understanding of machine learning and build on the \\nknowledge and intuition you’ve gained about language models: \\n• The Hundred-Page Machine Learning Book offers a concise yet thor-\\nough overview of core machine learning concepts, ranging from funda-\\nmental statistics to advanced algorithms. It’s an excellent companion to \\nthe language modeling material covered here. \\n• Machine Learning Engineering covers the practical aspects of design-\\ning, deploying, and maintaining ML systems at scale. If you’re looking \\nto move beyond experimentation and create robust, real-world ma-\\nchine learning applications, this book will guide you through every \\nstage of the machine learning engineering lifecycle.\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 200}, page_content='200'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 201}, page_content='201 \\nIndex \\n4 \\n4D parallelism, 154 \\nA \\naccuracy, 161 \\nactivation, 33 \\nadd-one smoothing. See: Laplace \\nsmoothing \\naffine transformation, 21 \\nAI, See \\nartificial intelligence, 16 \\nalgorithms \\ntask vector, 195 \\nalignment \\npreference-based, 196 \\nall-gather, 152 \\nartificial intelligence, 16 \\nconstitutional, 196 \\ngood old-fashioned, 18 \\nattention hea, 132 \\nattention weights, 122 \\nautograd. See: differentiation, \\nautomatic \\nautoregression, 77 \\nB \\nbackoff, 79 \\nbackpropagation, 49 \\nbackward pass, 49 \\nbag of words, 51, 84, 158 \\nbaseline, 158 \\nbias, 194 \\nbias term, 20 \\nBLAS, 30 \\nbootstrap resampling, 98 \\nBoW. See: bag of words \\nBPE. See: byte-pair encoding \\nBradley–Terry model, 98 \\nbroadcasting, 141 \\nBrown Corpus, 83 \\nbyte-pair encoding, 70 \\nC \\ncausal language model. See: \\nlanguage model, autoregressive \\ncentral tendency bias, 93 \\nchain of thought, 196 \\nchain rule, 26, 42 \\nchat LM. See: language model, chat \\nchat markup language. See: ChatML \\nChatML, 166 \\nclassification, 51 \\nbinary, 34, 41, 51 \\nmulticlass, 51 \\nclassification head, 181 \\nCLIP, 197 \\nCNN. See: neural network, \\nconvolutional \\ncodomain, 20 \\ncoefficient. See: weight term \\ncomputational graph, 35 \\nconfidence interval, 98 \\nconstant multiple rule, 25, 43 \\nconstant term. See: bias term'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 202}, page_content=\"202 \\n \\ncontext, 77 \\ncontext parallelism, 152 \\ncontext window, 115 \\nconvergence, 44, 102, 160, 161 \\ncorpus, 52 \\ncosine similarity, 31, 65, 190, 193 \\nCoT. See: chain of thought \\ncross-attention, 197 \\ncross-entropy, 57, 67, 111, 148, 182 \\nbinary, 41, 48, 55 \\ncuBLAS, 30 \\nD \\nDARE, 195 \\ndata leakage, 159 \\ndataset, 20 \\ntest, 83, 159 \\ntraining, 83, 159 \\ndecision tree, 18 \\ndecoder, See \\nTransformer, decoder-only, 118 \\ndecoding \\ngreedy, 172 \\ndeep learning, 134 \\ndense layer. See: fully connected \\nlayer \\nderivative \\nfirst, 23 \\npartial, 23 \\ndifferentiation \\nautomatic, 45 \\ndimensionality reduction, 68, 70 \\nDolma, 153 \\ndomain of function, 19 \\ndot product, 29, 123 \\ndropout, 198 \\nE \\nearly stopping, 198 \\nedit distance, 193 \\nelement-wise product, 30, 138 \\nElman RNN, 100 \\nElo rating, 96 \\nencoder \\nvision, 197 \\nencoding \\nbyte-pair, 110 \\nepoch, 111 \\nerror \\nmean squared, 23 \\nsquared, 22 \\nEuler's number, 34, 55, 85 \\nevaluation, 111 \\nexample, 20 \\nexpert, 195 \\nexplainability, 194 \\nF \\nFastText, 68 \\nfeature, 29 \\nfeature vector, 29, 51 \\nfinetuning, 89, 155, 156 \\nparameter-efficient Finetuning, 179 \\nfitting, 27 \\nFlashAttention, 151 \\nfloating-point operations, 154 \\nFLOPs. See: floating-point operations \\nFNN. See: neural network, \\nfeedforward \\nforward pass, 49, 61 \\nfrankenmerge, 195 \\nFSDP. See: Fully Sharded Data \\nParallel \\nfull finetune, 178\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 203}, page_content='203 \\nFully Sharded Data Parallel, 157 \\nfunction, 19 \\ncomposite, 25, 26, 33, 42, 134 \\nlinear, 20 \\nloss, 41 \\nFunction Calling, 196 \\nG \\ngate network. See: network, router \\ngeneralization, 83, 159, 198 \\nGloVe, 68 \\nGOFAI. See: artificial intelligence, \\ngood old-fashioned \\nGPT-2, 89 \\ngradient, 43 \\ngradient descent, 43 \\nmini-batch, 101 \\nground truth, 89 \\ngrouped-query attention, 151 \\nH \\nhallucination, 189 \\nhidden state, 100 \\nHugging Face Hub, 108, 155 \\nhyperparameter, 44, 111, 161 \\nI \\nin-context learning. See: prompting, \\nfew-shot \\ninference, 62, 139 \\ninput, 22 \\ninput sequence, 77 \\nintercept. See: bias term \\niteration, 44 \\nJ \\nJSON, 112, 158 \\nJSONL, 112, 158 \\nK \\nkernel methods, 19 \\nkey-value caching, 139 \\nknowledge distillation, 196 \\nL \\nlabeling, 54 \\nlanguage model, 76 \\nautoregressive, 77, 118, 123, 162, 179 \\nchat, 54, 67, 77, 95, 148, 182 \\nmasked, 78 \\nprogram-aided, 197 \\nvision, 197 \\nLaplace smoothing, 79, 88 \\nlayer \\nconcatenation and projection, 133 \\nembedding, 106 \\nfully connected, 36 \\ninput, 35 \\nof a neural network, 35 \\noutput, 35 \\nself-attention, 119, 177 \\nsparse MoE, 195 \\nlbfgs, 160 \\nlearning rate, 44 \\nlength of a vector. See: magnitude of \\na vector \\nLikert scale, 93 \\nLIMA, 167 \\nlinear algebra libraries, 30 \\nlinear transformation, 20, 29 \\nload balancing, 195 \\nlogarithm'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 204}, page_content='204 \\n \\nnatural, 41 \\nlogit, 55 \\nlog-likelihood \\nnegative, 85 \\nlong short-term memory, 117 \\nlongest common subsequence, 91 \\nLoRA. See: low-rank adaptation \\nLoRA adapter, 177 \\nLoRA scaling factor, 177 \\nloss \\nlogistic. See: cross-entropy, binary \\ntraining, 28 \\nloss function, 23 \\nlow-rank adaptation, 177 \\nquantized, 196 \\nLSTM. See: long short-term memory \\nM \\nmachine learning, 18, 19 \\nreinforcement, 22 \\nsupervised, 22 \\nunsupervised, 22 \\nmagnitude of a vector, 31 \\nmask \\nattention, 162 \\ncausal, 122, 123, 125, 146, 162 \\nmatrix, 38 \\ndocument-term, 53 \\nmatrix addition, 38 \\nmatrix multiplication, 39, 103 \\nbatch, 141 \\nmatrix transpose, 39 \\nmatrix-vector multiplication, 39, 128 \\nmaximum likelihood estimate, 78 \\nmergekit, 195 \\nminLSTM, 117 \\nmisalignment, 196 \\nMixtral \\n8x7B, 195 \\nmixture of experts, 195 \\nMLE. See: maximum likelihood \\nestimate \\nMLP. See: multilayer perceptron, See: \\nmultilayer perceptron \\nmodel, 19 \\nbase, 89 \\ncomposite, 34 \\nreward, 196 \\nmodel compression, 196 \\nmodel merging, 195 \\nmodel parallelism, 157 \\nmodel sharding, 157 \\nmodel soups, 195 \\nmodule API, 46, 60 \\nMoE. See: mixture of experts \\nMSE. See: error, mean squared \\nmulti-head attention, 131 \\nmultilayer perceptron, 36 \\nposition-wise, 124, 143, 177 \\nN \\nneedle in a haystack, 152 \\nnegative lookahead, 74 \\nnegative lookbehind, 74 \\nnetwork \\nrouter, 195 \\nneural network, 32 \\nconvolutional, 36 \\ndeep, 134 \\nfeedforward, 36, 61, 99 \\nrecurrent, 99 \\nneural networks, 18 \\nneuron \\nartificial, 35 \\nn-grams, 63 \\nnorm, 31'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 205}, page_content='205 \\nnotation \\ncapital-sigma, 29 \\nNucleus sampling. See: sampling, \\ntop-p \\nO \\none-hot encoding, 182 \\none-hot vector, 57 \\nopen-weight model, 150 \\noverfitting, 19, 83, 153, 161, 198 \\nover-parameterization, 196 \\nP \\npadding, 99 \\npairwise comparison, 95 \\nPAL. See: language model, program-\\naided \\nparallelism \\ncontext, 154 \\ndata, 154 \\npipeline, 154 \\ntensor, 154 \\nparameter, 20 \\npassthrough, 195 \\nPCA. See: principal component \\nanalysis \\nPEFT. See: finetuning, parameter-\\nefficient  \\npenalty \\nfrequency, 176 \\npresence, 176 \\nperceptron, 18 \\nperplexity, 85 \\nPhi 3.5 mini, 110 \\nprecision, 91 \\nprediction score, 41 \\npretraining, 89, 148 \\ndomain-specific, 190 \\nlong-context, 151 \\nprincipal component, 70 \\nprincipal component analysis, 70 \\nprobability \\nconditional, 76 \\nprobability distribution \\ndiscrete, 56, 77 \\nprojection matrix, 133, 142, 178 \\nprompt, 77 \\nsystem, 167, 197 \\nprompt engineering, 182 \\nprompting \\nfew-shot, 183 \\nprompting format, 166 \\nprompting style. See: prompting \\nformat \\npruning \\nstructured, 196 \\nunstructured, 196 \\npublic domain, 192 \\nPyTorch, 109 \\nQ \\nQLoRA. See: low-rank adaptation, \\nquantized \\nquantization, 48 \\npost-training, 196 \\nR \\nRAG. See: retrieval-augmented \\ngeneration \\nrandom forest, 19 \\nrank, 177 \\nranking, 95 \\nReAct. See: reason+act \\nreason+act, 197'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 206}, page_content='206 \\n \\nreasoning \\nmultimodal, 197 \\nrecall, 91 \\nregression \\nlinear, 23, 33, 41 \\nlogistic, 41, 158 \\nregular expression, 59 \\nregularization, 198 \\nL1, 198 \\nL2, 198 \\nreinforcement learning \\nfrom human feedback, 196 \\nReLU, 34, 134, 143, See: rectified \\nlinear unit \\nreproducibility, 58, 108 \\nresidual connection, 134, 144 \\nretrieval-augmented generation, 190 \\nRLHF, 196 \\nRMS. See: root mean square \\nRMSNorm. See: root mean square \\nnormalization \\nRNN. See: neural network, recurrent \\nroot mean square, 137 \\nroot mean square normalization, 137 \\nRoPE. See: rotary position embedding \\nrotary position embedding, 125 \\nrotation frequency, 128 \\nrotation matrix, 126 \\nROUGE, 89 \\nROUGE-1, 90 \\nROUGE-L, 91 \\nROUGE-N, 90, 91 \\nS \\nsampling \\ntop-k, 174 \\ntop-p, 175 \\nscalar, 29, 51 \\nscalar product. See: dot product \\nscikit-learn, 158 \\nscore \\nattention, 121 \\nmasked, 122 \\nscaled, 122 \\nself-attention, 120 \\nself-consistency, 197 \\nself-critique, 196 \\nsemantic similarity, 68 \\nsequential API, 46, 60 \\nset \\nfinite, 56 \\ntest, 198 \\ntraining, 28 \\nvalidation, 198 \\nsigmoid, 34, 41, 54 \\nsimple recurrent neural network. See: \\nElman RNN \\nskip connections. See: residual \\nconnections \\nskip-gram, 65 \\nskip-gram algorithm, 65 \\nSLERP, 195 \\nslope. See: weight term \\nsoftmax, 55, 115, 122, 172 \\nsolver, 160 \\nsparsity, 54, 64, 195 \\nstep. See: iteration \\nsubword, 52, 70 \\nsum of two vectors. See: vector sum \\nsum rule, 25, 43 \\nsupervised finetuning. See: finetuning \\nsupport vector machine, 19 \\nsurface form, 52 \\nSVM. See: support vector machine \\nsymbol, 71 \\nmerged, 71'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 207}, page_content=\"207 \\nT \\ntanh, 34, 103, See: hyperbolic \\ntangent \\ntarget, 22 \\ntemperature, 172 \\ntensor, 47 \\nTensorFlow, 109 \\ntesting, 62 \\nTIES-Merging, 195 \\ntoken, 52 \\ntokenization, 52 \\ntop-k routing, 195 \\nToT. See: tree of thought \\ntraining, 62, 138 \\nquantization-aware, 196 \\nsingle-epoch, 153 \\nTransformer, 118 \\ndecoder-only, 118 \\nSwitch, 195 \\ntranspose of a vector, 29 \\ntree of thought, 197 \\nU \\nunit, 35 \\nV \\nvanishing gradient problem, 134 \\nvector \\ncolumn, 29 \\ndense, 64 \\nembedding, 31 \\nrow, 29 \\nunit, 31 \\nzero, 31, 64, 99, 100 \\nvector component, See \\nvector dimension, 29 \\nvector dimension, 29 \\nvector dimensionality, 29 \\nvector size. See: vector \\ndimensionality \\nvector sum, 30 \\nversion control system, 188 \\nvision encoder, 197 \\nVLM. See: language model, vision \\nvocabulary, 52 \\nW \\nweight, 20 \\nweight term, 20 \\nword embedding, 63, 64 \\nword2vec, 65, 186 \\nWordNet, 69 \\nX \\nXavier initialization, 110 \\nxLSTM, 117 \\nZ \\nZipf's Law, 53\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext', 'creator': '', 'creationdate': '2025-01-25T22:04:23+00:00', 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'total_pages': 209, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-25T17:09:36-05:00', 'trapped': '', 'modDate': \"D:20250125170936-05'00'\", 'creationDate': 'D:20250125220423Z', 'page': 208}, page_content=''),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 0}, page_content='arXiv:2501.09223v1  [cs.CL]  16 Jan 2025\\nFoundations of\\nLarge Language Models\\nTong Xiao and Jingbo Zhu\\nJanuary 17, 2025\\nNLP Lab, Northeastern University & NiuTrans Research'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 1}, page_content='Copyright © 2021-2025 Tong Xiao and Jingbo Zhu\\nNATURAL LANGUAGE PROCESSING LAB, NORTHEASTERN UNIVERSITY\\n&\\nNIUTRANS RESEARCH\\nLicensed under the Creative Commons Attribution-NonCommercial 4.0 Unported License (the\\n“License”). You may not use this ﬁle except in compliance with the License. You may ob-\\ntain a copy of the License at http://creativecommons.org/licenses/by-nc/4.0. Unless\\nrequired by applicable law or agreed to in writing, software distributed under the License is dis-\\ntributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\\nexpress or implied. See the License for the speciﬁc language governing permissions and limita-\\ntions under the License.\\nJanuary 17, 2025'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 2}, page_content='Preface\\nLarge language models originated from natural language processing, but they have undoubtedly\\nbecome one of the most revolutionary technological advancements in the ﬁeld of artiﬁcial intelli-\\ngence in recent years. An important insight brought by large language models is that knowledge\\nof the world and languages can be acquired through large-scale language modeling tasks, and\\nin this way, we can create a universal model that handles diverse problems. This discovery has\\nprofoundly impacted the research methodologies in natural language processing and many related\\ndisciplines. We have shifted from training specialized systems from scratch using a large amount\\nof labeled data to a new paradigm of using large-scale pre-training to obtain foundation models,\\nwhich are then ﬁne-tuned, aligned, and prompted.\\nThis book aims to outline the basic concepts of large language models and introduce the\\nrelated techniques. As the title suggests, the book focuses more on the foundational aspects of\\nlarge language models rather than providing comprehensive coverage of all cutting-edge methods.\\nThe book consists of four chapters:\\n• Chapter 1 introduces the basics of pre-training. This is the foundation of large language\\nmodels, and common pre-training methods and model architectures will be discussed here.\\n• Chapter 2 introduces generative models, which are the large language models we commonly\\nrefer to today. After presenting the basic process of building these models, we will also\\nexplore how to scale up model training and handle long texts.\\n• Chapter 3 introduces prompting methods for large language models. We will discuss var-\\nious prompting strategies, along with more advanced methods such as chain-of-thought\\nreasoning and automatic prompt design.\\n• Chapter 4 introduces alignment methods for large language models. This chapter focuses\\non instruction ﬁne-tuning and alignment based on human feedback.\\nIf readers have some background in machine learning and natural language processing, along\\nwith a certain understanding of neural networks like Transformers, reading this book will be quite\\neasy. However, even without this prior knowledge, it is still perfectly ﬁne, as we have made the\\ncontent of each chapter as self-contained as possible, ensuring that readers will not be burdened\\nwith too much reading difﬁculty.\\nIn writing this book, we have gradually realized that it is more like a compilation of \"notes\" we\\nhave taken while learning about large language models. Through this note-taking writing style, we\\nhope to offer readers a ﬂexible learning path. Whether they wish to dive deep into a speciﬁc area\\nor gain a comprehensive understanding of large language models, they will ﬁnd the knowledge\\nand insights they need within these \"notes\".\\nWe would like to thank the students in our laboratory and all our friends who have shared\\nwith us their views on large language models and helped with corrections of errors in writing. In\\nparticular, we wish to thank Weiqiao Shan, Yongyu Mu, Chenglong Wang, Kaiyan Chang, Yuchun\\nFan, Hang Zhou, Xinyu Liu, Huiwen Bao, Tong Zheng, Junhao Ruan, and Qing Yang.\\nii'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 3}, page_content='Notation\\na\\nvariable\\na\\nrow vector or matrix\\nf(a)\\nfunction of a\\nmax f(a)\\nmaximum value of f(a)\\narg maxa f(a)\\nvalue of a that maximizes f(a)\\nx\\ninput token sequence to a model\\nxj\\ninput token at position j\\ny\\noutput token sequence produced by a model\\nyi\\noutput token at position i\\nθ\\nmodel parameters\\nPr(a)\\nprobability of a\\nPr(a|b)\\nconditional probability of a given b\\nPr(·|b)\\nprobability distribution of a variable given b\\nPrθ(a)\\nprobability of a as parameterized by θ\\nht\\nhidden state at time step t in sequential models\\nH\\nmatrix of all hidden states over time in a sequence\\nQ, K, V\\nquery, key, and value matrices in attention mechanisms\\nSoftmax(A)\\nSoftmax function that normalizes the input vector or matrix A\\nL\\nloss function\\nD\\ndataset used for training or ﬁne-tuning a model\\n∂L\\n∂θ\\ngradient of the loss function L with respect to the parameters θ\\nKL(p || q)\\nKL divergence between distributions p and q\\niii'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 4}, page_content='Contents\\n1\\nPre-training\\n1\\n1.1\\nPre-training NLP Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n1\\n1.1.1\\nUnsupervised, Supervised and Self-supervised Pre-training . . . . . . . .\\n2\\n1.1.2\\nAdapting Pre-trained Models . . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.2\\nSelf-supervised Pre-training Tasks . . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n1.2.1\\nDecoder-only Pre-training . . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n1.2.2\\nEncoder-only Pre-training\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n8\\n1.2.3\\nEncoder-Decoder Pre-training . . . . . . . . . . . . . . . . . . . . . . .\\n15\\n1.2.4\\nComparison of Pre-training Tasks . . . . . . . . . . . . . . . . . . . . .\\n20\\n1.3\\nExample: BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\n1.3.1\\nThe Standard Model . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\n1.3.2\\nMore Training and Larger Models . . . . . . . . . . . . . . . . . . . . .\\n27\\n1.3.3\\nMore Efﬁcient Models . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n27\\n1.3.4\\nMulti-lingual Models . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n28\\n1.4\\nApplying BERT Models\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n30\\n1.5\\nSummary\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n35\\n2\\nGenerative Models\\n36\\n2.1\\nA Brief Introduction to LLMs\\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\\n37\\n2.1.1\\nDecoder-only Transformers\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n38\\n2.1.2\\nTraining LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n40\\n2.1.3\\nFine-tuning LLMs\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n42\\n2.1.4\\nAligning LLMs with the World\\n. . . . . . . . . . . . . . . . . . . . . .\\n46\\n2.1.5\\nPrompting LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n51\\n2.2\\nTraining at Scale\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n56\\n2.2.1\\nData Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n56\\n2.2.2\\nModel Modiﬁcations . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n58\\n2.2.3\\nDistributed Training\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n60\\n2.2.4\\nScaling Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n63\\n2.3\\nLong Sequence Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n66\\n2.3.1\\nOptimization from HPC Perspectives\\n. . . . . . . . . . . . . . . . . . .\\n67\\n2.3.2\\nEfﬁcient Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n68\\n2.3.3\\nCache and Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n71\\n2.3.4\\nSharing across Heads and Layers\\n. . . . . . . . . . . . . . . . . . . . .\\n80\\niv'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 5}, page_content='v\\n2.3.5\\nPosition Extrapolation and Interpolation . . . . . . . . . . . . . . . . . .\\n82\\n2.3.6\\nRemarks\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n92\\n2.4\\nSummary\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n94\\n3\\nPrompting\\n96\\n3.1\\nGeneral Prompt Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n97\\n3.1.1\\nBasics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n97\\n3.1.2\\nIn-context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n99\\n3.1.3\\nPrompt Engineering Strategies . . . . . . . . . . . . . . . . . . . . . . .\\n101\\n3.1.4\\nMore Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n106\\n3.2\\nAdvanced Prompting Methods . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n115\\n3.2.1\\nChain of Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n115\\n3.2.2\\nProblem Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . .\\n117\\n3.2.3\\nSelf-reﬁnement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n124\\n3.2.4\\nEnsembling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n130\\n3.2.5\\nRAG and Tool Use . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n134\\n3.3\\nLearning to Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n138\\n3.3.1\\nPrompt Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n139\\n3.3.2\\nSoft Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n142\\n3.3.3\\nPrompt Length Reduction\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n152\\n3.4\\nSummary\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n153\\n4\\nAlignment\\n155\\n4.1\\nAn Overview of LLM Alignment . . . . . . . . . . . . . . . . . . . . . . . . . .\\n155\\n4.2\\nInstruction Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n157\\n4.2.1\\nSupervised Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n157\\n4.2.2\\nFine-tuning Data Acquisition . . . . . . . . . . . . . . . . . . . . . . . .\\n161\\n4.2.3\\nFine-tuning with Less Data . . . . . . . . . . . . . . . . . . . . . . . . .\\n166\\n4.2.4\\nInstruction Generalization\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n167\\n4.2.5\\nUsing Weak Models to Improve Strong Models . . . . . . . . . . . . . .\\n169\\n4.3\\nHuman Preference Alignment: RLHF . . . . . . . . . . . . . . . . . . . . . . .\\n172\\n4.3.1\\nBasics of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . .\\n173\\n4.3.2\\nTraining Reward Models . . . . . . . . . . . . . . . . . . . . . . . . . .\\n179\\n4.3.3\\nTraining LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n4.4\\nImproved Human Preference Alignment . . . . . . . . . . . . . . . . . . . . . .\\n187\\n4.4.1\\nBetter Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . .\\n187'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 6}, page_content='vi\\nNotation\\n4.4.2\\nDirect Preference Optimization\\n. . . . . . . . . . . . . . . . . . . . . .\\n193\\n4.4.3\\nAutomatic Preference Data Generation\\n. . . . . . . . . . . . . . . . . .\\n196\\n4.4.4\\nStep-by-step Alignment\\n. . . . . . . . . . . . . . . . . . . . . . . . . .\\n198\\n4.4.5\\nInference-time Alignment\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n200\\n4.5\\nSummary\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n201\\nBibliography\\n203'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 7}, page_content='CHAPTER 1\\nPre-training\\nThe development of neural sequence models, such as Transformers [Vaswani et al., 2017], along\\nwith the improvements in large-scale self-supervised learning, has opened the door to universal\\nlanguage understanding and generation. This achievement is largely motivated by pre-training:\\nwe separate common components from many neural network-based systems, and then train them\\non huge amounts of unlabeled data using self-supervision. These pre-trained models serve as\\nfoundation models that can be easily adapted to different tasks via ﬁne-tuning or prompting. As a\\nresult, the paradigm of NLP has been enormously changed. In many cases, large-scale supervised\\nlearning for speciﬁc tasks is no longer required, and instead, we only need to adapt pre-trained\\nfoundation models.\\nWhile pre-training has gained popularity in recent NLP research, this concept dates back\\ndecades to the early days of deep learning. For example, early attempts to pre-train deep learning\\nsystems include unsupervised learning for RNNs, deep feedforward networks, autoencoders, and\\nothers [Schmidhuber, 2015]. In the modern era of deep learning, we experienced a resurgence of\\npre-training, caused in part by the large-scale unsupervised learning of various word embedding\\nmodels [Mikolov et al., 2013b; Pennington et al., 2014]. During the same period, pre-training also\\nattracted signiﬁcant interest in computer vision, where the backbone models were trained on rel-\\natively large labeled datasets such as ImageNet, and then applied to different downstream tasks\\n[He et al., 2019; Zoph et al., 2020]. Large-scale research on pre-training in NLP began with the\\ndevelopment of language models using self-supervised learning. This family of models covers\\nseveral well-known examples like BERT [Devlin et al., 2019] and GPT [Brown et al., 2020], all\\nwith a similar idea that general language understanding and generation can be achieved by train-\\ning the models to predict masked words in a huge amount of text. Despite the simple nature of\\nthis approach, the resulting models show remarkable capability in modeling linguistic structure,\\nthough they are not explicitly trained to achieve this. The generality of the pre-training tasks\\nleads to systems that exhibit strong performance in a large variety of NLP problems, even outper-\\nforming previously well-developed supervised systems. More recently, pre-trained large language\\nmodels have achieved a greater success, showing the exciting prospects for more general artiﬁcial\\nintelligence [Bubeck et al., 2023].\\nThis chapter discusses the concept of pre-training in the context of NLP. It begins with a gen-\\neral introduction to pre-training methods and their applications. BERT is then used as an example\\nto illustrate how a sequence model is trained via a self-supervised task, called masked language\\nmodeling. This is followed by a discussion of methods for adapting pre-trained sequence mod-\\nels for various NLP tasks. Note that in this chapter, we will focus primarily on the pre-training\\nparadigm in NLP, and therefore, we do not intend to cover details about generative large language\\nmodels. A detailed discussion of these models will be left to subsequent chapters.\\n1.1\\nPre-training NLP Models\\nThe discussion of pre-training issues in NLP typically involves two types of problems: sequence\\nmodeling (or sequence encoding) and sequence generation. While these problems have different\\n1'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 8}, page_content='2\\nPre-training\\nforms, for simplicity, we describe them using a single model deﬁned as follows:\\no\\n=\\ng(x0, x1, ..., xm; θ)\\n=\\ngθ(x0, x1, ..., xm)\\n(1.1)\\nwhere {x0, x1, ..., xm} denotes a sequence of input tokens1, x0 denotes a special symbol (⟨s⟩or\\n[CLS]) attached to the beginning of a sequence, g(·; θ) (also written as gθ(·)) denotes a neural\\nnetwork with parameters θ, and o denotes the output of the neural network. Different problems\\ncan vary based on the form of the output o. For example, in token prediction problems (as in\\nlanguage modeling), o is a distribution over a vocabulary; in sequence encoding problems, o is a\\nrepresentation of the input sequence, often expressed as a real-valued vector sequence.\\nThere are two fundamental issues here.\\n• Optimizing θ on a pre-training task. Unlike standard learning problems in NLP, pre-training\\ndoes not assume speciﬁc downstream tasks to which the model will be applied. Instead, the\\ngoal is to train a model that can generalize across various tasks.\\n• Applying the pre-trained model gˆθ(·) to downstream tasks. To adapt the model to these\\ntasks, we need to adjust the parameters ˆθ slightly using labeled data or prompt the model\\nwith task descriptions.\\nIn this section, we discuss the basic ideas in addressing these issues.\\n1.1.1\\nUnsupervised, Supervised and Self-supervised Pre-training\\nIn deep learning, pre-training refers to the process of optimizing a neural network before it is\\nfurther trained/tuned and applied to the tasks of interest. This approach is based on an assumption\\nthat a model pre-trained on one task can be adapted to perform another task. As a result, we do\\nnot need to train a deep, complex neural network from scratch on tasks with limited labeled data.\\nInstead, we can make use of tasks where supervision signals are easier to obtain. This reduces the\\nreliance on task-speciﬁc labeled data, enabling the development of more general models that are\\nnot conﬁned to particular problems.\\nDuring the resurgence of neural networks through deep learning, many early attempts to\\nachieve pre-training were focused on unsupervised learning. In these methods, the parame-\\nters of a neural network are optimized using a criterion that is not directly related to speciﬁc tasks.\\nFor example, we can minimize the reconstruction cross-entropy of the input vector for each layer\\n[Bengio et al., 2006]. Unsupervised pre-training is commonly employed as a preliminary step\\nbefore supervised learning, offering several advantages, such as aiding in the discovery of better\\nlocal minima and adding a regularization effect to the training process [Erhan et al., 2010]. These\\nbeneﬁts make the subsequent supervised learning phase easier and more stable.\\nA second approach to pre-training is to pre-train a neural network on supervised learning\\ntasks. For example, consider a sequence model designed to encode input sequences into some\\n1Here we assume that tokens are basic units of text that are separated through tokenization. Sometimes, we will use\\nthe terms token and word interchangeably, though they have closely related but slightly different meanings in NLP.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 9}, page_content='1.1 Pre-training NLP Models\\n3\\nrepresentations. In pre-training, this model is combined with a classiﬁcation layer to form a clas-\\nsiﬁcation system. This system is then trained on a pre-training task, such as classifying sentences\\nbased on sentiment (e.g., determining if a sentence conveys a positive or negative sentiment).\\nThen, we adapt the sequence model to a downstream task. We build a new classiﬁcation system\\nbased on this pre-trained sequence model and a new classiﬁcation layer (e.g., determining if a\\nsequence is subjective or objective). Typically, we need to ﬁne-tune the parameters of the new\\nmodel using task-speciﬁc labeled data, ensuring the model is optimally adjusted to perform well\\non this new type of data. The ﬁne-tuned model is then employed to classify new sequences for\\nthis task. An advantage of supervised pre-training is that the training process, either in the pre-\\ntraining or ﬁne-tuning phase, is straightforward, as it follows the well-studied general paradigm\\nof supervised learning in machine learning. However, as the complexity of the neural network\\nincreases, the demand for more labeled data also grows. This, in turn, makes the pre-training task\\nmore difﬁcult, especially when large-scale labeled data is not available.\\nA third approach to pre-training is self-supervised learning. In this approach, a neural net-\\nwork is trained using the supervision signals generated by itself, rather than those provided by\\nhumans. This is generally done by constructing its own training tasks directly from unlabeled\\ndata, such as having the system create pseudo labels. While self-supervised learning has recently\\nemerged as a very popular method in NLP, it is not a new concept. In machine learning, a related\\nconcept is self-training where a model is iteratively improved by learning from the pseudo labels\\nassigned to a dataset. To do this, we need some seed data to build an initial model. This model\\nthen generates pseudo labels for unlabeled data, and these pseudo labels are subsequently used to\\niteratively reﬁne and bootstrap the model itself. Such a method has been successfully used in sev-\\neral NLP areas, such as word sense disambiguation [Yarowsky, 1995] and document classiﬁcation\\n[Blum and Mitchell, 1998]. Unlike the standard self-training method, self-supervised pre-training\\nin NLP does not rely on an initial model for annotating the data. Instead, all the supervision sig-\\nnals are created from the text, and the entire model is trained from scratch. A well-known example\\nof this is training sequence models by successively predicting a masked word given its preceding\\nor surrounding words in a text. This enables large-scale self-supervised learning for deep neural\\nnetworks, leading to the success of pre-training in many understanding, writing, and reasoning\\ntasks.\\nFigure 1.1 shows a comparison of the above three pre-training approaches. Self-supervised\\npre-training is so successful that most current state-of-the-art NLP models are based on this\\nparadigm. Therefore, in this chapter and throughout this book, we will focus on self-supervised\\npre-training. We will show how sequence models are pre-trained via self-supervision and how the\\npre-trained models are applied.\\n1.1.2\\nAdapting Pre-trained Models\\nAs mentioned above, two major types of models are widely used in NLP pre-training.\\n• Sequence Encoding Models. Given a sequence of words or tokens, a sequence encoding\\nmodel represents this sequence as either a real-valued vector or a sequence of vectors, and\\nobtains a representation of the sequence. This representation is typically used as input to\\nanother model, such as a sentence classiﬁcation system.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 10}, page_content='4\\nPre-training\\nUnsupervised\\nSupervised\\nPre-training\\nTraining\\nUnlabeled\\nData\\nLabeled\\nData\\n(a) Unsupervised Pre-training\\nSupervised\\nSupervised\\nPre-training\\nTuning\\nLabeled\\nData\\nTask 1\\nLabeled\\nData\\nTask 2\\n(b) Supervised Pre-training\\nSelf-\\nSupervised\\nSupervised\\nZero/Few\\nShot Learning\\nPre-training\\nTuning\\nPrompting\\nUnlabeled\\nData\\nLabeled\\nData\\n(c) Self-supervised Pre-training\\nFig. 1.1: Illustration of unsupervised, supervised, and self-supervised pre-training. In unsupervised pre-training, the\\npre-training is performed on large-scale unlabeled data. It can be viewed as a preliminary step to have a good starting\\npoint for the subsequent optimization process, though considerable effort is still required to further train the model\\nwith labeled data after pre-training. In supervised pre-training, the underlying assumption is that different (supervised)\\nlearning tasks are related. So we can ﬁrst train the model on one task, and transfer the resulting model to another task\\nwith some training or tuning effort. In self-supervised pre-training, a model is pre-trained on large-scale unlabeled data\\nvia self-supervision. The model can be well trained in this way, and we can efﬁciently adapt it to new tasks through\\nﬁne-tuning or prompting.\\n• Sequence Generation Models. In NLP, sequence generation generally refers to the prob-\\nlem of generating a sequence of tokens based on a given context. The term context has\\ndifferent meanings across applications. For example, it refers to the preceding tokens in\\nlanguage modeling, and refers to the source-language sequence in machine translation2.\\nWe need different techniques for applying these models to downstream tasks after pre-training.\\nHere we are interested in the following two methods.\\n1.1.2.1\\nFine-tuning of Pre-trained Models\\nFor sequence encoding pre-training, a common method of adapting pre-trained models is ﬁne-\\ntuning. Let Encodeθ(·) denote an encoder with parameters θ, for example, Encodeθ(·) can be a\\nstandard Transformer encoder. Provided we have pre-trained this model in some way and obtained\\nthe optimal parameters ˆθ, we can employ it to model any sequence and generate the corresponding\\nrepresentation, like this\\nH\\n=\\nEncodeˆθ(x)\\n(1.2)\\nwhere x is the input sequence {x0, x1, ..., xm}, and H is the output representation which is a\\nsequence of real-valued vectors {h0, h1, ..., hm}. Because the encoder does not work as a stan-\\ndalone NLP system, it is often integrated as a component into a bigger system. Consider, for\\nexample, a text classiﬁcation problem in which we identify the polarity (i.e., positive, negative,\\n2More precisely, in auto-regressive decoding of machine translation, each target-language token is generated based\\non both its preceding tokens and source-language sequence.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 11}, page_content='1.1 Pre-training NLP Models\\n5\\nand neutral) of a given text. We can build a text classiﬁcation system by stacking a classiﬁer\\non top of the encoder. Let Classifyω(·) be a neural network with parameters ω. Then, the text\\nclassiﬁcation model can be expressed in the form\\nPrω,ˆθ(·|x)\\n=\\nClassifyω(H)\\n=\\nClassifyω(Encodeˆθ(x))\\n(1.3)\\nHere Prω,ˆθ(·|x) is a probability distribution over the label set {positive, negative, neutral}, and\\nthe label with the highest probability in this distribution is selected as output. To keep the notation\\nuncluttered, we will use Fω,ˆθ(·) to denote Classifyω(Encodeˆθ(·)).\\nBecause the model parameters ω and ˆθ are not optimized for the classiﬁcation task, we cannot\\ndirectly use this model. Instead, we must use a modiﬁed version of the model that is adapted to\\nthe task. A typical way is to ﬁne-tune the model by giving explicit labeling in downstream tasks.\\nWe can train Fω,ˆθ(·) on a labeled dataset, treating it as a common supervised learning task. The\\noutcome of the ﬁne-tuning is the parameters ˜ω and ˜θ that are further optimized. Alternatively,\\nwe can freeze the encoder parameters ˆθ to maintain their pre-trained state, and focus solely on\\noptimizing ω. This allows the classiﬁer to be efﬁciently adapted to work in tandem with the\\npre-trained encoder.\\nOnce we have obtained a ﬁne-tuned model, we can use it to classify a new text. For example,\\nsuppose we have a comment posted on a travel website:\\nI love the food here. It’s amazing!\\nWe ﬁrst tokenize this text into tokens3, and then feed the token sequence xnew into the ﬁne-tuned\\nmodel F˜ω,˜θ(·). The model generates a distribution over classes by\\nF˜ω,˜θ(xnew)\\n=\\nh\\nPr(positive|xnew)\\nPr(negative|xnew)\\nPr(neutral|xnew)\\ni\\n(1.4)\\nAnd we select the label of the entry with the maximum value as output. In this example it is\\npositive.\\nIn general, the amount of labeled data used in ﬁne-tuning is small compared to that of the\\npre-training data, and so ﬁne-tuning is less computationally expensive. This makes the adaption\\nof pre-trained models very efﬁcient in practice: given a pre-trained model and a downstream task,\\nwe just need to collect some labeled data, and slightly adjust the model parameters on this data. A\\nmore detailed discussion of ﬁne-tuning can be found in Section 1.4.\\n1.1.2.2\\nPrompting of Pre-trained Models\\nUnlike sequence encoding models, sequence generation models are often employed independently\\nto address language generation problems, such as question answering and machine translation,\\nwithout the need for additional modules. It is therefore straightforward to ﬁne-tune these models\\n3The text can be tokenized in many different ways. One of the simplest is to segment the text into English words\\nand punctuations {I, love, the, food, here, ., It, ’s, amazing, !}'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 12}, page_content='6\\nPre-training\\nas complete systems on downstream tasks. For example, we can ﬁne-tune a pre-trained encoder-\\ndecoder multilingual model on some bilingual data to improve its performance on a speciﬁc trans-\\nlation task.\\nAmong various sequence generation models, a notable example is the large language models\\ntrained on very large amounts of data. These language models are trained to simply predict the next\\ntoken given its preceding tokens. Although token prediction is such a simple task that it has long\\nbeen restricted to “language modeling” only, it has been found to enable the learning of the general\\nknowledge of languages by repeating the task a large number of times. The result is that the\\npre-trained large language models exhibit remarkably good abilities in token prediction, making\\nit possible to transform numerous NLP problems into simple text generation problems through\\nprompting the large language models. For example, we can frame the above text classiﬁcation\\nproblem as a text generation task\\nI love the food here. It’s amazing! I’m\\nHere\\nindicates the word or phrase we want to predict (call it the completion). If the predicted\\nword is happy, or glad, or satisﬁed or a related positive word, we can classify the text as positive.\\nThis example shows a simple prompting method in which we concatenate the input text with I’m\\nto form a prompt. Then, the completion helps decide which label is assigned to the original text.\\nGiven the strong performance of language understanding and generation of large language\\nmodels, a prompt can instruct the models to perform more complex tasks. Here is a prompt where\\nwe prompt the LLM to perform polarity classiﬁcation with an instruction.\\nAssume that the polarity of a text is a label chosen from {positive, negative,\\nneutral}. Identify the polarity of the input.\\nInput: I love the food here. It’s amazing!\\nPolarity:\\nThe ﬁrst two sentences are a description of the task. Input and Polarity are indicators of the input\\nand output, respectively. We expect the model to complete the text and at the same time give the\\ncorrect polarity label. By using instruction-based prompts, we can adapt large language models to\\nsolve NLP problems without the need for additional training.\\nThis example also demonstrates the zero-shot learning capability of large language models,\\nwhich can perform tasks that were not observed during the training phase. Another method for\\nenabling new capabilities in a neural network is few-shot learning. This is typically achieved\\nthrough in-context learning (ICT). More speciﬁcally, we add some samples that demonstrate how\\nan input corresponds to an output. These samples, known as demonstrations, are used to teach\\nlarge language models how to perform the task. Below is an example involving demonstrations'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 13}, page_content='1.2 Self-supervised Pre-training Tasks\\n7\\nAssume that the polarity of a text is a label chosen from {positive, negative,\\nneutral}. Identify the polarity of the input.\\nInput: The trafﬁc is terrible during rush hours, making it difﬁcult to reach the\\nairport on time.\\nPolarity: Negative\\nInput: The weather here is wonderful.\\nPolarity: Positive\\nInput: I love the food here. It’s amazing!\\nPolarity:\\nPrompting and in-context learning play important roles in the recent rise of large language\\nmodels. We will discuss these issues more deeply in Chapter 3. However, it is worth noting\\nthat while prompting is a powerful way to adapt large language models, some tuning efforts are\\nstill needed to ensure the models can follow instructions accurately. Additionally, the ﬁne-tuning\\nprocess is crucial for aligning the values of these models with human values. More detailed\\ndiscussions of ﬁne-tuning can be found in Chapter 4.\\n1.2\\nSelf-supervised Pre-training Tasks\\nIn this section, we consider self-supervised pre-training approaches for different neural architec-\\ntures, including decoder-only, encoder-only, and encoder-decoder architectures. We restrict our\\ndiscussion to Transformers since they form the basis of most pre-trained models in NLP. How-\\never, pre-training is a broad concept, and so we just give a brief introduction to basic approaches\\nin order to make this section concise.\\n1.2.1\\nDecoder-only Pre-training\\nThe decoder-only architecture has been widely used in developing language models [Radford et al.,\\n2018]. For example, we can use a Transformer decoder as a language model by simply removing\\ncross-attention sub-layers from it. Such a model predicts the distribution of tokens at a position\\ngiven its preceding tokens, and the output is the token with the maximum probability. The stan-\\ndard way to train this model, as in the language modeling problem, is to minimize a loss function\\nover a collection of token sequences. Let Decoderθ(·) denote a decoder with parameters θ. At\\neach position i, the decoder generates a distribution of the next tokens based on its preceding\\ntokens {x0, ..., xi}, denoted by Prθ(·|x0, ..., xi) (or pθ\\ni+1 for short). Suppose we have the gold-\\nstandard distribution at the same position, denoted by pgold\\ni+1 . For language modeling, we can think\\nof pgold\\ni+1 as a one-hot representation of the correct predicted word. We then deﬁne a loss function\\nL(pθ\\ni+1, pgold\\ni+1 ) to measure the difference between the model prediction and the true prediction. In\\nNLP, the log-scale cross-entropy loss is typically used.\\nGiven a sequence of m tokens {x0, ..., xm}, the loss on this sequence is the sum of the loss'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 14}, page_content='8\\nPre-training\\nover the positions {0, ..., m −1}, given by\\nLossθ(x0, ..., xm)\\n=\\nm−1\\nX\\ni=0\\nL(pθ\\ni+1, pgold\\ni+1 )\\n=\\nm−1\\nX\\ni=0\\nLogCrossEntropy(pθ\\ni+1, pgold\\ni+1 )\\n(1.5)\\nwhere LogCrossEntropy(·) is the log-scale cross-entropy, and pgold\\ni+1 is the one-hot representation\\nof xi+1.\\nThis loss function can be extended to a set of sequences D. In this case, the objective of\\npre-training is to ﬁnd the best parameters that minimize the loss on D\\nˆθ\\n=\\narg min\\nθ\\nX\\nx∈D\\nLossθ(x)\\n(1.6)\\nNote that this objective is mathematically equivalent to maximum likelihood estimation, and can\\nbe re-expressed as\\nˆθ\\n=\\narg max\\nθ\\nX\\nx∈D\\nlog Prθ(x)\\n=\\narg max\\nθ\\nX\\nx∈D\\ni−1\\nX\\ni=0\\nlog Prθ(xi+1|x0, ..., xi)\\n(1.7)\\nWith these optimized parameters ˆθ, we can use the pre-trained language model Decoderˆθ(·)\\nto compute the probability Prˆθ(xi+1|x0, ..., xi) at each position of a given sequence.\\n1.2.2\\nEncoder-only Pre-training\\nAs deﬁned in Section 1.1.2.1, an encoder Encoderθ(·) is a function that reads a sequence of\\ntokens x = x0...xm and produces a sequence of vectors H = h0...hm4. Training this model is\\nnot straightforward, as we do not have gold-standard data for measuring how good the output of\\nthe real-valued function is. A typical approach to encoder pre-training is to combine the encoder\\nwith some output layers to receive supervision signals that are easier to obtain. Figure 1.2 shows\\na common architecture for pre-training Transformer encoders, where we add a Softmax layer on\\ntop of the Transformer encoder. Clearly, this architecture is the same as that of the decoder-based\\nlanguage model, and the output is a sequence of probability distributions\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\npW,θ\\n1...\\npW,θ\\nm\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n=\\nSoftmaxW(Encoderθ(x))\\n(1.9)\\n4If we view hi as a row vector, H can be written as\\nH\\n=\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nh0\\n...\\nhm\\n\\uf8f9\\n\\uf8fa\\uf8fb\\n(1.8)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 15}, page_content='1.2 Self-supervised Pre-training Tasks\\n9\\nx0\\nx1\\nx2\\nx3\\nx4\\n(masked)\\ne0\\ne1\\ne2\\ne3\\ne4\\nEncoder\\nSoftmax\\nmodel reconstructs the masked token\\nE.g., evaluate how well the\\nSelf-supervision\\n(a) Pre-training\\nx0\\nx1\\nx2\\nx3\\nx4\\ne0\\ne1\\ne2\\ne3\\ne4\\nPre-trained Encoder\\nPrediction Network\\nOutput for Downstream Tasks\\n(b) Applying the Pre-trained Encoder\\nFig. 1.2: Pre-training a Transformer encoder (left) and then applying the pre-trained encoder (right). In the pre-training\\nphase, the encoder, together with a Softmax layer, is trained via self-supervision. In the application phase, the Softmax\\nlayer is removed, and the pre-trained encoder is combined with a prediction network to address speciﬁc problems. In\\ngeneral, for better adaptation to these tasks, the system is ﬁne-tuned using labeled data.\\nHere pW,θ\\ni\\nis the output distribution Pr(·|x) at position i. We use SoftmaxW(·) to denote that\\nthe Softmax layer is parameterized by W, that is, SoftmaxW(H) = Softmax(H · W). For\\nnotation simplicity, we will sometimes drop the superscripts W and θ afﬁxed to each probability\\ndistribution.\\nThe difference between this model and standard language models is that the output pi has\\ndifferent meanings in encoder pre-training and language modeling. In language modeling, pi is\\nthe probability distribution of predicting the next word. This follows an auto-regressive decoding\\nprocess: a language model only observes the words up to position i and predicts the next. By\\ncontrast, in encoder pre-training, the entire sequence can be observed at once, and so it makes no\\nsense to predict any of the tokens in this sequence.\\n1.2.2.1\\nMasked Language Modeling\\nOne of the most popular methods of encoder pre-training is masked language modeling, which\\nforms the basis of the well-known BERT model [Devlin et al., 2019]. The idea of masked lan-\\nguage modeling is to create prediction challenges by masking out some of the tokens in the input\\nsequence and training a model to predict the masked tokens. In this sense, the conventional lan-\\nguage modeling problem, which is sometimes called causal language modeling, is a special case\\nof masked language modeling: at each position, we mask the tokens in the right-context, and pre-\\ndict the token at this position using its left context. However, in causal language modeling we\\nonly make use of the left-context in word prediction, while the prediction may depend on tokens\\nin the right-context. By contrast, in masked language modeling, all the unmasked tokens are used\\nfor word prediction, leading to a bidirectional model that makes predictions based on both left and\\nright-contexts.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 16}, page_content='10\\nPre-training\\nMore formally, for an input sequence x = x0...xm, suppose that we mask the tokens at po-\\nsitions A(x) = {i1, ..., iu}. Hence we obtain a masked token sequence ¯x where the token at\\neach position in A(x) is replaced with a special symbol [MASK]. For example, for the following\\nsequence\\nThe early bird catches the worm\\nwe may have a masked token sequence like this\\nThe [MASK] bird catches the [MASK]\\nwhere we mask the tokens early and worm (i.e., i1 = 2 and i2 = 6).\\nNow we have two sequences x and ¯x. The model is then optimized so that we can correctly\\npredict x based on ¯x. This can be thought of as an autoencoding-like process, and the train-\\ning objective is to maximize the reconstruction probability Pr(x|¯x). Note that there is a simple\\nposition-wise alignment between x and ¯x. Because an unmasked token in ¯x is the same as the to-\\nken in x at the same position, there is no need to consider the prediction for this unmasked token.\\nThis leads to a simpliﬁed training objective which only maximizes the probabilities for masked\\ntokens. We can express this objective in a maximum likelihood estimation fashion\\n( ˆ\\nW, ˆθ)\\n=\\narg max\\nW,θ\\nX\\nx∈D\\nX\\ni∈A(x)\\nlog PrW,θ\\ni\\n(xi|¯x)\\n(1.10)\\nor alternatively express it using the cross-entropy loss\\n( c\\nW, ˆθ)\\n=\\narg min\\nW,θ\\nX\\nx∈D\\nX\\ni∈A(x)\\nLogCrossEntropy(pW,θ\\ni\\n, pgold\\ni\\n)\\n(1.11)\\nwhere PrW,θ\\nk\\n(xk|¯x) is the probability of the true token xk at position k given the corrupted input\\nx, and pW,θ\\nk\\nis the probability distribution at position k given the corrupted input x. To illustrate,\\nconsider the above example where two tokens of the sequence “the early bird catches the worm”\\nare masked. For this example, the objective is to maximize the sum of log-scale probabilities\\nLoss\\n=\\nlog Pr(x2 = early|¯x = [CLS] The [MASK]\\n|\\n{z\\n}\\n¯x2\\nbird catches the [MASK]\\n|\\n{z\\n}\\n¯x6\\n) +\\nlog Pr(x6 = worm|¯x = [CLS] The [MASK]\\n|\\n{z\\n}\\n¯x2\\nbird catches the [MASK]\\n|\\n{z\\n}\\n¯x6\\n)\\n(1.12)\\nOnce we obtain the optimized parameters c\\nW and ˆθ, we can drop c\\nW. Then, we can further\\nﬁne-tune the pre-trained encoder Encoderˆθ(·) or directly apply it to downstream tasks.\\n1.2.2.2\\nPermuted Language Modeling\\nWhile masked language modeling is simple and widely applied, it introduces new issues. One\\ndrawback is the use of a special token, [MASK], which is employed only during training but not'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 17}, page_content='1.2 Self-supervised Pre-training Tasks\\n11\\nat test time. This leads to a discrepancy between training and inference. Moreover, the auto-\\nencoding process overlooks the dependencies between masked tokens. For example, in the above\\nexample, the prediction of x2 (i.e., the ﬁrst masked token) is made independently of x6 (i.e., the\\nsecond masked token), though x6 should be considered in the context of x2.\\nThese issues can be addressed using the permuted language modeling approach to pre-\\ntraining [Yang et al., 2019]. Similar to causal language modeling, permuted language modeling\\ninvolves making sequential predictions of tokens. However, unlike causal modeling where predic-\\ntions follow the natural sequence of the text (like left-to-right or right-to-left), permuted language\\nmodeling allows for predictions in any order. The approach is straightforward: we determine an\\norder for token predictions and then train the model in a standard language modeling manner, as\\ndescribed in Section 1.2.1. Note that in this approach, the actual order of tokens in the text remains\\nunchanged, and only the order in which we predict these tokens differs from standard language\\nmodeling. For example, consider a sequence of 5 tokens x0x1x2x3x4. Let ei represent the em-\\nbedding of xi (i.e., combination of the token embedding and positional embedding). In standard\\nlanguage modeling, we would generate this sequence in the order of x0 →x1 →x2 →x3 →x4.\\nThe probability of the sequence can be modeled via a generation process.\\nPr(x)\\n=\\nPr(x0) · Pr(x1|x0) · Pr(x2|x0, x1) · Pr(x3|x0, x1, x2) ·\\nPr(x4|x0, x1, x2, x3)\\n=\\nPr(x0) · Pr(x1|e0) · Pr(x2|e0, e1) · Pr(x3|e0, e1, e2) ·\\nPr(x4|e0, e1, e2, e3)\\n(1.13)\\nNow, let us consider a different order for token prediction: x0 →x4 →x2 →x1 →x3. The\\nsequence generation process can then be expressed as follows:\\nPr(x)\\n=\\nPr(x0) · Pr(x4|e0) · Pr(x2|e0, e4) · Pr(x1|e0, e4, e2) ·\\nPr(x3|e0, e4, e2, e1)\\n(1.14)\\nThis new prediction order allows for the generation of some tokens to be conditioned on a\\nbroader context, rather than being limited to just the preceding tokens as in standard language\\nmodels. For example, in generating x3, the model considers both its left-context (i.e., e0, e1, e2)\\nand right-context (i.e., e4). The embeddings e0, e1, e2, e4 incorporate the positional information\\nof x0, x1, x2, x4, preserving the original order of the tokens. As a result, this approach is somewhat\\nakin to masked language modeling: we mask out x3 and use its surrounding tokens x0, x1, x2, x4\\nto predict this token.\\nThe implementation of permuted language models is relatively easy for Transformers. Be-\\ncause the self-attention model is insensitive to the order of inputs, we do not need to explicitly\\nreorder the sequence to have a factorization like Eq. (1.14). Instead, permutation can be done\\nby setting appropriate masks for self-attention. For example, consider the case of computing\\nPr(x1|e0, e4, e2). We can place x0, x1, x2, x3, x4 in order and block the attention from x3 to x1\\nin self-attention, as illustrated below'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 18}, page_content='12\\nPre-training\\nx0\\nx1\\nx2\\nx3\\nx4\\nMasks for Self-attention:\\nBlue box = valid attention\\nGray box = blocked attention\\nFor a more illustrative example, we compare the self-attention masking results of causal language\\nmodeling, masked language modeling and permuted language modeling in Figure 1.3.\\n1.2.2.3\\nPre-training Encoders as Classiﬁers\\nAnother commonly-used idea to train an encoder is to consider classiﬁcation tasks.\\nIn self-\\nsupervised learning, this is typically done by creating new classiﬁcation challenges from the unla-\\nbeled text. There are many different ways to design the classiﬁcation tasks. Here we present two\\npopular tasks.\\nA simple method, called next sentence prediction (NSP), is presented in BERT’s original\\npaper [Devlin et al., 2019]. The assumption of NSP is that a good text encoder should capture\\nthe relationship between two sentences. To model such a relationship, in NSP we can use the\\noutput of encoding two consecutive sentences SentA and SentB to determine whether SentB is\\nthe next sentence following SentA. For example, suppose SentA = ’It is raining .’ and SentB =\\n’I need an umbrella .’. The input sequence of the encoder could be\\n[CLS] It is raining . [SEP] I need an umbrella . [SEP]\\nwhere [CLS] is the start symbol (i.e., x0) which is commonly used in encoder pre-training, and\\n[SEP] is a separator that separates the two sentences. The processing of this sequence follows a\\nstandard procedure of Transformer encoding: we ﬁrst represent each token xi as its corresponding\\nembedding ei, and then feed the embedding sequence {e0, ..., em} into the encoder to obtain the\\noutput sequence {h0, ..., hm}. Since h0 is generally considered as the representation of the entire\\nsequence, we add a Softmax layer on top of it to construct a binary classiﬁcation system. This\\nprocess is illustrated as follows\\ntoken: [CLS] It\\nis\\nraining\\n.\\n[SEP]\\nI\\nneed an umbrella\\n.\\n[SEP]\\nembedding:\\ne0\\ne1\\ne2\\ne3\\ne4\\ne5\\ne6\\ne7\\ne8\\ne9\\ne10\\ne11\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\nEncoder\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\nencoding:\\nh0\\nh1\\nh2\\nh3\\nh4\\nh5\\nh6\\nh7\\nh8\\nh9\\nh10\\nh11\\n↓\\nSoftmax\\n↓\\nIs Next or Not?'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 19}, page_content='1.2 Self-supervised Pre-training Tasks\\n13\\nx0\\nx0\\nx1\\nx1\\nx2\\nx2\\nx3\\nx3\\nx4\\nx4\\nPr(x0) = 1\\nPr(x1|e0)\\nPr(x2|e0, e1)\\nPr(x3|e0, e1, e2)\\nPr(x4|e0, e1, e2, e3)\\n(a) Causal Language Modeling (order: x0 →x1 →x2 →x3 →x4)\\nx0\\nx0\\nx1\\nx1\\nx2\\nx2\\nx3\\nx3\\nx4\\nx4\\nmasked\\nmasked\\nmasked\\nmasked\\n1\\nPr(x1|e0, emask, e2, emask, e4)\\n1\\nPr(x3|e0, emask, e2, emask, e4)\\n1\\n(b) Masked Language Modeling (order: x0, [MASK], x2, [MASK], x4 →x1, x3)\\nx0\\nx0\\nx1\\nx1\\nx2\\nx2\\nx3\\nx3\\nx4\\nx4\\nPr(x0) = 1\\nPr(x1|e0, e4, e2)\\nPr(x2|e0, e4)\\nPr(x3|e0, e4, e2, e1)\\nPr(x4|e0)\\n(c) Permuted Language Modeling (order: x0 →x4 →x2 →x1 →x3)\\nFig. 1.3: Comparison of self-attention masking results of causal language modeling, masked language modeling and\\npermuted language modeling. The gray cell denotes the token at position j does not attend to the token at position i.\\nThe blue cell (i, j) denotes that the token at position j attends to the token at position i. emask represents the embedding\\nof the symbol [MASK], which is a combination of the token embedding and the positional embedding.\\nIn order to generate training samples, we need two sentences each time, one for SentA and\\nthe other for SentB. A simple way to do this is to utilize the natural sequence of two consecu-\\ntive sentences in the text. For example, we obtain a positive sample by using actual consecutive\\nsentences, and a negative sample by using randomly sampled sentences. Consequently, training\\nthis model is the same as training a classiﬁer. Typically, NSP is used as an additional training loss'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 20}, page_content='14\\nPre-training\\nfunction for pre-training based on masked language modeling.\\nA second example of training Transformer encoders as classiﬁers is to apply classiﬁcation-\\nbased supervision signals to each output of an encoder. For example, Clark et al. [2019] in their\\nELECTRA model, propose training a Transformer encoder to identify whether each input token\\nis identical to the original input or has been altered in some manner. The ﬁrst step of this method\\nis to generate a new sequence from a given sequence of tokens, where some of the tokens are\\naltered. To do this, a small masked language model (call it the generator) is applied: we randomly\\nmask some of the tokens, and train this model to predict the masked tokens. For each training\\nsample, this masked language model outputs a token at each masked position, which might be\\ndifferent from the original token. At the same time, we train another Transformer encoder (call it\\nthe discriminator) to determine whether each predicted token is the same as the original token or\\naltered. More speciﬁcally, we use the generator to generate a sequence where some of the tokens\\nare replaced. Below is an illustration.\\noriginal:\\n[CLS]\\nThe\\nboy\\nspent\\nhours\\nworking\\non\\ntoys\\n.\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\nmasked:\\n[CLS]\\nThe\\nboy\\nspent\\n[MASK]\\nworking\\non\\n[MASK]\\n.\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\nGenerator (small masked language model)\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\nreplaced:\\n[CLS]\\nThe\\nboy\\nspent\\ndecades\\nworking\\non\\ntoys\\n.\\nThen, we use the discriminator to label each of these tokens as orginal or replaced, as follows\\nreplaced: [CLS]\\nThe\\nboy\\nspent\\ndecades working\\non\\ntoys\\n.\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\nDiscriminator (the model we want)\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\n↓\\nlabel: original original original original replaced\\noriginal\\noriginal original original\\nFor training, the generator is optimized as a masked language model with maximum likelihood\\nestimation, and the discriminator is optimized as a classiﬁer using a classiﬁcation-based loss. In\\nELECTRA, the maximum likelihood-based loss and the classiﬁcation-based loss are combined for\\njointly training both the generator and discriminator. An alternative approach is to use generative\\nadversarial networks (GANs), that is, the generator is trained to fool the discriminator, and the dis-\\ncriminator is trained to distinguish the output of the generator from the true distribution. However,\\nGAN-style training complicates the training task and is more difﬁcult to scale up. Nevertheless,\\nonce training is complete, the generator is discarded, and the encoding part of the discriminator is\\napplied as the pre-trained model for downstream tasks.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 21}, page_content='1.2 Self-supervised Pre-training Tasks\\n15\\n1.2.3\\nEncoder-Decoder Pre-training\\nIn NLP, encoder-decoder architectures are often used to model sequence-to-sequence problems,\\nsuch as machine translation and question answering. In addition to these typical sequence-to-\\nsequence problems in NLP, encoder-decoder models can be extended to deal with many other\\nproblems. A simple idea is to consider text as both the input and output of a problem, and so\\nwe can directly apply encoder-decoder models. For example, given a text, we can ask a model to\\noutput a text describing the sentiment of the input text, such as positive, negative, and neutral.\\nSuch an idea allows us to develop a single text-to-text system to address any NLP problem.\\nWe can formulate different problems into the same text-to-text format. We ﬁrst train an encoder-\\ndecoder model to gain general-purpose knowledge of language via self-supervision. This model\\nis then ﬁne-tuned for speciﬁc downstream tasks using targeted text-to-text data.\\n1.2.3.1\\nMasked Encoder-Decoder Pre-training\\nIn Raffel et al. [2020]’s T5 model, many different tasks are framed as the same text-to-text task.\\nEach sample in T5 follows the format\\nSource Text\\n→\\nTarget Text\\nHere →separates the source text, which consists of a task description or instruction and the input\\ngiven to the system, from the target text, which is the response to the input task. As an example,\\nconsider a task of translating from Chinese to English. A training sample can be expressed as\\n[CLS] Translate from Chinese to English: 你好！\\n→\\n⟨s⟩Hello!\\nwhere [CLS] and ⟨s⟩are the start symbols on the source and target sides, respectively5.\\nLikewise, we can express other tasks in the same way. For example\\n[CLS] Answer: when was Albert Einstein born?\\n→\\n⟨s⟩He was born on March 14, 1879.\\n[CLS] Simplify: the professor, who has has published numerous papers in his ﬁeld,\\nwill be giving a lecture on the topic next week.\\n→\\n⟨s⟩The experienced professor will give a lecture next week.\\n[CLS] Score the translation from English to Chinese. English: when in Rome, do as\\nthe Romans do. Chinese: 人在罗马就像罗马人一样做事。\\n→\\n⟨s⟩0.81\\nwhere instructions are highlighted in gray. An interesting case is that in the last example we\\n5We could use the same start symbol for different sequences. Here we use different symbols to distinguish the\\nsequences on the encoder and decoder-sides.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 22}, page_content='16\\nPre-training\\nreframe the scoring problem as the text generation problem. Our goal is to generate a text repre-\\nsenting the number 0.81, rather than outputting it as a numerical value.\\nThe approach described above provides a new framework of universal language understanding\\nand generation. Both the task instructions and the problem inputs are provided to the system\\nin text form. The system then follows the instructions to complete the task. This method puts\\ndifferent problems together, with the beneﬁt of training a single model that can perform many\\ntasks simultaneously.\\nIn general, ﬁne-tuning is necessary for adapting the pre-trained model to a speciﬁc downstream\\ntask. In this process, one can use different ways to instruct the model for the task, such as using a\\nshort name of the task as the preﬁx to the actual input sequence or providing a detailed description\\nof the task. Since the task instructions are expressed in text form and involved as part of the input,\\nthe general knowledge of instruction can be gained through learning the language understanding\\nmodels in the pre-training phase. This may help enable zero-shot learning. For example, pre-\\ntrained models can generalize to address new problems where the task instructions have never\\nbeen encountered.\\nThere have been several powerful methods of self-supervised learning for either Transformer\\nencoders or decoders. Applying these methods to pre-train encoder-decoder models is relatively\\nstraightforward. One common choice is to train encoder-decoder models as language models. For\\nexample, the encoder receives a sequence preﬁx, while the decoder generates the remaining se-\\nquence. However, this differs from standard causal language modeling, where the entire sequence\\nis autoregressively generated from the ﬁrst token. In our case, the encoder processes the preﬁx at\\nonce, and then the decoder predicts subsequent tokens in the manner of causal language modeling.\\nPut more precisely, this is a preﬁx language modeling problem: a language model predicts the\\nsubsequent sequence given a preﬁx, which serves as the context for prediction.\\nConsider the following example\\n[CLS] The puppies are frolicking\\n|\\n{z\\n}\\nPreﬁx\\n→\\n⟨s⟩outside the house .\\n|\\n{z\\n}\\nSubsequent Sequence\\nWe can directly train an encoder-decoder model using examples like this. Then, the encoder learns\\nto understand the preﬁx, and the decoder learns to continue writing based on this understanding.\\nFor large-scale pre-training, it is easy to create a large number of training examples from unlabeled\\ntext.\\nIt is worth noting that for pre-trained encoder-decoder models to be effective in multi-lingual\\nand cross-lingual tasks, such as machine translation, they should be trained with multi-lingual\\ndata. This typically requires that the vocabulary includes tokens from all the languages. By\\ndoing so, the models can learn shared representations across different languages, thereby enabling\\ncapabilities in both language understanding and generation in a multi-lingual and cross-lingual\\ncontext.\\nA second approach to pre-training encoder-decoder models is masked language modeling. In\\nthis approach, as discussed in Section 1.2.2, tokens in a sequence are randomly replaced with a\\nmask symbol, and the model is then trained to predict these masked tokens based on the entire\\nmasked sequence.\\nAs an illustration, consider the task of masking and reconstructing the sentence'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 23}, page_content='1.2 Self-supervised Pre-training Tasks\\n17\\nThe puppies are frolicking outside the house .\\nBy masking two tokens (say, frolicking and the), we have the BERT-style input and output of the\\nmodel, as follows\\n[CLS] The puppies are [MASK] outside [MASK] house .\\n→\\n⟨s⟩\\nfrolicking\\nthe\\nHere\\ndenotes the masked position at which we do not make token predictions. By varying the\\npercentage of the tokens in the text, this approach can be generalized towards either BERT-style\\ntraining or language modeling-style training [Song et al., 2019]. For example, if we mask out all\\nthe tokens, then the model is trained to generate the entire sequence\\n[CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\\n→\\n⟨s⟩The puppies are frolicking outside the house .\\nIn this case, we train the decoder as a language model.\\nNote that, in the context of the encoder-decoder architecture, we can use the encoder to read\\nthe masked sequence, and use the decoder to predict the original sequence. With this objective,\\nwe essentially have a denoising autoencoder: the encoder transforms a corrupted input into some\\nhidden representation, and the decoder reconstructs the uncorrupted input from this hidden repre-\\nsentation. Here is an example of input and output for denoising training.\\n[CLS] The puppies are [MASK] outside [MASK] house .\\n→\\n⟨s⟩The puppies are frolicking outside the house .\\nBy learning to map from this corrupted sequence to its uncorrupted counterpart, the model gains\\nthe ability to understand on the encoder side and to generate on the decoder side. See Figure 1.4\\nfor an illustration of how an encoder-decoder model is trained with BERT-style and denoising\\nautoencoding objectives.\\nAs we randomly select tokens for masking, we can certainly mask consecutive tokens [Joshi et al.,\\n2020]. Here is an example.\\n[CLS] The puppies are [MASK] outside [MASK] [MASK] .\\n→\\n⟨s⟩The puppies are frolicking outside the house .\\nAnother way to consider consecutive masked tokens is to represent them as spans. Here we\\nfollow Raffel et al. [2020]’s work, and use [X], [Y] and [Z] to denote sentinel tokens that cover\\none or more consecutive masked tokens. Using this notation, we can re-express the above training\\nexample as\\n[CLS] The puppies are [X] outside [Y] .\\n→\\n⟨s⟩[X] frolicking [Y] the house [Z]'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 24}, page_content='18\\nPre-training\\n[CLS] The puppies are\\n[M]\\nin\\n[M] house\\n.\\nEncoder\\nDecoder\\nfrolicking\\n[M]\\n[M]\\n[M]\\n⟨s⟩\\n[M]\\nthe\\n[M]\\n[M]\\n[M]\\n[M]frolicking[M]\\nthe\\n[M]\\n[M]\\nLoss\\n(a) Training an encoder-decoder model with BERT-style masked language modeling\\n[CLS] The puppies are\\n[M]\\nin\\n[M] house\\n.\\nEncoder\\nDecoder\\nfrolicking\\nare\\npuppies\\nThe\\n⟨s⟩\\nin\\nthe\\nhouse\\nThe puppies are frolicking in\\nthe\\nhouse\\n.\\n(b) Training an encoder-decoder model with denoising autoencoding\\nLoss over the sequence\\nFig. 1.4: Training an encoder-decoder model using BERT-style and denoising autoencoding methods. In both methods,\\nthe input to the encoder is a corrupted token sequence where some tokens are masked and replaced with [MASK] (or\\n[M] for short). The decoder predicts these masked tokens, but in different ways. In BERT-style training, the decoder\\nonly needs to compute the loss for the masked tokens, while the remaining tokens in the sequence can be simply treated\\nas [MASK] tokens. In denoising autoencoding, the decoder predicts the sequence of all tokens in an autoregressive\\nmanner. As a result, the loss is obtained by accumulating the losses of all these tokens, as in standard language\\nmodeling.\\nThe idea is that we represent the corrupted sequence as a sequence containing placeholder\\nslots. The training task is to ﬁll these slots with the correct tokens using the surrounding context.\\nAn advantage of this approach is that the sequences used in training would be shorter, making the\\ntraining more efﬁcient. Note that masked language modeling provides a very general framework\\nfor training encoder-decoder models. Various settings can be adjusted to have different training\\nversions, such as altering the percentage of tokens masked and the maximum length of the masked\\nspans.\\n1.2.3.2\\nDenoising Training\\nIf we view the problem of training encoder-decoder models as a problem of training denoising\\nautoencoders, there will typically be many different methods for introducing input corruption and\\nreconstructing the input. For instance, beyond randomly masking tokens, we can also alter some\\nof them or rearrange their order.\\nSuppose we have an encoder-decoder model that can map an input sequence x to an output'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 25}, page_content='1.2 Self-supervised Pre-training Tasks\\n19\\nsequence y\\ny\\n=\\nDecodeω(Encodeθ(x))\\n=\\nModelθ,ω(x)\\n(1.15)\\nwhere θ and ω are the parameters of the encoder and the decoder, respectively. In denoising\\nautoencoding problems, we add some noise to x to obtain a noisy, corrupted input xnoise. By\\nfeeding xnoise into the encoder, we wish the decoder to output the original input. The training\\nobjective can be deﬁned as\\n(ˆθ, ˆω)\\n=\\narg max\\nθ,ω\\nLoss(Modelθ,ω(xnoise), x)\\n(1.16)\\nHere the loss function Loss(Modelθ,ω(xnoise), x) evaluates how well the model Modelθ,ω(xnoise)\\nreconstructs the original input x. We can choose the cross-entropy loss as usual.\\nAs the model architecture and the training approach have been developed, the remaining issue\\nis the corruption of the input. Lewis et al. [2020], in their BART model, propose corrupting the\\ninput sequence in several different ways.\\n• Token Masking. This is the same masking method that we used in masked language mod-\\neling. The tokens in the input sequence are randomly selected and masked.\\n• Token Deletion. This method is similar to token masking. However, rather than replacing\\nthe selected tokens with a special symbol [MASK], these tokens are removed from the\\nsequence. See the following example for a comparison of the token masking and token\\ndeletion methods.\\nOriginal (x):\\nThe puppies are frolicking outside the house .\\nToken Masking (xnoise):\\nThe puppies are [MASK] outside [MASK] house .\\nToken Deletion (xnoise):\\nThe puppies are frolicking outside the house .\\nwhere the underlined tokens in the original sequence are masked or deleted.\\n• Span Masking. Non-overlapping spans are randomly sampled over the sequence. Each\\nspan is masked by [MASK]. We also consider spans of length 0, and, in such cases, [MASK]\\nis simply inserted at a position in the sequence. For example, we can use span masking to\\ncorrupt the above sequence as\\nOriginal (x):\\nThe 0 puppies are frolicking outside the house .\\nSpan Masking (xnoise):\\nThe [MASK] puppies are [MASK] house .\\nHere the span frolicking outside the is replaced with a single [MASK]. 0 indicates a length-\\n0 span, and so we insert an [MASK] between The and puppies. Span masking introduces\\nnew prediction challenges in which the model needs to know how many tokens are gener-\\nated from a span. This problem is very similar to fertility modeling in machine translation\\n[Brown et al., 1993].'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 26}, page_content='20\\nPre-training\\nIf we consider a sequence consisting of multiple sentences, additional methods of corruption\\ncan be applied. In the BART model, there are two such methods.\\n• Sentence Reordering. This method randomly permutes the sentences so that the model can\\nlearn to reorder sentences in a document. Consider, for example, two consecutive sentences\\nHard work leads to success . Success brings happiness .\\nWe can reorder the two sentences to have a corrupted input sequence\\nSuccess brings happiness . Hard work leads to success .\\n• Document Rotation. The goal of this task is to identify the start token of the sequence.\\nFirst, a token is randomly selected from the sequence. Then, the sequence is rotated so that\\nthe selected token is the ﬁrst token. For example, suppose we select the token leads from\\nthe above sequence. The rotated sequence is\\nleads to success . Success brings happiness .\\nHard work\\nHard work\\nselected\\nwhere the subsequence Hard work before leads is appended to the end of the sequence.\\nFor pre-training, we can apply multiple corruption methods to learn robust models, for ex-\\nample, we randomly choose one of them for each training sample. In practice, the outcome of\\nencoder-decoder pre-training depends heavily on the input corruption methods used, and so we\\ntypically need to choose appropriate training objectives through careful experimentation.\\n1.2.4\\nComparison of Pre-training Tasks\\nSo far, we have discussed a number of pre-training tasks. Since the same training objective can\\napply to different architectures (e.g., using masked language modeling for both encoder-only and\\nencoder-decoder pre-training), categorizing pre-training tasks based solely on model architecture\\ndoes not seem ideal. Instead, we summarize these tasks based on the training objectives.\\n• Language Modeling. Typically, this approach refers to an auto-regressive generation pro-\\ncedure of sequences. At one time, it predicts the next token based on its previous context.\\n• Masked Language Modeling. Masked Language Modeling belongs to a general mask-\\npredict framework. It randomly masks tokens in a sequence and predicts these tokens using\\nthe entire masked sequence.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 27}, page_content='1.3 Example: BERT\\n21\\n• Permuted Language Modeling. Permuted language modeling follows a similar idea to\\nmasked language modeling, but considers the order of (masked) token prediction. It reorders\\nthe input sequence and predicts the tokens sequentially. Each prediction is based on some\\ncontext tokens that are randomly selected.\\n• Discriminative Training. In discriminative training, supervision signals are created from\\nclassiﬁcation tasks. Models for pre-training are integrated into classiﬁers and trained to-\\ngether with the remaining parts of the classiﬁers to enhance their classiﬁcation performance.\\n• Denoising Autoencoding. This approach is applied to the pre-training of encoder-decoder\\nmodels. The input is a corrupted sequence and the encoder-decoder models are trained to\\nreconstruct the original sequence.\\nTable 1.1 illustrates these methods and their variants using examples. The use of these ex-\\namples does not distinguish between models, but we mark the model architectures where the\\npre-training tasks can be applied. In each example, the input consists of a token sequence, and the\\noutput is either a token sequence or some probabilities. For generation tasks, such as language\\nmodeling, superscripts are used to indicate the generation order on the target side. If the super-\\nscripts are omitted, it indicates that the output sequence can be generated either autoregressively\\nor simultaneously. On the source side, we assume that the sequence undergoes a standard Trans-\\nformer encoding process, meaning that each token can see the entire sequence in self-attention.\\nThe only exception is in permuted language modeling, where an autoregressive generation pro-\\ncess is implemented by setting attention masks on the encoder side. To simplify the discussion,\\nwe remove the token ⟨s⟩from the target-side of each example.\\nWhile these pre-training tasks are different, it is possible to compare them in the same frame-\\nwork and experimental setup [Dong et al., 2019; Raffel et al., 2020; Lewis et al., 2020]. Note that\\nwe cannot list all the pre-training tasks here as there are many of them. For more discussions on\\npre-training tasks, the interested reader may refer to some surveys on this topic [Qiu et al., 2020;\\nHan et al., 2021].\\n1.3\\nExample: BERT\\nIn this section, we introduce BERT models, which are among the most popular and widely used\\npre-trained sequence encoding models in NLP.\\n1.3.1\\nThe Standard Model\\nThe standard BERT model, which is proposed in Devlin et al. [2019]’s work, is a Transformer\\nencoder trained using both masked language modeling and next sentence prediction tasks. The\\nloss used in training this model is a sum of the loss of the two tasks.\\nLossBERT\\n=\\nLossMLM + LossNSP\\n(1.17)\\nAs is regular in training deep neural networks, we optimize the model parameters by minimizing\\nthis loss. To do this, a number of training samples are collected. During training, a batch of'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 28}, page_content='22\\nPre-training\\nMethod Enc Dec E-D Input\\nOutput\\nCausal LM\\n•\\n•\\nThe1 kitten2 is3 chasing4 the5 ball6 .7\\nPreﬁx LM\\n•\\n•\\n[C] The kitten is\\nchasing1 the2 ball3 .4\\nMasked LM\\n•\\n•\\n[C] The kitten [M] chasing the [M] .\\nis\\nball\\nMASS-style\\n•\\n•\\n[C] The kitten [M] [M] [M] ball .\\nis chasing the\\nBERT-style\\n•\\n•\\n[C] The kitten [M] playing the [M] .\\nkitten is chasing\\nball\\nPermuted LM\\n•\\n[C] The kitten is chasing the ball .\\nThe5 kitten7 is6 chasing1 the4 ball2 .3\\nNext Sentence\\n•\\n[C] The kitten is chasing the ball .\\nPr(IsNext | representation-of-[C])\\nPrediction\\nBirds eat worms .\\nSentence\\n•\\nEncode a sentence as ha and\\nScore(ha, hb)\\nComparison\\nanother sentence as hb\\nToken Classiﬁcation\\n•\\n[C] The kitten is chasing the ball .\\nPr(·|The) Pr(·|kitten) ... Pr(·|.)\\nToken Reordering\\n•\\n[C] . kitten the chasing The is ball\\nThe1 kitten2 is3 chasing4 the5 ball6 .7\\nToken Deletion\\n•\\n[C] The kitten is chasing the ball .\\nThe1 kitten2 is3 chasing4 the5 ball6 .7\\nSpan Masking\\n•\\n[C] The kitten [M] is [M] .\\nThe1 kitten2 is3 chasing4 the5 ball6 .7\\nSentinel Masking\\n•\\n[C] The kitten [X] the [Y]\\n[X]1 is2 chasing3 [Y]4 ball5 .6\\nSentence\\n•\\n[C] The ball rolls away swiftly . The The1 kitten2 is3 chasing4 the5 ball6 .7\\nReordering\\nkitten is chasing the ball .\\nThe8 ball9 rolls10 away11 swiftly12 .13\\nDocument\\n•\\n[C] chasing the ball . The ball rolls\\nThe1 kitten2 is3 chasing4 the5 ball6 .7\\nRotation\\naway swiftly . The kitten is\\nThe8 ball9 rolls10 away11 swiftly12 .13\\nTable 1.1: Comparison of pre-training tasks, including language modeling, masked language modeling, permuted\\nlanguage modeling, discriminative training, and denoising autoencoding. [C] = [CLS], [M] = [MASK], [X], [Y] =\\nsentinel tokens. Enc, Dec and E-D indicate whether the approach can be applied to encoder-only, decoder-only,\\nencoder-decoder models, respectively. For generation tasks, superscripts are used to represent the order of the tokens.\\ntraining samples is randomly selected from this collection at a time, and LossBERT is accumulated\\nover these training samples. Then, the model parameters are updated via gradient descent or its\\nvariants. This process is repeated many times until some stopping criterion is satisﬁed, such as\\nwhen the training loss converges.\\n1.3.1.1\\nLoss Functions\\nIn general, BERT models are used to represent a single sentence or a pair of sentences, and thus\\ncan handle various downstream language understanding problems. In this section we assume that\\nthe input representation is a sequence containing two sentences SentA and SentB, expressed as\\n[CLS] SentA [SEP] SentB [SEP]\\nHere we follow the notation in BERT’s paper and use [SEP] to denote the separator.\\nGiven this sequence, we can obtain LossMLM and LossNSP separately. For masked language\\nmodeling, we predict a subset of the tokens in the sequence. Typically, a certain percentage of the'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 29}, page_content='1.3 Example: BERT\\n23\\ntokens are randomly selected, for example, in the standard BERT model, 15% of the tokens in\\neach sequence are selected. Then the sequence is modiﬁed in three ways\\n• Token Masking. 80% of the selected tokens are masked and replaced with the symbol\\n[MASK]. For example\\nOriginal:\\n[CLS] It is raining . [SEP] I need an umbrella . [SEP]\\nMasked:\\n[CLS] It is [MASK] . [SEP] I need [MASK] umbrella . [SEP]\\nwhere the selected tokens are underlined. Predicting masked tokens makes the model learn\\nto represent tokens from their surrounding context.\\n• Random Replacement. 10% of the selected tokens are changed to a random token. For\\nexample,\\nOriginal:\\n[CLS] It is raining . [SEP] I need an umbrella . [SEP]\\nRandom Token:\\n[CLS] It is raining . [SEP] I need an hat . [SEP]\\nThis helps the model learn to recover a token from a noisy input.\\n• Unchanged. 10% of the selected tokens are kept unchanged. For example,\\nOriginal:\\n[CLS] It is raining . [SEP] I need an umbrella . [SEP]\\nUnchanged Token:\\n[CLS] It is raining . [SEP] I need an umbrella . [SEP]\\nThis is not a difﬁcult prediction task, but can guide the model to use easier evidence for\\nprediction.\\nLet A(x) be the set of selected positions of a given token sequence x, and ¯x be the modiﬁed\\nsequence of x. The loss function of masked language modeling can be deﬁned as\\nLossMLM\\n=\\n−\\nX\\ni∈A(x)\\nlog Pri(xi|¯x)\\n(1.18)\\nwhere Pri(xi|¯x) is the probability of predicting xi at the position i given ¯x. Figure 1.5 shows a\\nrunning example of computing LossMLM.\\nFor next sentence prediction, we follow the method described in Section 1.2.2.3. Each training\\nsample is classiﬁed into a label set {IsNext, NotNext}, for example,\\nSequence:\\n[CLS] It is raining . [SEP] I need an umbrella . [SEP]\\nLabel:\\nIsNext\\nSequence:\\n[CLS] The cat sleeps on the windowsill . [SEP] Apples grow on trees . [SEP]\\nLabel:\\nNotNext'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 30}, page_content='24\\nPre-training\\n[CLS] It is raining . [SEP] I need an umbrella . [SEP]\\nInput:\\nSelect tokens with a probability of 15%\\n[CLS] It is raining . [SEP] I need an umbrella . [SEP]\\nToken Selection:\\nMask selected tokens with a probability of 80%\\n[CLS] It is [MASK] . [SEP] I need [MASK] umbrella . [SEP]\\nToken Masking:\\nAlter selected tokens with a probability of 10%\\n[CLS] It is [MASK] . [SEP] I need [MASK] hat . [SEP]\\nToken:\\nReplacement\\nKeep selected tokens unchanged with a probability of 10%\\n[CLS] It is [MASK] . [SEP] I need [MASK] hat . [SEP]\\nUnchanged:\\nTrain the Transformer encoder with the modiﬁed sequence\\n[CLS]\\nIt\\nis\\n[MASK]\\n.\\n[SEP]\\nI\\nneed [MASK] hat\\n.\\n[SEP]\\ne0\\ne1\\ne2\\ne3\\ne4\\ne5\\ne6\\ne7\\ne8\\ne9\\ne10\\ne11\\nh0\\nh1\\nh2\\nh3\\nh4\\nh5\\nh6\\nh7\\nh8\\nh9\\nh10\\nh11\\ntraining\\nI\\nan\\numbrella\\nTransformer Encoder\\nFig. 1.5: A running example of BERT-style masked language modeling. First, 15% tokens are randomly selected.\\nThese selected tokens are then processed in one of three ways: replaced with a [MASK] token (80% of the time),\\nreplaced with a random token (10% of the time), or kept unchanged (10% of the time). The model is trained to predict\\nthese selected tokens based on the modiﬁed sequence. ei represents the embedding of the token at the position i. Gray\\nboxes represent the Softmax layers.\\nThe output vector of the encoder for the ﬁrst token [CLS] is viewed as the sequence representation,\\ndenoted by hcls (or h0). A classiﬁer is built on top of hcls. Then, we can compute the probability of\\na label c given hcls, i.e., Pr(c|hcls). There are many loss functions one can choose for classiﬁcation\\nproblems. For example, in maximum likelihood training, we can deﬁne LossNSP as\\nLossNSP\\n=\\n−log Pr(cgold|hcls)\\n(1.19)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 31}, page_content='1.3 Example: BERT\\n25\\nwhere cgold is the correct label for this sample.\\n1.3.1.2\\nModel Setup\\nAs shown in Figure 1.6, BERT models are based on the standard Transformer encoder architecture.\\nThe input is a sequence of embeddings, each being the sum of the token embedding, the positional\\nembedding, and the segment embedding.\\ne\\n=\\nx + epos + eseg\\n(1.20)\\nBoth the token embedding (x) and positional embedding (epos) are regular, as in Transformer\\nmodels. The segment embedding (eseg) is a new type of embedding that indicates whether a token\\nbelongs to SentA or SentB. This can be illustrated by the following example.\\nToken [CLS]\\nIt\\nis\\nraining\\n.\\n[SEP]\\nI\\nneed\\nan\\numbrella\\n.\\n[SEP]\\nx\\nx0\\nx1\\nx2\\nx3\\nx4\\nx5\\nx6\\nx7\\nx8\\nx9\\nx10\\nx11\\nepos PE(0) PE(1) PE(2) PE(3) PE(4) PE(5) PE(6) PE(7) PE(8)\\nPE(9)\\nPE(10) PE(11)\\neseg\\neA\\neA\\neA\\neA\\neA\\neA\\neB\\neB\\neB\\neB\\neB\\neB\\nThe main part of BERT models is a multi-layer Transformer network. A Transformer layer\\nconsists of a self-attention sub-layer and an FFN sub-layer. Both of them follow the post-norm\\narchitecture: output = LNorm(F(input) + input), where F(·) is the core function of the sub-\\nlayer (either a self-attention model or an FFN), and LNorm(·) is the layer normalization unit.\\nTypically, a number of Transformer layers are stacked to form a deep network. At each position of\\nthe sequence, the output representation is a real-valued vector which is produced by the last layer\\nof the network.\\nThere are several aspects one may consider in developing BERT models.\\n• Vocabulary Size (|V |). In Transformers, each input token is represented as an entry in a\\nvocabulary V . Large vocabularies can cover more surface form variants of words, but may\\nlead to increased storage requirements.\\n• Embedding Size (de). Every token is represented as a de-dimensional real-valued vector.\\nAs presented above, this vector is the sum of the token embedding, positional embedding,\\nand segment embedding, all of which are also de-dimensional real-valued vectors.\\n• Hidden Size (d). The input and output of a sub-layer are of d dimensions. Besides, most\\nof the hidden states of a sub-layer are d-dimensional vectors. In general, d can be roughly\\nviewed as the width of the network.\\n• Number of Heads (nhead). In self-attention sub-layers, one needs to specify the number\\nof heads used in multi-head self-attention. The larger this number is, the more sub-spaces\\nattention is performed. In practical systems, we often set nhead ≥4.\\n• FFN Hidden Size (dﬀn). The size of the hidden layer of the FFNs used in Transformers is\\ntypically larger than d. For example, a typical setting is dﬀn = 4d. For larger Transformers,\\nsuch as recent large models, dﬀn may be set to a very large value.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 32}, page_content='26\\nPre-training\\nSelf-attention\\nPosition\\nToken\\nSegment\\nx0x1...xm\\nLayer Normalization\\nFFN\\nLayer Normalization\\nOutput Layer\\n...\\nh0h1...hm\\nlayers\\nInput\\nxi corresponds to an entry of V\\nEmbedding\\ne = x + epos + eseg ∈Rde\\nSelf-attention Sub-layer\\nhidden size: d\\nnumber of heads: nhead\\nFFN Sub-layer\\nhidden size: d\\nFFN hidden size: dﬀn\\nEncoder Output\\nhi ∈Rd is the contextual\\nrepresentation of xi\\nFig. 1.6: The model Architecture of BERT (Transformer encoder). The input tokens are ﬁrst represented as embed-\\ndings, each of which is the sum of the corresponding token embedding, positional embedding and segment embedding.\\nThen, the embedding sequence is processed by a stack of Transformer layers. Each layer in this stack includes a self-\\nattention sub-layer and a FFN sub-layer. The output of the BERT model is a sequence of vectors produced by the ﬁnal\\nTransformer layer.\\n• Model Depth (L). Using deep networks is an effective way to improve the expressive power\\nof Transformers. For BERT models, L is typically set to 12 or 24. However, networks with\\neven greater depth are also feasible and can be applied for further enhancements.\\nDifferent settings of these hyper-parameters lead to different model sizes. There are two\\nwidely-used BERT models.\\n• BERTbase: d = 768, L = 12, nhead = 12, total number of parameters = 110M.\\n• BERTlarge: d = 1, 024, L = 14, nhead = 16, total number of parameters = 340M.\\nTraining BERT models follows the standard training process of Transformers. Training larger\\nmodels such as BERTlarge requires more training effort and time. This is a common problem\\nfor pre-training, especially when a model is trained on a very large amount of data. In practice,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 33}, page_content='1.3 Example: BERT\\n27\\nthere are often considerations of training efﬁciency. For example, a practice is to ﬁrst train a\\nBERT model on relatively short sequences for a large number of training steps, and then continue\\ntraining it on full-length sequences for the remaining training steps.\\n1.3.2\\nMore Training and Larger Models\\nBERT is a milestone model in NLP, sparking many subsequent efforts to improve it. One direction\\nis to scale up the model itself, including increasing training data and developing larger models.\\nRoBERTa, an extension of the standard BERT model, is an example of such efforts [Liu et al.,\\n2019]. It introduces two major improvements. First, simply using more training data and more\\ncompute can improve BERT models without need of changing the model architectures. Second,\\nremoving the NSP loss does not decrease the performance on downstream tasks if the training is\\nscaled up. These ﬁndings suggest exploring a general direction of pre-training: we can continue\\nto improve pre-training by scaling it up on simple pre-training tasks.\\nA second approach to improving BERT models is to increase the number of model parame-\\nters. For example, in He et al. [2021]’s work, a 1.5 billion-parameter BERT-like model is built by\\nincreasing both the model depth and hidden size. However, scaling up BERT and various other\\npre-trained models introduces new challenges in training, for example, training very large models\\noften becomes unstable and difﬁcult to converge. This makes the problem more complicated, and\\nrequires careful consideration of various aspects, including model architecture, parallel computa-\\ntion, parameter initialization, and so on. In another example, Shoeybi et al. [2019] successfully\\ntrained a 3.9 billion-parameter BERT-like model, where hundreds of GPUs were used to manage\\nthe increased computational demands.\\n1.3.3\\nMore Efﬁcient Models\\nCompared to its predecessors, BERT is a relatively large model for the time it was proposed.\\nThis increase in model size results in larger memory requirements and a consequent slowdown in\\nsystem performance. Developing smaller and faster BERT models is part of the broader challenge\\nof building efﬁcient Transformers, which has been extensively discussed in Tay et al. [2020]’s\\nwork and Xiao and Zhu [2023]’s work. However, a deeper discussion of this general topic is\\nbeyond the scope of our current discussion. Here we instead consider a few efﬁcient variants of\\nBERT.\\nSeveral threads of research are of interest to NLP researchers in developing efﬁcient BERT\\nmodels. First, work on knowledge distillation, such as training student models with the output\\nof well-trained teacher models, shows that smaller BERT models can be obtained by transferring\\nknowledge from larger BERT models. Given that BERT models are multi-layer networks with\\nseveral different types of layers, knowledge distillation can be applied at different levels of repre-\\nsentation. For example, beyond distilling knowledge from the output layers, it is also possible to\\nincorporate training loss that measures the difference in output of hidden layers between teacher\\nmodels and student models [Sun et al., 2020; Jiao et al., 2020]. Indeed, knowledge distillation has\\nbeen one of the most widely-used techniques for learning small pre-trained models.\\nSecond, conventional model compression methods can be directly applied to compress BERT\\nmodels. One common approach is to use general-purpose pruning methods to prune the Trans-\\nformer encoding networks [Gale et al., 2019]. This generally involves removing entire layers'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 34}, page_content='28\\nPre-training\\n[Fan et al., 2019] or a certain percentage of parameters in the networks [Sanh et al., 2020; Chen et al.,\\n2020]. Pruning is also applicable to multi-head attention models. For example, Michel et al.\\n[2019] show that removing some of the heads does not signiﬁcantly decrease the performance\\nof BERT models, but speeds up the inference of these models. Another approach to compress-\\ning BERT models is quantization [Shen et al., 2020]. By representing model parameters as low-\\nprecision numbers, the models can be greatly compressed. While this method is not speciﬁc to\\nBERT models, it proves effective for large Transformer-based architectures.\\nThird, considering that BERT models are relatively deep and large networks, another thread\\nof research uses dynamic networks to adapt these models for efﬁcient inference. An idea in this\\nparadigm is to dynamically choose the layers for processing a token, for example, in depth-\\nadaptive models we exit at some optimal depth and thus skip the rest of the layers in the layer\\nstack [Xin et al., 2020; Zhou et al., 2020]. Similarly, we can develop length-adaptive models in\\nwhich the length of the input sequence is dynamically adjusted. For example, we can skip some of\\nthe tokens in the input sequence so that the model can reduce computational load on less important\\ntokens, enhancing overall efﬁciency.\\nFourth, it is also possible to share parameters across layers to reduce the size of BERT models.\\nA simple way to do this is to share the parameters of a whole Transformer layer across the layer\\nstack [Dehghani et al., 2018; Lan et al., 2020]. In addition to the reduced number of parameters,\\nthis enables reuse of the same layer in a multi-layer Transformer network, leading to savings of\\nmemory footprint at test time.\\n1.3.4\\nMulti-lingual Models\\nThe initial BERT model was primarily focused on English. Soon after this model was proposed,\\nit was extended to many languages. One simple way to do this is to develop a separate model\\nfor each language. Another approach, which has become more popular in recent work on large\\nlanguage models, is to train multi-lingual models directly on data from all the languages. In\\nresponse, multi-lingual BERT (mBERT) models were developed by training them on text from\\n104 languages 6. The primary difference from monolingual BERT models is that mBERT models\\nuse larger vocabularies to cover tokens from multiple languages. As a result, the representations\\nof tokens from different languages are mapped into the same space, allowing for the sharing of\\nknowledge across languages via this universal representation model.\\nOne important application of multi-lingual pre-trained models is cross-lingual learning. In the\\ncross-lingual setting, we learn a model on tasks in one language, and apply it to the same tasks\\nin another language. In cross-lingual text classiﬁcation, for example, we ﬁne-tune a multi-lingual\\npre-trained model on English annotated documents. Then, we use the ﬁne-tuned model to classify\\nChinese documents.\\nAn improvement to multi-lingual pre-trained models like mBERT is to introduce bilingual data\\ninto pre-training. Rather than training solely on monolingual data from multiple languages, bilin-\\ngual training explicitly models the relationship between tokens in two languages. The resulting\\nmodel will have innate cross-lingual transfer abilities, and thus can be easily adapted to different\\nlanguages. Lample and Conneau [2019] propose an approach to pre-training cross-lingual lan-\\nguage models (XLMs). In their work, a cross-lingual language model can be trained in either the\\ncausal language modeling or masked language modeling manner. For masked language modeling\\n6https://github.com/google-research/bert/'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 35}, page_content='1.3 Example: BERT\\n29\\npre-training, the model is treated as an encoder. The training objective is the same as BERT: we\\nmaximize the probabilities of some randomly selected tokens which are either masked, replaced\\nwith random tokens, or kept unchanged in the input. If we consider bilingual data in pre-training,\\nwe sample a pair of aligned sentences each time. Then, the two sentences are packed together to\\nform a single sequence used for training. For example, consider an English-Chinese sentence pair\\n鲸鱼是哺乳动物。↔Whales are mammals .\\nWe can pack them to obtain a sequence, like this\\n[CLS] 鲸鱼是哺乳动物。[SEP] Whales are mammals . [SEP]\\nWe then select a certain percentage of the tokens and replace them with [MASK].\\n[CLS] [MASK] 是[MASK] 动物。[SEP] Whales [MASK] [MASK] . [SEP]\\nThe goal of pre-training is to maximize the product of the probabilities of the masked tokens given\\nthe above sequence. By performing training in this way, the model can learn to represent both the\\nEnglish and Chinese sequences, as well as to capture the correspondences between tokens in the\\ntwo languages. For example, predicting the Chinese token 鲸鱼may require the information\\nfrom the English token Whales. Aligning the representations of the two languages essentially\\ntransforms the model into a “translation” model. So this training objective is also called transla-\\ntion language modeling. Figure 1.7 shows an illustration of this approach.\\nA beneﬁt of multi-lingual pre-trained models is their inherent capability of handling code-\\nswitching. In NLP and linguistics, code-switching refers to switching among languages in a text.\\nFor example, the following is a mixed language text containing both Chinese and English:\\n周末我们打算去做hiking ，你想一起来吗？\\n(We plan to go hiking this weekend, would you like to join us?)\\nFor multi-lingual pre-trained models, we do not need to identify whether a token is Chinese or\\nEnglish. Instead, every token is just an entry of the shared vocabulary. This can be imagined as\\ncreating a “new” language that encompasses all the languages we want to process.\\nThe result of multi-lingual pre-training is inﬂuenced by several factors. Given that the model\\narchitecture is ﬁxed, one needs to specify the size of the shared vocabulary, the number (or per-\\ncentage) of samples in each language, the size of the model, and so on. Conneau et al. [2020]\\npoint out several interesting issues regarding large-scale multi-lingual pre-training for XLM-like\\nmodels. First, as the number of supported languages increases, a larger model is needed to handle\\nthese languages. Second, a larger shared vocabulary is helpful for modeling the increased diver-\\nsity in languages. Third, low-resource languages more easily beneﬁt from cross-lingual transfer\\nfrom high-resource languages, particularly when similar high-resource languages are involved in\\npre-training. However, interference may occur if the model is trained for an extended period,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 36}, page_content='30\\nPre-training\\n[CLS] [MASK]\\n是\\n[MASK] 动物\\n。\\n[SEP] Whales [MASK] [MASK]\\n.\\n[SEP]\\n(zh)\\n(zh)\\n(zh)\\n(zh)\\n(zh)\\n(zh)\\n(zh)\\n(en)\\n(en)\\n(en)\\n(en)\\n(en)\\ne0\\ne1\\ne2\\ne3\\ne4\\ne5\\ne6\\ne7\\ne8\\ne9\\ne10\\ne11\\nh0\\nh1\\nh2\\nh3\\nh4\\nh5\\nh6\\nh7\\nh8\\nh9\\nh10\\nh11\\n鲸鱼\\n哺乳\\nare mammals\\nTransformer Encoder\\nFig. 1.7: An illustration of translation language modeling. For ease of understanding, we present a simple example\\nwhere all the selected tokens are masked. The model is trained to predict these masked tokens. As the sequence\\ncontains tokens in two languages, predicting a token in one language allows access to tokens in the other language,\\nthereby enabling cross-lingual modeling. In Lample and Conneau [2019]’s work, an input embedding (i.e., ei) is the\\nsum of the token embedding, positional embedding, and language embedding. This requires that each token is assigned\\nwith a language label. Thus we can distinguish tokens in different languages. In multi-lingual pre-training, particularly\\nin work using shared vocabularies, specifying the language to which a token belongs is not necessary. The use of\\nlanguage embeddings in turn makes it difﬁcult to handle code-switching. Therefore, we assume here that all token\\nrepresentations are language-independent.\\nmeaning the overall performance of the pre-trained model starts decreasing at a certain point dur-\\ning pre-training. Thus, in practical systems, one may need to stop the pre-training early to prevent\\ninterference.\\n1.4\\nApplying BERT Models\\nOnce a BERT model is pre-trained, it can then be used to solve NLP problems. But BERT models\\nare not immediately ready for performing speciﬁc downstream tasks. In general, additional ﬁne-\\ntuning work is required to make them adapt. As a ﬁrst step, we need a predictor to align the\\noutput of the model with the problem of interest. Let BERTˆθ(·) be a BERT model with pre-\\ntrained parameters ˆθ, and Predictω(·) be a prediction network with parameters ω. By integrating\\nthe prediction network with the output of the BERT model, we develop a model to tackle the\\ndownstream tasks. This model can be expressed as\\ny\\n=\\nPredictω(BERTˆθ(x))\\n(1.21)\\nwhere x is the input and y is the output that ﬁts the problem. For example, in classiﬁcation\\nproblems, the model outputs a probability distribution over labels.\\nThen, we collect a set of labeled samples D, and ﬁne-tune the model by\\n(˜ω, ˜θ)\\n=\\narg min\\nω,ˆθ+\\nX\\n(x,ygold)∈D\\nLoss(yω,ˆθ+, ygold)\\n(1.22)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 37}, page_content='1.4 Applying BERT Models\\n31\\nwhere (x, ygold) represents a tuple of an input and its corresponding output. The notation of this\\nequation seems a bit complicated, but the training/tuning process is standard. We optimize the\\nmodel by minimizing the loss over the tuning samples. The outcome is the optimized parameters\\n˜ω and ˜θ. The optimization starts with the pre-trained parameters ˆθ. Here we use ˆθ+ to indicate that\\nthe parameters are initialized with ˆθ, and use yω,ˆθ+ to denote the model output computed using\\nthe parameters ω and ˆθ+.\\nWith the ﬁne-tuned parameters ˜ω and ˜θ, we can apply the model Predict˜ω(BERT˜θ(·)) to new\\ndata of the same tasks for which the model was ﬁne-tuned. The form of the downstream tasks\\ndetermines the input and output formats of the model, as well as the architecture of the prediction\\nnetwork. In the following we list some tasks to which BERT models are generally suited.\\n• Classiﬁcation (Single Text). One of the most widely-used applications of BERT models is\\ntext classiﬁcation. In this task, a BERT model receives a sequence of tokens and encodes\\nit as a sequence of vectors. The ﬁrst output vector hcls (or h0) is typically used as the\\nrepresentation of the entire text. The prediction network takes hcls as input to produce a\\ndistribution of labels. Let [CLS]x1x2...xm be an input text. See below for an illustration of\\nBERT-based text classiﬁcation.\\n[CLS]\\nx1\\nx2\\n...\\nxm\\n[SEP]\\necls\\ne1\\ne2\\n...\\nem\\nem+1\\nhcls\\nh1\\nh2\\n...\\nhm\\nhm+1\\nClass\\nBERT\\nHere the gray box denotes the prediction network. Many NLP problems can be categorized\\nas text classiﬁcation tasks, and there have been several text classiﬁcation benchmarks for\\nevaluating pre-trained models. For example, we can classify texts by their grammatical cor-\\nrectness (grammaticality) or emotional tone (sentiment) [Socher et al., 2013; Warstadt et al.,\\n2019]. Note that the prediction network could be any classiﬁcation model, such as a deep\\nneural network or a more traditional classiﬁcation model. The entire model can then be\\ntrained or ﬁne-tuned in the manner of a standard classiﬁcation model. For example, the pre-\\ndiction network can be simply a Softmax layer and the model parameters can be optimized\\nby maximizing the probabilities of the correct labels.\\n• Classiﬁcation (Pair of Texts). Classiﬁcation can also be performed on a pair of texts. Sup-\\npose we have two texts, x1...xm and y1...yn. We can concatenate these texts to form a single\\nsequence with a length len. Then, we predict a label for this combined text sequence based\\non the hcls vector, as follows'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 38}, page_content='32\\nPre-training\\n[CLS]\\nx1\\nx2\\n...\\nxm\\n[SEP]\\ny1\\ny2\\n...\\nyn\\n[SEP]\\nText 1\\nText 2\\necls\\ne1\\ne2\\n...\\nem\\nem+1\\nem+2\\nem+3\\n...\\nelen−1\\nelen\\nhcls\\nh1\\nh2\\n...\\nhm\\nhm+1 hm+2 hm+3\\n...\\nhlen−1\\nhlen\\nClass\\nBERT\\nwhere len = n + m + 2. Text pair classiﬁcation covers several problems, including se-\\nmantic equivalence judgement (determine whether two texts are semantically equivalent)\\n[Dolan and Brockett, 2005], text entailment judgement (determine whether a hypothesis\\ncan be logically inferred or entailed from a premise) [Bentivogli and Giampiccolo, 2011;\\nWilliams et al., 2018], grounded commonsense inference (determine whether an event is\\nlikely to happen given its context) [Zellers et al., 2018], and question-answering inference\\n(determine whether an answer corresponds to a given question).\\n• Regression. Instead of generating a label distribution, we can have the prediction network\\noutput a real-valued score. For example, by adding a Sigmoid layer to the prediction net-\\nwork, the system can be employed to compute the similarity between two given sentences.\\nThe architecture is the same as that of BERT-based classiﬁcation systems, with only the\\nchange of the output layer.\\n[CLS]\\nx1\\nx2\\n...\\nxm\\n[SEP]\\ny1\\ny2\\n...\\nyn\\n[SEP]\\nText 1\\nText 2\\necls\\ne1\\ne2\\n...\\nem\\nem+1\\nem+2\\nem+3\\n...\\nelen−1\\nelen\\nhcls\\nh1\\nh2\\n...\\nhm\\nhm+1 hm+2 hm+3\\n...\\nhlen−1\\nhlen\\nNumber (similarity, evaluation score, etc.)\\nBERT\\nFor training or ﬁne-tuning, we can minimize the regression loss of the model output as\\nusual.\\n• Sequence Labeling. Sequence labeling is a machine learning approach applicable to a wide\\nrange of NLP problems. This approach assigns a label to each token in an input sequence,\\nand some linguistic annotations can then be derived from this sequence of labels. An ex-\\nample of sequence labeling in NLP is part-of-speech (POS) tagging. We label each word\\nin a sentence with its corresponding POS tag. Another example is named entity recognition\\n(NER) in which we label each word with an NER tag, and named entities are identiﬁed\\nusing these tags. See below for an illustration of the model architecture for NER.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 39}, page_content='1.4 Applying BERT Models\\n33\\n[CLS]\\nx1\\nx2\\n...\\nxm\\n[SEP]\\necls\\ne1\\ne2\\n...\\nem\\nem+1\\nhcls\\nh1\\nh2\\n...\\nhm\\nhm+1\\n{B, I, O}{B, I, O}\\n{B, I, O}\\nTag\\nTag\\nTag\\nBERT\\nHere {B, I, O} is the tag set of NER. For example, B-ORG means the beginning of an\\norganization, I-ORG means the word is inside an organization, and O means the word does\\nnot belong to any named entity. This NER model can output a distribution over the tag set\\nat each position, denoted as pi. The training or ﬁne-tuning of the model can be performed\\nover these distributions {p1, ..., pm}. For example, suppose pi(tagi) is the probability of\\nthe correct tag at position i. The training loss can be deﬁned to be the negative likelihood\\nLoss\\n=\\n−1\\nm\\nm\\nX\\ni=1\\nlog pi(tagi)\\n(1.23)\\nFinding the best label sequence given a trained NER model is a well-studied issue in NLP.\\nThis is often achieved via dynamic programming, which, in the context of path ﬁnding over\\na lattice, has linear complexity [Huang, 2009].\\n• Span Prediction. Some NLP tasks require predicting a span in a text. A common example\\nis reading comprehension. In this task, we are given a query x1...xm and a context text\\ny1...yn. The goal is to identify a continuous span in y1...yn that best answers the query.\\nThis problem can be framed as a sequence labeling-like task in which we predict a label for\\neach yj to indicate the beginning or ending of the span. Following Seo et al. [2017], we add\\ntwo networks on top of the BERT output for yj: one for generating the probability of yj\\nbeing the beginning of the span (denoted by pbeg\\nj\\n), and one for generating the probability\\nof yj being the ending of the span (denoted by pend\\nj\\n). The resulting model architecture is\\nshown as follows\\n[CLS]\\nx1\\nx2\\n...\\nxm\\n[SEP]\\ny1\\ny2\\n...\\nyn\\n[SEP]\\nQuery\\nContext Text\\necls\\ne1\\ne2\\n...\\nem\\nem+1\\nem+2\\nem+3\\n...\\nelen−1\\nelen\\nhcls\\nh1\\nh2\\n...\\nhm\\nhm+1 hm+2 hm+3\\n...\\nhlen−1\\nhlen\\n(pbeg\\n1\\n)\\nBeg\\n(pend\\n1\\n)\\nEnd\\n(pbeg\\n2\\n)\\nBeg\\n(pend\\n2\\n)\\nEnd\\n(pbeg\\nn\\n)\\nBeg\\n(pend\\nn\\n)\\nEnd\\nBERT'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 40}, page_content='34\\nPre-training\\nWe pack the query and context text together to obtain the input sequence. The prediction\\nnetworks are only applied to outputs for the context text, generating the probabilities pbeg\\nj\\nand pend\\nj\\nat each position. The loss can be computed by summing the log likelihoods of the\\ntwo models across the entire context text.\\nLoss\\n=\\n−1\\nn\\nn\\nX\\nj=1\\n\\x00 log pbeg\\nj\\n+ log pend\\nj\\n\\x01\\n(1.24)\\nAt test time, we search for the best span by\\n(ˆj1, ˆj2)\\n=\\narg max\\n1≤j1≤j2≤n\\n\\x00 log pbeg\\nj1 + log pend\\nj2\\n\\x01\\n(1.25)\\n• Encoding for Encoder-decoder Models. While our focus in this section has been primarily\\non language understanding problems, it is worth noting that BERT models can be applied\\nto a broader range of NLP tasks. In fact, BERT models can be used in all the scenarios\\nwhere we need to encode a piece of text. One application that we have not mentioned is\\ntext generation which includes a range of tasks such as machine translation, summarization,\\nquestion answering, and dialogue generation. These tasks can be formulated as sequence-\\nto-sequence problems: we use an encoder to represent the source text, and a decoder to\\ngenerate the corresponding target text. A straightforward method to apply BERT models\\nis to consider them as encoders. Before ﬁne-tuning, we can initialize the parameters of the\\nencoder with those from a pre-trained BERT model. Then, the encoder-decoder model can\\nbe ﬁne-tuned on pairs of texts as usual. The following shows the architecture of a neural\\nmachine translation system where a BERT model is applied on the source side.\\n[CLS]\\nx1\\n...\\nxm\\n[SEP]\\nSource Text\\nex\\ncls\\nex\\n1\\n...\\nex\\nm\\nex\\nm+1\\nBERT (Encoder)\\nAdapter\\n⟨s⟩\\ny1\\ny2\\n...\\nyn−1\\ney\\n0\\ney\\n1\\ney\\n2\\n...\\ney\\nn−1\\nDecoder\\ny1\\ny2\\ny3\\n...\\nyn\\nTarget Text\\nHere x1...xm denotes the source sequence, y1...yn denotes the target sequence, ex\\n1...ex\\nm\\ndenotes the embedding sequence of x1...xm, and ey\\n1...ey\\nn denotes the embedding sequence\\nof y1...yn. The adapter, which is optional, maps the output of the BERT model to the form\\nthat is better suited to the decoder.\\nFine-tuning BERT models is a complicated engineering problem, inﬂuenced by many factors,\\nsuch as the amount of ﬁne-tuning data, the model size, and the optimizer used in ﬁne-tuning.\\nIn general, we wish to ﬁne-tune these models sufﬁciently so that they can perform well in the\\ndownstream tasks. However, ﬁne-tuning BERT models for speciﬁc tasks may lead to overﬁtting,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 41}, page_content='1.5 Summary\\n35\\nwhich in turn reduces their ability to generalize to other tasks. For example, suppose we have a\\nBERT model that performs well on a particular task. If we then ﬁne-tune it for new tasks, this\\nmay decrease its performance on the original task. This problem is related to the catastrophic\\nforgetting problem in continual training, where a neural network forgets previously learned in-\\nformation when updated on new samples. In practical applications, a common way to alleviate\\ncatastrophic forgetting is to add some old data into ﬁne-tuning and train the model with more\\ndiverse data. Also, one may use methods specialized to catastrophic forgetting, such as experi-\\nence replay [Rolnick et al., 2019] and elastic weight consolidation [Kirkpatrick et al., 2017]. The\\ninterested reader can refer to some surveys for more detailed discussions of this issue in continual\\nlearning [Parisi et al., 2019; Wang et al., 2023a;e].\\n1.5\\nSummary\\nIn this chapter we have discussed the general idea of pre-training in NLP. In particular, we have dis-\\ncussed self-supervised pre-training and its application to encode-only, decoder-only, and encoder-\\ndecoder architectures. Moreover, we have presented and compared a variety of pre-training tasks\\nfor these architectures. As an example, BERT is used to illustrate how sequence models are pre-\\ntrained via masked language modeling and applied to different downstream tasks.\\nRecent years have shown remarkable progress in NLP, led by the large-scale use of self-\\nsupervised pre-training. And sweeping advances are being made across many tasks, not only\\nin NLP but also in computer vision and other areas of AI. One idea behind these advances is that a\\nsigniﬁcant amount of knowledge about the world can be learned by simply training these AI sys-\\ntems on huge amounts of unlabeled data. For example, a language model can learn some general\\nknowledge of a language by repeatedly predicting masked words in large-scale text. As a result,\\nthis pre-trained language model can serve as a foundation model, which can be easily adapted to\\naddress speciﬁc downstream NLP tasks. This paradigm shift in NLP has enabled the development\\nof incredibly powerful systems for language understanding, generation, and reasoning [Manning,\\n2022]. However, it is important to recognize that we are still in the early stages of creating truly in-\\ntelligent systems, and there is a long way to go. Nevertheless, large-scale pre-training has opened\\na door to intelligent systems that researchers have long aspired to develop, though several key re-\\nsearch areas remain open for exploration, such as learning intelligence efﬁciently using reasonably\\nsmall-sized data and acquiring complex reasoning and planning abilities.\\nNote that this chapter is mostly introductory and cannot cover all aspects of pre-training. For\\nexample, there are many methods to ﬁne-tune a pre-trained model, offering different ways to better\\nadapt the model to diverse situations. Moreover, large language models, which are considered one\\nof the most signiﬁcant achievements in AI in recent years, are skipped in this section. We leave\\nthe discussion of these topics to the following chapters.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 42}, page_content='CHAPTER 2\\nGenerative Models\\nOne of the most signiﬁcant advances in NLP in recent years might be the development of large\\nlanguage models (LLMs). This has helped create systems that can understand and generate nat-\\nural languages like humans. These systems have even been found to be able to reason, which\\nis considered a very challenging AI problem. With these achievements, NLP made big strides\\nand entered a new era of research in which difﬁcult problems are being solved, such as building\\nconversational systems that can communicate with humans smoothly.\\nThe concept of language modeling or probabilistic language modeling dates back to early ex-\\nperiments conducted by Shannon [1951]. In his work, a language model was designed to estimate\\nthe predictability of English — how well can the next letter of a text be predicted when the pre-\\nceding N letters are known. Although Shannon’s experiments were preliminary, the fundamental\\ngoals and methods of language modeling have remained largely unchanged over the decades since\\nthen. For quite a long period, particularly before 2010, the dominant approach to language mod-\\neling was the n-gram approach [Jurafsky and Martin, 2008]. In n-gram language modeling, we\\nestimate the probability of a word given its preceding n −1 words, and thus the probability of a\\nsequence can be approximated by the product of a series of n-gram probabilities. These proba-\\nbilities are typically estimated by collecting smoothed relative counts of n-grams in text. While\\nsuch an approach is straightforward and simple, it has been extensively used in NLP. For example,\\nthe success of modern statistical speech recognition and machine translation systems has largely\\ndepended on the utilization of n-gram language models [Jelinek, 1998; Koehn, 2010].\\nApplying neural networks to language modeling has long been attractive, but a real break-\\nthrough appeared as deep learning techniques advanced. A widely cited study is Bengio et al.\\n[2003]’s work where n-gram probabilities are modeled via a feed-forward network and learned\\nby training the network in an end-to-end fashion. A by-product of this neural language model\\nis the distributed representations of words, known as word embeddings. Rather than represent-\\ning words as discrete variables, word embeddings map words into low-dimensional real-valued\\nvectors, making it possible to compute the meanings of words and word n-grams in a continu-\\nous representation space. As a result, language models are no longer burdened with the curse of\\ndimensionality, but can represent exponentially many n-grams via a compact and dense neural\\nmodel.\\nThe idea of learning word representations through neural language models inspired subsequent\\nresearch in representation learning in NLP. However, this approach did not attract signiﬁcant in-\\nterest in developing NLP systems in the ﬁrst few years after its proposal. Starting in about 2012,\\nthough, advances were made in learning word embeddings from large-scale text via simple word\\nprediction tasks. Several methods, such as Word2Vec, were proposed to effectively learn such\\nembeddings, which were then successfully applied in a variety of NLP systems [Mikolov et al.,\\n2013a;b]. As a result of these advances, researchers began to think of learning representations of\\nsequences using more powerful language models, such as LSTM-based models [Sutskever et al.,\\n2014; Peters et al., 2018]. And further progress and interest in sequence representation exploded\\nafter Transformer was proposed. Alongside the rise of Transformer, the concept of language mod-\\neling was generalized to encompass models that learn to predict words in various ways. Many\\n36'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 43}, page_content='2.1 A Brief Introduction to LLMs\\n37\\npowerful Transformer-based models were pre-trained using these word prediction tasks, and suc-\\ncessfully applied to a variety of downstream tasks [Devlin et al., 2019].\\nIndeed, training language models on large-scale data has led NLP research to exciting times.\\nWhile language modeling has long been seen as a foundational technique with no direct link to\\nthe goals of artiﬁcial intelligence that researchers had hoped for, it helps us see the emergence of\\nintelligent systems that can learn a certain degree of general knowledge from repeatedly predicting\\nwords in text. Recent research demonstrates that a single, well-trained LLM can handle a large\\nnumber of tasks and generalize to perform new tasks with a small adaptation effort [Bubeck et al.,\\n2023]. This suggests a step towards more advanced forms of artiﬁcial intelligence, and inspires\\nfurther exploration into developing more powerful language models as foundation models.\\nIn this chapter, we consider the basic concepts of generative LLMs. For simplicity, we use the\\nterms large language models or LLMs to refer to generative models like GPT, though this term\\ncan broadly cover other types of models like BERT. We begin by giving a general introduction\\nto LLMs, including the key steps of building such models. We then discuss two scaling issues of\\nLLMs: how LLMs are trained at scale, and how LLMs can be improved to handle very long texts.\\nFinally, we give a summary of these discussions.\\n2.1\\nA Brief Introduction to LLMs\\nIn this section we give an introduction to the basic ideas of LLMs as required for the rest of this\\nchapter and the following chapters. We will use terms word and token interchangeably. Both\\nof them refer to the basic units used in language modeling, though their original meanings are\\ndifferent.\\nBefore presenting details, let us ﬁrst consider how language models work. The goal of lan-\\nguage modeling is to predict the probability of a sequence of tokens occurring. Let {x0, x1, ..., xm}\\nbe a sequence of tokens, where x0 is the start symbol ⟨s⟩(or ⟨SOS⟩)1. The probability of this se-\\nquence can be deﬁned using the chain rule\\nPr(x0, ..., xm)\\n=\\nPr(x0) · Pr(x1|x0) · Pr(x2|x0, x1) · · · Pr(xm|x0, ..., xm−1)\\n=\\nm\\nY\\ni=0\\nPr(xi|x0, ..., xi−1)\\n(2.1)\\nor alternatively in a logarithmic form\\nlog Pr(x0, ..., xm)\\n=\\nm\\nX\\ni=0\\nlog Pr(xi|x0, ..., xi−1)\\n(2.2)\\nHere Pr(xi|x0, ..., xi−1) is the probability of the token xi given all its previous tokens {x0, ..., xi−1}\\n2.\\nIn the era of deep learning, a typical approach to language modeling is to estimate this\\n1The start symbol can also be [CLS] following BERT models.\\n2We assume that when i\\n=\\n0, Pr(xi|x0, ..., xi−1)\\n=\\nPr(x0)\\n=\\n1.\\nHence Pr(x0, ..., xm)\\n=\\nPr(x0) Pr(x1, ..., xm|x0) = Pr(x1, ..., xm|x0).'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 44}, page_content='38\\nGenerative Models\\nContext\\nPredict\\nDecision Rule\\nSequence Probability\\n⟨s⟩a\\nb\\narg maxx2∈V Pr(x2|⟨s⟩a)\\nPr(⟨s⟩) · Pr(a|⟨s⟩)· Pr(b|⟨s⟩a)\\n⟨s⟩a b\\nc\\narg maxx3∈V Pr(x3|⟨s⟩a b)\\nPr(⟨s⟩) · Pr(a|⟨s⟩) · Pr(b|⟨s⟩a)·\\nPr(c|⟨s⟩a b)\\n⟨s⟩a b c\\nd\\narg maxx4∈V Pr(x4|⟨s⟩a b c)\\nPr(⟨s⟩) · Pr(a|⟨s⟩) · Pr(b|⟨s⟩a)·\\nPr(c|⟨s⟩a b)· Pr(d|⟨s⟩a b c)\\nTable 2.1: Illustration of generating the three tokens b c d given the preﬁx ⟨s⟩a via a language model. In each step,\\nthe model picks a token xi from V so that Pr(xi|x0, ..., xi−1) is maximized. This token is then appended to the end\\nof the context sequence. In the next step, we repeat the same process, but based on the new context.\\nprobability using a deep neural network. Neural networks trained to accomplish this task re-\\nceive a sequence of tokens x0, ..., xi−1 and produce a distribution over the vocabulary V (de-\\nnoted by Pr(·|x0, ..., xi−1)). The probability Pr(xi|x0, ..., xi−1) is the value of the i-th entry of\\nPr(·|x0, ..., xi−1).\\nWhen applying a trained language model, a common task is to ﬁnd the most likely token given\\nits previous context tokens. This token prediction task can be described as\\nˆxi\\n=\\narg max\\nxi∈V\\nPr(xi|x0, ..., xi−1)\\n(2.3)\\nWe can perform word prediction multiple times to generate a continuous text: each time we\\npredict the best token ˆxi, and then add this predicted token to the context for predicting the next\\ntoken ˆxi+1. This results in a left-to-right generation process implementing Eqs. (2.1) and (2.2). To\\nillustrate, consider the generation of the following three words given the preﬁx ‘⟨s⟩a’, as shown\\nin Table 2.1. Now we discuss how LLMs are constructed, trained, and applied.\\n2.1.1\\nDecoder-only Transformers\\nAs is standard practice, the input of a language model is a sequence of tokens (denoted by\\n{x0, ..., xm−1}).\\nFor each step, an output token is generated, shifting the sequence one po-\\nsition forward for the next prediction.\\nTo do this, the language model outputs a distribution\\nPr(·|x0, ..., xi−1) at each position i, and the token xi is selected according to this distribution.\\nThis model is trained by maximizing the log likelihood Pm\\ni=1 log Pr(xi|x0, ..., xi−1)3.\\nHere, we focus on the decoder-only Transformer architecture, as it is one of the most popular\\nmodel architectures used in LLMs. The input sequence of tokens is represented by a sequence\\nof de-dimensional vectors {e0, ..., em−1}. ei is the sum of the token embedding of xi and the\\npositional embedding of i. The major body of the model is a stack of Transformer blocks (or\\nlayers). Each Transformer block has two stacked sub-layers, one for self-attention modeling and\\none for FFN modeling. These sub-layers can be deﬁned using the post-norm architecture\\noutput\\n=\\nLNorm(F(input) + input)\\n(2.4)\\n3Note that Pm\\ni=1 log Pr(xi|x0, ..., xi−1) = Pm\\ni=0 log Pr(xi|x0, ..., xi−1) since log Pr(x0) = 0.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 45}, page_content='2.1 A Brief Introduction to LLMs\\n39\\nor the pre-norm architecture\\noutput\\n=\\nLNorm(F(input)) + input\\n(2.5)\\nwhere input and output denote the input and output, both being an m × d matrix. The i-th rows\\nof input and output can be seen as contextual representations of the i-th token in the sequence.\\nF(·) is the core function of a sub-layer. For FFN sub-layers, F(·) is a multi-layer FFN. For\\nself-attention sub-layers, F(·) is a multi-head self-attention function. In general, self-attention is\\nexpressed in a form of QKV attention\\nAttqkv(Q, K, V)\\n=\\nSoftmax(QKT\\n√\\nd\\n+ Mask)V\\n(2.6)\\nwhere Q, K and V ∈Rm×d are the queries, keys, and values, respectively. It is important to\\nnote that only previous tokens are considered when predicting a token. So a masking variable\\nMask ∈Rm×m is incorporated into self-attention to achieve this. The entry (i, k) of Mask has\\na value of 0 if i ≤k, and a value of −inf otherwise.\\nGiven a representation H ∈Rm×d, the multi-head self-attention function can be deﬁned as\\nF(H)\\n=\\nMerge(head1, ..., headτ)Whead\\n(2.7)\\nwhere Merge(·) representees a concatenation of its inputs, and Whead ∈Rd×d represents a pa-\\nrameter matrix. headj is the output of QKV attention on a sub-space of representation\\nheadj\\n=\\nAttqkv(Q[j], K[j], V[j])\\n(2.8)\\nQ[j],K[j],and V[j] are the queries, keys, and values projected onto the j-th sub-space via linear\\ntransformations\\nQ[j]\\n=\\nHWq\\nj\\n(2.9)\\nK[j]\\n=\\nHWk\\nj\\n(2.10)\\nV[j]\\n=\\nHWv\\nj\\n(2.11)\\nwhere Wq\\nj, Wk\\nj , and Wv\\nj ∈Rd× d\\nτ are the parameter matrices of the transformations.\\nSuppose we have L Transformer blocks. A Softmax layer is built on top of the output of the\\nlast block. The Softmax layer outputs a sequence of m distributions over the vocabulary, like this\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nPr(·|x0, ..., xm−1)\\n...\\nPr(·|x0, x1)\\nPr(·|x0)\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n=\\nSoftmax(HLWo)\\n(2.12)\\nwhere HL is the output of the last Transformer block, and Wo ∈Rd×|V | is the parameter matrix.\\nFigure 2.1 shows the Transformer architecture for language modeling. Applying this language'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 46}, page_content='40\\nGenerative Models\\nx0\\nx1\\n...\\nxm−1\\ne0\\ne1\\n...\\nem−1\\nhL\\n0\\nhL\\n1\\n...\\nhL\\nm−1\\n...\\nPr(x1|x0)\\nPr(x2|x0x1)\\nPr(xm|x0x1...xm−1)\\nx1\\nx2\\n...\\nxm\\nLanguage Model\\nz0\\nz1\\n...\\nzm−1\\nPost-norm or Pre-norm\\nPost-norm or Pre-norm\\nSelf-attention\\nFFN\\nL Blocks\\nFig. 2.1: The Transformer-decoder architecture for language modeling. The central components are L stacked Trans-\\nformer blocks, each comprising a self-attention sub-layer and an FFN sub-layer. To prevent the model from accessing\\nthe right-context, a masking variable is incorporated into self-attention. The output layer uses a Softmax function to\\ngenerate a probability distribution for the next token, given the sequence of previous tokens. During inference, the\\nmodel takes the previously predicted token to predict the next one, repeating this process until the end of the sequence\\nis reached. {z0, ..., zm−1} denote the inputs of a Transformer block, and {hL\\n0 , ..., hL\\nm−1} denote the outputs of the\\nlast Transformer block.\\nmodel follows an autoregressive process. Each time the language model takes a token xi−1 as\\ninput and predicts a token xi that maximizes the probability Pr(xi|x0, ..., xi−1). It is important\\nto note that, despite different implementation details, many LLMs share the same architecture\\ndescribed above. These models are called large because both their depth and width are signiﬁcant.\\nTable 2.2 shows the model sizes for a few LLMs, as well as their model setups.\\n2.1.2\\nTraining LLMs\\nNow suppose that we are given a training set D comprising K sequences. The log-likelihood of\\neach sequence x = x0...xm in D can be calculated using a language model\\nLθ(x)\\n=\\nm\\nX\\ni=1\\nlog Prθ(xi|x0, ..., xi−1)\\n(2.13)\\nHere the subscript θ afﬁxed to L(·) and Pr(·) denotes the parameters of the language model. Then,\\nthe objective of maximum likelihood training is deﬁned as\\nˆθ\\n=\\narg max\\nθ\\nX\\nx∈D\\nLθ(x)\\n(2.14)\\nTraining Transformer-based language models with the above objective is commonly viewed\\nas a standard optimization process for neural networks. This can be achieved using gradient de-\\nscent algorithms, which are widely supported by off-the-shelf deep learning toolkits. Somewhat'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 47}, page_content='2.1 A Brief Introduction to LLMs\\n41\\nLLM\\n# of Parameters\\nDepth L\\nWidth d\\n# of Heads\\n(Q/KV)\\nGPT-1 [Radford et al., 2018]\\n0.117B\\n12\\n768\\n12/12\\nGPT-2 [Radford et al., 2019]\\n1.5B\\n48\\n1,600\\n25/25\\nGPT-3 [Brown et al., 2020]\\n175B\\n96\\n12,288\\n96/96\\nLLaMA2 [Touvron et al., 2023b]\\n7B\\n32\\n4,096\\n32/32\\n13B\\n40\\n5,120\\n40/40\\n70B\\n80\\n8,192\\n64/64\\nLLaMA3/3.1 [Dubey et al., 2024]\\n8B\\n32\\n4,096\\n32/8\\n70B\\n80\\n8,192\\n64/8\\n405B\\n126\\n16,384\\n128/8\\nGemma2 [Team et al., 2024]\\n2B\\n26\\n2,304\\n8/4\\n9B\\n42\\n3,584\\n16/8\\n37B\\n46\\n4,608\\n32/16\\nQwen2.5 [Yang et al., 2024]\\n0.5B\\n24\\n896\\n14/2\\n7B\\n28\\n3,584\\n28/4\\n72B\\n80\\n8,192\\n64/8\\nDeepSeek-V3 [Liu et al., 2024a]\\n671B\\n61\\n7,168\\n128/128\\nFalcon [Penedo et al., 2023]\\n7B\\n32\\n4,544\\n71/71\\n40B\\n60\\n8,192\\n128/128\\n180B\\n80\\n14,848\\n232/232\\nMistral [Jiang et al., 2023a]\\n7B\\n32\\n4,096\\n32/32\\nTable 2.2: Comparison of some LLMs in terms of model size, model depth, model width, and number of heads (a/b\\nmeans a heads for queries and b heads for both keys and values).\\nsurprisingly, better results were continuously yielded as language models were evolved into more\\ncomputationally intensive models and trained on larger datasets [Kaplan et al., 2020]. These suc-\\ncesses have led NLP researchers to continue increasing both the training data and model size in\\norder to build more powerful language models.\\nHowever, as language models become larger, we confront new training challenges, which\\nsigniﬁcantly change the problem compared to training relatively small models. One of these\\nchallenges arises from the need for large-scale distributed systems to manage the data, model\\nparameters, training routines, and so on. Developing and maintaining such systems requires a\\nsigniﬁcant amount of work in both software and hardware engineering, as well as expertise in deep\\nlearning. A related issue is that when the training is scaled up, we need more computing resources\\nto ensure the training process can be completed in an acceptable time. For example, it generally\\nrequires hundreds or thousands of GPUs to train an LLM with tens of billions of parameters\\nfrom scratch. This requirement drastically increases the cost of training such models, especially\\nconsidering that many training runs are needed as these models are developed. Also, from the\\nperspective of deep learning, the training process can become unstable if the neural networks are\\nvery deep and/or the model size is very large. In response, we typically need to modify the model\\narchitecture to adapt LLMs to large-scale training. In Section 2.2 we will present more discussions\\non these issues.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 48}, page_content='42\\nGenerative Models\\n2.1.3\\nFine-tuning LLMs\\nOnce we have pre-trained an LLM, we can then apply it to perform various NLP tasks. Tradi-\\ntionally language models are used as components of other systems, for example, they are widely\\napplied to score translations in statistical machine translation systems. By contrast, in generative\\nAI, LLMs are considered complete systems and are employed to address NLP problems by mak-\\ning use of their generation nature. A common approach is to describe the task we want to address\\nin text and then prompt LLMs to generate text based on this description. This is a standard text\\ngeneration task where we continue or complete the text starting from a given context.\\nMore formally, let x = x0...xm denote a token sequence of context given by users, and\\ny = y1...yn denote a token sequence following the context. Then, the inference of LLMs can be\\ndeﬁned as a problem of ﬁnding the most likely sequence y based on x:\\nˆy\\n=\\narg max\\ny\\nlog Pr(y|x)\\n=\\narg max\\ny\\nn\\nX\\ni=1\\nlog Pr(yi|x0, ..., xm, y1, ..., yi−1)\\n(2.15)\\nHere Pn\\ni=1 log Pr(yi|x0, ..., xm, y1, ..., yi−1) essentially expresses the same thing as the right-\\nhand side of Eq. (2.2). It models the log probability of predicting tokens from position m + 1,\\nrather than position 0. Throughout this chapter and subsequent ones, we will employ separate\\nvariables x and y to distinguish the input and output of an LLM, though they can be seen as sub-\\nsequences from the same sequence. By adopting such notation, we see that the form of the above\\nequation closely resembles those used in other text generation models in NLP, such as neural\\nmachine translation models.\\nTo illustrate how LLMs are applied, consider the problem of determining the grammaticality\\nfor a given sentence. We can deﬁne a template like this\\n{*sentence*}\\nQuestion: Is this sentence grammatically correct?\\nAnswer:\\nHere\\nrepresents the text we intend to generate. {*sentence*} is a placeholder variable that\\nwill be replaced by the actual sentence provided by the users. For example, suppose we have a\\nsentence “John seems happy today.”. We can replace the {*sentence*} in the template with this\\nsentence to have an input to the language model\\nJohn seems happy today.\\nQuestion: Is this sentence grammatically correct?\\nAnswer:\\nTo perform the task, the language model is given the context x =“John seems happy today .\\\\n\\nQuestion : Is this sentence grammatically correct?\\\\n Answer :”4. It then generates the following\\n4\\\\n is a special character used for line breaks.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 49}, page_content='2.1 A Brief Introduction to LLMs\\n43\\ntext as the answer, based on the context. For example, the language model may output “Yes” (i.e.,\\ny = “Yes”) if this text is the one with the maximum probability of prediction given this context.\\nLikewise, we can deﬁne more templates to address other tasks. For example, we can translate\\nan English sentence into Chinese using the following template\\n{*sentence*}\\nQuestion: What is the Chinese translation of this English sentence?\\nAnswer:\\nor using an instruction-like template\\n{*sentence*}\\nTranslate this sentence from English into Chinese.\\nor using a code-like template.\\n[src-lang] = English [tgt-lang] = Chinese [input] = {*sentence*}\\n[output] =\\nThe above templates provide a simple but effective method to “prompt” a single LLM to per-\\nform various tasks without adapting the structure of the model. However, this approach requires\\nthat the LLM can recognize and follow the instructions or questions. One way to do this is to incor-\\nporate training samples with instructions and their corresponding responses into the pre-training\\ndataset. While this method is straightforward, building and training LLMs from scratch is com-\\nputationally expensive. Moreover, making instruction-following data effective for pre-training\\nrequires a signiﬁcant amount of such data, but collecting large-scale labeled data for all tasks of\\ninterest is very difﬁcult.\\nA second method, which has been a de facto standard in recent research, is to adapt LLMs\\nvia ﬁne-tuning. As such, the token prediction ability learned in the pre-training phase can be\\ngeneralized to accomplish new tasks. The idea behind ﬁne-tuning is that some general knowledge\\nof language has been acquired in pre-training, but we need a mechanism to activate this knowledge\\nfor applying it to new tasks. To achieve this, we can slightly ﬁne-tune the model parameters using\\ninstruction-following data. This approach is called instruction ﬁne-tuning.\\nAn instruction ﬁne-tuning sample, which is represented by a sequence of tokens, can be seen\\nas a tuple consisting of an input and the desired output. Here, the input includes instructions,\\nsystem information (or system preﬁx), and any other user-provided information5. To illustrate,\\nconsider the following examples (blue text = input and underlined text = output).\\n5System information refers to a sequence of tokens added at the beginning of an input in order to guide the behavior\\nof an LLM, such as, you are a helpful assistant and should not output toxic content.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 50}, page_content='44\\nGenerative Models\\nRead the following sentence, and answer whether it is grammatically correct.\\nLLMs are powerful models but are expensive to build.\\nYes\\nLLMs are powerful models but are expensive to build.\\nDoes this sentence make sense grammatically?\\nAnswer Option\\n- Yes\\n- No\\nYes\\nYes or No? Yes = the following sentence is grammatically correct. No = it con-\\ntains grammatical errors.\\nLLMs are powerful models but are expensive to build.\\nYes\\nAll these samples describe the same binary classiﬁcation task, but with different instructions.\\nTo increase the diversity in the ﬁne-tuning data and make LLMs generalize, we can deﬁne more\\ntasks using instructions.\\nWrite a short story about three characters.\\nCharacter 1: A boy whose name is Andy.\\nCharacter 2: Andy’s dog.\\nCharacter 3: Andy’s friend, Jane.\\nIn the heart of a bustling city, Andy, his loyal dog Rocket, and his friend\\nJane discovered a hidden garden beneath the glow of twilight. Surrounded by\\nancient trees and whispering winds, they shared dreams and secrets, forging an\\nunbreakable bond. As the night deepened, they made a pact under the stars to\\nalways ﬁnd magic in the mundane. Together, they realized that true adventure lay\\nin the moments spent with friends, and every day was a treasure waiting to be\\ndiscovered.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 51}, page_content='2.1 A Brief Introduction to LLMs\\n45\\nShow me a recipe for making ice cream.\\nIngredients:\\n2 cups heavy cream; 1 cup milk; 3/4 cup sugar; 2 tsp vanilla extract;\\nPinch of salt.\\nSteps:\\n1. Whisk all ingredients until sugar dissolves.\\n2. Chill mixture for 2 hours in the fridge.\\n3. Freeze in a shallow dish, stirring every 30 minutes, 3-4 times.\\n4. Freeze solid for 2 hours.\\n5. Serve your homemade ice cream.\\nIf you buy 5 apples and each apple costs $1.20, how much do you spend in total?\\n$6.00\\nWrite a Python program to calculate the sum of squares of the following numbers.\\n1 , 2 , 10 , -9 , 78\\nnumbers = [1,2,10,-9 ,78]\\nsum_of_squares = sum(x**2 for x in numbers)\\nprint(sum_of_squares)\\nTo acquire instruction-following abilities, a certain amount of ﬁne-tuning data is required. This\\ndata may include diverse instructions and possible responses. It has been found that scaling the\\nnumber of ﬁne-tuning tasks is beneﬁcial for improving the performance of LLMs [Chung et al.,\\n2022]. Note that although more ﬁne-tuning data is favorable, the amount of this data is generally\\norders of magnitude smaller than that of the pre-training data. For example, LLMs can be ﬁne-\\ntuned with tens or hundreds of thousands of samples, or even fewer if these samples are of high\\nquality [Zhou et al., 2023a; Chen et al., 2023b], whereas pre-training such models may require\\nbillions or trillions of tokens, resulting in signiﬁcantly larger computational demands and longer\\ntraining times [Touvron et al., 2023a].\\nIt is also worth noting that we should not expect the ﬁne-tuning data to cover all the down-\\nstream tasks to which we intend to apply LLMs. A common understanding of how the pre-training\\n+ ﬁne-tuning approach works is that LLMs have gained knowledge for understanding instructions\\nand generating responses in the pre-training phase. However, these abilities are not fully activated\\nuntil we introduce some form of supervision. The general instruction-following behavior emerges\\nas we ﬁne-tune the models with a relatively small amount of labeled data. As a result, we can\\nachieve some level of zero-shot learning: the ﬁne-tuned models can handle new tasks that they\\nhave not been explicitly trained or ﬁne-tuned for [Sanh et al., 2022; Wei et al., 2022a]. This zero-\\nshot learning ability distinguishes generative LLMs from earlier pre-trained models like BERT,\\nwhich are primarily ﬁne-tuned for speciﬁc tasks.\\nOnce we have prepared a collection of instruction-described data, the ﬁne-tuning process is\\nrelatively simple. This process can be viewed as a standard training process as pre-training, but on\\na much smaller training dataset. Let Dtune be the ﬁne-tuning dataset and ˆθ be the model parameters'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 52}, page_content='46\\nGenerative Models\\noptimized via pre-training. We can modify Eq. (2.14) to obtain the objective of ﬁne-tuning\\n˜θ\\n=\\narg max\\nˆθ+\\nX\\nsample∈Dtune\\nLˆθ+(sample)\\n(2.16)\\nHere ˜θ denotes the optimal parameters. The use of notation ˆθ+ means that the ﬁne-tuning starts\\nwith the pre-trained parameters ˆθ.\\nFor each sample ∈Dtune, we divide it into an input segment xsample and an output segment\\nysample, that is,\\nsample\\n=\\n[ysample, xsample]\\n(2.17)\\nWe then deﬁne the loss function to be\\nLˆθ+(sample)\\n=\\n−log Prˆθ+(ysample|xsample)\\n(2.18)\\nIn other words, we compute the loss over the sub-sequence ysample, rather than the entire sequence.\\nIn a practical implementation of back-propagation for this equation, the sequence [ysample, xsample]\\nis constructed in the forward pass as usual. However, in the backward pass, error gradients are\\npropagated back only through the parts of the network that correspond to ysample, leaving the rest\\nof the network unchanged. As an example, consider a sequence\\n⟨s⟩Square this number . 2 .\\n|\\n{z\\n}\\nContext (Input)\\nThe result is 4 .\\n|\\n{z\\n}\\nPrediction (Output)\\nThe loss is calculated and back propagated only for The result is 4 ..\\nInstruction ﬁne-tuning also requires substantial engineering work. In order to achieve satis-\\nfactory results, one may experiment with different settings of the learning rate, batch size, number\\nof ﬁne-tuning steps, and so on. This typically requires many ﬁne-tuning runs and evaluations. The\\ncost and experimental effort of ﬁne-tuning remain critical and should not be overlooked, though\\nthey are much lower than those of the pre-training phase.\\nWhile we focus on instruction ﬁne-tuning for an illustrative example here, ﬁne-tuning tech-\\nniques play an important role in developing various LLMs and are more widely used. Examples\\ninclude ﬁne-tuning LLMs as chatbots using dialog data, and adapting these models to handle very\\nlong sequences. The wide application of ﬁne-tuning has led researchers to improve these tech-\\nniques, such as designing more efﬁcient ﬁne-tuning algorithms. While the research on ﬁne-tuning\\nis fruitful, in this section we just give a ﬂavour of the key steps involved. We will see more detailed\\ndiscussions on this topic in the following chapters.\\n2.1.4\\nAligning LLMs with the World\\nInstruction ﬁne-tuning provides a simple way to adapt LLMs to tasks that can be well deﬁned. This\\nproblem can broadly be categorized as an alignment problem. Here, alignment is referred to as a\\nprocess of guiding LLMs to behave in ways that align with human intentions. The guidance can\\ncome from labeled data, human feedback, or any other form of human preferences. For example,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 53}, page_content='2.1 A Brief Introduction to LLMs\\n47\\nwe want LLMs not only to be accurate in following instructions, but also to be unbiased, truthful,\\nand harmless. So we need to supervise the models towards human values and expectations. A\\ncommon example is that when we ask an LLM how to build a weapon, it may provide a list of key\\nsteps to do so if it is not carefully aligned. However, a responsible model should recognize and\\navoid responding to requests for harmful or illegal information. Alignment in this case is crucial\\nfor ensuring that LLMs act responsibly and in accordance with ethical guidelines.\\nA related concept to alignment is AI safety. One ultimate goal of AI is to build intelligent\\nsystems that are safe and socially beneﬁcial. To achieve this goal we should keep these systems\\nrobust, secure, and subjective, in any conditions of real-world use, even in conditions of misuse\\nor adverse use. For LLMs, the safety can be increased by aligning them with appropriate human\\nguidance, such as human labeled data and interactions with users during application.\\nAlignment is difﬁcult as human values and expectations are diverse and shifting. Sometimes,\\nit is hard to describe precisely what humans want, unless we see the response of LLMs to user\\nrequests. This makes alignment no longer a problem of tuning LLMs on predeﬁned tasks, but a\\nbigger problem of training them with the interactions with the real world.\\nAs a result of the concerns with controlling AI systems, there has been a surge in research\\non the alignment issue for LLMs. Typically, two alignment steps are adopted after LLMs are\\npre-trained on large-scale unlabeled data.\\n• Supervised Fine-tuning (SFT). This involves continuing the training of pre-trained LLMs\\non new, task-oriented, labelled data. A commonly used SFT technique is instruction ﬁne-\\ntuning. As described in the previous subsection, by learning from instruction-response\\nannotated data, LLMs can align with the intended behaviors for following instructions,\\nthereby becoming capable of performing various instruction-described tasks. Supervised\\nﬁne-tuning can be seen as following the pre-training + ﬁne-tuning paradigm, and offers a\\nrelatively straightforward method to adapt LLMs.\\n• Learning from Human Feedback. After an LLM ﬁnishes pre-training and supervised ﬁne-\\ntuning, it can be used to respond to user requests if appropriately prompted. But this model\\nmay generate content that is unfactual, biased, or harmful. To make the LLM more aligned\\nwith the users, one simple approach is to directly learn from human feedback. For example,\\ngiven some instructions and inputs provided by the users, experts are asked to evaluate how\\nwell the model responds in accordance with their preferences and interests. This feedback\\nis then used to further train the LLM for better alignment.\\nA typical method for learning from human feedback is to consider it as a reinforcement learn-\\ning (RL) problem, known as reinforcement learning from human feedback (RLHF) [Ouyang et al.,\\n2022]. The RLHF method was initially proposed to address general sequential decision-making\\nproblems [Christiano et al., 2017], and was later successfully employed in the development of\\nthe GPT series models [Stiennon et al., 2020]. As a reinforcement learning approach, the goal of\\nRLHF is to learn a policy by maximizing some reward from the environment. Speciﬁcally, two\\ncomponents are built in RLHF:\\n• Agent. An agent, also called an LM agent, is the LLM that we want to train. This agent\\noperates by interacting with its environment: it receives a text from the environment and'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 54}, page_content='48\\nGenerative Models\\noutputs another text that is sent back to the environment. The policy of the agent is the\\nfunction deﬁned by the LLM, that is, Pr(y|x).\\n• Reward Model. A reward model is a proxy of the environment. Each time the agent\\nproduces an output sequence, the reward model assigns this output sequence a numerical\\nscore (i.e., the reward). This score tells the agent how good the output sequence is.\\nIn RLHF, we need to perform two learning tasks: 1) reward model learning, which involves\\ntraining a reward model using human feedback on the output of the agent, and 2) policy learning,\\nwhich involves optimizing a policy guided by the reward model using reinforcement learning\\nalgorithms. Here is a brief outline of the key steps involved in RLHF.\\n• Build an initial policy using pre-training and instruction ﬁne-tuning.\\n• Use the policy to generate multiple outputs for each input, and then collect human feedback\\non these outputs (e.g., comparisons of the outputs).\\n• Learn a reward model from the human feedback.\\n• Fine-tune the policy with the supervision from the reward model.\\nFigure 2.2 shows an overview of RLHF. Given that this section serves only as a brief intro-\\nduction to concepts of LLMs, a detailed discussion of RLHF techniques will not be included. We\\ninstead illustrate the basic ideas behind RLHF using a simple example.\\nSuppose we have trained an LLM via pre-training and instruction ﬁne-tuning. This LLM is\\ndeployed to respond to requests from users. For example, a user may input\\nHow can I live a more environmentally friendly life?\\nWe use the LLM to generate 4 different outputs (denoted by {y1, ..., y4}) by sampling the\\noutput space\\nOutput 1 (y1):\\nConsider switching to an electric vehicle or bicycle instead of\\ntraditional cars to reduce carbon emissions and protect our planet.\\nOutput 2 (y2):\\nAdopt a minimalist lifestyle. Own fewer possessions to reduce\\nconsumption and the environmental impact of manufacturing and\\ndisposal.\\nOutput 3 (y3):\\nGo off-grid. Generate your own renewable energy and collect\\nrainwater to become completely self-sufﬁcient and reduce reliance\\non non-renewable resources.\\nOutput 4 (y4):\\nSupport local farm products to reduce the carbon footprint of\\ntransporting food, while enjoying fresh, healthy food.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 55}, page_content='2.1 A Brief Introduction to LLMs\\n49\\nLLM\\nPre-training Data\\nI love the food here! ...\\nHow can I get there? ...\\nSFT Data\\nweather in London . ...\\nWrite a poem about the\\nPre-training &\\nSupervised ﬁne-tuning\\n(a) Learning an Initial LLM\\nLLM\\nUser Input\\nenvironmentally friendly?\\nHow can I live more\\nModel Output\\n3. ............\\n4. ............\\n1. ............\\n2. ............\\nPredicting\\nComparisons\\ny1 ≻y4 ≻y2 ≻y3\\nAnnotating Data with Human Preferences\\n(b) Annotating Data with Human Preferences\\nReward Model\\nComparison Data\\n{(x, yk1 ≻yk2 )}\\nTraining\\n(c) Training the Reward Model\\nLLM\\n(Policy)\\nDataset D\\nx ∼D\\nInput-output Pairs\\n{x, y}\\nSampling y via the Policy Pr(y|x)\\nReward Model\\nReward Scores\\n{r(x, y)}\\nEvaluate the Input-output Pairs\\n(d) Training/Fine-tuning the Policy\\nRL Fine-tuning\\nFig. 2.2: An overview of RLHF. There are 4 key steps involved: a) training an initial LLM (i.e., policy) using pre-\\ntraining and supervised ﬁne-tuning; b) collecting human preference data by ranking the outputs of the LLM; c) training\\na reward model using the ranking results; d) RL ﬁne-tuning of the policy based on the reward model. Double line\\narrows mean training or ﬁne-tuning.\\nWe then ask annotators to evaluate these outputs. One straightforward way is to assign a rating\\nscore to each output. In this case, the reward model learning problem can be framed as a task of\\ntraining a regression model. But giving numerical scores to LLM outputs is not an easy task for\\nannotators. It is usually difﬁcult to design an annotation standard that all annotators can agree on\\nand easily follow. An alternative method, which is more popular in the development of LLMs, is\\nto rank these outputs. For example, a possible ranking of the above outputs is\\ny1 ≻y4 ≻y2 ≻y3'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 56}, page_content='50\\nGenerative Models\\nA reward model is then trained using this ranking result. In general, a reward model in RLHF\\nis a language model that shares the same architecture as the target LLM, but with a smaller model\\nsize. Given the input x and output yk, we concatenate them to form a sequence seqk = [x, yk].\\nThis sequence is processed from left to right using forced decoding. Since each position can\\nonly access its left context in language modeling, the output of the top-most Transformer layer at\\nthe ﬁrst position cannot be used as the representation of the sequence. Instead, a special symbol\\n(e.g., ⟨\\\\s⟩) is added to the end of the sequence, and the corresponding output of the Transformer\\nlayer stack is considered as the representation of the entire sequence. An output layer, such as a\\nlinear transformation layer, is built on top of this representation to generate the reward, denoted\\nby R(seqk) or R(x, yk).\\nWe train this reward model using ranking loss. For example, a pair-wise ranking loss function\\ncan be written in the form\\nLossω(Dr)\\n=\\n−E(x,yk1,yk2)∼Dr log(Sigmoid(Rω(x, yk1) −Rω(x, yk2)))\\n(2.19)\\nwhere ω represents the parameters of the reward model, and Dr represents a set of tuples of an\\ninput and a pair of outputs. (x, yk1, yk2) ∼Dr is a sampling operation which draws a sample\\n(x, yk1, yk2) from Dr with some probability. As an example, suppose we ﬁrst draw a model\\ninput x with a uniform distribution and then draw a pair of model outputs with a probability of\\nyk1 ≻yk2 given x (denoted by Pr(yk1 ≻yk2|x)). The corresponding loss function is given by\\nLossω(Dr)\\n=\\n−\\nX\\nPr(x) · Pr(yk1 ≻yk2|x) · log(Sigmoid(Rω(x, yk1) −Rω(x, yk2)))\\n=\\n−1\\nK\\nX\\nPr(yk1 ≻yk2|x) · log(Sigmoid(Rω(x, yk1) −Rω(x, yk2)))\\n(2.20)\\nwhere K represents the number of model inputs involved in sampling. While the form of these\\nfunctions may seem complex, their idea is simple: we penalize the model if the predicted ranking\\nof two outputs differs from the human-labeled ranking. By contrast, the model receives a bonus,\\nif the predicted ranking matches the human-labeled ranking.\\nWe can train the reward model by minimizing the above ranking loss\\nˆω\\n=\\narg min\\nω\\nLossω(Dr)\\n(2.21)\\nThe resulting model Rˆω(·) can be employed to evaluate any given pair of input and output. Note\\nthat although the reward model is trained using a ranking-based objective, it is used for scoring.\\nThis allows it to provide continuous supervision signals, which is very beneﬁcial for training other\\nmodels.\\nWe now turn to the policy learning problem. A commonly adopted objective is to maximize\\nthe reward on a set of input-output pairs. Following an analogous form of Eq. (2.16), we obtain a\\nsimple training objective for RL ﬁne-tuning\\n˜θ\\n=\\narg max\\nˆθ+\\nE(x,yˆθ+)∼DrlftRˆω(x, yˆθ+)\\n(2.22)\\nwhere the optimal parameters ˜θ are obtained by ﬁne-tuning the pre-trained parameters ˆθ. Drlft is'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 57}, page_content='2.1 A Brief Introduction to LLMs\\n51\\nthe RL ﬁne-tuning dataset. For each sample (x, yˆθ+), x is sampled from a prepared dataset of\\ninput sequences, and yˆθ+ is sampled from the distribution Prˆθ+(y|x) given by the policy.\\nIn practice, more advanced reinforcement learning algorithms, such as proximal policy opti-\\nmization (PPO), are often used for achieving more stable training, as well as better performance.\\nWe leave the detailed discussion of reinforcement learning algorithms to the following parts of\\nthis book where RLHF is extensively used for alignment.\\nAn interesting question arises here: why not consider learning from human preferences as\\na standard supervised learning problem? This question is closely related to our aforementioned\\ndiscussion on the difﬁculty of data annotation. Often, describing human values and goals is chal-\\nlenging, and it is even more difﬁcult for humans to provide outputs that are well aligned. As an\\nalternative, annotating the preferences of a given list of model outputs offers a simpler task. By\\ndoing so, we can create a model that understands human preferences, which can then be used as\\na reward model for training policies. From the perspective of machine learning, RLHF is par-\\nticularly useful for scenarios where the desired behavior of an agent is difﬁcult to demonstrate\\nbut can be easily recognized by humans. Another advantage of RLHF is its ability to explore the\\nsample space. By employing sampling techniques, models trained with reinforcement learning\\ncan venture beyond the annotated data set to explore additional samples. This exploratory ability\\nallows RLHF to discover potentially beneﬁcial policies that are not immediately apparent from\\nthe labeled data alone.\\n2.1.5\\nPrompting LLMs\\nWe have so far shown that LLMs can be used to perform various tasks by giving them appropriate\\nprompts. There are no restrictions on these prompts, which can include any information we wish\\nto ask or communicate with LLMs, such as natural language instructions and the context of con-\\nversations. Since this approach requires no additional training or tuning, adapting LLMs becomes\\nhighly efﬁcient once they are developed. This somewhat inﬂuences the paradigms in NLP: we\\nno longer need to develop speciﬁc systems for individual tasks but can instead use a single, well-\\ntrained LLM to perform different tasks by prompting it. An appealing aspect of LLM prompting\\narises as a result: users can easily have “customized” systems by designing their own prompts\\nfor LLMs. Given the important role played by prompting in LLMs, prompting engineering has\\nbecome a very active area of research in NLP.\\nThe term prompt is used in many different ways in the literature. In this chapter, this term\\nrefers to the entire input to LLMs, and so we use the terms prompt and model input interchange-\\nably. Before discussing prompting further, let us ﬁrst see a few examples where the prompts\\nare more complex than those presented in the previous subsections. Note that this subsection is\\nnot aimed at writing high-quality prompts but rather at highlighting some interesting issues in\\nprompting LLMs.\\nOne of the popular ways to use LLMs is to assign them a “role” played in generating responses.\\nFor example, LLMs can act as a psychologist when answering questions.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 58}, page_content='52\\nGenerative Models\\nPlease explain what delayed gratiﬁcation is?\\nNote, you are a researcher with a deep background in psychology. Your own\\nﬁeld of study is developmental psychology in children, with a focus on aspects\\nof control and intelligence. You are very familiar with the concept of delayed\\ngratiﬁcation and have applied it to your own research. You have published papers\\non delayed gratiﬁcation and conducted related experiments.\\nDelayed gratiﬁcation is the process of resisting an immediate reward in\\nanticipation of receiving a more valuable reward in the future. It is a signiﬁcant\\nconcept in developmental psychology, especially in relation to how children learn\\nself-control and develop cognitive abilities. Research shows that the ability to\\ndelay gratiﬁcation is associated with positive outcomes in academic performance,\\nsocial competence, and emotional well-being. Studies and experiments in this\\narea highlight the importance of cultivating this skill early in life to support\\nlong-term success and resilience.\\nAnother example is the use of LLMs in detecting and correcting errors such as syntactic or\\nsemantic mistakes in text. For an LLM which is trained on both code and natural language data,\\nwe may use it for code debugging6.\\nFix the bugs in this C language program.\\n#include <stdio.h>\\nint main() {\\nprintg(\"Hello, World!\")\\nreturn 0;\\n}\\n1. The function name ’printg’ is incorrect . It should be ’printf’.\\n2. There is a missing semicolon at the end of the ’printf’ function call.\\nHere is the corrected version of the program:\\n#include <stdio.h>\\nint main() {\\nprintf(\"Hello, World!\");\\nreturn 0;\\n}\\nAlso, LLMs can be ﬁne-tuned to act as a conversation partner 7.\\n6In this example, the code is not tokenized for easier reading.\\n7To ﬁne-tune an LLM for multi-turn dialogue, one needs to consider conversation history in the context for pre-\\ndicting the response in the current round of conversation. This makes the actual prompt used in response generation\\nrelatively longer than that used in single-turn dialogue.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 59}, page_content='2.1 A Brief Introduction to LLMs\\n53\\nHi! I’m an assistant. How can I help you?\\nWho won the FIFA World Cup 2022?\\nArgentina won the FIFA World Cup 2022.\\nWhere was it held?\\nThe 2022 FIFA World Cup was held in Qatar.\\nHow many times has Argentina won the World Cup?\\nArgentina has won the FIFA World Cup three times.\\n...\\nThese examples and previous ones have shown that appropriate responses can be generated\\nvia prompts involving clear instructions and questions. However, when problem solving requires\\nknowledge that is not explicitly speciﬁed, LLMs may make mistakes, even though the instructions\\nare sufﬁciently clear and precise. A family of challenging tasks for LLMs involves arithmetic\\nreasoning and commonsense reasoning. For example, we can ask an LLM to solve primary school\\nmath problems presented in natural language.\\nJack has 7 apples. He ate 2 of them for dinner, but then his mom gave him 5 more\\napples. The next day, Jack gave 3 apples to his friend John. How many apples\\ndoes Jack have left in the end?\\nThe answer is 10.\\nThe correct answer should be 7, so the model output is incorrect.\\nOne approach to addressing such issues is to incorporate learning into prompts, called in-\\ncontext learning or (ICL). The idea of ICL is to demonstrate the ways to solve problems in\\nprompts, and condition predictions on these demonstrations. Here is an example where a similar\\nproblem and the corresponding answer are presented in the prompt (green = demonstrations).\\nTom has 12 marbles. He wins 7 more marbles in a game with his friend but then\\nloses 5 marbles the next day. His brother gives him another 3 marbles as a gift.\\nHow many marbles does Tom have now?\\nThe answer is 17.\\nJack has 7 apples. He ate 2 of them for dinner, but then his mom gave him 5 more\\napples. The next day, Jack gave 3 apples to his friend John. How many apples\\ndoes Jack have left in the end?\\nThe answer is 12.\\nBut the LLM still made mistakes this time. A reason for this might be that solving math\\nproblems does not only involve problem-answer mappings but also, to a larger extent, the under-\\nlying logical inference in multiple steps. A method to improve the inference abilities of LLMs\\nis chain-of-thought prompting (COT prompting) [Wei et al., 2022c]. In COT prompting, we\\ndecompose complex reasoning problems into multiple problem-solving intermediate steps. These'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 60}, page_content='54\\nGenerative Models\\nsteps are demonstrated in prompts so that LLMs can be prompted to learn to mimic the underlying\\nthought processes in solving the problems. COT prompting has been found to be very useful for\\nachieving good results on challenging tasks, such as the GSM8K mathematical reasoning bench-\\nmark.\\nConsider the above demonstration example Tom has 12 marbles. He wins 7 more marbles in\\na game with his friend but then loses 5 marbles the next day. His brother gives him another 3\\nmarbles as a gift. How many marbles does Tom have now ?. The corresponding logical reasoning\\nsteps can be structured:\\n1. Tom starts with 12 marbles.\\n2. He wins 7 more marbles in a game, so we add those to his initial count: 12 marbles + 7\\nmarbles = 19 marbles.\\n3. The next day, Tom loses 5 marbles, so we subtract those from his current total: 19 marbles\\n- 5 marbles = 14 marbles.\\n4. His brother gives him 3 more marbles, so we add those to his current total: 14 marbles + 3\\nmarbles = 17 marbles.\\n5. After all these events, Tom has 17 marbles.\\nWe can add these reasoning steps into the prompt, and get the correct answer.\\nTom has 12 marbles. He wins 7 more marbles in a game with his friend but then\\nloses 5 marbles the next day. His brother gives him another 3 marbles as a gift.\\nHow many marbles does Tom have now?\\nTom initially has 12 marbles. After winning a game, he gains 7 more, increasing\\nhis total to 19 marbles. He loses 5 marbles the next day, bringing his total down\\nto 14 marbles. His brother gifts him 3 more marbles, increasing his total to 17\\nmarbles. Therefore, Tom now has 17 marbles. So the answer is 17.\\nJack has 7 apples. He ate 2 of them for dinner, but then his mom gave him 5 more\\napples. The next day, Jack gave 3 apples to his friend John. How many apples\\ndoes Jack have left in the end?\\nJack starts with 7 apples. He eats 2 apples for dinner, so we subtract 2 from 7,\\nleaving him with 5 apples. His mom gives him 5 more apples, adding 5 to his\\ncurrent total. Now he has 5 + 5 = 10 apples. The next day, Jack gives 3 apples to\\nhis friend John. Subtracting 3 from 10, Jack is left with 7 apples. Therefore, Jack\\nhas 7 apples left. So the answer is 7.\\nThis approach is also called one-shot COT prompting. Likewise, we can add more demon-\\nstrations to have a few-shot COT prompting approach.\\nFurthermore, a special kind of COT, zero-shot COT, introduces instructions that provoke\\nLLMs to think, without presenting any intermediate reasoning steps in the prompts [Kojima et al.,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 61}, page_content='2.1 A Brief Introduction to LLMs\\n55\\n2022]. An interesting example is that by appending a simple sentence “Let’s think step by step.”\\nto the end of a prompt, LLMs can generate reasoning steps to reach the ﬁnal answer.\\nJack has 7 apples. He ate 2 of them for dinner, but then his mom gave him 5 more\\napples. The next day, Jack gave 3 apples to his friend John. How many apples\\ndoes Jack have left in the end?\\nLet’s think step by step.\\n1. Initial Quantity: Jack starts with 7 apples.\\n2. After Dinner: He eats 2 apples, so 7 −2 = 5 apples remain.\\n3. His Mom Gives More: His mom gives him 5 more apples, so 5 + 5 = 10\\napples are now with him.\\n4. Giving to John: The next day, Jack gives 3 apples to his friend John,\\nso 10 −3 = 7 apples are left.\\nIn the end, Jack has 7 apples left.\\nZero-shot, one-shot, and few-shot learning are common concepts in the area of in-context\\nlearning for LLMs and are not restricted to COT prompting. Broadly speaking, any prompting\\nthat involves only simple instructions without any demonstrations can be considered a form of\\nzero-shot learning. This zero-shot learning ability emerges as LLMs are pre-trained and/or ﬁne-\\ntuned. Also, one-shot and few-shot learning methods are more often considered when LLMs do\\nnot acquire the corresponding zero-shot learning ability. These methods are therefore important\\nfor in-context learning when addressing new tasks. Examples include those for performing various\\nNLP tasks by demonstrating task-formatted samples. See the following examples for sentiment\\nsentence classiﬁcation and phrase translation via few-shot learning.\\nGiven the following text snippets, classify their sentiment as Positive, Negative,\\nor Neutral.\\nExample 1: “I had an amazing day at the park!”\\nSentiment: Positive\\nExample 2: “The service at the restaurant was terrible.”\\nSentiment: Negative\\nExample 3: “I think it’s going to rain today.”\\nSentiment: Neutral\\nText: “This movie was a fantastic journey through imagination.”\\nSentiment: Positive'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 62}, page_content='56\\nGenerative Models\\nTranslate the following Chinese phrases into English.\\nExample 1: “你好”\\nTranslation: “Hello”\\nExample 2: “谢谢你”\\nTranslation: “Thank you”\\nPhrase to translate: “早上好”\\nTranslation: “Good Morning”\\nAbove, we have presented examples to illustrate the fundamental in-context learning capa-\\nbilities of prompting LLMs. This section, however, does not include more advanced prompting\\ntechniques in order to keep the content concise and compact. More discussions on prompting can\\nbe found in Chapter 3.\\n2.2\\nTraining at Scale\\nAs a ﬁrst step in developing LLMs, we need to train these models on large amounts of data.\\nThe training task is itself standard: the objective is to maximize the likelihood, which can be\\nachieved via gradient descent. However, as we scale up both the model size and the amount\\nof data, the problem becomes very challenging, for example, large models generally make the\\ntraining unstable. In this section, we discuss several issues of large-scale training for LLMs,\\nincluding data preparation, model modiﬁcation, and distributed training. We also discuss the\\nscaling laws for LLMs, which help us understand their training efﬁciency and effectiveness.\\n2.2.1\\nData Preparation\\nThe importance of data cannot be overstated in NLP. As larger neural networks are developed,\\nthe demand for data continues to increase. For example, developing LLMs may require trillions\\nof tokens in pre-training (see Table 2.3), orders of magnitude larger than those used in training\\nconventional NLP models. In general, we may want to gather as much training data as possible.\\nHowever, larger training datasets do not mean better training results, and the development of\\nLLMs raises new issues in creating or collecting these datasets.\\nA ﬁrst issue is the quality of data. High-quality data has long been seen as crucial for training\\ndata-driven NLP systems. Directly using raw text from various sources is in general undesirable.\\nFor example, a signiﬁcant portion of the data used to train recent LLMs comes from web scraping,\\nwhich may contain errors and inappropriate content, such as toxic information and fabricated\\nfacts. Also, the internet is ﬂooded with machine-generated content due to the widespread use of\\nAI, presenting further challenges for processing and using web-scraped data. Researchers have\\nfound that training LLMs on unﬁltered data is harmful [Raffel et al., 2020]. Improving data quality\\ntypically involves incorporating ﬁltering and cleaning steps in the data processing workﬂow. For\\nexample, Penedo et al. [2023] show that by adopting a number of data processing techniques, 90%\\nof their web-scraped data can be removed for LLM training. In addition to large-scale web-scraped\\ndata, LLM training data often includes books, papers, user-generated data on social media, and\\nso on. Most of the latest LLMs are trained on such combined datasets, which are found to be'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 63}, page_content='2.2 Training at Scale\\n57\\nLLM\\n# of Tokens\\nData\\nGPT3-175B [Brown et al., 2020]\\n0.5T\\nWebpages, Books, Wikipedia\\nFalcon-180B [Almazrouei et al., 2023]\\n3.5T\\nWebpages, Books, Conversations,\\nCode, Technical Articles\\nLLaMA2-65B [Touvron et al., 2023a]\\n1.0T ∼1.4T\\nWebpages, Code, Wikipedia,\\nBooks, Papers, Q&As\\nPaLM-450B [Chowdhery et al., 2022]\\n0.78T\\nWebpages, Books, Conversations,\\nCode, Wikipedia, News\\nGemma-7B [Gemma Team, 2024]\\n6T\\nWebpages, Mathematics, Code\\nTable 2.3: Amounts of training data used in some LLMs in terms of the number of tokens.\\nimportant for the strong performance of the resulting models.\\nA second issue is the diversity of data. We want the training data to cover as many types of\\ndata as possible, so that the trained models can adapt to different downstream tasks easily. It has\\nbeen widely recognized that the quality and diversity of training data both play very important\\nroles in LLMs. An interesting example is that incorporating programming code into training data\\nhas been found to be beneﬁcial for LLMs. The beneﬁts are demonstrated not only in enhancing the\\nprogramming abilities of LLMs, but also in improving reasoning for complex problems, especially\\nthose requiring COT prompting. The concept “diversity” can be extended to include language\\ndiversity as well. For example, many LLMs are trained on multi-lingual data, and therefore we\\ncan handle multiple languages using a single model. While this approach shows strong abilities\\nin multi-lingual and cross-lingual tasks, its performance on speciﬁc languages largely depends on\\nthe volume and quality of the data for those languages. It has been shown in some cases to provide\\npoor results for low-resource languages.\\nA third issue is the bias in training data. This is not a problem that is speciﬁc to LLMs but\\nexists in many NLP systems. A common example is gender bias, where LLMs show a preference\\nfor one gender over another. This can partly be attributed to class imbalance in the training data,\\nfor example, the term nurses is more often associated with women. In order to debias the data,\\nit is common practice to balance the categories of different language phenomena, such as gender,\\nethnicity, and dialects. The bias in data is also related to the diversity issue mentioned above.\\nFor example, since many LLMs are trained and aligned with English-centric data, they are bi-\\nased towards the cultural values and perspectives prevalent among English-speaking populations.\\nIncreasing language diversity in training data can somewhat mitigate the bias.\\nAnother issue with collecting large-scale data is the privacy concern. If LLMs are trained\\non data from extensive sources, this potentially leads to risks regarding the exposure of sensitive\\ninformation, such as intellectual property and personal data. This is particularly concerning given\\nthe capacity of LLMs to represent patterns from the data they are trained on, which might in-\\nadvertently involve memorizing and reproducing speciﬁc details. A simple approach to privacy\\nprotection is to remove or anonymize sensitive information. For example, anonymization tech-\\nniques can be applied to remove personally identiﬁable information from training data to prevent\\nLLMs from learning from such data. However, in practice, erasing or redacting all sensitive data\\nis difﬁcult. Therefore, many LLMs, particularly those launched for public service, typically work\\nwith systems that can detect the potential exposure of sensitive data, or are ﬁne-tuned to reject'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 64}, page_content='58\\nGenerative Models\\ncertain requests that could lead to information leakage.\\n2.2.2\\nModel Modiﬁcations\\nTraining LLMs is difﬁcult. A commonly encountered problem is that the training process be-\\ncomes more unstable as LLMs get bigger. For example, one needs to choose a small learning rate\\nto achieve stable training with gradient descent, but this in turn results in much longer training\\ntimes. Sometimes, even when the training conﬁguration is carefully designed, training may di-\\nverge at certain points during optimization. The training of LLMs is generally inﬂuenced by many\\nfactors, such as parameter initialization, batching, and regularization. Here, we focus on common\\nmodiﬁcations and improvements to the standard Transformer architecture, which are considered\\nimportant in developing trainable LLMs.\\n2.2.2.1\\nLayer Normalization with Residual Connections\\nLayer normalization is used to stabilize training for deep neural networks. It is a process of\\nsubtracting the mean and dividing by the standard deviation. By normalizing layer output in\\nthis way, we can effectively reduce the covariate shift problem and improve the training stability.\\nIn Transformers, layer normalization is typically used together with residual connections. As\\ndescribed in Section 2.1.1, a sub-layer can be based on either the post-norm architecture, in which\\nlayer normalization is performed right after a residual block, or the pre-norm architecture, in\\nwhich layer normalization is performed inside a residual block. While both of these architectures\\nare widely used in Transformer-based systems [Wang et al., 2019], the pre-norm architecture has\\nproven to be especially useful in training deep Transformers. Given this, most LLMs are based on\\nthe pre-norm architecture, expressed as output = LNorm(F(input)) + input.\\nA widely-used form of the layer normalization function is given by\\nLNorm(h)\\n=\\nα · h −µ\\nσ + ǫ + β\\n(2.23)\\nwhere h is a d-dimensional real-valued vector, µ is the mean of all the entries of h, and σ is the\\ncorresponding standard deviation. ǫ is introduced for the sake of numerical stability. α ∈Rd and\\nβ ∈Rd are the gain and bias terms.\\nA variant of layer normalization, called root mean square (RMS) layer normalization, only\\nre-scales the input vector but does not re-center it [Zhang and Sennrich, 2019]. The RMS layer\\nnormalization function is given by\\nLNorm(h)\\n=\\nα ·\\nh\\nσrms + ǫ + β\\n(2.24)\\nwhere σrms is the root mean square of h, that is, σrms = (1\\nd\\nPd\\nk=1 h2\\nk)\\n1\\n2 . This layer normalization\\nfunction is used in LLMs like the LLaMA series.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 65}, page_content='2.2 Training at Scale\\n59\\n2.2.2.2\\nActivation Functions in FFNs\\nIn Transformers, FFN sub-layers are designed to introduce non-linearities into representation\\nlearning, and are found to be useful for preventing the representations learned by self-attention\\nfrom degeneration8 [Dong et al., 2021]. A standard form of the FFNs used in these sub-layers can\\nbe expressed as\\nFFN(h)\\n=\\nσ(hWh + bh)Wf + bf\\n(2.25)\\nwhere Wh ∈Rd×dh, bh ∈Rdh, Wf ∈Rdh×d, and bf ∈Rd are the parameters, and dh is the\\nhidden size. σ(·) is the activation function of the hidden layer. A common choice for σ(·) is the\\nrectiﬁed linear unit (ReLU), given by\\nσrelu(h)\\n=\\nmax(0, h)\\n(2.26)\\nIn practical implementations, increasing dh is helpful and thus it is often set to a larger number\\nin LLMs. But a very large hidden size poses challenges for both training and deployment. In this\\ncase, the design of the activation function plays a relatively more important role in wide FFNs.\\nThere are several alternatives to the ReLU in LLMs. One of these is the gaussian error linear\\nunit (GeLU) which can be seen as a smoothed version of the ReLU. Rather than controlling the\\noutput by the sign of the input, the GeLU function weights its input by the percentile Pr(h ≤h).\\nHere h is a d-dimensional vector whose entries are drawn from the standard normal distribution\\nGaussian(0, 1)9. Speciﬁcally, the GeLU function is deﬁned to be\\nσgelu(h)\\n=\\nh Pr(h ≤h)\\n=\\nhΦ(h)\\n(2.27)\\nwhere Φ(h) is the cumulative distribution function of Gaussian(0, 1), which can be implemented\\nin convenient ways [Hendrycks and Gimpel, 2016]. The GeLU function has been adopted in\\nseveral LLMs, such as BERT, GPT-3, and BLOOM.\\nAnother family of activation functions which is popular in LLMs is gated linear unit (GLU)-\\nbased functions. The basic form of GLUs is given by\\nσglu(h)\\n=\\nσ(hW1 + b1) ⊙(W2 + b2)\\n(2.28)\\nwhere W1 ∈Rd×d, b1 ∈Rd, W2 ∈Rd×d, and b2 ∈Rd are model parameters. Different choices\\nof σ(·) result in different versions of GLU functions. For example, if σ(·) is deﬁned to be the\\nGeLU function, we will have the GeGLU function\\nσgeglu(h)\\n=\\nσgelu(hW1 + b1) ⊙(W2 + b2)\\n(2.29)\\nThis activation function has been successfully applied in LLMs like Gemma.\\nAs another example, consider σ(·) to be the Swish function σswish(h) = h ⊙Sigmoid(ch)\\n8Here degeneration refers to the phenomenon in which the rank of a matrix is reduced after some processing.\\n9Pr(h ≤h) is an informal notation. It refers to a vector, with each entry representing the percentile for the\\ncorresponding entry of h.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 66}, page_content='60\\nGenerative Models\\n[Ramachandran et al., 2017]. Then, the SwiGLU function is given by\\nσswiglu(h)\\n=\\nσswish(hW1 + b1) ⊙(W2 + b2)\\n(2.30)\\nBoth the PaLM and LLaMA series are based on the SwiGLU function. For more discussions of\\nGLUs, the reader can refer to Shazeer [2020]’s work.\\n2.2.2.3\\nRemoving Bias Terms\\nAnother popular model design is to remove the bias terms in afﬁne transformations used in LLMs.\\nThis treatment can be applied to layer normalization, transformations of the inputs to QKV atten-\\ntion, and FFNs. For example, we can modify Eq. (2.25) to obtain an FFN with no bias terms\\nFFN(h)\\n=\\nσ(hWh)Wf\\n(2.31)\\nChowdhery et al. [2022] report that removing bias terms helps improve the training stability\\nof LLMs. This method has been used in several recent LLMs, such as LLaMA and Gemma.\\n2.2.2.4\\nOther Issues\\nMany LLMs also involve modiﬁcations to their positional embedding models. For example, one\\ncan replace sinusoidal positional encodings with rotary position embeddings so that the learned\\nLLMs can handle long sequences better. These models will be discussed in Section 2.3.\\nNote that while model modiﬁcations are common in training LLMs, the stability of training\\ncan be improved in many different ways. For example, increasing the batch size as the training\\nproceeds has been found to be useful for some LLMs. In general, achieving stable and efﬁcient\\nlarge-scale LLM training requires carefully designed setups, including learning schedules, opti-\\nmizer choices, training parallelism, mixed precision training, and so on. Some of these issues are\\nhighly engineered, and therefore, we typically need a number of training runs to obtain satisfactory\\nLLMs.\\n2.2.3\\nDistributed Training\\nTraining LLMs requires signiﬁcant amounts of computational resources. A common approach to\\nimproving training efﬁciency is to use large-scale distributed systems. Fortunately, alongside the\\nrise of neural networks in AI, deep learning-oriented software and hardware have been developed,\\nmaking it easier to implement LLMs and perform computations. For example, one can now easily\\nﬁne-tune an LLM using deep learning software frameworks and a machine with multiple GPUs.\\nHowever, scaling up the training of LLMs is still challenging, and requires signiﬁcant efforts in\\ndeveloping hardware and software systems for stable and efﬁcient distributed training.\\nAn important consideration of distributed training is parallelism. There are several forms\\nof parallelism: data parallelism, model parallelism, tensor parallelism, and pipeline parallelism.\\nDespite different ways to distribute computations across devices, these parallelism methods are\\nbased on a similar idea: the training problem can be divided into smaller tasks that can be ex-\\necuted simultaneously. The issue of parallelism in training LLMs has been extensively studied'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 67}, page_content='2.2 Training at Scale\\n61\\n[Narayanan et al., 2021; Fedus et al., 2022]. Here we sketch the basic concepts.\\n• Data Parallelism. This method is one of the most widely used parallelism methods for\\ntraining neural networks. To illustrate, consider the simplest case where the standard delta\\nrule is used in gradient descent\\nθt+1\\n=\\nθt −lr · ∂Lθt(Dmini)\\n∂θt\\n(2.32)\\nwhere the new parameters θt+1 is obtained by updating the latest parameters θt with a small\\nstep lr in the direction of the negative loss gradient. ∂Lθt(Dmini)\\n∂θt\\nis the gradient of the loss\\nwith respect to the parameters θt, and is computed on a minibatch of training sample Dmini.\\nIn data parallelism, we divide Dmini into N smaller batches, denoted by {D1, ..., DN}.\\nThen, we distribute these batches to N workers, each with a corresponding batch. Once\\nthe data is distributed, these workers can work at the same time. The gradient of the entire\\nminibatch is obtained by aggregating the gradients computed by the workers, like this\\n∂Lθt(Dmini)\\n∂θt\\n=\\n∂Lθt(D1)\\n∂θt\\n|\\n{z\\n}\\nworker 1\\n+ ∂Lθt(D2)\\n∂θt\\n|\\n{z\\n}\\nworker 2\\n+ · · · + ∂Lθt(DN)\\n∂θt\\n|\\n{z\\n}\\nworker N\\n(2.33)\\nIn ideal cases where the workers coordinate well and the communication overhead is small,\\ndata parallelism can achieve nearly an N-fold speed-up for training.\\n• Model Parallelism. Although data parallelism is simple and effective, it requires each\\nworker to run the entire LLM and perform the complete forward and backward process.\\nAs LLMs grow larger, it sometimes becomes unfeasible to load and execute an LLM on a\\nsingle device. In this case, we can decouple the LLM into smaller components and run these\\ncomponents on different devices. One simple way to do this is to group consecutive layers\\nin the layer stack and assign each group to a worker. The workers operate in the order of\\nthe layers in the stack, that is, in the forward pass we process the input from lower-level to\\nupper-level layers, and in the backward pass we propagate the error gradients from upper-\\nlevel to lower-level layers. Consider, for example, a Transformer decoder with L stacked\\nblocks. To distribute the computation load, each block is assigned to a worker. See the\\nfollowing illustration for a single run of the forward and backward passes of this model.\\nWorker L\\nBL (↑) BL (↓)\\n...\\n...\\n...\\nWorker 2\\nB2 (↑)\\nB2 (↓)\\nWorker 1\\nB1 (↑)\\nB1 (↓)\\nHere Bl denotes the computation of block l, and the symbols ↑and ↓denote the forward and\\nbackward passes, respectively. Note that this parallelism method forces the workers to run\\nin sequence, so a worker has to wait for the previous worker to ﬁnish their job. This results\\nin the devices being idle for most of the time. In practical systems, model parallelism is\\ngenerally used together with other parallelism mechanisms to maximize the use of devices.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 68}, page_content='62\\nGenerative Models\\n• Tensor Parallelism. Parallelism can also be performed in a single computation step. A\\ncommon example is splitting a large parameter matrix into chunks, multiplying an input\\ntensor with each of these chunks separately, and then concatenating the results of these\\nmultiplications to form the output. For example, consider the multiplication of the repre-\\nsentation h ∈Rd with the parameter matrix Wh ∈Rd×dh in an FFN sub-layer (see Eq.\\n(2.25)). We can slice the matrix Wh ∈Rd×dh vertically to a sequence of M sub-matrices\\nWh\\n=\\nh\\nW1\\nh\\nW2\\nh\\n...\\nWM\\nh\\ni\\n(2.34)\\nwhere each sub-matrix Wk\\nh has a shape of d × dh\\nM . The multiplication of h with Wh can be\\nexpressed as\\nhWh\\n=\\nh\\nh\\nW1\\nh\\nW2\\nh\\n...\\nWM\\nh\\ni\\n=\\nh\\nhW1\\nh\\nhW2\\nh\\n...\\nhWM\\nh\\ni\\n(2.35)\\nWe can perform matrix multiplications {hW1\\nh, hW2\\nh, ..., hWM\\nh } on M devices separately.\\nAs a result, we distribute a large matrix multiplication across multiple devices, each of\\nwhich may have relatively small memory. From the perspective of the design of modern\\nGPUs, tensor parallelism over GPUs provides a two-level, tile-based approach to parallel\\ncomputing. First, at a higher level, we decompose a matrix multiplication into sub-matrix\\nmultiplications that can directly ﬁt into the memory of GPUs. Then, at a lower level, we\\nexecute these sub-matrix multiplications on GPUs using tile-based parallel algorithms that\\nare speciﬁcally optimized for GPUs.\\n• Pipeline Parallelism. Above, in model parallelism, we have described a simple approach\\nto spreading groups of model components across multiple devices. But this method is in-\\nefﬁcient because only one device is activated at a time during processing. Pipeline par-\\nallelism addresses this issue by introducing overlaps between computations on different\\ndevices [Harlap et al., 2018; Huang et al., 2019]. To do this, a batch of samples is divided\\ninto a number of micro-batches, and then these micro-batches are processed by each worker\\nas usual. Once a micro-batch is processed by a worker and passed to the next one, the\\nfollowing micro-batch immediately occupies the same worker. In other words, we create\\na pipeline in which different computation steps can overlap if multiple jobs are given to\\nthe pipeline. The following shows an illustration of pipeline parallelism for processing 3\\nmicro-batches.\\nWorker L\\nBL,1\\nBL,2\\nBL,3\\nBL,1\\nBL,2\\nBL,3\\n...\\n...\\n...\\nWorker 2\\nB2,1\\nB2,2\\nB2,3\\nB2,1\\nB2,2\\nB2,3\\nWorker 1\\nB1,1\\nB1,2\\nB1,3\\nB1,1\\nB1,2\\nB1,3\\nHere Bl,k represents the processing of the k-th micro-batch by the l-th worker. Ideally we\\nwould like to maximize the number of micro-batches, and thus minimize the idle time of the'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 69}, page_content='2.2 Training at Scale\\n63\\nworkers. However, in practice, using small micro-batches often reduces GPU utilization and\\nincreases task-switching costs. This may, in turn, decrease the overall system throughput.\\nThe ultimate goal of parallel processing is to achieve linear growth in efﬁciency, that is, the\\nnumber of samples that can be processed per unit of time increases linearly with the number of\\ndevices. However, distributed training is complicated, and inﬂuenced by many factors in addition\\nto the parallelism method we choose. One problem, which is often associated with distributed\\nsystems, is the cost of communication. We can think of a distributed system as a group of net-\\nworked nodes. Each of these nodes can perform local computation or pass data to other nodes. If\\nthere are a large number of such nodes, it will be expensive to distribute and collect data across\\nthem. Sometimes, the time savings brought about by parallelism are offset by the communica-\\ntion overhead of a large network. Another problem with large-scale distributed systems is that\\nthe synchronization of nodes introduces additional costs. As is often the case, some nodes may\\ntake longer to work, causing others to wait for the slowest ones. While we can use asynchronous\\ntraining to handle heterogeneity in computational resources, this may lead to stale gradients and\\nnon-guaranteed convergence. Moreover, as more nodes are added to the network, there is more\\nchance to have crashed nodes during training. In this case, we need to ensure that the whole\\nsystem is fault tolerant. In many practical settings, to increase scalability, one needs to take into\\naccount additional issues, including architecture design, data transfer and computation overlap,\\nload balancing, memory bandwidth and so on.\\nTraining LLMs is so computationally expensive that, even though distributed training is al-\\nready in use, researchers and engineers often still employ various model compression and speed-\\nup methods to improve training efﬁciency [Weng, 2021]. One example is mixed precision training,\\nin which low precision data (such as FP16 and FP8 data) is used for gradient computation on each\\nindividual node, and single or double precision data (such as FP32/FP64 data) is used for updating\\nthe model [Micikevicius et al., 2018]. A key operation in this approach is gradient accumulation\\nwhere gradients need to be accumulated and synchronized across nodes. However, due to the\\nnon-associativity of ﬂoating-point addition, this can lead to slight numerical differences in accu-\\nmulated gradients on different nodes, which may affect model convergence and ﬁnal performance.\\nThis problem is more obvious if there are a large number of nodes involved in distributed training,\\nespecially given that low-precision numerical computations may encounter overﬂow and under-\\nﬂow issues, as well as inconsistencies across different hardware devices. Therefore, the design of\\ndistributed systems needs to consider these numerical computation issues to ensure satisfactory\\nresults and convergence.\\n2.2.4\\nScaling Laws\\nThe success of LLMs reveals that training larger language models using more resources can lead\\nto improved model performance. Researchers have explained this as scaling laws of LLMs. More\\nspeciﬁcally, scaling laws describe the relationships between the performance of LLMs and the\\nattributes of LLM training, such as the model size, the amount of computation used for training,\\nand the amount of training data. For example, Hestness et al. [2017] show that the performance of\\ndeep neural networks is a power-law-like function of the training data size. In the beginning, when\\nthe amount of training data is not large, the performance of the model improves slowly. Afterward,\\nwhen more training data is used, the model enters a phase of rapid performance improvement, and\\nthe performance curve resembles a power-law curve. Ultimately, the improvement in performance'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 70}, page_content='64\\nGenerative Models\\nSlow Reduction\\nPhase\\nPower-law Reduction\\nPhase\\nConvergence\\nPhase\\n(Irreducible Error)\\nTraining Dataset Size (Log-scale)\\nNumber of Test Errors (Log-scale)\\nFig. 2.3: A scaling law of test error against a variable of interest (e.g., training dataset size) [Hestness et al., 2017]. The\\ncurve of the scaling law can be divided into three phases. At the beginning, the number of test errors decreases slowly\\nwhen more training data is used, but this only lasts for a short period. In the second phase, the number of test errors\\ndecreases drastically, and the curve becomes a power law curve. After that, the error reduction slows down again in the\\nthird phase. Note that there are irreducible errors that cannot be eliminated, regardless of the amount of training data.\\nbecomes slow again, and more data does not lead to signiﬁcant gains. Figure 2.3 shows an example\\nof such curves.\\nIn NLP, a traditional view holds that the performance gains will disappear at a certain point\\nas the training is scaled up. However, recent results show that, if we consider the problem on\\na larger scale, scaling up training is still a very effective method for obtaining stronger LLMs.\\nFor example, both closed-source and open-source LLMs can beneﬁt from more data, even though\\ntrillions of tokens have already been used for training.\\nWith the increase in the scale of model training, LLMs exhibit new capabilities, known as the\\nemergent abilities of LLMs. For example, Wei et al. [2022b] studied the scaling properties of\\nLLMs across different model sizes and amounts of computational resources. Their work shows\\nthat some abilities emerge when we scale the model size to certain level. The appearance of\\nemergent abilities has demonstrated the role of scaled training in enhancing the performance of\\nLLMs, and it has also, to some extent, motivated researchers to continuously attempt to train larger\\nmodels. As larger and stronger LMs continue to appear, our understanding of the scaling laws\\ncontinues to mature. This helps researchers predict the performance of LLMs during training and\\nestimate the minimal computational resources required to achieve a given level of performance.\\nTo understand how model performance scales with various factors considered during training,\\nit is common to express the model performance as a function of these factors. For example, in\\nthe simplest case, we can express the loss or error of an LLM as a function of a single variable of\\ninterest. However, there are no universal scaling laws that can describe this relationship. Instead,\\ndifferent functions are proposed to ﬁt the learning curves of LLMs.\\nLet x be the variable of interest (such as the number of model parameters) and L(x) be the\\nloss of the model given x (such as the cross-entropy loss on test data). The simplest form of L(x)\\nis a power law\\nL(x)\\n=\\naxb\\n(2.36)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 71}, page_content='2.2 Training at Scale\\n65\\n105\\n107\\n109\\n2.4\\n3.2\\n4.0\\n4.8\\n5.6\\nNumber of Parameters\\nTest Loss\\nL(N) = (\\nN\\n8.8·1013 )−0.076\\n108\\n109\\n2.7\\n3\\n3.3\\n3.6\\n3.9\\n4.2\\nDataset Size\\nTest Loss\\nL(D) = (\\nD\\n5.4·1013 )−0.095\\nFig. 2.4: Test loss against model size (N) and training dataset size (D) (data points are plotted for illustrative purposes).\\nWe plot test loss as a function of N, which is deﬁned as L(N) = \\x00N\\n8.8×1013\\n\\x01−0.076, and a function of D, which is\\ndeﬁned as L(D) = \\x00D\\n5.4×1013\\n\\x01−0.095 [Kaplan et al., 2020].\\nwhere a and b are parameters that are estimated empirically. Despite its simplicity, this func-\\ntion has successfully interpreted the scaling ability of language models and machine transla-\\ntion systems in terms of model size (denoted by N) and training dataset size (denoted by D)\\n[Gordon et al., 2021; Hestness et al., 2017]. For example, Kaplan et al. [2020] found that the per-\\nformance of their language model improves as a power law of either N or D after an initial\\ntransient period, and expressed these relationships using L(N) =\\n\\x00N\\n8.8×1013\\n\\x01−0.076 and L(D) =\\n\\x00D\\n5.4×1013\\n\\x01−0.095 (see Figure 2.4).\\nAn improvement to this scaling law is to add an irreducible error term to the power law. The\\nform of L(x) is then given by\\nL(x)\\n=\\naxb + ǫ∞\\n(2.37)\\nwhere ǫ∞is the irreducible error that accounts for the error due to unknown variables, which is\\npresent even as x →∞. Eq. (2.37) is one of the most widely used forms for designing scaling\\nlaws of LLMs. For example, Rosenfeld et al. [2020] developed a scaling law that involves both\\nmodel scaling and dataset scaling, like this\\nL(N, D)\\n=\\naNb + cDd + ǫ∞\\n(2.38)\\nAn example of such formulation is the Chinchilla scaling law. It states that the test loss per\\ntoken is the sum of the inverse proportion functions of N and D, with an additional irreducible\\nerror term. Hoffmann et al. [2022] express this scaling law as\\nL(N, D)\\n=\\n406.4\\nN 0.34\\n| {z }\\nmodel scaling\\n+\\n410.7\\nD0.28\\n| {z }\\ndataset scaling\\n+\\n1.69\\n|{z}\\nirreducible error\\n(2.39)\\nAll the scaling laws mentioned above are based on monotonic functions. So they cannot cover\\nfunctions with inﬂection points, such as double descent curves. In response, researchers have\\nexplored more sophisticated functions to ﬁt the learning curves. Examples of such functions can'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 72}, page_content='66\\nGenerative Models\\nbe found in Alabdulmohsin et al. [2022] and Caballero et al. [2023]’s work.\\nThe signiﬁcance of scaling laws lies in providing directional guidance for LLM research: if\\nwe are still in the region of the power law curve, using more resources to train larger models is a\\nvery promising direction. While this result “forces” big research groups and companies to invest\\nmore in computational resources to train larger models, which is very expensive, scaling laws\\ncontinuously push the boundaries of AI further away. On the other hand, understanding scaling\\nlaws helps researchers make decisions in training LLMs. For example, given the computational\\nresources at hand, the performance of LLMs may be predicted.\\nOne last note on scaling laws in this section. For LLMs, a lower test loss does not always\\nimply better performance on all downstream tasks. To adapt LLMs, there are several steps such\\nas ﬁne-tuning and prompting that may inﬂuence the ﬁnal result. Therefore, the scaling laws for\\ndifferent downstream tasks might be different in practice.\\n2.3\\nLong Sequence Modeling\\nWe have already seen that, in large-scale training, larger language models can be developed by us-\\ning more data and computational resources. However, scaling up can also occur in other directions.\\nFor instance, in many applications, LLMs are adapted to process signiﬁcantly long sequences. An\\ninteresting example is that we pre-train an LLM on extensive texts of normal length and then ap-\\nply it to deal with very long token sequences, far beyond the length encountered in pre-training.\\nHere we use Pr(y|x) to denote the text generation probability where x is the context and y is the\\ngenerated text. There are broadly three types of long sequence modeling problems.\\n• Text generation based on long context (i.e., x is a long sequence). For example, we\\ngenerate a short summary for a very long text.\\n• Long text generation (i.e., y is a long sequence). For example, we generate a long story\\nbased on a few keywords.\\n• Long text generation based on long context (i.e., both x and y are long sequences). For\\nexample, we translate a long document from Chinese to English.\\nRecently, NLP researchers have been more interested in applying and evaluating LLMs on\\ntasks where extremely long input texts are involved. Imagine an LLM, which reads a C++ source\\nﬁle containing tens of thousands of lines, and outlines the functionality of the program correspond-\\ning to the source ﬁle. Such models, capable of handling extensive textual contexts, are sometimes\\ncalled long-context LLMs. In this section we will restrict ourselves to long-context LLMs, but\\nthe methods discussed here can be applicable to other problems.\\nFor Transformers, dealing with long sequences is computationally expensive, as the computa-\\ntional cost of self-attention grows quadratically with the sequence length. This makes it infeasible\\nto train and deploy such models for very long inputs. Two strands of research have tried to adapt\\nTransformers to long-context language modeling.\\n• The ﬁrst explores efﬁcient training methods and model architectures to learn self-attention\\nmodels from long-sequence data.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 73}, page_content='2.3 Long Sequence Modeling\\n67\\n• The other adapts pre-trained LLMs to handle long sequences with modest or no ﬁne-tuning\\nefforts.\\nHere, we will discuss the former brieﬂy since it can be found in general discussions of efﬁcient\\nTransformer architectures [Tay et al., 2020; Xiao and Zhu, 2023]. We will focus on the latter,\\nhighlighting popular methods in recent LLMs. We will also discuss the strengths and limitations\\nof these long-sequence models.\\n2.3.1\\nOptimization from HPC Perspectives\\nWe begin our discussion by considering improvements to standard Transformer models from the\\nperspectives of high-performance computing. Most of these improvements, though not speciﬁ-\\ncally designed for LLMs, have been widely applied across various deep learning models [Kim et al.,\\n2023]. A commonly used approach is to adopt a low-precision implementation of Transformers.\\nFor example, we can use 8-bit or 16-bit ﬁxed-point data types for arithmetic operations, instead\\nof 32-bit or 64-bit ﬂoating-point data types. Using these low-precision data types can increase\\nthe efﬁciency and memory throughput, so that longer sequences can be processed more easily.\\nAn alternative approach is to improve Transformers by using hardware-aware techniques. For\\nexample, on modern GPUs, the efﬁciency of Transformers can be improved by using IO-aware\\nimplementations of the self-attention function [Dao et al., 2022; Kwon et al., 2023].\\nAnother way to handle long sequences is through sequence parallelism [Li et al., 2023b;\\nKorthikanti et al., 2023]. Speciﬁcally, consider the general problem of attending the query qi\\nat the position i to the keys K and values V. We can divide K by rows and obtain a set of sub-\\nmatrices {K[1], ..., K[nu]}, each corresponding to a segment of the sequence. Similarly, we can\\nobtain the sub-matrices of V, denoted by {V[1], ..., V[nu]}. Then, we assign each pair of K[u] and\\nV[u] to a computing node (e.g., a GPU of a GPU cluster). The assigned nodes can run in parallel,\\nthereby parallelizing the attention operation.\\nRecall that the output of the self-attention model can be written as\\nAttqkv(qi, K, V)\\n=\\nm−1\\nX\\nj=0\\nαi,jvj\\n(2.40)\\nwhere αi,j is the attention weight between positions i and j. In Transformers, αi,j is obtained\\nby normalizing the rescaled version of the dot product between qi and kj. Let βi,j denote the\\nattention score between qi and kj. We have\\nβi,j\\n=\\nqi · kj\\n√\\nd\\n+ Mask(i, j)\\n(2.41)\\nwhere Mask(i, j) is the masking variable for (i, j). Then, we deﬁne the attention weight αi,j to\\nbe\\nαi,j\\n=\\nSoftmax(βi,j)\\n=\\nexp(βi,j)\\nP\\nj′ exp(βi,j′)\\n(2.42)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 74}, page_content='68\\nGenerative Models\\nOn each computing node, we need to implement these equations. Given the keys and values\\nassigned to this node, computing the numerator of the right-hand side of Eq. (2.42) (i.e., exp(βi,j))\\nis straightforward, as all the required information is stored on the node. However, computing the\\ndenominator of the right-hand side of Eq. (2.42) involves a sum of exp(βi,j′) over all j′s, which\\nrequires transferring data to and from other nodes. To illustrate, suppose that vj and kj are placed\\non node u. We can rewrite Eq. (2.42) as\\nαi,j\\n=\\nnode u\\nz\\n}|\\n{\\nexp(βi,j)\\nX\\nkj′∈K[1]\\nexp(βi,j′)\\n|\\n{z\\n}\\nnode 1\\n+ · · · +\\nX\\nkj′∈K[u]\\nexp(βi,j′)\\n|\\n{z\\n}\\nnode u\\n+ · · · +\\nX\\nkj′∈K[nu]\\nexp(βi,j′)\\n|\\n{z\\n}\\nnode nu\\n(2.43)\\nwhere the notation kj′ ∈K[u] represents that kj′ is a row vector of K[u]. In a straightforward\\nimplementation, we ﬁrst perform the summations {P\\nkj′∈K[u] exp(βi,j′)} separately on the corre-\\nsponding nodes. Then, we collect these summation results from different nodes to combine them\\ninto a ﬁnal result. This corresponds to a collective operation in the context of parallel processing.\\nThere are many efﬁcient implementations of such operations, such as the all-reduce algorithms.\\nHence the sum of all exp(βi,j) values can be computed using optimized routines in collective\\ncommunication toolkits.\\nGiven the attention weights {αi,j}, we then compute the attention results using Eq. (2.40).\\nThe problem can be re-expressed as\\nAttqkv(qi, K, V)\\n=\\nX\\nvj′∈V[1]\\nαi,j′vj′\\n|\\n{z\\n}\\nnode 1\\n+ · · · +\\nX\\nvj′∈V[u]\\nαi,j′vj′\\n|\\n{z\\n}\\nnode u\\n+ · · · +\\nX\\nvj′∈V[nu]\\nαi,j′vj′\\n|\\n{z\\n}\\nnode nu\\n(2.44)\\nLike Eq. (2.43), Eq. (2.44) can be implemented as a summation program in parallel process-\\ning. First, perform the weighted summations of values on different nodes simultaneously. Then,\\nwe collect the results from these nodes via collective operations.\\nNote that, although this section primarily focuses on long sequence modeling, much of the mo-\\ntivation for sequence parallelism comes from the distributed training methods of deep networks,\\nas discussed in Section 2.2.3. As a result, the implementation of these methods can be based on\\nthe same parallel processing library.\\n2.3.2\\nEfﬁcient Architectures\\nOne difﬁculty of applying Transformers to long sequences is that self-attention has a quadratic\\ntime complexity with respect to the sequence length. Moreover, a key-value cache (or KV cache\\nfor short) is maintained during inference, and its size increases as more tokens are processed. Al-\\nthough the KV cache grows linearly with the sequence length, for extremely long input sequences,\\nthe memory footprint becomes signiﬁcant and it is even infeasible to deploy LLMs for such tasks.\\nAs a result, the model architecture of long-context LLMs generally moves away from the standard'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 75}, page_content='2.3 Long Sequence Modeling\\n69\\nTransformer, turning instead to the development of more efﬁcient variants and alternatives.\\nOne approach is to use sparse attention instead of standard self-attention. This family of\\nmodels is based on the idea that only a small number of tokens are considered important when\\nattending to a given token, and so most of the attention weights between tokens are close to zero.\\nAs a consequence, we can prune most of the attention weights and represent the attention model\\nin a compressed form. To illustrate, consider the self-attention model\\nAttqkv(Q, K, V)\\n=\\nα(Q, K)V\\n(2.45)\\nwhere the attention weight matrix α(Q, K) ∈Rm×m is obtained by\\nα(Q, K)\\n=\\nSoftmax(QKT\\n√\\nd\\n+ Mask)\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nα0,0\\n0\\n0\\n...\\n0\\nα1,0\\nα1,1\\n0\\n...\\n0\\nα2,0\\nα2,1\\nα2,2\\n...\\n0\\n...\\n...\\n...\\n...\\n...\\nαm−1,0\\nαm−1,1\\nαm−1,2\\n...\\nαm−1,m−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n(2.46)\\nEach row vector\\nh\\nαi,0\\n...\\nαi,i\\n0\\n...\\n0\\ni\\ncorresponds to a distribution of attending the i-th\\ntoken to every token of the sequence. Since language models predict next tokens only based on\\ntheir left-context, we normally write the output of the attention model at position i as\\nAttqkv(qi, K≤i, V≤i)\\n=\\nh\\nαi,0\\n...\\nαi,i\\ni\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nv0\\n...\\nvi\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n=\\ni\\nX\\nj=0\\nαi,jvj\\n(2.47)\\nwhere K≤i =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nk0\\n...\\nki\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fband V≤i =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nv0\\n...\\nvi\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fbare the keys and values up to position i.\\nIn the original version of self-attention\\nh\\nαi,0\\n...\\nαi,i\\ni\\nis assumed to be dense, that is, most of\\nthe values are non-zero. In sparse attention, some of the entries of\\nh\\nαi,0\\n...\\nαi,i\\ni\\nare considered\\nnon-zero, and the remaining entries are simply ignored in computation. Suppose G ⊆{0, ..., i} is\\nthe set of indices of the non-zero entries. For language models, the output of the sparse attention\\nmodel at position i is given by\\nAttsparse(qi, K≤i, V≤i)\\n=\\nX\\nj∈G\\nα′\\ni,jvj\\n(2.48)\\nHere {α′\\ni,j} are normalized over G. Hence their values are different from the original attention\\nweights (in fact we have α′\\ni,j > αi,j). The sparsity of the model is determined by how large G is.\\nSparse attention models differ in the way we deﬁne G. One simple approach is to deﬁne G based'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 76}, page_content='70\\nGenerative Models\\non heuristically designed patterns. For example, a widely-used pattern involves having G cover a\\nwindow of tokens located near position i [Parmar et al., 2018].\\nWhile sparse attention reduces the computation through the use of sparse operations, such\\nmodels still have signiﬁcant limitations as we must keep the entire KV cache (i.e., K≤i and V≤i)\\nduring inference. If the sequence is very long, storing this cache will become highly memory-\\nintensive. To address this, we can consider a different form of attention models where the KV\\ncache is not explicitly retained. Linear attention is one such approach [Katharopoulos et al., 2020].\\nIt uses a kernel function φ(·) to project each query and key onto points q′\\ni = φ(qi) and k′\\ni = φ(ki),\\nrespectively. By removing the Softmax function under such transformations10, the form of the\\nresulting attention model is given by\\nAttqkv(qi, K≤i, V≤i)\\n≈\\nAttlinear(q′\\ni, K′\\n≤i, V≤i)\\n=\\nq′\\niµi\\nq′\\niνi\\n(2.49)\\nwhere µi and νi are variables that are computed in the recurrent forms\\nµi\\n=\\nµi−1 + k′T\\ni vi\\n(2.50)\\nνi\\n=\\nνi−1 + k′T\\ni\\n(2.51)\\nµi and νi can be seen as representations of the history up to position i. A beneﬁt of this model is\\nthat we need not keep all past queries and values. Instead only the latest representations µi and\\nνi are used. So the computational cost of each step is a constant, and the model can be easily\\nextended to deal with long sequences.\\nIn fact, this sequential approach to long sequence modeling arises naturally when we adopt a\\nviewpoint of recurrent models. Such models read one token (or a small number of tokens) at a\\ntime, update the recurrent state using these inputs, and then discard them before the next token\\narrives. The output at each step is generated based only on the recurrent state, rather than on all the\\nprevious states. The memory footprint is determined by the recurrent state which has a ﬁxed size.\\nRecurrent models can be used in real-time learning scenarios where data arrives in a stream and\\npredictions can be made at any time step. In NLP, applying recurrent models to language mod-\\neling is one of the earliest successful attempts to learn representations of sequences. Although\\nTransformer has been used as the foundational architecture in LLMs, recurrent models are still\\npowerful models, especially for developing efﬁcient LLMs. More recently, recurrent models have\\nstarted their resurgence in language modeling and have been reconsidered as a promising alterna-\\ntive to Transformers [Gu and Dao, 2023]. Figure 2.5 shows a comparison of the models discussed\\nin this subsection.\\n10In the new space after this transformation, the Softmax normalization can be transformed into the simple scaling\\nnormalization.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 77}, page_content='2.3 Long Sequence Modeling\\n71\\nqi\\nki\\nki−1\\nki−2\\n· · ·\\nk1\\nk0\\nvi\\nvi−1\\nvi−2\\n· · ·\\nv1\\nv0\\nAttqkv(qi, K≤i, V≤i)\\n(a) Standard Self-attention\\nqi\\nki\\nki−1\\nki−2\\n· · ·\\nk1\\nk0\\nvi\\nvi−1\\nvi−2\\n· · ·\\nv1\\nv0\\nAttqkv(qi, {k1, ki}, {v1, vi})\\n(b) Sparse Attention\\nqi\\nki\\nki−1\\nki−2\\n· · ·\\nk1\\nk0\\nvi\\nvi−1\\nvi−2\\n· · ·\\nv1\\nv0\\nνi = νi−1 + k′T\\ni\\nµi = µi−1 + k′T\\ni vi\\nνi\\nµi\\n⇒\\n⇒\\nAttlinear(qi, K≤i, V≤i) = q′\\niµi\\nq′\\niνi\\n(c) Linear Attention\\nhi\\nhi−1\\nhi−2\\nhi−3\\n· · ·\\nh1\\nh0\\ninputi\\nhi = f(hi−1, inputi)\\n(d) Recurrent Models\\nFig. 2.5: Illustrations of self-attention, sparse attention, linear attention and recurrent models. Blue boxes = cached\\nstates for producing the output at position i. f(·) = a recurrent cell.\\n2.3.3\\nCache and Memory\\nLLMs based on the standard Transformer architecture are global models. The inference for these\\nmodels involves storing the entire left-context in order to make predictions for future tokens. This\\nrequires a KV cache where the representations (i.e., keys and values) of all previously-generated'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 78}, page_content='72\\nGenerative Models\\ntokens are kept, and the cost of caching grows as the inference proceeds. Above, we have dis-\\ncussed methods for optimizing this cache via efﬁcient attention approaches, such as sparse atten-\\ntion and linear attention. Another idea, which may have overlap with the previous discussion, is\\nto explicitly encode the context via an additional memory model.\\n2.3.3.1\\nFixed-size KV Cache\\nA straightforward approach is to represent the keys and values using a ﬁxed-size memory model.\\nSuppose we have a memory Mem which retains the contextual information. We can write the\\nattention operation at position i in a general form\\nAtt(qi, Mem)\\n=\\nAttqkv(qi, K≤i, V≤i)\\n(2.52)\\nIn this model, Mem is simply the KV cache, i.e., Mem = (K≤i, V≤i). Thus the size of\\nMem is determined by i. If we deﬁne Mem as a ﬁxed-size variable, then the cost of performing\\nAtt(qi, Mem) will be ﬁxed. There are several alternative ways to design Mem.\\n• One of the simplest methods is to consider a ﬁxed-size window of previous keys and values.\\nMem is therefore given by\\nMem\\n=\\n(K[i−nc+1,i], V[i−nc+1,i])\\n(2.53)\\nwhere nc denotes the size of the window. The notation K[i−nc+1,i] and V[i−nc+1,i] denote\\nthe keys and values over positions from i −nc + 1 to i.11 This model can be seen as a type\\nof local attention model.\\n• It is also possible to deﬁne Mem as a pair of summary vectors, which leads to a more\\ncompressed representation of the history. A simple way to summarize the previous keys\\nand values is to use the moving average of them. For example, Mem can be deﬁned as the\\nunweighted moving average of the previous nc keys and values\\nMem\\n=\\n\\x10Pi\\nj=i−nc+1 kj\\nnc\\n,\\nPi\\nj=i−nc+1 vj\\nnc\\n\\x11\\n(2.54)\\nAlternatively, we can use a weighted version of moving average\\nMem\\n=\\n\\x10Pi\\nj=i−nc+1 βj−i+nckj\\nPnc\\nj=1 βj\\n,\\nPi\\nj=i−nc+1 βj−i+ncvj\\nPnc\\nj=1 βj\\n\\x11\\n(2.55)\\nHere {β1, ..., βnc} are the coefﬁcients, which can be either learned as model parameters\\nor determined via heuristics. For example, they can be set to increasing coefﬁcients (i.e.,\\nβ1 < β2 < ... < βnc−1 < βnc) in order to give larger weight to positions that are closer to\\n11More formally, we write K[i−nc+1,i] =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nki−nc+1\\n...\\nki\\n\\uf8f9\\n\\uf8fa\\uf8fband V[i−nc+1,i] =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nvi−nc+1\\n...\\nvi\\n\\uf8f9\\n\\uf8fa\\uf8fb. Sometimes we denote\\nK[i−nc+1,i] by {ki−nc+1, ..., ki} and V[i−nc+1,i] by {vi−nc+1, ..., vi} for notation simplicity.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 79}, page_content='2.3 Long Sequence Modeling\\n73\\ni. We can extend the moving average to include all the positions up to i. This leads to the\\ncumulative average of the keys and values, given in the form\\nMem\\n=\\n\\x10Pi\\nj=0 kj\\ni + 1\\n,\\nPi\\nj=0 vj\\ni + 1\\n\\x11\\n(2.56)\\nIn general, the cumulative average can be written using a recursive formula\\nMemi\\n=\\n(ki, vi) + i · Memi−1\\ni + 1\\n(2.57)\\nwhere Memi and Memi−1 denote the cumulative averages of the current and previous po-\\nsitions, respectively. An advantage of this model is that we only need to store a single\\nkey-value pair during inference, rather than storing all the key-value pairs. Note that the\\nabove memory models are related to recurrent models, and more advanced techniques have\\nbeen used to develop alternatives to self-attention mechanisms in Transformers [Ma et al.,\\n2023].\\n• The memory Mem can also be a neural network. At each step, it takes both the previous\\noutput of the memory and the current states of the model as input, and produces the new\\noutput of the memory. This neural network can be formulated as the function\\nMem\\n=\\nUpdate(Skv, Mempre)\\n(2.58)\\nHere Mem and Mempre represent the outputs of the memory at the current step and the\\nprevious step, respectively. Skv is a set of key-value pairs, representing the recent states of\\nthe model. This formulation is general and allows us to develop various memory models by\\nselecting different Update(·) and Skv conﬁgurations. For example, if Skv only contains the\\nlatest key-value pair (ki, vi) and Update(·) is deﬁned as a recurrent cell, then Eq. (2.58)\\ncan be expressed as an RNN-like model\\nMem\\n=\\nf((ki, vi), Mempre)\\n(2.59)\\nwhere f(·) is a recurrent cell. Recurrence can also be applied to segment-level modeling\\nfor efﬁciency consideration. A simple approach is that we can divide the sequence into\\nsegments, and treat Skv as a segment. Applying recurrent models to Update(·) will result in\\nmemory models that operate on segments. A special example is that we deﬁne Update(·) as\\nan FIFO function that adds Skv into the memory and removes the oldest key-value segment\\nfrom the memory, given by\\nMem\\n=\\nFIFO(Skv, Mempre)\\n(2.60)\\nConsider a memory which includes two segments, one for current segment, and one for the\\nprevious segment. In the attention operation, each position can access the history key-value\\npairs in two closest consecutive segments. This essentially deﬁnes a local memory, but it\\nand its variants have been widely used segment-level recurrent models [Dai et al., 2019;\\nHutchins et al., 2022; Bulatov et al., 2022].\\n• The above memory models can be extended to involve multiple memories. An example'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 80}, page_content='74\\nGenerative Models\\nof this approach is compressive Transformer [Rae et al., 2019]. It employs two distinct\\nﬁxed-size memories: one for modeling local context (denoted by Mem), and the other for\\nmodeling and compressing long-term history (denoted by CMem). The KV cache in this\\nmodel is the combination of Mem and CMem. The attention function can be written as\\nAttcom(qi, Mem, CMem)\\n=\\nAttqkv(qi, [Mem, CMem])\\n(2.61)\\nwhere [Mem, CMem] is a combined memory of Mem and CMem. As with other segment-\\nlevel models, the compressive Transformer model operates on segments of the sequence.\\nEach segment is a sequence of ns consecutive tokens, and we denote Sk\\nkv as the key-value\\npairs corresponding to the tokens of the k-th segment. When a new segment arrives, Mem\\nis updated in an FIFO fashion: we append the nc key-value pairs in Sk\\nkv to Mem, and then\\npop the ns oldest key-value pairs from Mem, which is given by\\nMem\\n=\\nFIFO(Sk\\nkv, Mempre)\\n(2.62)\\nThe popped key-value pairs are then used to update the compressive memory CMem. These\\nns key-value pairs are compressed into ns\\nc key-value pairs via a compression network.\\nCMem is an FIFO which appends the compressed ns\\nc key-value pairs to the tail of the\\nqueue, and drops the ﬁrst ns\\nc key-value pairs of the queue. It is given by\\nCMem\\n=\\nFIFO(Ck\\nkv, CMempre)\\n(2.63)\\nwhere Ck\\nkv represents the set of compressed key-value pairs. Implicit in the compressive\\nTransformer model is that local context should be represented explicitly with minimal in-\\nformation loss, while long-range context can be more compressed.\\n• We have already seen that both global and local contexts are useful and can be mod-\\neled using attention models. This view motivates the extension to attention models for\\ncombining both local and long-term memories [Ainslie et al., 2020; Zaheer et al., 2020;\\nGupta and Berant, 2020]. A simple but widely-used approach is to involve the ﬁrst few to-\\nkens of the sequence in attention, serving as global tokens. This approach is usually applied\\nalong with other sparse attention models. An advantage of incorporating global tokens of\\nthe sequence is that it helps smooth the output distribution of the Softmax function used in\\nattention weight computation, and thus stabilizes model performance when the context size\\nis very large [Xiao et al., 2024]. One drawback, however, is that using a ﬁxed-size global\\nmemory may result in information loss. When dealing with long sequences, we need to\\nenlarge the KV cache for sufﬁcient representations of the context, but this in turn increases\\nthe computational cost.\\nFigure 2.6 shows illustrations of the above approaches. Note that, while we focus on optimiza-\\ntion of the KV cache here, this issue is closely related to those discussed in the previous section.\\nAll of the methods we have mentioned so far can broadly be categorized as efﬁcient attention\\napproaches, which are widely used in various Transformer variants.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 81}, page_content='2.3 Long Sequence Modeling\\n75\\n· · ·\\n· · ·\\ni\\ni −1\\ni −2\\ni −3\\ni −4\\ni −5\\ni −6\\ni −7\\nKeys\\nValues\\nSize = 4 × 2\\nMemory\\n(a) Window-based Cache\\n· · ·\\n· · ·\\ni\\ni −1\\ni −2\\ni −3\\ni −4\\ni −5\\ni −6\\ni −7\\nKeys\\nValues\\nSize = 1 × 2\\nMemory\\nki−3+ki−2+ki−1+ki\\n4\\n⇒\\nvi−3+vi−2+vi−1+vi\\n4\\n⇒\\n(b) Moving Average-based Cache\\n· · ·\\n· · ·\\ni\\ni −1\\ni −2\\ni −3\\ni −4\\ni −5\\ni −6\\ni −7\\nKeys\\nValues\\nSize = 1 × 2\\nMemory\\nMem = Update( Skv , Mempre)\\n⇒\\n(c) Recurrent Network as Cache\\n· · ·\\n· · ·\\ni\\ni −1\\ni −2\\ni −3\\ni −4\\ni −5\\ni −6\\ni −7\\nKeys\\nValues\\nSize = 4 × 2\\nMemory\\nSize = 2 × 2\\nMemory\\nCompressed\\n(d) Hybrid Cache (Compressed Memory + Local Memory)\\nFig. 2.6: Illustrations of ﬁxed-size KV caches in LLMs. Blue boxes represent the keys and values generated during\\nLLM inference, green boxes represent the keys and values stored or encoded in the primary memory, and orange boxes\\nrepresent the keys and values stored or encoded in the compressed memory.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 82}, page_content='76\\nGenerative Models\\n2.3.3.2\\nMemory-based Models\\nThe modeling of memories discussed above was based on updates to the KV cache, and the re-\\nsulting models are typically referred to as internal memories. We now consider another family\\nof models, called external memories, which operate as independent models to access large-scale\\ncontexts for LLMs. Many such models are based on memory-based methods which have been\\nextensively discussed in machine learning [Bishop, 2006]. A common example is nearest neigh-\\nbor algorithms: we store context representations in a datastore, and try to ﬁnd the most similar\\nstored representations to match a given query. The retrieved context representations are then used\\nto improve attention for this query.\\nHere, we consider the k-nearest neighbors (k-NN) method which is one of the most popular\\nmemory-based methods. Since our focus is language modeling in this section, we deﬁne a sample\\nin the datastore as a key-value pair corresponding to some context state. Note that “context” is a\\nbroad concept here, not just a sequence preﬁx in text generation. One might, for example, view\\nthe entire dataset as the context for predicting tokens. This allows us to retrieve the closest context\\nsituation in a set of sequences, rather than a given sequence preﬁx. Although we will restrict\\nourselves to context modeling for a single sequence, in this subsection, we discuss a relatively\\nmore general case.\\nSuppose we have a set of keys {kj} with corresponding values {vj}, and suppose we store\\nthese key-value pairs in a vector database12. For each query qi, we ﬁnd its k nearest neighbours by\\ngrowing the radius of the sphere centered as qi until it contains k data points in {kj}. This results\\nin a set of k keys along with their corresponding values, denoted by Memknn. As before, we\\ndenote Mem as the local memory for the query, such as the KV cache of neighboring tokens. Our\\ngoal is to attend query qi to both the local memory Mem and the long-term memory Memknn.\\nThere are, of course, several ways to incorporate Mem and Memknn into the attention model.\\nFor example, we might simply combine them to form a single KV cache [Mem, Memknn], and\\nattend qi to [Mem, Memknn] via standard QKV attention. Or we might use Mem and Memknn\\nin separate attention steps. An example of such approaches is the model developed by Wu et al.\\n[2021]. It linearly combines the two types of attention, given by\\nAtt(qi, Mem, Memknn)\\n=\\ng ⊙Attlocal + (1 −g) ⊙Attknn\\n(2.64)\\nAttlocal\\n=\\nAtt(qi, Mem)\\n(2.65)\\nAttknn\\n=\\nAtt(qi, Memknn)\\n(2.66)\\nHere g ∈Rd is the coefﬁcient vector, which can be the output of a learned gate.\\nGiven the k-NN-based memory model described above, the remaining task is to determine\\nwhich key-value pairs are retained in the datastore. For standard language modeling tasks, we\\nconsider the previously seen tokens in a sequence as the context, so we can add the keys and\\nvalues of all these tokens into the datastore. In this case, the resulting k-NN-based attention\\nmodel is essentially equivalent to a sparse attention model [Gupta et al., 2021].\\nAlternatively, we can extend the context from one sequence to a collection of sequences.\\nFor example, we might collect all key-value pairs across the sequences in a training dataset and\\nadd them to the datastore to model a larger context. Thus, LLMs can predict tokens based on a\\n12A vector database, or vector store, is a database that provides highly optimized retrieval interfaces for ﬁnding stored\\nvectors that closely match a query vector.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 83}, page_content='2.3 Long Sequence Modeling\\n77\\ngeneralized context. A problem with this approach is that the computational cost would be large\\nif many sequences are involved. Since these sequences are part of our training data, we can build\\nand optimize an index for the vectors in the datastore before running the LLMs. As a result, the\\nretrieval of similar vectors can be very efﬁcient, as in most vector databases.\\nIn fact, all the above-mentioned methods can be viewed as instances of a retrieval-based ap-\\nproach. Instead of using retrieval results to improve attention, we can apply this approach in other\\nways as well. One application of k-NN-based search is k-NN language modeling (or k-NN LM)\\n[Khandelwal et al., 2020]. The idea is that, although it is attempting to extend the context used\\nin self-attention by incorporating nearest neighbors in representation learning, in practice, similar\\nhidden states in Transformers are often highly predictive of similar tokens in subsequent positions.\\nIn k-NN LM, each item in the datastore is a key-value tuple (z, w), where z represents a hidden\\nstate of the LLM at a position, and w represents the corresponding prediction. A typical way to\\ncreate the datastore is to collect the output vector of the Transformer layer stack and the corre-\\nsponding next token for each position of each sequence in a training dataset. During inference,\\nwe have a representation hi given a preﬁx. Given this representation, we ﬁrst search the datastore\\nfor k closest matching data items {(z1, w1), ..., (zk, wk)}. Here {w1, ..., wk} are thought of as\\nreference tokens for prediction, and thus can be used to guide the token prediction based on hi.\\nOne common way to make use of reference tokens is to deﬁne a distribution over the vocabulary\\nV ,\\nPrknn(·|hi)\\n=\\nSoftmax(\\nh\\n−d0\\n· · ·\\n−d|V |\\ni\\n)\\n(2.67)\\nwhere dv equals the distance between hi and zj if wj equals the v-th entry of V , and equals 0\\notherwise. We use a linear function with a coefﬁcient λ that interpolates between the retrieval-\\nbased distribution Prknn(·|hi) and the LLM output distribution Prlm(·|hi)\\nPr(·|hi)\\n=\\nλ · Prknn(·|hi) + (1 −λ) · Prlm(·|hi)\\n(2.68)\\nThen, as usual, we can choose the next token y by maximizing the probability Pr(y|hi).\\nAs with information retrieval (IR) systems, the datastore can also manage texts and provide\\naccess to relevant texts for a query. For example, we can store a collection of text documents\\nin a search engine with full-text indexing, and then search it for documents that match a given\\ntext-based query. Applying IR techniques to LLMs leads to a general framework called retrieval-\\naugmented generation (RAG). The RAG framework works as follows. We use the context x as\\nthe query and ﬁnd the k most relevant document pieces {c1, ..., ck} from the datastore via efﬁcient\\nIR techniques13. These search results are combined with the original context via a prompting\\n13In piratical applications, queries are typically generated using a query generation system, which may expand it\\nwith variations of tokens and query intent.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 84}, page_content='78\\nGenerative Models\\ntemplate g(·)14, resulting in an augmented input for the LLM\\nx′\\n=\\ng(c1, ..., ck, x)\\n(2.69)\\nThen, we use x′ as the context and predict the following text using the model Pr(y|x′). One\\nadvantage of RAG is that we need not modify the architecture of LLMs, but instead augment the\\ninput to LLMs via an additional IR system. Figure 2.7 shows a comparison of the use of different\\nexternal memories in LLMs.\\n2.3.3.3\\nMemory Capacity\\nA memory model in LLMs, in the form of a simple key-value cache or a datastore, can broadly\\nbe seen as an encoder of contextual information. Ideally, before we say that a memory model\\nis representative of the entire context in token prediction, we need to make sure that the model\\ncan accurately represent any part of the context. The standard KV cache is one such model that\\ncompletely stores all past history. In this case, the model is said to have adequate capacity for\\nmemorizing the context. In many practical applications, however, complete memorization is not\\nrequired. Instead, the goal is to enable LLMs to access important contextual information. As a\\nresult, efﬁcient and compressed memory models are developed, as described in this section. Note\\nthat, the longer the sequence, the more difﬁcult it becomes for a low-capacity memory model to\\ncapture important contextual information. It is therefore common practice to simply increase the\\nmodel capacity when processing long contexts.\\nWhile high-capacity models are generally favorable, they are difﬁcult to train and deploy. A\\nchallenging scenario is that the tokens arrive in a stream and the context continuously grows.\\nDeveloping LLMs for such tasks is difﬁcult as we need to train Transformers on extremely long\\nsequences. A possible way to address this difﬁculty is to use non-parametric methods, such as\\nretrieval-based methods. For example, as discussed above, we can use a vector database to store\\npreviously generated key-value pairs, and thus represent the context by this external memory\\nmodel. Although this approach side-steps the challenge of representing long context in Trans-\\nformers, building and updating external memory models are computationally expensive. These\\nmodels are more often used in problems where the context is given in advance and ﬁxed during\\ninference, and hence unsuitable for streaming context modeling.\\nIn cases where the size of the context continuously grows, applying ﬁxed-size memory models\\nis a commonly used approach. For example, in recurrent models, a sequence of arbitrary length\\ncan be summarized into a set of hidden states by which we have a ﬁxed computational cost per\\nstep. While recurrent models were initially found to be not very good at handling long-distance\\ndependencies in sequence modeling in early applications of deep learning to NLP, recent advance-\\nments have shown that their variants are now effective in modeling extremely long sequences.\\n[Bulatov et al., 2022; Hutchins et al., 2022; Munkhdalai et al., 2024; Ma et al., 2024].\\n14For example, the template could be:\\nmessage = {*c1*} ... {*ck*}\\ninput: {*x*}\\noutput:'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 85}, page_content='2.3 Long Sequence Modeling\\n79\\n· · ·\\n· · ·\\nDatastore\\nqi\\nSearch\\nKV Cache\\nk Nearest\\nNeighbors\\nAtt(qi, Mem)\\nAtt(qi, Memknn)\\ng ⊙Att(qi, Mem) + (1 −g) ⊙Att(qi, Memknn)\\nKeys/values in LLM\\nKeys/values in Datastore\\n(a) k-NN Search Augmented Attention\\n· · ·\\n· · ·\\nDatastore\\nqi\\nSearch\\nKV Cache\\nk Nearest\\nNeighbors\\nAtt(qi, Mem)\\nAtt(qi, Mem)\\n· · ·\\nDistribution Pr(·)\\nDistribution Prknn(·)\\nOutput Distribution\\nKeys/values in LLM\\nKeys in Datastore\\nPredicted Tokens\\n(b) k-NN Language Modeling\\nInput Context:\\nx = What is deep learning?\\nDatastore\\nSearch\\n· · ·\\nc2 = Machine learning is ...\\nc1 = Deep network is ...\\nk Nearest\\nNeighbors\\nWhat is deep learning?\\nMessage: deep network ... machine learning ...\\nLLM\\n(c) Retrieval-augmented Generation\\nFig. 2.7: Illustrations of external memories (or datastores) for language modeling.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 86}, page_content='80\\nGenerative Models\\nThere is no general deﬁnition of memory capacity in LLMs. A simple approach might consider\\nhow much storage is used to retain contextual information. For example, memory capacity could\\nbe deﬁned by the size of the KV cache in Transformers or the vector database used in retrieval-\\nbased methods. A related concept is model complexity. In machine learning, there are several\\nways to deﬁne the model complexity of a model. One of the simplest methods is by counting the\\nnumber of parameters. However, it should be emphasized that the memory models discussed here\\nprimarily serve to store information, rather than add trainable parameters. Therefore, a model with\\na large memory capacity is not necessarily more complex. Nevertheless, in practice determining\\nthe capacity of a memory model is not straightforward. In general, we need to control the trade-off\\nbetween maximizing the performance and controlling the memory footprint.\\n2.3.4\\nSharing across Heads and Layers\\nIn Transformers, the KV cache is a data structure that can be dynamically adjusted along multiple\\ndimensions, such as heads, layers, and sequence length. For example, consider an LLM with L\\nlayers. Each layer has τ attention heads, and each head produces a dh-dimensional output. During\\ninference, we store the keys and values for up to m tokens. The space complexity of this caching\\nmechanism is O(L · τ · dh · m). As we have seen previously, this complexity can be reduced by\\ncaching the keys and values for fewer tokens. For example, in sliding window attention, a ﬁxed-\\nsize window is used to cache the keys and values in local context. And this model has a space\\ncomplexity of O(L · τ · dh · mw), with mw being the size of the window.\\nIn addition to reducing m, we can also decrease the size of the KV cache along other di-\\nmensions. A widely-used approach is to enable sharing across heads in multi-head self-attention.\\nRecall from Section 2.1.1 that multi-head self-attention uses multiple sets of queries, keys, and\\nvalues (each set is called a head), each performing the QKV attention mechanism as usual. This\\ncan be expressed as\\nOutput\\n=\\nMerge(head1, ..., headτ)Whead\\n(2.70)\\nwhere headj ∈Rdh is computed using the standard QKV attention function\\nheadj = Attqkv(q[j]\\ni , K[j]\\n≤i, V[j]\\n≤i)\\n(2.71)\\nHere, q[j]\\ni , K[j]\\n≤i, and V[j]\\n≤i are the query, keys, and values that are projected onto the j-th feature\\nsub-space. So this model can be interpreted as performing attention on a group of feature sub-\\nspaces in parallel (see Figure 2.8 (b)). The KV cache needs to retain the keys and values for all\\nthese heads, that is, {(K[1]\\n≤i, V[1]\\n≤i), ..., (K[τ]\\n≤i, V[τ]\\n≤i)}.\\nOne reﬁnement to the multi-head attention model, called multi-query attention (MQA), is to\\nshare keys and values across heads, while allowing queries to be unique for each head [Shazeer,\\n2019]. In MQA, there is a single set of keys and values (K≤i, V≤i). In addition, there are τ\\nqueries {q[1]\\ni , ..., q[τ]\\ni }, each corresponding to a different head. For each head, we have\\nheadj = Attqkv(q[j]\\ni , K≤i, V≤i)\\n(2.72)\\nFigure 2.8 (c) illustrates this model. By sharing keys and values, the size of the KV cache would'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 87}, page_content='2.3 Long Sequence Modeling\\n81\\nquery\\nkey\\nvalue\\n(a) Single-head Attention\\nquery\\nkey\\nvalue\\n(b) Multi-head Attention\\nquery\\nkey\\nvalue\\n(c) Multi-query Attention\\nquery\\nkey\\nvalue\\n(d) Grouped Query Attention\\nquery\\nkey\\nvalue\\nLayer l\\nLayer l −1\\nSharing\\n(e) Cross-layer Multi-head Attention\\nFig. 2.8: Illustration of QKV attention based on different multi-head and sharing mechanisms. (a) = single-head\\nattention, and (b-e) = attention with multiple heads.\\nbe O(L · dh · m).\\nGrouped query attention (GQA) is a natural extension to multi-head attention and MQA\\n[Ainslie et al., 2023]. In GQA, heads are divided into ng groups, each corresponding to a shared\\nset of keys and values. Hence we have ng sets of keys and values {(K[1]\\n≤i, V[1]\\n≤i), ..., (K[ng]\\n≤i , V[ng]\\n≤i )}.\\nSee Figure 2.8 (d) for an illustration. Let g(j) be the group id for the j-th head. The GQA model\\ncan be expressed as\\nheadj = Attqkv(q[j]\\ni , K[g(j)]\\n≤i\\n, V[g(j)]\\n≤i\\n)\\n(2.73)\\nThe size of the KV cache of GQA is O(L·ng ·dh·m). One beneﬁt of GQA is that we can trade-off\\nbetween computational efﬁciency and model expressiveness by adjusting ng. When ng = τ, the\\nmodel becomes the standard multi-head attention model. By contrast, when ng = 1, it becomes'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 88}, page_content='82\\nGenerative Models\\nthe GQA model.\\nSharing can also be performed across layers. Such a method falls into the family of shared\\nweight and shared activation methods, which have been extensively used in Transformers [Dehghani et al.,\\n2018; Lan et al., 2020]. For example, one can share KV activations or attention weights across\\nlayers to reduce both computation and memory footprints [Xiao et al., 2019; Brandon et al., 2024].\\nFigure 2.8 (e) shows an illustration of this method, where a query in a layer directly accesses the\\nKV cache of a lower-level layer.\\n2.3.5\\nPosition Extrapolation and Interpolation\\nSince Transformer layers are order-insensitive to input, we need some way to encode positional\\ninformation in the input tokens. To do this, it is common to add positional embeddings to token\\nembeddings, and then feed these combined embeddings into the Transformer layer stack as input.\\nIn this case, the embedding at position i can be expressed as\\nei\\n=\\nxi + PE(i)\\n(2.74)\\nwhere xi ∈Rd denotes the token embedding, and PE(i) ∈Rd denotes the positional embedding.\\nIn general, the token embedding xi is a position-independent vector, and so the positional embed-\\nding PE(i) is used to encode the positional context. A straightforward approach is to treat PE(i)\\nas a learnable variable and train it alongside other model parameters. In this way, we can learn\\na unique representation for each position, and thus distinguish the tokens appearing at different\\npositions of a sequence.\\nRepresentations of positions using learned vectors can work well in tasks where the sequences\\nat training and test times are of similar lengths. In practice, however, we often impose length\\nrestrictions on sequences during training to prevent excessive computational costs, but wish to\\napply the trained models to much longer sequences during inference. In this case, using learned\\npositional embeddings has obvious drawbacks, as there are no trained embeddings for positions\\nthat are not observed in the training phase.\\nAn alternative approach to modeling positional information is to develop positional embed-\\ndings that can generalize: once trained, the embedding model can be used to handle longer se-\\nquences. Suppose that we train a positional embedding model on sequences with a maximum\\nlength of ml, and we wish to apply the trained model to a sequence of length m (m >> ml). If\\nthe embedding model is limited in the range of positions that we can observe from training data,\\nthen this model will simply fail to deal with new data outside that range. See Figure 2.9 (a) for\\nan illustration where the learned embedding model cannot model data points outside the training\\ndomain if it lacks the ability to extrapolate.\\nThere are several approaches to making positional embedding models generalize. They can\\nbe grouped into two classes.\\n• Extrapolation. The model learned on observed data points (i.e., positions) can be directly\\nemployed to assign meaningful values to data points beyond the original range. For ex-\\nample, suppose we have a series of numbers 1, 2, ..., 10, and we want to understand the\\nmeaning of a new number, 15. Knowing that these numbers are natural numbers used for\\nordering, we can easily infer that 15 is a number that follows 10, even though 15 has not'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 89}, page_content='2.3 Long Sequence Modeling\\n83\\n0\\n1,024\\n2,048\\n−1\\n0\\n1\\nSequence Length\\n(a) Encoding with No Generalization\\nValue\\n0\\n1,024\\n2,048\\n−1\\n0\\n1\\nSequence Length\\n(b) Extrapolation\\nValue\\n0\\n1,024\\n2,048\\n−1\\n0\\n1\\nSequence Length\\n(c) Interpolation\\nValue\\nFig. 2.9: Illustrations of different positional embedding methods for a range of positions. Blue points represent the\\npositions that have been observed during training, and red points represent the positions that are newly observed at test\\ntime. In sub-ﬁgure (a), the encoding model only memorizes the points seen during training, and cannot generalize. In\\nsub-ﬁgures (b) and (c), the model can generalize through extrapolation and interpolation.\\nbeen observed before. Figure 2.9 (b) shows an example of this approach, where a function\\nis learned to ﬁt the data points within a speciﬁc range and then applied to estimate the values\\nof data points outside that range.\\n• Interpolation. This approach maps a larger range of data points into the original obser-\\nvation range. For example, suppose we have a model designed for numbers in the range\\n[1, 10]. When given a new range of [1, 20], we can scale this down by dividing every num-\\nber by 2, thereby ﬁtting all numbers into [1, 10]. This scaling allows us to use the model\\ntrained on the range [1, 10] to describe data points in the expanded range of [1, 20]. See\\nFigure 2.9 (c) for an illustration of this approach.\\nIn fact, positional embeddings in many systems have achieved some level of generalization.\\nFor example, sinusoidal encoding, the most common positional embedding method, employs sine\\nand cosine functions that can naturally extend to sequences of any length. Although this approach\\nmight seem direct and simple, it does not perform well when we signiﬁcantly extend the sequences\\nfor processing. In this subsection, we will discuss several alternative methods based on either\\nextrapolation or interpolation.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 90}, page_content='84\\nGenerative Models\\n2.3.5.1\\nAttention with Learnable Biases\\nOne problem with Eq. (2.74) is that the embedding model treats each token independently and\\ntherefore ignores the distance between different tokens. A common improvement to this model,\\ncalled relative positional embedding, is to consider the pairwise relationship between tokens\\n[Shaw et al., 2018]. The general idea behind this is to obtain the offset between any pair of posi-\\ntions and incorporate it into the self-attention model. One of the simplest forms of self-attention\\nwith relative positional embedding is given by\\nAttqkv(qi, K≤i, V≤i)\\n=\\ni\\nX\\nj=0\\nα(i, j)vj\\n(2.75)\\nα(i, j)\\n=\\nSoftmax(qikT\\nj + PE(i, j)\\n√\\nd\\n+ Mask(i, j))\\n(2.76)\\nThe only difference between this model and the original self-attention model is that a bias term\\nPE(i, j) is added to the query-key product in this new model. Intuitively, PE(i, j) can be inter-\\npreted as a distance penalty for the pair of positions i and j. As i moves away from j, the value of\\nPE(i, j) decreases.\\nPE(i, j) can be deﬁned in several different ways. Here, we consider the T5 version of relative\\npositional embedding, called the T5 bias [Raffel et al., 2020]. For each pair of query qi and key\\nkj, the offset between them is deﬁned to be15\\nd(i, j)\\n=\\ni −j\\n(2.77)\\nA simple design for the bias PE(i, j) is to share the same learnable variable for all query-key\\npairs with the same offset, i.e., PE(i, j) = ui−j, where ui−j is the variable corresponding to\\nthe offset i −j. However, simply assigning a unique value to each offset will restrict this model\\nto observed offsets. When i −j is larger than the maximum trained offset, the model cannot\\ngeneralize.\\nThe T5 bias instead adopts a generalization of this model. Rather than assigning each query-\\nkey offset a unique bias term, it groups difference offsets into “buckets”, each corresponding to\\none learnable parameter. More speciﬁcally, the bias terms for nb + 1 buckets are given as follows.\\n• For buckets 0 to nb+1\\n2\\n−1, each bucket corresponds to one offset, that is, bucket 0 ↔offset\\n0, bucket 1 ↔offset 1, bucket 2 ↔offset 2, and so on. We express this as b(i −j) = i −j.\\n• For buckets nb+1\\n2\\nto nb, the size of each bucket increases logarithmically. For example, the\\nbucket number for a given offset i −j ≥nb+1\\n2\\ncan be deﬁned as\\nb(i −j)\\n=\\nnb + 1\\n2\\n+ ⌊log(i −j) −log(nb+1\\n2\\n)\\nlog(distmax) −log(nb+1\\n2\\n) · nb + 1\\n2\\n⌋\\n(2.78)\\nwhere the parameter distmax is typically set to a relatively large number to indicate the\\n15For language modeling, a query is only allowed to attend to its left-context, and so we have i −j ≥0. In the more\\ngeneral case of self-attention, where a token can attend to all tokens in the sequence, we may have negative offsets\\nwhen i < j.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 91}, page_content='2.3 Long Sequence Modeling\\n85\\n0\\n1\\n2\\n3\\n· · ·\\n14\\n15\\n16\\n17\\n18\\n· · ·\\n32\\nBucket\\nOffset\\n(i −j)\\n0\\n1\\n2\\n3\\n14\\n15 16 ∼20 21 ∼26\\n27 ∼33\\n802 ∼∞\\nﬁxed bucket size\\nlogarithmically increased bucket size\\nFig. 2.10: Illustration of distributing query-key offsets into buckets in the T5 model (nb = 32 and distmax = 1024).\\nBoxes represent buckets. In the ﬁrst half of the buckets, we use a ﬁxed bucket size. In the second half of the buckets,\\nwe increase the bucket size logarithmically. The last bucket contains all the query-key offsets that are not covered by\\nprevious buckets.\\nmaximum offset we may encounter.\\n• When i −j > distmax, we place i −j in the last bucket. In other words, bucket nb contains\\nall the offsets that are not assigned to the previous buckets.\\nTogether, these can be expressed as the function\\nb(i −j)\\n=\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\ni −j\\n0 ≤i −j < nb+1\\n2\\nmin(nb, nb+1\\n2\\n+ ⌊\\nlog(i−j)−log( nb+1\\n2\\n)\\nlog(distmax)−log( nb+1\\n2\\n) · nb+1\\n2\\n⌋)\\ni −j ≥nb+1\\n2\\n(2.79)\\nFigure 2.10 shows an illustration of these buckets. We see that in the ﬁrst half of the buckets,\\neach bucket is associated with only one value of i −j, while in the second half, the bucket size\\nincreases as i −j grows. The last bucket is designed to handle sequences of arbitrarily long\\nlengths.\\nAll PE(i, j)s in a bucket share the same bias term ub(i−j). Substituting PE(i, j) = ub(i−j)\\ninto Eq. (2.76), the attention weight for qi and kj becomes16\\nα(i, j)\\n=\\nSoftmax(qikT\\nj + ub(i−j)\\n√\\nd\\n+ Mask(i, j))\\n(2.81)\\nThe parameters {u0, ..., unb} are learned as common parameters during training. It should\\nbe emphasized that this model can generalize to long sequences. This is because PE(i, j)s with\\nsimilar query-key offsets share the same parameter, and this sharing strategy is particularly im-\\nportant for achieving good generalization, given that large query-key offsets are rare in training.\\nIn practice, we often set nb to a moderate number, and thus it can help control the overﬁtting of\\npositional embedding models.\\n16Note that, in Raffel et al. [2020]’s T5 model, the rescaling operation for the query-key product is removed. The\\nattention weight α(i, j) is then given by\\nα(i, j)\\n=\\nSoftmax(qikT\\nj + ub(i−j) + Mask(i, j))\\n(2.80)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 92}, page_content='86\\nGenerative Models\\n2.3.5.2\\nAttention with Non-learned Biases\\nRelative positional embedding models are based on a set of learned biases for the query-key prod-\\nuct in self-attention. An alternative approach is to give these biases ﬁxed values via heuristics,\\nrather than training them on a particular dataset. One beneﬁt of this heuristics-based approach is\\nthat it does not rely on a training process and thus can be directly applied to any sequences once\\nthe biases are set.\\nOne example of such an approach is Press et al. [2022]’s approach, called attention with\\nlinear biases or ALiBi for short. In the ALiBi approach, the bias term is deﬁned as the negative\\nscaled query-key offset\\nPE(i, j)\\n=\\n−β · (i −j)\\n=\\nβ · (j −i)\\n(2.82)\\nwhere β is the scaling factor. Adding this term to the query-key product, we obtain a new form of\\nattention weights\\nα(i, j)\\n=\\nSoftmax(qikT\\nj + β · (j −i)\\n√\\nd\\n+ Mask(i, j))\\n(2.83)\\nThis model can be interpreted as adding a ﬁxed penalty to qikT\\nj whenever j moves one step\\naway from i. So we do not need to adapt it to a range of sequence lengths, and can employ it to\\nmodel arbitrarily long sequences. See Figure 2.11 for a comparison of the T5 bias and the ALiBi\\nbias.\\nIn general, the scalar β should be tuned on a validation dataset. However, Press et al. [2022]\\nfound that setting β to values decreasing geometrically by a factor of\\n1\\n2a for multi-head attention\\nperforms well on a variety of tasks. Speciﬁcally, for a self-attention sub-layer involving nhead\\nheads, the scalar for the k-th head is given by\\nβk\\n=\\n1\\n2\\n8\\nk\\n(2.84)\\nThe ALiBi approach provides a simple form of relative positional embeddings. There are\\nother similar methods for designing query-key biases using the offset i −j. Table 2.4 shows a\\ncomparison of such biases. As an aside it is worth noting that the form of the right-hand side\\nof Eq. (2.82) is very similar to length features used in conventional feature-based systems. For\\nexample, in statistical machine translation systems, such features are widely used to model word\\nreordering problems, resulting in models that can generalize well across different translation tasks\\n[Koehn, 2010].\\n2.3.5.3\\nRotary Positional Embedding\\nAs with sinusoidal embeddings, rotary positional embeddings are based on hard-coded values for\\nall dimensions of an embedding [Su et al., 2024]. Recall that in the sinusoidal embedding model,\\npositions are represented as combinations of sine and cosine functions with different frequencies.\\nThese embeddings are then added to token embeddings to form the inputs to the Transformer'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 93}, page_content='2.3 Long Sequence Modeling\\n87\\nq0kT\\n0\\nq1kT\\n0\\nq1kT\\n1\\nq2kT\\n0\\nq2kT\\n1\\nq2kT\\n2\\nq3kT\\n0\\nq3kT\\n1\\nq3kT\\n2\\nq3kT\\n3\\nq4kT\\n0\\nq4kT\\n1\\nq4kT\\n2\\nq4kT\\n3\\nq4kT\\n4\\nq5kT\\n0\\nq5kT\\n1\\nq5kT\\n2\\nq5kT\\n3\\nq5kT\\n4\\nq5kT\\n5\\nq6kT\\n0\\nq6kT\\n1\\nq6kT\\n2\\nq6kT\\n3\\nq6kT\\n4\\nq6kT\\n5\\nq6kT\\n6\\nqikT\\nj\\nu0\\nu1\\nu0\\nu2\\nu1\\nu0\\nu2\\nu2\\nu1\\nu0\\nu3\\nu2\\nu2\\nu1\\nu0\\nu3\\nu3\\nu2\\nu2\\nu1\\nu0\\nu3\\nu3\\nu3\\nu2\\nu2\\nu1\\nu0\\nBias (ub(i−j))\\n+\\n(a) The T5 bias (nb = 3 and distmax = 5)\\nq0kT\\n0\\nq1kT\\n0\\nq1kT\\n1\\nq2kT\\n0\\nq2kT\\n1\\nq2kT\\n2\\nq3kT\\n0\\nq3kT\\n1\\nq3kT\\n2\\nq3kT\\n3\\nq4kT\\n0\\nq4kT\\n1\\nq4kT\\n2\\nq4kT\\n3\\nq4kT\\n4\\nq5kT\\n0\\nq5kT\\n1\\nq5kT\\n2\\nq5kT\\n3\\nq5kT\\n4\\nq5kT\\n5\\nq6kT\\n0\\nq6kT\\n1\\nq6kT\\n2\\nq6kT\\n3\\nq6kT\\n4\\nq6kT\\n5\\nq6kT\\n6\\nqikT\\nj\\n0\\n−1β\\n0\\n−2β −1β\\n0\\n−3β −2β −1β\\n0\\n−4β −3β −2β −1β\\n0\\n−5β −4β −3β −2β\\n−β\\n0\\n−6β −5β −4β −3β −2β\\n−β\\n0\\nBias (−β(i −j))\\n+\\n(b) The ALiBi bias\\nFig. 2.11: Query-key products with biases (above = the T5 bias and below = the ALiBi bias). The color scale of the\\nbiases ranges from light blue denoting small absolute values to deep blue denoting large absolute values.\\nlayer stack. Rotary positional embeddings instead model positional context as rotations to token\\nembeddings in a complex space. This leads to a model expressed in the form of multiplicative\\nembeddings\\nei\\n=\\nxiR(i)\\n(2.85)\\nwhere R(i) ∈Rd×d is the rotation matrix representing the rotations performed on the token\\nembedding xi ∈Rd.\\nFor simplicity, we will ﬁrst consider embeddings with only two dimensions and return to a\\ndiscussion of the more general formulation later. Suppose we have a 2-dimensional token embed-\\nding x =\\nh\\nx1\\nx2\\ni\\n. We can represent it as a vector in a plane, originating at the origin (0, 0)\\nand terminating at (x1, x2). A counterclockwise rotation of this vector refers to an operation of'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 94}, page_content='88\\nGenerative Models\\nEntry\\nQuery-Key Bias (PE(i, j))\\nT5 [Raffel et al., 2020]\\nub(i−j)\\nALiBi [Press et al., 2022]\\n−β · ( i −j )\\nKerple [Chi et al., 2022]\\n−β1( i −j )β2\\n(power)\\n−β1 log(1 + β2( i −j ))\\n(logarithmic)\\nSandwich [Chi et al., 2023]\\nP ¯d/2\\nk=1 cos\\n\\x00( i −j )/100002k/ ¯d\\x01\\nFIRE [Li et al., 2024]\\nf\\n\\x00ψ( i −j )/ψ(max(mlen, i))\\n\\x01\\nTable 2.4: Query-key biases as relative positional embeddings. β, β1, β2, ¯d, and mlen are hyper-parameters. In the T5\\nmodel, b(i −j) denotes the bucket assigned to i −j. In the FIRE model, ψ(·) is a monotonically increasing function\\nsuch as ψ(x) = log(cx + 1), and f(·) is an FFN.\\nmoving the vector around the origin while maintaining its magnitude, as shown in Figure 2.12 (a).\\nThe degree of rotation is usually deﬁned by a speciﬁc angle, denoted by θ. The rotation can be\\nexpressed mathematically in the form\\nRo(x, θ)\\n=\\nxRθ\\n=\\nh\\nx1\\nx2\\ni \"\\ncos θ\\nsin θ\\n−sin θ\\ncos θ\\n#\\n=\\nh\\ncos θ · x1 −sin θ · x2\\nsin θ · x1 + cos θ · x2\\ni\\n(2.86)\\nwhere Rθ =\\n\"\\ncos θ\\nsin θ\\n−sin θ\\ncos θ\\n#\\nis the rotation matrix. If two or more rotations are performed on the\\nsame vector, we can rotate the vector further. This follows from the fact that the composition of\\nsuccessive rotations is itself a rotation. More formally, rotating a vector by an angle θ for t times\\ncan be expressed as\\nRo(x, tθ)\\n=\\nxRtθ\\n=\\nh\\ncos tθ · x1 −sin tθ · x2\\nsin tθ · x1 + cos tθ · x2\\ni\\n(2.87)\\nIf we interpret t as the position of a token represented by x in a sequence, then we will ﬁnd\\nthat the above equation deﬁnes a simple positional embedding model. As shown in Figure 2.12\\n(b), we start moving the token from position 0. Each time we move one step forward, the vector\\nis rotated by the angle θ. Upon arriving at the position t, the representation of the token with\\npositional context is given by Ro(x, iθ). As the rotations do not change the magnitude of the\\nembedding, the original “meaning” of the token is retained. The positional information is injected\\ninto the embedding, when it gets rotated.\\nA popular way to understand vector rotation is to deﬁne it in complex spaces. It is easy\\nto transform each vector x =\\nh\\nx1\\nx2\\ni\\nin the 2D Euclidean space R2 to a complex number\\nx′ = x1 + ix2 in the complex space C via a bijective linear map. Then, the rotation of x with the\\nangle tθ corresponds to the multiplication by eitθ. Given that eitθ = cos tθ + i sin tθ, the rotation'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 95}, page_content='2.3 Long Sequence Modeling\\n89\\nx1\\nx2\\nθ\\nvector x\\nxRθ\\nrotated vector\\n(a) Single-step Rotation\\nx1\\nx2\\nθ\\nθ\\nθ\\nx\\nxRθ\\nxR2θ\\nxR3θ\\n(b) Multi-step Rotation\\nx1\\nx2\\n7θ\\n7θ\\nsleeping4\\nsleeping11\\ncat2\\ncat9\\nThe1 cat2 is3 sleeping4 peacefully5\\nin6 the7 warm8 sunlight9 .10\\nEvery1 afternoon2 ,3 you4 ’ll5 ﬁnd6 that7\\nthe8 cat9 is10 sleeping11 on12 my13 bed14 .15\\n(c) Angles between embeddings of two tokens at different positions\\nFig. 2.12: Illustrations of vector rotations in a plane. Sub-ﬁgures (a) and (b) show rotations of a vector in a single\\nstep and multiple steps, respectively. Sub-ﬁgure (c) shows the embeddings of tokens cat and sleeping in two different\\nsentences. We show these sentences with a subscript afﬁxed to each token to indicate its position. If we represent\\ntokens as vectors, we can add positional information by rotating these vectors. This rotation preserves the “distances”\\nbetween the vectors. For example, given that the distance between cat and sleeping is the same in both sentences, the\\nangle between their embeddings also remains the same during rotation.\\noperation can be re-expressed in the form\\nxRtθ\\n7→\\nx′eitθ\\n=\\n(x1 + ix2)(cos tθ + i sin tθ)\\n=\\ncos tθ · x1 −sin tθ · x2 + i(sin tθ · x1 + cos tθ · x2)\\n(2.88)\\nHere we denote the token representation x′eitθ by C(x, tθ). The inner product of the representa-\\ntions of the tokens at positions t and s can be written as\\n⟨C(x, tθ), C(y, sθ)⟩\\n=\\n(x′y′)ei(t−s)θ\\n(2.89)\\nwhere y′ is the complex conjugate of y′. As can be seen, the result of this inner product involves\\na term t −s, and so it can model the offset between the two tokens.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 96}, page_content='90\\nGenerative Models\\nNow we go back to representations in the 2D Euclidean space. The dot-product of Ro(x, tθ)\\nand Ro(y, sθ) is can be written as a function of (t −s)θ\\nRo(x, tθ)[Ro(y, sθ)]T\\n=\\nxRtθ[yRsθ]T\\n=\\nxRtθ[Rsθ]TyT\\n=\\nxR(t−s)θyT\\n(2.90)\\nGiven this result, if we consider Ro(x, tθ) and Ro(y, sθ) as the query and the key, then the self-\\nattention operation will implicitly involve the modeling of relative positional context.\\nThis rotary positional embedding can be extended to multi-dimensional embeddings. For\\na d-dimensional token embedding x =\\nh\\nx1\\nx2\\n...\\nxd\\ni\\n, we can treat it as a d\\n2-dimensional\\ncomplex vector x′ =\\nh\\nx′\\n1\\nx′\\n2\\n...\\nx′\\nd/2\\ni\\n=\\nh\\nx1 + ix2\\nx3 + ix4\\n...\\nxd−1 + ixd\\ni\\n, where\\neach consecutive pair of items forms a complex number. Then, the rotary positional embedding in\\nthe complex space is given by\\nC(x, tθ)\\n=\\nd/2\\nX\\nk=1\\nx′\\nkeitθk⃗ek\\n(2.91)\\nwhere ⃗ek is the standard basis vector with a single non-zero value in the k-th coordinate and 0’s\\nelsewhere [Biderman et al., 2021].\\nAlthough this formula involves a complicated expression, its equivalent form in the d-dimensional\\nEuclidean space is relatively easy to understand. We can write it as\\nRo(x, tθ)\\n=\\nh\\nx1\\nx2\\n...\\nxd\\ni\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nRtθ1\\nRtθ2\\n...\\nRtθd/2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n(2.92)\\nwhere Rtθk =\\n\"\\ncos tθk\\nsin tθk\\n−sin tθk\\ncos tθk\\n#\\n. θ =\\nh\\nθ1, ..., θd/2\\ni\\nare the parameters for controlling the an-\\ngles of rotations in different dimensions. Typically, θk is set to 10000−2(k−1)\\nd\\n, which is analogous\\nto the setting in sinusoidal embeddings.\\nIn a practical implementation, Eq. (2.92) can be rewritten into a form that relies solely on the\\nelement-wise product and addition of vectors.\\nRo(x, tθ)\\n=\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nx1\\nx2\\n...\\nxd−1\\nxd\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nT\\n⊙\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\ncos tθ1\\ncos tθ1\\n...\\ncos tθd/2\\ncos tθd/2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nT\\n+\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n−x2\\nx1\\n...\\n−xd\\nxd−1\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nT\\n⊙\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\nsin tθ1\\nsin tθ1\\n...\\nsin tθd/2\\nsin tθd/2\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nT\\n(2.93)\\nFinally, we rewrite Eq. (2.85) to obtain the form of the embedding at position i'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 97}, page_content='2.3 Long Sequence Modeling\\n91\\nei\\n=\\nRo(xi, iθ)\\n(2.94)\\n2.3.5.4\\nPosition Interpolation\\nIn position interpolation, our goal is to map the positions in the new sequence to match the ob-\\nserved range in training. Suppose the sequence length for training ranges from 0 to ml. When\\nm > ml at test time, we represent the positions in [0, m] such that our representations ﬁt [0, ml].\\nTo illustrate, consider the rotary positional embedding model described above. The embedding\\nof each token is described by a model Ro(xi, iθ) in which θ =\\nh\\nθ1, ..., θd/2\\ni\\nare the parameters.\\nRo(xi, iθ) can be cast in the form of a linear combination of two periodic functions (see Eq.\\n(2.93))\\ncos iθ\\n=\\nh\\ncos iθ1\\n...\\ncos iθd/2\\ni\\n(2.95)\\nsin iθ\\n=\\nh\\nsin iθ1\\n...\\nsin iθd/2\\ni\\n(2.96)\\nθk is a exponential function of k and takes the form\\nθk\\n=\\nb−2(k−1)\\nd\\n(2.97)\\nwhere b is the base. The period of cos iθk and sin iθk is\\nTk\\n=\\n2π · b\\n2(k−1)\\nd\\n(2.98)\\nThe key idea behind position interpolation is to adjust this period so that the new positions can\\nbe encoded within the range [0, ml]. One way to achieve this is to scale up Tk by m\\nml , given by\\nT ′\\nk\\n=\\nm\\nml\\n· 2π · b\\n2(k−1)\\nd\\n(2.99)\\nHence all points in [0, m] are compressed into [0, ml]. This linear scaling can be easily realized\\nby modifying the input to the embedding model [Chen et al., 2023c]. The new model with linear\\npositional interpolation is given by\\nRo′(xi, iθ)\\n=\\nRo(xi, ml\\nm iθ)\\n(2.100)\\nAnother method of positional interpolation is to scale the base17. Suppose that the base b is\\nscaled by λ. We wish the period of this new model in the last dimension of θ (i.e., dimension d\\n2)\\nto be equal to that of the linear positional interpolation model. This can be expressed as\\n2π · (λb)\\n2( d\\n2 −1)\\nd\\n=\\nm\\nml\\n· 2π · b\\n2( d\\n2 −1)\\nd\\n(2.101)\\n17This\\nmethod\\nwas\\nﬁrst\\nproposed\\nin\\nhttps://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/\\nntkaware_scaled_rope_allows_llama_models_to_have/'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 98}, page_content='92\\nGenerative Models\\nSolving this equation, we obtain\\nλ\\n=\\n\\x00 m\\nml\\n\\x01\\nd\\n2( d\\n2 −1)\\n=\\n\\x00 m\\nml\\n\\x01\\nd\\nd−2\\n(2.102)\\nThis gives an embedding model\\nRo′(xi, iθ)\\n=\\nRo(xi, iθ′)\\n(2.103)\\nwhere\\nθ′ =\\nh\\n(λb)−0\\nd , (λb)−2\\nd , ..., (λb)−d−2\\nd\\ni\\n(2.104)\\nNote that scaling the base provides a non-uniform method for scaling the periods across dif-\\nferent dimensions of θ. This method has been found to be helpful for extending LLMs to longer\\nsequences, and several improvements have been developed [Peng et al., 2024; Ding et al., 2024].\\n2.3.6\\nRemarks\\nIn this section, we have presented a variety of methods for long-context language modeling. We\\nclose this section by discussing some interesting issues related to these methods.\\n2.3.6.1\\nNeed for Long Context\\nOne of the ultimate goals of long-context LLMs is that these models can precisely encode inﬁnite\\ncontext. The so-called inﬁnite context refers more to the fact that an LLM can continuously read\\nwords. This motivates LLMs that can handle extremely long context or stream data. As discussed\\nin Section 2.3.3, it is common to use ﬁxed-size memory models to process continuously expanding\\ncontext. Many such systems are based on recurrent architectures or their variants, because they\\nare inherently suited to model time series problems where the effects of past inputs continue\\nindeﬁnitely. Another way to achieve inﬁnite memory is to develop alternatives to self-attention\\nmodels, for example, one can use continuous-space attention models to encode context, which\\nremoves the dependency on context length [Martins et al., 2022].\\nWhen studying long-context LLMs, it is natural to wonder what mechanisms may explain the\\nuse of long context in language modeling. Can we compress the representation of inﬁnite context\\ninto a relatively small-sized model? Are all context tokens useful for predicting next tokens? How\\ndo LLMs prepare for token prediction when they see the context? Can we know in advance which\\ncontextual information will be critical for prediction? General answers to all these questions\\nare not obvious, but they inspire follow-on research of explainable models, and some interesting\\nresults have been found. For example, Deletang et al. [2024] conducted extensive experiments\\nto show that LLMs are powerful in-context compressors. Although viewing predictive models\\nas compression models has long been studied in machine learning, it also provides insights into\\nour understanding of the LLM scaling laws. Pal et al. [2023] and Wu et al. [2024] investigated\\nwhether the features learned up to the current step, though not intentionally, are already sufﬁcient'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 99}, page_content='2.3 Long Sequence Modeling\\n93\\nfor predicting tokens at the following steps. Note that the need for long-context in language\\nmodeling is highly dependent on the problem that we address. A related issue is where to apply\\nLLMs and how to evaluate them. For example, in summarization tasks we may only need to distill\\nand focus on a few key aspects of the text, while in retrieval-like tasks we need to “memorize”\\nthe entire context so that the relevant information can be accessed. We will discuss the evaluation\\nissue later in this subsection.\\n2.3.6.2\\nPre-training or Adapting LLMs?\\nTraining LLMs requires signiﬁcant computational costs. Although it is straightforward to train\\nLLMs on long sequence data, the training becomes computationally unwieldy for large data sets. It\\nis common practice to pre-train LLMs on general datasets, and then adapt them with modest ﬁne-\\ntuning effort. For example, LLMs with relative or rotary positional embeddings can be directly\\ntrained on large-scale data in the pre-training phase. While the resulting models may exhibit some\\nabilities to extrapolate lengths in the inference phase, it may be more effective to ﬁne-tune them\\non longer sequences.\\nIdeally, we would like to pre-train LLMs with standard Transformer architectures and adapt\\nthem to new tasks. This allows us to use many off-the-shelf LLMs and efﬁciently adapt them to\\nhandle long sequences. However, when new architectures are adopted, it seems inevitable that\\nwe need to train these models from scratch. This poses practical difﬁculties for developing long-\\ncontext LLMs, as we cannot leverage well-developed, pre-trained models and must instead train\\nthem ourselves. On the other hand, ﬁne-tuning is still an effective way to adapt LLMs with certain\\narchitectures that are different from those in pre-training. An example is models augmented with\\nexternal memories. In these models, the pre-trained LLMs are ﬁxed, and the focus is on how\\nto make these LLMs collaborate with the memory models. In RAG, for instance, it is common\\nto ﬁne-tune LLMs to improve their use of retrieval-augmented inputs. Another example of ﬁne-\\ntuning LLMs for long-context modeling is that we train an LLM with full attention models, and\\nthen replace them with sparse attention models in the ﬁne-tuning phase. The pre-trained LLM\\nprovides initial values of model parameters used in a different model, and this model is then ﬁne-\\ntuned as usual.\\n2.3.6.3\\nEvaluating Long-context LLMs\\nEvaluating long-context LLMs is important, but it is a new issue in NLP. The general idea is that,\\nif we input a long context to an LLM, then we can check from the output of the LLM whether it\\nunderstands the entire context and makes use of it in predicting following tokens. In conventional\\nresearch of NLP, such evaluations are often aimed at examining the ability of NLP models in\\nhandling long-range dependencies. However, the size of contexts used in recent LLMs is much\\nlarger than that used in NLP systems a few years ago. This motivates researchers to develop new\\nevaluation benchmarks and metrics for long-context LLMs.\\nOne approach is to use the perplexity metric. However, in spite of its apparent simplicity, this\\nmethod tends to reﬂect more on the LLMs’ ability to make use of local context rather than global\\ncontext. It is therefore tempting to develop evaluation methods that are speciﬁc to long-context\\nLLMs. Popular methods include various synthetic tasks where artiﬁcially generated or modiﬁed'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 100}, page_content='94\\nGenerative Models\\ndata is used to evaluate speciﬁc capabilities of long-context LLMs. In needle-in-a-haystack18 and\\npasskey retrieval tasks [Mohtashami and Jaggi, 2024; Chen et al., 2023c], for instance, LLMs are\\nrequired to identify and extract a small, relevant piece of information from a large volume of given\\ntext. The assumption here is that an LLM with sufﬁcient memory should remember earlier parts\\nof the text as it processes new information. This LLM can thus pick out the relevant details, which\\nmight be sparse and hidden among much irrelevant information, from the text. Alternatively,\\nin copy memory tasks (or copy tasks for short), LLMs are used to repeat the input text or a\\nspeciﬁc segment multiple times. These tasks were initially proposed to test the extent to which\\nrecurrent models can retain and recall previously seen tokens [Hochreiter and Schmidhuber, 1997;\\nArjovsky et al., 2016], and have been adopted in evaluating recent LLMs [Bulatov et al., 2022;\\nGu and Dao, 2023].\\nAnother approach to evaluating long-context LLMs is to test them on NLP tasks that involve\\nvery long input sequences. Examples include long-document or multi-document summarization,\\nlong-document question answering, code completion, and so on. A beneﬁt of this approach is that\\nit can align evaluations with user expectations.\\nAlthough many methods have been developed, there is still no general way to evaluate long-\\ncontext LLMs [Liu et al., 2024c]. One problem is that most of these methods focus on speciﬁc\\naspects of LLMs, rather than their fundamental ability to model very long contexts. Even though\\nan LLM can pick out the appropriate piece of text from the input, we cannot say that it truly un-\\nderstands the entire context. Instead, it might just remember some important parts of the context,\\nor even simply recall the answer via the model learned in pre-training. Moreover, the data used\\nin many tasks is small-scale and relatively preliminary, leading to discrepancies between evalu-\\nation results and actual application performance. A more interesting issue is that the results of\\nLLMs are inﬂuenced by many other factors and experimental setups, for example, using different\\nprompts can lead to very different outcomes. This makes evaluation even more challenging be-\\ncause improvements may not solely result from better modeling of long contexts, and there is a\\nrisk of overclaiming our results. Nevertheless, many open questions remain in the development\\nand evaluation of long-context LLMs. For example, these models still suffer from limitations\\nsuch as restricted context length and high latency. Studying these issues is likely to prove valuable\\nfuture directions.\\n2.4\\nSummary\\nIn this chapter, we have discussed the concept of LLMs and related techniques. This can be consid-\\nered a general, though not comprehensive, introduction to LLMs, laying the foundation for further\\ndiscussions on more advanced topics in subsequent chapters. Furthermore, we have explored two\\nways to scale up LLMs. The ﬁrst focuses on the large-scale pre-training of LLMs, which is cru-\\ncial for developing state-of-the-art models. The second focuses on methods for adapting LLMs to\\nlong inputs, including optimizing attention models, designing more efﬁcient and compressed KV\\ncaches, incorporating memory models, and exploring better positional embeddings.\\nThe strength of LLMs lies in their ability to break the constraints of training NLP models for\\na limited number of speciﬁc tasks. Instead, LLMs learn from large amounts of text through the\\nsimple task of token prediction — we predict the next token in a sentence given its prior tokens.\\n18https://github.com/gkamradt/LLMTest_NeedleInAHaystack'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 101}, page_content='2.4 Summary\\n95\\nA general view is that, by repeating this token prediction task a large number of times, LLMs can\\nacquire some knowledge of the world and language, which can then be applied to new tasks. As a\\nresult, LLMs can be prompted to perform any task by framing it as a task of predicting subsequent\\ntokens given prompts. This emergent ability in language models comes from several dimensions,\\nsuch as scaling up training, model size, and context size. It is undeniable that scaling laws are\\ncurrently the fundamental principle adopted in developing large language models, although sim-\\nply increasing model size has yet to prove sufﬁcient for achieving AGI. These continuously scaled\\nLLMs have been found to show capabilities in general-purpose language understanding, genera-\\ntion, and reasoning. More recently, it has been found that scaling up the compute at inference time\\ncan also lead to signiﬁcant improvements in complex reasoning tasks [OpenAI, 2024].\\nGiven their amazing power, LLMs have attracted considerable interest, both in terms of tech-\\nniques and applications. As a result, the explosion of research interest in LLMs has also led to a\\nvast number of new techniques and models. However, we do not attempt to provide a comprehen-\\nsive literature review on all aspects of LLMs, given the rapid evolution of the ﬁeld. Nevertheless,\\none can still gain knowledge about LLMs from general reviews [Zhao et al., 2023; Minaee et al.,\\n2024] or more focused discussions on speciﬁc topics [Ruan et al., 2024].'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 102}, page_content='CHAPTER 3\\nPrompting\\nIn the context of LLMs, prompting refers to the method of providing an LLM with a speciﬁc input\\nor cue to generate a desired output or perform a task. For example, if we want the LLM to translate\\na sentence from English to Chinese, we can prompt it like this\\nTranslate the text from English to Chinese.\\nText: The early bird catches the worm.\\nTranslation:\\nPrompting is crucial for LLMs because it directly inﬂuences how effectively these models under-\\nstand and respond to user queries. A well-crafted prompt can guide an LLM to generate more\\naccurate, relevant, and contextually appropriate responses. Furthermore, this process can be iter-\\natively reﬁned. By analyzing the responses of the LLM, users can adjust their prompts to align\\nmore closely with their speciﬁc needs. Given the importance of prompting in applying LLMs,\\nprompt design has become an essential skill for users and developers working with LLMs. This\\nleads to an active research area, called prompt engineering, in which we design effective prompts\\nto make better use of LLMs and enhance their practical utility in real-world applications.\\nAn important concept related to prompting is in-context learning. When prompting an LLM,\\nwe can add new information to the context, such as demonstrations of problem-solving. This\\nallows the LLM to learn from this context how to solve the problem. Here is an example of\\nprompting LLMs with a few demonstrations of how to classify text based on sentiment polarity.\\nHere are some examples of text classiﬁcation.\\nExample 1: We had a delightful dinner together. →Label: Positive\\nExample 2: I’m frustrated with the delays. →Label: Negative\\nWhat is the label for “That comment was quite hurtful.”?\\nLabel:\\nIn-context learning is often seen as an emergent ability of LLMs that arises after pre-training.\\nThough LLMs can be trained or tuned to perform new tasks, in-context learning provides a very\\nefﬁcient way to adapt these models without any training or tuning effort. Perhaps this is one of\\nthe most notable features of LLMs: they indeed learn general knowledge about the world and\\nlanguage during pre-training, which we can easily apply to new challenges. Moreover, in-context\\nlearning reﬂects the broader trend of making AI systems more generalizable and user-friendly.\\nInstead of requiring specialized engineers to ﬁne-tune models for every unique task, users can\\ninteract with LLMs in a more intuitive way, simply providing examples or adjusting the context\\nas needed.\\nIn this chapter, we focus on prompting techniques in LLMs. We begin by considering several\\ninteresting prompt designs commonly used in prompt engineering. Then, we discuss a series of\\n96'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 103}, page_content='3.1 General Prompt Design\\n97\\nreﬁnements to these methods. Finally, we explore approaches for automating prompt design.\\n3.1\\nGeneral Prompt Design\\nThis section presents basic concepts in prompt design, along with examples of how to prompt\\nLLMs for various NLP tasks. Since the effectiveness of prompting is highly dependent on the\\nLLMs being used, prompts often vary across different LLMs, making it difﬁcult to provide a\\ncomprehensive list of prompts for all LLMs and downstream tasks. Therefore, this discussion is\\nnot focused on any speciﬁc LLM. Instead, the goal is to provide guiding principles for prompt\\ndesign.\\n3.1.1\\nBasics\\nThe term prompt is used in many different ways. In this chapter we deﬁne a prompt as the input\\ntext to an LLM, denoted by x. The LLM generates a text y by maximizing the probability Pr(y|x).\\nIn this generation process, the prompt acts as the condition on which we make predictions, and it\\ncan contain any information that helps describe and solve the problem.\\nA prompt can be obtained using a prompt template (or template for short) [Liu et al., 2023a].\\nA template is a piece of text containing placeholders or variables, where each placeholder can\\nbe ﬁlled with speciﬁc information. Here are two templates for asking the LLM for weekend\\nsuggestions.\\nPlease give me some suggestions for a fun weekend.\\nIf {∗premise∗}, what are your suggestions for a fun weekend.\\nIn the ﬁrst template, we simply instruct the LLM to return some suggestions. So the template\\nis just a piece of text with no variables. In the second template, the variable {∗premise∗} needs to\\nbe speciﬁed by the users to provide a premise for making suggestions. For example, if we input\\npremise\\n=\\nthe weather is nice this weekend\\nthen we can generate a prompt\\nIf the weather is nice this weekend,\\nwhat are your suggestions for a fun weekend.\\nWe can also design a template with multiple variables. Here is an example in which we\\ncompare the two sentences in terms of their semantic similarity.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 104}, page_content='98\\nPrompting\\nHere is a sentence\\n{∗sentence1∗}\\nHere is another sentence\\n{∗sentence2∗}\\nCompute the semantic similarity between the two sentences\\nA popular way to format prompts is to write each input or output in a “name:content” style.\\nFor example, we can describe a conversation between two people, named John and David, and use\\nthe LLM to continue the conversation. A template of such prompts is given by\\nJohn: {∗utterance1∗}\\nDavid: {∗utterance2∗}\\nJohn: {∗utterance3∗}\\nDavid: {∗utterance4∗}\\nJohn: {∗utterance5∗}\\nDavid: {∗utterance6∗}\\nJohn: {∗utterance7∗}\\nDavid:\\nThe “name:content” format can be used to deﬁne the task that we want the LLM to perform.\\nFor example, given that “Q” and “A” are commonly used abbreviations for “Question” and “An-\\nswer”, respectively, we can use the following template to do question-answering.\\nQ: {∗question∗}\\nA:\\nThis format can be used to describe more complex tasks. For example, the following is an\\nexample of providing a speciﬁcation for a translation task\\nTask: Translation\\nSource language: English\\nTarget language: Chinese\\nStyle: Formal text\\nTemplate: Translate the following sentence: {∗sentence∗}\\nIn practical systems, it is common to represent and store such data in key-value pairs, such as the\\nJSON format1.\\nWhen the problem is difﬁcult to describe in an attribute-based manner, it is more common\\nto instruct LLMs with a clear and detailed description. There are many ways to do this. One\\n1The JSON representation is'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 105}, page_content='3.1 General Prompt Design\\n99\\nexample is to assign a role to LLMs and provide sufﬁcient context. The following is a template\\nthat instructs an LLM to act as an expert and answer questions from children.\\nYou are a computer scientist with extensive knowledge in the ﬁeld\\nof deep learning.\\nPlease explain the following computer-related concept to a child\\naround 10 years old, using simple examples whenever possible.\\n{∗concept∗}\\nHere the text “You are a computer scientist ... deep learning. ” is sometimes called system\\ninformation, and is provided to help the LLM understand the context or constraints of the task it\\nis being asked to perform.\\n3.1.2\\nIn-context Learning\\nLearning can occur during inference. In-context learning is one such method, where prompts\\ninvolve demonstrations of problem-solving, and LLMs can learn from these demonstrations how\\nto solve new problems. Since we do not update model parameters in this process, in-context\\nlearning can be viewed as a way to efﬁciently activate and reorganize the knowledge learned in\\npre-training without additional training or ﬁne-tuning. This enables quick adaptation of LLMs to\\nnew problems, pushing the boundaries of what pre-trained LLMs can achieve without task-speciﬁc\\nadjustments.\\nIn-context learning can be illustrated by comparing three methods: zero-shot learning, one-\\nshot learning and few-shot learning. Zero-shot learning, as its name implies, does not involve a\\ntraditional “learning” process. It instead directly applies LLMs to address new problems that were\\nnot observed during training. In practice, we can repetitively adjust prompts to guide the LLMs in\\ngenerating better responses, without demonstrating problem-solving steps or providing examples.\\nConsider the following example. Suppose we want to use an LLM as an assistant that can help\\ncorrect English sentences. A zero-shot learning prompt is given by\\n{\\n\"Task\": \"Translation\"\\n\"Source language\": \"English\"\\n\"Target language\": \"Chinese\"\\n\"Style\": \"Formal text\"\\n\"Template\": \"Translate the following sentence: {∗sentence∗}\"\\n}'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 106}, page_content='100\\nPrompting\\nSYSTEM\\nYou are a helpful assistant, and are great at grammar correction.\\nUSER\\nYou will be provided with a sentence in English. The task is\\nto output the correct sentence.\\nInput: She don’t like going to the park.\\nOutput:\\nHere the gray words are used to indicate different ﬁelds of the prompt.\\nIn one-shot learning, we extend this prompt by adding a demonstration of how to correct\\nsentences, thereby allowing the LLM to learn from this newly-added experience.\\nSYSTEM\\nYou are a helpful assistant, and are great at grammar correction.\\nDEMO\\nYou will be provided with a sentence in English. The task is\\nto output the correct sentence.\\nInput: There is many reasons to celebrate.\\nOutput: There are many reasons to celebrate.\\nUSER\\nYou will be provided with a sentence in English. The task is\\nto output the correct sentence.\\nInput: She don’t like going to the park.\\nOutput:\\nFurthermore, we can add more demonstrations to enable few-shot learning.\\nSYSTEM\\nYou are a helpful assistant, and are great at grammar correction.\\nDEMO1\\nYou will be provided with a sentence in English. The task is\\nto output the correct sentence.\\nInput: There is many reasons to celebrate.\\nOutput: There are many reasons to celebrate.\\nDEMO2\\nYou will be provided with a sentence in English. The task is\\nto output the correct sentence.\\nInput: Me and my friend goes to the gym every day.\\nOutput: My friend and I go to the gym every day.\\nUSER\\nYou will be provided with a sentence in English. The task is\\nto output the correct sentence.\\nInput: She don’t like going to the park.\\nOutput:\\nIn few-shot learning, we essentially provide a pattern that maps some inputs to the corre-\\nsponding outputs. The LLM attempts to follow this pattern in making predictions, provided that\\nthe prompt includes a sufﬁcient number of demonstrations, although generally small. It is also'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 107}, page_content='3.1 General Prompt Design\\n101\\npossible to use simpler patterns to achieve this. For example, one can use the following few-shot\\nlearning prompt for translating words from Chinese to English.\\nDEMO\\n现在\\n→\\nnow\\n来\\n→\\ncome\\n去\\n→\\ngo\\n男孩\\n→\\nboy\\nUSER\\n女孩\\n→\\nIf the LLM is powerful enough, few-shot learning can enable it to address complex prob-\\nlems, such as mathematical reasoning. For example, consider the following task of summing two\\nnumbers and then dividing the sum by their product.\\nDEMO\\n12 5\\n→\\n(12 + 5)/(12 × 5) = 0.283\\n3 1\\n→\\n(3 + 1)/(3 × 1) = 1.33\\n−9 4\\n→\\n(−9 + 4)/(−9 × 4) = 0.138\\n15 15\\n→\\n(15 + 15)/(15 × 15) = 0.133\\nUSER\\n19 73\\n→\\nIn many practical applications, the effectiveness of in-context learning relies heavily on the\\nquality of prompts and the fundamental abilities of pre-trained LLMs. On one hand, we need a\\nsigniﬁcant prompt engineering effort to develop appropriate prompts that help LLMs learn more\\neffectively from demonstrations. On the other hand, stronger LLMs can make better use of in-\\ncontext learning for performing new tasks. For example, suppose we wish to use an LLM to\\ntranslate words from Inuktitut to English. If the LLM lacks pre-training on Inuktitut data, its\\nunderstanding of Inuktitut will be weak, and it will be difﬁcult for the model to perform well in\\ntranslation regardless of how we prompt it. In this case, we need to continue training the LLM\\nwith more Inuktitut data, rather than trying to ﬁnd better prompts.\\nIt might be interesting to explore how in-context learning emerges during pre-training and\\nwhy it works during inference. One simple understanding is that LLMs have gained some knowl-\\nedge of problem-solving, but there are many possible predictions, which are hard to distinguish\\nwhen the models confront new problems. Providing demonstrations can guide the LLMs to fol-\\nlow the “correct” paths. Furthermore, some researchers have tried to interpret in-context learn-\\ning from several different perspectives, including Bayesian inference [Xie et al., 2022], gradient\\ndecent [Dai et al., 2023; Von Oswald et al., 2023], linear regression [Akyürek et al., 2023], meta\\nlearning [Garg et al., 2022], and so on.\\n3.1.3\\nPrompt Engineering Strategies\\nDesigning prompts is highly empirical. In general, there are many ways to prompt an LLM for\\nperforming the same task, and we need to perform a number of trial-and-error runs to ﬁnd a\\nsatisfactory prompt. To write good prompts more efﬁciently, one can follow certain strategies.\\nExamples of common prompting principles include'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 108}, page_content='102\\nPrompting\\n• Describing the task as clearly as possible. When we apply an LLM to solve a problem,\\nwe need to provide a precise, speciﬁc, and clear description of the problem and instruct the\\nLLM to perform as we expect. This is particularly important when we want the output of\\nthe LLM to meet certain expectations. For example, suppose we are curious about climate\\nchange. A simple prompt for asking the LLM to provide some information is\\nTell me about climate change.\\nSince this instruction is too general, the LLM may generate a response that addresses any\\naspect of climate change, which may not align with our speciﬁc interests. In this case, we\\ncan instead use prompts that are speciﬁc and detailed. One such example is\\nProvide a detailed explanation of the causes and effects of climate change,\\nincluding the impact on global temperatures, weather patterns, and sea\\nlevels. Also, discuss possible solutions and actions being taken to mitigate\\nthese effects.\\nNow suppose we intend to explain climate change to a 10-year-old child. We can adjust the\\nabove prompt further.\\nExplain the causes and effects of climate change to a 10-year-old child.\\nTalk about how it affects the weather, sea levels, and temperatures. Also,\\nmention some things people are doing to help. Try to explain in simple\\nterms and do not exceed 500 words.\\n• Guiding LLMs to think. LLMs have exhibited surprisingly good capabilities to “think”.\\nA common example is that well-developed LLMs have achieved impressive performance\\nin mathematical reasoning tasks, which are considered challenging. In prompt engineering,\\nthe “thinking” ability of LLMs needs to be activated through appropriate prompting, espe-\\ncially for problems that require signiﬁcant reasoning efforts. In many cases, an LLM that\\nis instructed to “think” can produce completely different results compared with the same\\nLLM that is instructed to perform the task straightforwardly. For example, Kojima et al.\\n[2022] found that simply appending “Let’s think step by step” to the end of each prompt\\ncan improve the performance of LLMs on several reasoning tasks. LLMs can be prompted\\nto “think” in a number of ways. One method is to instruct LLMs to generate steps for rea-\\nsoning about the problem before reaching the ﬁnal answer. For example, consider a task of\\nsolving mathematical problems. See below for a simple prompt for this task.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 109}, page_content='3.1 General Prompt Design\\n103\\nYou are a mathematician. You will be provided with a math problem.\\nPlease solve the problem.\\nSince solving math problems requires a detailed reasoning process, LLMs would probably\\nmake mistakes if they attempted to work out the answer directly. So we can explicitly ask\\nLLMs to follow a given reasoning process before coming to a conclusion.\\nYou are a mathematician. You will follow these detailed reasoning steps\\nwhen solving math problems.\\nStep 1: Problem Interpretation.\\nThe mathematician carefully listens to your query and understands the in-\\ntricate details of the mathematical challenge you have presented.\\nStep 2: Strategy Formulation.\\nDrawing upon their extensive knowledge, the mathematician chooses the\\nmost effective strategy tailored to the type of math problem, whether it is\\nalgebra, calculus, or geometry.\\nStep 3: Detailed Calculation.\\nWith precision and expertise, the mathematician performs the necessary\\ncalculations step by step, adhering to all mathematical principles.\\nStep 4: Solution Review.\\nBefore providing the ﬁnal answer, the mathematician meticulously checks\\nthe calculations for accuracy and offers a concise explanation or rationale\\nfor the solution.\\nYou will be provided with a math problem. Please solve the problem.\\n{∗problem∗}\\nAnother method to guide LLMs to “think” is through multiple rounds of interaction with\\nLLMs. For example, as a ﬁrst step, we can instruct LLMs to solve the problem directly\\nYou will be provided with a math problem. Please solve the problem.\\n{∗problem∗}\\nNow we have an initial answer to the problem. As a second step, we prompt LLMs to\\nevaluate the correctness of the answer and, if necessary, rework it to ﬁnd a better solution.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 110}, page_content='104\\nPrompting\\nYou will be provided with a math problem, along with a solution. Evaluate\\nthe correctness of this solution, and identify any errors if present. Then,\\nwork out your own solution.\\nProblem: {∗problem∗}\\nSolution: {∗solution∗}\\nThe prompts presented here are closely related to a long line of research on reasoning prob-\\nlems in LLMs. It is impossible to provide a complete discussion of all related issues because\\nthis topic covers a large family of methods. But we will see a relatively more detailed dis-\\ncussion on how to improve prompting through more reasoning in Section 3.2.\\n• Providing reference information. As discussed in the previous section, we can include\\ndemonstrations in prompts and allow LLMs to in-context learn from these demonstrations\\nhow to perform the task. In fact, given the remarkable ability of language understanding of\\nLLMs, we can add any type of text into the prompts and so these models can predict based\\non enriched contexts. In many applications, we have various information that is relevant\\nto user queries. Instead of using LLMs to make unconstrained predictions, we often want\\nLLMs to produce outputs that are conﬁned to the relevant text. One such example is RAG,\\nwhere the relevant text for the user query is provided by calling an IR system, and we\\nprompt LLMs to generate responses based on this provided relevant text. The following\\nprompt shows an example.\\nYou are an expert that can generate answers to input queries. You have now\\nbeen provided with a query and the corresponding context information.\\nPlease generate an answer based on this context information. Note that\\nyou need to provide the answer in your own words, not just copy from the\\ncontext provided.\\nContext information: {∗IR-result∗}\\nQuery: {∗query∗}\\nIf the context information is highly reliable, we can even restrict LLMs to answering using\\nonly the provided text. An example prompt is shown as follows'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 111}, page_content='3.1 General Prompt Design\\n105\\nYou are an expert tasked with generating answers from input queries. You\\nhave been provided with a query and corresponding context information,\\norganized in a table where each row represents a useful record. Please\\ngenerate an answer using only this context information. Ensure that you\\nprovide the answer in your own words.\\nContext information: {∗table∗}\\nQuery: {∗query∗}\\nWhen dealing with real-world problems, we often have prior knowledge and additional\\ninformation about the problems that help produce better answers. Considering such infor-\\nmation in prompting is generally helpful in improving the result.\\n• Paying attention to prompt formats. In general, the performance of LLMs is highly\\nsensitive to the prompts we input. Sometimes a small modiﬁcation to a prompt can lead to a\\nbig change in model output. An interesting example is that changing the order of sentences\\nin a prompt may cause LLMs to generate different results. To make prompts easy to read\\nand reduce ambiguity, it is common to format them in a way that ensures clarity. One\\nexample is that we deﬁne several ﬁelds for prompts and ﬁll different information in each\\nﬁeld. Another example is we can use code-style prompts for LLMs which can understand\\nand generate both natural language and code. See the following for a code-style prompt that\\nperforms translation where one demonstration is presented.\\n[English] = [I have an apple.]\\n[German] = [Ich habe einen Apfel.]\\n[English] = [I have an orange.]\\n[German] =\\nLLMs can receive text in various formats. This allows us to use control characters, XML\\ntags, and speciﬁc formatting to represent complex data. And it is useful to specify how the\\ninput and output should be formatted or structured. For example, we can delimit sections of\\ntext using quotes and prompt LLMs accordingly (e.g., adding a sentence like “the input text\\nis delimited by double quotes” to the prompt).\\nAbove, we have discussed only a few strategies for writing good prompts. There are, of course,\\nmany such methods, and one needs to develop their own through practice. Interested readers can\\nrefer to various online documents for more information, such as OpenAI’s manual on the GPT\\nseries models2.\\n2See\\nhttps://platform.openai.com/docs/guides/prompt-engineering/\\nsix-strategies-for-getting-better-results.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 112}, page_content='106\\nPrompting\\n3.1.4\\nMore Examples\\nIn this subsection, we consider more examples of prompting LLMs to perform various NLP tasks.\\nThe motivation here is not to give standard prompts for these tasks, but rather to use simple\\nexamples to illustrate how LLMs can be prompted to deal with NLP problems.\\n3.1.4.1\\nText Classiﬁcation\\nText classiﬁcation is perhaps one of the most common problems in NLP. Many tasks can be\\nbroadly categorized as assigning pre-deﬁned labels to a given text. Here we consider the polarity\\nclassiﬁcation problem in sentiment analysis. We choose polarity classiﬁcation for illustration be-\\ncause it is one of the most popular and well-deﬁned text classiﬁcation tasks. In a general setup of\\npolarity classiﬁcation, we are required to categorize a given text into one of three categories: neg-\\native, positive, or neutral. Below is a simple prompt for doing this (for easy reading, we highlight\\nthe task description in the prompt).\\nAnalyze the polarity of the following text and classify it as positive, negative, or\\nneutral.\\nText:\\nThe service at the restaurant was slower than expected, which was a bit frustrat-\\ning.\\nThe polarity of the text can be classiﬁed as positive.\\nTo make the example complete, we show the response generated by the LLM (underlined text).\\nAlthough the answer is correct, the LLM gives this answer not in labels but in text describing\\nthe result. The problem is that LLMs are designed to generate text but not to assign labels to text\\nand treat classiﬁcation problems as text generation problems. As a result, we need another system\\nto map the LLM’s output to the label space (call it label mapping), that is, we extract “positive”\\nfrom “The polarity of the text can be classiﬁed as positive”. This is trivial in most cases because\\nwe can identify label words via simple heuristics. But occasionally, LLMs may not express the\\nclassiﬁcation results using these label words. In this case, the problem becomes more complicated,\\nas we need some way to map the generated text or words to predeﬁned label words.\\nOne method to induce output labels from LLMs is to reframe the problem as a cloze task. For\\nexample, the following shows a cloze-like prompt for polarity classiﬁcation.\\nAnalyze the polarity of the following text and classify it as positive, negative, or\\nneutral.\\nText:\\nThe service at the restaurant was slower than expected, which was a bit frustrat-\\ning.\\nThe polarity of the text is positive'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 113}, page_content='3.1 General Prompt Design\\n107\\nWe can use LLMs to complete the text and ﬁll the blank with the most appropriate word. Ide-\\nally, we wish the ﬁlled word would be positive, negative, or neutral. However, LLMs are not\\nguaranteed to generate these label words. One method to address this problem is to constrain the\\nprediction to the set of label words and select the one with the highest probability. Then, the output\\nlabel is given by\\nlabel\\n=\\narg max\\ny∈Y\\nPr(y|x)\\n(3.1)\\nwhere y denotes the word ﬁlled in the blank, and Y denotes the set of label words\\n{positive, negative, neutral}.\\nAnother method of using LLMs to generate labels is to constrain the output with prompts. For\\nexample, we can prompt LLMs to predict within a controlled set of words. Here is an example.\\nAnalyze the polarity of the following text and classify it as positive, negative, or\\nneutral.\\nText:\\nThe service at the restaurant was slower than expected, which was a bit frustrat-\\ning.\\nWhat is the polarity of the text?\\nJust answer: positive, negative, or neutral.\\nPositive\\nSentiment analysis is a common NLP problem that has probably been well understood by\\nLLMs through pre-training or ﬁne-tuning. Thus we can prompt LLMs using simple instructions\\nto perform the task. However, for new classiﬁcation problems, it may be necessary to provide\\nadditional details about the task, such as the classiﬁcation standards, so that the LLMs can perform\\ncorrectly. To do this, we can add a more detailed description of the task and/or demonstrate\\nclassiﬁcation examples in the prompts. To illustrate, consider the following example.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 114}, page_content='108\\nPrompting\\nAnalyze the polarity of the following text and classify it as positive, negative, or\\nneutral. Here’s what each category represents:\\nPositive: This indicates that the text conveys a positive emotion or attitude. For\\nexample, texts expressing happiness, satisfaction, excitement, or admiration are\\nconsidered positive.\\nNegative: This refers to a text that expresses a negative emotion or attitude. It\\nencompasses feelings of sadness, anger, frustration, or criticism.\\nNeutral: Neutral sentiment is used to describe texts that do not exhibit clear posi-\\ntive or negative emotions but instead convey informational, factual, or indifferent\\ntones.\\nText:\\nThe service at the restaurant was slower than expected, which was a bit frustrat-\\ning.\\nWhat is the polarity of the text?\\nPositive\\nWhile it seems straightforward to use LLMs for classiﬁcation problems, there are still issues\\nthat have not been well addressed. For example, when dealing with a large number of categories,\\nit remains challenging to effectively prompt LLMs. Note that if we face a very difﬁcult classiﬁca-\\ntion problem and have a certain amount of labeled data, ﬁne-tuning LLMs or adopting “BERT +\\nclassiﬁer”-like architectures is also desirable.\\n3.1.4.2\\nInformation Extraction\\nMany NLP problems can be regarded as information extraction problems, involving the identiﬁ-\\ncation or extraction of speciﬁc pieces of information from unstructured text. This information can\\ninclude named entities, relationships, events, and other relevant data points. The goal of infor-\\nmation extraction is to transform raw data into a format that can be easily analyzed and used in\\nvarious downstream applications.\\nAs information extraction covers a wide range of problems, we cannot discuss them all here.\\nInstead, we start with the task of named entity recognition — a task that has long been a concern\\nin NLP. Named entity recognition is a process that detects and classiﬁes key information in text\\ninto speciﬁc groups. These key pieces of information, known as named entities, typically include\\nproper names and are categorized into distinct classes such as people, locations, organizations,\\ndates, monetary values, and percentages. Consider ﬁrst a simple example of extracting person\\nnames from a text3.\\n3The text is from https://edition.cnn.com/travel'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 115}, page_content='3.1 General Prompt Design\\n109\\nIdentify all person names in the provided text.\\nText:\\nIs the UK really doing that badly or have travel trends shifted, post-pandemic?\\nFor Tom Jenkins, CEO of the European Tourism Organisation, it’s the latter. “I\\nthink the UK is doing perfectly well but we’ll see more people going to Europe,”\\nhe says of 2024, adding that there’s “allegedly a slight plateau of American de-\\nmand for the UK.”\\nThe person name identiﬁed in the provided text is: Tom Jenkins\\nThen, we can extract all named entities.\\nIdentify and classify all named entities in the provided text into categories such\\nas person names, locations, dates, and organizations. List each entity with its type\\non one line.\\nText:\\nIs the UK really doing that badly or have travel trends shifted, post-pandemic?\\nFor Tom Jenkins, CEO of the European Tourism Organisation, it’s the latter. “I\\nthink the UK is doing perfectly well but we’ll see more people going to Europe,”\\nhe says of 2024, adding that there’s “allegedly a slight plateau of American de-\\nmand for the UK.”\\n1. Tom Jenkins - Person Name\\n2. European Tourism Organisation - Organization\\n3. UK - Location\\n4. Europe - Location\\n5. 2024 - Date\\nGiven these identiﬁed named entities, we can further process the text using other information\\nextraction tasks. For example, we can identify the relationships between these named entities (call\\nit relation extraction). The corresponding prompt is shown as follows.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 116}, page_content='110\\nPrompting\\nGiven a text and a list of named entities identiﬁed within it, analyze and describe\\nthe relationships between these entities. Explain how each entity is contextually\\nrelated to others.\\nText:\\nIs the UK really doing that badly or have travel trends shifted, post-pandemic?\\nFor Tom Jenkins, CEO of the European Tourism Organisation, it’s the latter. “I\\nthink the UK is doing perfectly well but we’ll see more people going to Europe,”\\nhe says of 2024, adding that there’s “allegedly a slight plateau of American de-\\nmand for the UK.”\\nNamed Entity:\\nTom Jenkins, European Tourism Organisation, UK, Europe, 2024\\nTom Jenkins - European Tourism Organisation: Tom Jenkins is the CEO of the\\nEuropean Tourism Organisation. This indicates a professional association where\\nTom Jenkins likely has a signiﬁcant role in directing the organization’s activities,\\nespecially those related to tourism in Europe.\\n...\\nIf LLMs have been ﬁne-tuned with instruction following for information extraction tasks, it\\nis relatively easy to perform various information extraction tasks. For example, the following is a\\nprompt template for information extraction.\\nYou will be provided with a text. Your task is to {∗task-description∗}\\nText: {∗text∗}\\nHere task-description is the instruction for the task. Examples of the instruction include\\nextract the keywords from the text.\\nextract the key events from the text.\\ndetect the coreference and anaphoric links between entities from the text.\\nand so on. Of course, we can provide a more detailed description of the task to prompt LLMs\\nmore accurately.\\n3.1.4.3\\nText Generation\\nIn NLP, the term text generation is used broadly to cover a wide range of different problems, many\\nof which focus on generating text based on given text. These problems can be roughly categorized\\ninto two classes: 1) continual writing based on the input text (call it text completion), and 2)\\ntransformation of the input text into another text (call it text transformation). Here, we illustrate\\nthe prompts for the former and consider the prompts for the latter in the following discussion.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 117}, page_content='3.1 General Prompt Design\\n111\\nA simple example of text completion is continuing to write a story starting with a few given\\nsentences.\\nYou are a writer who is great at writing stories. You are provided with a few\\nsentences to start with. Please continue the story from the provided sentences.\\nStart of the story:\\nOne sunny afternoon, Lily discovered a tiny, golden key hidden under a rock in\\nher backyard. Curious, she picked it up and wondered what it might unlock. Just\\nthen, her cat, Whiskers, began to meow excitedly, leading her towards the old\\noak tree at the edge of the garden.\\nLily followed Whiskers, her heart pounding with excitement and a touch of\\nnervousness. The old oak tree had always seemed mysterious to her, its gnarled\\nbranches casting long shadows even on the brightest days. As they reached the\\ntree, Whiskers darted ahead and started pawing at something buried beneath a\\npile of autumn leaves ...\\nLikewise, we can create a conversation given the conversational context.\\nYou are provided with a conversation between a tourist and a taxi driver. Please\\ncomplete the conversation.\\nTourist: Could you take me to the downtown museum, please?\\nTaxi Driver: Of course! First time in the city?\\nTourist: Yes, any must-see places around?\\nTaxi Driver: Deﬁnitely check out the botanical gardens after the museum.\\nThey’re nearby and quite beautiful.\\nTourist: Great, thanks for the tip!\\nTaxi Driver: No problem. Enjoy your visit!\\nAnother application of text completion is generating text based on user requirements. The\\nrequirements can include style, tone, length, and any speciﬁc content that the text should contain.\\nFor example, users may need a formal report, a creative story, or a piece of programming code.\\nHere is a prompt for generating a ﬁve-character regulated Chinese poem.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 118}, page_content='112\\nPrompting\\nYou are a poet. Please write a traditional Chinese poem. Here is the requirement.\\nTheme: Spring’s rejuvenation\\nStructure: Five-character regulated poem\\nEmotion: Convey a sense of happiness and renewal through vivid spring imagery.\\n春意盎然\\n翠柳风中舞，\\n红花雨后新。\\n山明水又绿，\\n天宽鸟自频。\\nIf the LLM is trained to generate language and code, we can prompt it to perform code com-\\npletion tasks. Here is an example.\\nPlease write a Python function to calculate the average of a list of numbers.\\ndef calculate_average(numbers):\\nif numbers:\\n# Check if the list is not empty\\nreturn sum(numbers) / len(numbers)\\nelse:\\nreturn 0 # Return 0 if the list is empty\\n3.1.4.4\\nText Transformation\\nTypical text transformation tasks include machine translation, summarization, text style transfer,\\nand so on. The prompts used to perform these tasks are not signiﬁcantly different from the ones\\npresented previously. Here, we present a few examples to showcase these prompts.\\nTranslate the following text from English to Spanish.\\nText:\\nThe quick brown fox jumps over the lazy dog.\\nEl rápido zorro marrón salta sobre el perro perezoso.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 119}, page_content='3.1 General Prompt Design\\n113\\nSummarize the following this article in no more than 50 words:\\nArticle:\\nIn recent years, urban areas have been facing signiﬁcant challenges related to sus-\\ntainability and environmental management. Cities are experiencing higher rates\\nof pollution, increased trafﬁc congestion, and greater demands on infrastructure.\\nThis growth has led to numerous environmental issues, including elevated lev-\\nels of air and water pollution, increased waste production, and strained public\\nservices ...\\nUrban areas are grappling with sustainability challenges, such asrising pollution,\\ntrafﬁc congestion, and infrastructure demands ...\\nRewrite this text in a formal tone.\\nText:\\nHey, what’s up? Long time no see!\\nHello, how have you been? It has been quite some time since we last met!\\n3.1.4.5\\nQuestion Answering\\nThe question-answering format is inherently simple. For a given question, there is an answer that\\ncorresponds to it. For example, in open-domain question answering, we expect the system to\\nreturn an answer in response to a user-submitted question. Prompt templates for general-purpose\\nquestion answering can be:\\n{∗question∗}\\nQuestion: {∗question∗}\\nAnswer:\\nQuestion answering is important in NLP because many problems can be framed as question-\\nanswering tasks. In particular, many recent reasoning tasks are deﬁned in the form of question\\nanswering. For example, in the MMLU benchmark [Hendrycks et al., 2021], each example con-\\nsists of a multiple-choice question, and LLMs are required to select the correct answer. See the\\nfollowing for an example prompt for answering a question in this dataset.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 120}, page_content='114\\nPrompting\\n(Z, ∗) is a group with a ∗b = a + b + 1 for all a, b in Z. The inverse of a is\\n(A) 0\\n(B) −2\\n(C) a −2\\n(D) (2 + a) ∗−1\\nD\\nAnother widely-used benchmark is the GSM8K dataset [Cobbe et al., 2021]. It consists of\\nthousands of grade school math word problems. For each problem, we use LLMs to return a\\nsolution in natural language. For example, the following is a prompt used for solving a problem\\nin GSM8K.\\nDEMO\\nQ: Jerry’s two daughters play softball on different teams. They each have\\n8 games this season. Each team practices 4 hours for every game they\\nplay. If each game lasts for 2 hours, how many hours will Jerry spend at\\nthe ﬁeld watching his daughters play and practice altogether?\\nA: Jerry will spend 8 games * 2 hours per game = ≪8 ∗2 = 16 ≫16\\nhours watching one daughter play her games. He will spend 16 ∗2 =≪\\n16 ∗2 = 32 ≫32 hours watching both daughters play their games. He\\nwill spend 8 games * 4 hours of practice = ≪8 ∗4 = 32 ≫32 hours\\nwatching one daughter practice. He will spend 32∗2 =≪32∗2 = 64 ≫\\n64 hours watching both daughters practice. He will spend a total of 32\\nhours watching games + 64 hours watching practice =≪32 + 64 =\\n96 ≫96 hours. #### 96\\nQ: Mary bought six apples from the store. From the apples she bought,\\nfor each that Mary ate, she planted two trees from the remaining ones.\\nHow many apples did Mary eat?\\nA: She planted eight trees. This means she used half of that amount of\\napples, which is 8 trees /2 trees/apple =≪8/2 = 4 ≫4 apples. That\\nmeans that she planted four of the six apples she bought, leaving only 6\\napples −4 apples =≪6 −4 = 2 ≫2 apples to be eaten. #### 2\\nUSER\\nQ: Boris has 100 apples. Beck has 23 fewer apples than Boris. If Boris\\ngives Beck 10 apples, how many fewer apples does Beck have than Boris\\nnow?\\nA: Boris starts with 100 apples and gives Beck 10 apples, leaving\\nhim with 100 apples −10 apples =≪100 −10 = 90 ≫90 apples.\\nInitially,\\nBeck has 23 fewer apples than Boris,\\ngiving him\\na\\nstarting count of 100 −23 =≪100 −23 = 77 ≫77 apples.\\nAfter\\nreceiving 10 apples from Boris, Beck’s total comes to 77 apples +10\\napples =≪77 + 10 = 87 ≫87 apples. Consequently, Beck now has\\n90 −87 =≪90 −87 = 3 ≫3 fewer apples than Boris. #### 3'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 121}, page_content='3.2 Advanced Prompting Methods\\n115\\nHere a few-shot prompt is adopted. The LLM learns from these demonstrations of problem-\\nsolution pairs not only the way of problem-solving but also the way of formatting the output. For\\nexample, the ﬁnal result of calculation follows the #### token, and ≪... ≫annotates the detailed\\ncalculation steps (called calculation annotation)4.\\n3.2\\nAdvanced Prompting Methods\\nSo far in this chapter, we have introduced the basic concepts related to LLM prompting and pre-\\nsented a number of prompts for NLP tasks. We now consider several techniques for enhancing the\\neffectiveness of prompting.\\n3.2.1\\nChain of Thought\\nWe have encountered the concept of chain of thought (CoT) several times in this chapter and\\nprevious ones [Wei et al., 2022c; Chowdhery et al., 2022]. CoT methods provide a simple way\\nto prompt LLMs to generate step-by-step reasoning for complex problems, thereby approaching\\ntasks in a more human-like manner. Rather than coming to a conclusion directly, the CoT methods\\ninstruct LLMs to generate reasoning steps or to learn from demonstrations of detailed reasoning\\nprocesses provided in the prompts. To illustrate CoT, we consider the problem of algebraic calcu-\\nlation, as commonly described in the literature. Suppose we are given a algebraic problem\\nCalculate the average of the numbers 2, 4, and 6.\\nWe can consider it as the question and prompt an LLM to answer it.\\nQ: Please calculate the average of the numbers 2, 4, and 9.\\nA: The answer is 6.\\nIt seems difﬁcult for the LLM to directly give a correct answer. A simple improvement is to\\nadd demonstrations of similar problems in the prompt, and thus the LLM can learn from these\\ndemonstrations.\\nQ: Please calculate the average of the numbers 1, 3, 5, and 7.\\nA: The answer is 4.\\nQ: Please calculate the average of the numbers 2, 4, and 9.\\nA: The answer is 7.\\nThe problem here is that, although we have shown a similar question-answer pair, it remains\\ndifﬁcult for the LLM to reason out the correct answer. In CoT, not only can LLMs learn from the\\n4During prediction, a calculator is used when we see ≪... ≫. More speciﬁcally, once the LLM encounters “=”\\nin a ≪... ≫, then the calculator calculates the expression on the left-hand side of “=”. This method helps reduce the\\ncalculation errors made by LLMs.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 122}, page_content='116\\nPrompting\\ncorrespondence between questions and answers but they may gain more from detailed problem-\\nsolving steps that used to derive the answers. To do this, we can incorporate some reasoning steps\\ninto the prompt to obtain a CoT prompt.\\nQ: Please calculate the mean square of the numbers 1, 3, 5, and 7.\\nA: Calculate the square of each number: 12 = 1, 32 = 9, 52 = 25, and 72 = 49.\\nSum the squares, 1 + 9 + 25 + 49 = 84. There are 4 numbers in total. Divide\\nthe sum by the number of items, 84/4 = 21. The answer is 21.\\nQ: Please calculate the average of the numbers 2, 4, and 9.\\nA: Calculate 2 + 4 + 9, which equals 15. There are three numbers. Divide the\\ntotal sum by the count, resulting in 15/3 = 5. The answer is 5.\\nHere we highlight the reasoning steps in green. By providing a detailed reasoning process for\\na similar problem, the LLM learns to reason and thereby generates a problem-solving path that\\nleads to the correct answer.\\nThere are several beneﬁts of using CoT prompting.\\nFirst, CoT allows LLMs to decom-\\npose complex problems into smaller, sequential reasoning steps. This somewhat mirrors human\\nproblem-solving behaviors, making it particularly effective for tasks requiring detailed, multi-step\\nreasoning. Second, CoT makes the reasoning process more transparent and interpretable. Since all\\nreasoning steps are visible, we can understand and interpret how a conclusion was reached. Third,\\nif users can follow the logic behind the reasoning process, they will be more likely to trust the pre-\\ndictions of an LLM. This is particularly important when applying LLMs in ﬁelds like medicine,\\neducation, and ﬁnance. Fourth, CoT is an in-context learning approach, and thus, it is applicable\\nto most well-trained, off-the-shelf LLMs. Moreover, CoT provides efﬁcient ways to adapt LLMs\\nto different types of problems. It can even inspire more creative solutions by exploring various\\nalternative reasoning paths, which might not be obvious when arriving at a conclusion directly.\\nThe method described above requires providing one or more examples of CoT reasoning, typ-\\nically called the few-shot CoT method. By contrast, the zero-shot CoT method does not require\\nsuch examples. It instead prompts LLMs to reason step-by-step by incorporating speciﬁc instruc-\\ntions in prompts. For example, below is a zero-shot CoT prompt.\\nQ: Please calculate the average of the numbers 2, 4, and 9.\\nA: Let’s think step-by-step.\\nWe have three numbers:\\n2, 4, and 9.\\nAdd these numbers together,\\n2 + 4 + 9 = 15. Determine how many numbers there are, which in this case\\nis three. The average is calculated by dividing the total sum by the number of\\nelements. Completing the division gives 15/3 = 5. So the answer is 5.\\nFollowing the instruction “Let’s think step by step”, the LLM is prompted to generate detailed\\nreasoning steps. As discussed in Kojima et al. [2022]’s work, prompting with such instructions\\nmay result in LLMs generating only the reasoning steps without a clear conclusion. In this case,\\na second round of prompting can be used to extract the answer from these reasoning steps. For\\nexample, Kojima et al. [2022] create a second prompt which combines both the input and output'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 123}, page_content='3.2 Advanced Prompting Methods\\n117\\nin the ﬁrst round of prompting. Using this combined input, the LLM can continue its reasoning\\nprocess and then generate the correct answer. Furthermore, it is possible to prompt LLMs to\\nreason using instructions other than “Let’s think step by step”, such as “Let’s think logically” and\\n“Please show me your thinking steps ﬁrst”.\\nWhile we have illustrated CoT methods using an algebraic reasoning problem, these methods\\ncan be applied to a variety of different problems. Typical problem-solving scenarios for CoT\\ninclude mathematical reasoning, logical reasoning, commonsense reasoning, symbolic reasoning,\\ncode generation, and so on. See Figure 3.1 for more examples of applying CoT in various tasks.\\nCoT today is one of the most active ﬁelds of prompt engineering. This has not only led to im-\\nproved performance for LLM prompting but has opened the door to a wide range of methods for\\nstudying and verifying reasoning capabilities of LLMs. Although we have focused on the basic\\nidea of CoT in this section, it can be improved in several ways. For example, we can consider the\\nreasoning process as a problem of searching through many possible paths, each of which may con-\\nsist of multiple intermediate states (i.e., reasoning steps). In general, we wish the search space to\\nbe well-deﬁned and sufﬁciently large, so that we are more likely to ﬁnd the optimal result. For this\\nreason, an area of current LLM research is aimed at designing better structures for representing\\nreasoning processes, allowing LLMs to tackle more complex reasoning challenges. These struc-\\ntures include tree-based structures [Yao et al., 2024], graph-based structures [Besta et al., 2024],\\nand so on. By using these compact representations of reasoning paths, LLMs can explore a wider\\nrange of decision-making paths, analogous to System 2 thinking5. Another line of research fo-\\ncuses on prompting LLMs with multi-round interactions. This involves decomposing complex\\nproblems into sub-problems, verifying and reﬁning model outputs, employing model ensembling,\\nand so on. Note that these methods and the issues involved are not limited to CoT. In fact, they\\nare often used as more general approaches to improving LLMs, while CoT can be seen as a way\\nto test the capabilities of LLMs. We will see discussions of some of these issues in the following\\nsubsections.\\nBefore leaving our discussion of CoT, we should consider its practical limitations. One of them\\nis the need for detailed, multi-step reasoning demonstrations in few-shot CoT scenarios, which\\nmay be difﬁcult to obtain, either automatically or manually. Also, there is no standard method for\\nbreaking down complex problems into simpler problem-solving steps. This often heavily depends\\non the user’s experience. In addition, errors in intermediate steps can also affect the accuracy of\\nthe ﬁnal conclusion. For further discussion on the pros and cons of CoT, the interested reader can\\nrefer to recent surveys on this topic [Chu et al., 2023; Yu et al., 2023; Zhang et al., 2023a].\\n3.2.2\\nProblem Decomposition\\nWe have seen that LLMs can beneﬁt from solving a complex problem by breaking it down into\\nsimpler problem-solving tasks. Such an approach can be seen as an example of a broader paradigm\\nknown as problem decomposition, which has been extensively explored and discussed in psy-\\nchology and computer science. From the psychological perspective, complex problem-solving\\nrefers to a process of addressing a problem using knowledge that helps overcome the barriers of\\n5System 1 and System 2 thinking, as described by Kahneman [2011], represent two different modes of cognitive\\nprocessing. System 1 is fast, automatic, intuitive, and emotional. This mode of thinking operates effortlessly and\\nquickly, and is often what guides our daily decisions, judgments, and impressions. System 2 is slow, deliberate, and\\nanalytical. It is activated when we need to perform complex computations.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 124}, page_content='118\\nPrompting\\nQ: What home entertainment equipment re-\\nquires cable?\\nAnswer Choices: (a) radio shack (b) substa-\\ntion (c) television (d) cabinet\\nA: The answer must require cable. Of the\\nabove choices, only television requires cable.\\nSo the answer is (c).\\nQ: What type of person typically contracts\\nillness?\\nAnswer Choices: (a) hospital (b) head (c)\\nsick person (d) elderly person (e) doctor’s of-\\nﬁce\\nA:\\nThe answer must be a person suscepti-\\nble to illness. Of the choices, \"elderly per\\nson\" is correct because they typically have\\nweaker immune systems.\\nSo the answer is (d).\\nQ: Do hamsters provide food for any ani-\\nmals?\\nA: Hamsters are prey animals. Prey are food\\nfor predators. Thus, hamsters provide food\\nfor some animals. So the answer is yes.\\nQ: Can you use Microsoft Ofﬁce without in-\\nternet?\\nA: Yes, you can use Microsoft Ofﬁce appli-\\ncations , although features like cloud stor-\\nage and online collaboration require inter-\\nnet. So the answer is yes.\\n(a) CSQA\\n(b) StrategyQA\\nQ: Complete the rest of the sequence, mak-\\ning sure that the parentheses are closed prop-\\nerly. Input: [ {\\nA: Let’s think step by step.\\n0: empty stack\\n1: [ ; stack: [\\n2: { ; stack: [ {\\nSo the answer is } ].\\nQ: Complete the rest of the sequence, mak-\\ning sure that the parentheses are closed prop-\\nerly. Input: < [ [\\nA: Let’s think step by step.\\n0: empty stack\\n1: < ; stack: <\\n2: [ ; stack: < [\\n3: [ ; stack: < [ [\\nSo the answer is ] ] >.\\nQ: Take the last letters of the words in \"Elon\\nMusk\" and concatenate them.\\nA: The last letter of \"Elon\" is \"n\". The last\\nletter of \"Musk\" is \"k\". Concatenating them\\nis \"nk\". The answer is nk.\\nQ: Take the last letters of each words in \"Re-\\nnata Mariela Mona Kristin\" and concatenate\\nthem.\\nA: The last letter of \"Renata\" is \"a\". The\\nlast letter of \"Mariela\" is \"a\". The last let-\\nter of \"Mona\" is \"a\"\". The last letter of\\n\"Kristin\" is \"n\". Concatenating them is\\n\"aaan\". The answer is aaan.\\n(c) Dyck languages\\n(d) Last Letter Concatenation\\nFig. 3.1: CoT in four different reasoning tasks, including CSQA, StrategyQA, Dyck languages, and Last Letter Con-\\ncatenation. The CoT parts are highlighted in green.\\nthe problem6. There are generally no standard or clear paths to a solution for a complex prob-\\nlem. However, it is often advantageous to employ strategies that decompose the problem, thereby\\nmaking it easier to tackle the corresponding sub-problems with less effort. For example, consider\\nwriting a blog about the risks of AI. If we simply prompt an LLM with the instruction “Please\\nwrite a blog about the risks of AI”, the LLM may generate a blog with arbitrary structures and\\n6A relatively formal deﬁnition can be found in Frensch and Funke [2014]’s book: complex problem-solving occurs\\nto overcome barriers between a given state and a desired goal state by means of behavioral and/or cognitive, multi-step\\nactivities.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 125}, page_content='3.2 Advanced Prompting Methods\\n119\\nwriting styles. A better method, instead, could be to outline the blog and provide more detailed\\ninformation about each section. Consider the following prompt\\nYou are a blog writer. Please follow the provided outline below to write a blog\\nabout the risks of AI.\\n• Introduction\\nIntroduce AI, its relevance, and the importance of understanding its risks for youth.\\n• Privacy Concerns\\nDiscuss how AI might compromise personal privacy through interactions online.\\n• Misinformation\\nExplore AI’s role in spreading misinformation and inﬂuencing young people’s deci-\\nsions.\\n• Cyberbullying\\nHighlight how AI tools can be utilized in cyberbullying and the impact on mental\\nhealth.\\n• Tips for Safe AI Use\\nOffer guidelines for responsible AI usage and promote critical thinking.\\n• Conclusion\\nRecap main points and encourage proactive engagement with AI ethics.\\nHere we give the title and major points for each section. Then, the LLM can use this structure to\\nbreak down the writing task by ﬁlling in content for these sections. Note that the way to structure\\nthe blog can be provided by humans or even generated automatically. For example, we can use\\nthe LLM to ﬁrst generate the outline, and then ask it to follow this outline to complete the writing.\\nIn computer science, decomposing complex problems is a commonly used strategy in software\\nand hardware system design. A well-known example is the divide-and-conquer paradigm, which\\nis often used to design algorithms for computation problems that can be reduced to simpler, more\\nmanageable problems. For example, consider a problem of determining whether a document\\ndiscusses the risks of AI. We can instruct the LLM with the following prompt.\\nYou are provided with a text. Please determine whether it discusses the risks of\\nAI.\\n{∗document∗}\\nIf the document is long, the computation will be expensive. Alternatively, we can divide\\nthe document into relatively short segments and perform the same task on each segment. These\\nsegments can be processed in parallel to further reduce the computational cost. Next, we determine'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 126}, page_content='120\\nPrompting\\nthe relevancy of each segment to the topic of AI risks. The ﬁnal output is then generated using\\nanother prompt.\\nYour task is to determine whether a text discusses the risks of AI. This text has\\nbeen divided into segments, and you have obtained the relevancy of each segment\\nto the topic of AI risks. Based on this, please provide your ﬁnal result.\\nSegment 1: {∗relevancy-to-the-topic1∗}\\nSegment 2: {∗relevancy-to-the-topic2∗}\\nSegment 3: {∗relevancy-to-the-topic3∗}\\n...\\nNow let us return to a more general discussion of problem decomposition in prompting. While\\nproblem decomposition can be applied to various NLP problems, it has been more extensively\\ndiscussed and tested in reasoning tasks recently. For complex reasoning tasks, we often need\\na multi-step reasoning path to reach a correct conclusion. We can use LLMs to achieve this in\\nthree different ways. First, LLMs can directly reach the conclusion. In other words, they can\\npredict without explicit reasoning processes, and there is a hidden and uninterpretable reasoning\\nmechanism. Second, LLMs are prompted to generate a multi-step reasoning path that leads to the\\nconclusion, like CoT. However, we run LLMs just once, and all intermediate steps in reasoning\\nare generated in a single prediction. Third, we break down the original problem into a number of\\nsub-problems, which are either addressed in separate runs of LLMs or tackled using other systems.\\nHere we focus our attention on the third approach, which is closely related to problem decompo-\\nsition. Note, however, that a more comprehensive discussion could cover all these approaches,\\nwhile the ﬁrst two have been discussed to some extent in this chapter.\\nA general framework for problem decomposition involves two elements.\\n• Sub-problem Generation. This involves decomposing the input problem into a number of\\nsub-problems.\\n• Sub-problem Solving. This involves solving each sub-problem and deriving intermediate\\nand ﬁnal conclusions through reasoning.\\nThese two issues can be modeled in different ways, leading to various problem decomposition\\nmethods. One approach is to treat them as separate steps in a two-step process. For example,\\nconsider the blog writing task described at the beginning of this subsection. In the ﬁrst step, we\\ndecompose the entire problem into sub-problems all at once (i.e., outline the blog). In the second\\nstep, we solve the sub-problems either sequentially or in another order (i.e., ﬁll in content for\\neach section as needed). The ﬁnal output of this process combines the results from solving each\\nsub-problem. While this method is simple and straightforward, it assumes that the problem is\\ncompositional, making it more suitable for tasks like writing and code generation.\\nHowever, many real-world problems require complex reasoning. One key characteristic of\\nthese problems is that the reasoning steps may not be ﬁxed. The reasoning path can vary for\\ndifferent problems, and each step of reasoning may depend on the outcomes of prior steps. In'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 127}, page_content='3.2 Advanced Prompting Methods\\n121\\nsuch cases, it is undesirable to use ﬁxed sub-problem generation in advance. Instead, sub-problems\\nshould be generated dynamically based on the input problem, and, if possible, generated on the\\nﬂy during the reasoning process. This makes problem decomposition more challenging compared\\nwith designing divide-and-conquer algorithms. Ideally, we would like to jointly design both the\\nsystems for sub-problem generation and sub-problem solving. But a more practical and widely\\nused approach is to adopt separate models for these tasks. A straightforward way to achieve this\\nis to adapt an LLM for these tasks by either prompting or tuning the model.\\nHere we consider a method based on the above idea, called least-to-most prompting [Zhou et al.,\\n2023b]. The motivation for this method arises from the challenges of solving difﬁcult reasoning\\nproblems — those that cannot be addressed by simply generalizing from a few examples. For\\nthese problems, a more effective problem-solving strategy is to follow a progressive sequence of\\nsub-problems that systematically lead to the conclusion. More speciﬁcally, in the least-to-most\\nprompting method, sub-problem generation is performed by prompting an LLM with instructions\\nand/or demonstrations. For example, below is a 2-shot prompt for sub-problem generation in\\nleast-to-most prompting.\\nTASK\\nYour task is to decompose a problem into several sub-problems. You will\\nbe given a few examples to illustrate how to achieve this.\\nDEMO\\nQ: In a community, 5% of the population are infants, 15% are children,\\n40% are adults, and 40% are seniors. Which group makes up the largest\\nportion of the population?\\nA: To answer the question “Which group makes up the largest portion of the\\npopulation?”, we need to know: “How many percent are infants?”, “How\\nmany percent are children?”, “How many percent are adults?”, “How many\\npercent are seniors?”.\\nQ: Alice, Bob, and Charlie brought beads for their group project in their\\ncraft class. Alice has twice as many beads as Bob, and Bob has ﬁve times\\nas many beads as Charlie. If Charlie has 6 beads, how many beads can they\\nuse for their craft project?\\nA: To answer the question “How many beads can they use for their craft\\nproject?”, we need to know: “How many beads does Bob have?”, “How\\nmany beads does Alice have?”.\\nUSER\\nQ: The environmental study conducted from 2015 to 2020 revealed that the\\naverage temperature in the region increased by 2.3 degrees Celsius. What\\nwas the duration of the environmental study?\\nA: To answer the question “What was the duration of the environmental\\nstudy?”, we need to know: “When did the environmental study start?”,\\n“When did the environmental study end?”.\\nBy learning from the examples, the LLM can generate two sub-problems for answering the\\nnew problem “What was the duration of the environmental study?” (highlighted in blue and\\norange). Given these sub-problems, we solve them sequentially. For each sub-problem, we take\\nall previously-generated QA pairs as context, and then produce the answer. For the example above,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 128}, page_content='122\\nPrompting\\nwe need to answer the ﬁrst sub-problem by prompting the LLM, like this\\nThe environmental study conducted from 2015 to 2020 revealed that\\nthe average temperature in the region increased by 2.3 degrees Celsius.\\nSUB-PROB1\\nQ: When did the environmental study start?\\nA: The environmental study started in 2015.\\nOnce we have the answer to the ﬁrst sub-problem, we proceed to the second one. This time,\\nwe include both the ﬁrst sub-problem and its corresponding answer in the input.\\nThe environmental study conducted from 2015 to 2020 revealed that\\nthe average temperature in the region increased by 2.3 degrees Celsius.\\nSUB-PROB1\\nQ: When did the environmental study start?\\nA: The environmental study started in 2015.\\nSUB-PROB2\\nQ: When did the environmental study end?\\nA: The environmental study started in 2020.\\nFinally, we use the LLM to solve the original problem given the answers to all the sub-\\nproblems.\\nThe environmental study conducted from 2015 to 2020 revealed that\\nthe average temperature in the region increased by 2.3 degrees Celsius.\\nSUB-PROB1\\nQ: When did the environmental study start?\\nA: The environmental study started in 2015.\\nSUB-PROB2\\nQ: When did the environmental study end?\\nA: The environmental study started in 2020.\\nFINAL\\nQ: What was the duration of the environmental study?\\nA: The duration of the environmental study was 5 years.\\nThe least-to-most method offers a basic approach to prompting LLMs to generate and solve\\nsub-problems separately. We can improve it in several ways. One simple improvement is to apply\\nvarious advanced prompting techniques, which do not require changes to the problem decom-\\nposition framework. For example, we can incorporate CoT into the prompting to enhance the\\nreasoning performance of sub-problem generation and solving.\\nAnother improvement is to explore methods for better decomposing problems and organizing\\nproblem-solving paths. To describe these approaches, we will use the symbol p0 to denote the'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 129}, page_content='3.2 Advanced Prompting Methods\\n123\\ninput problem, and use the symbols {p1, ..., pn} to denote the sub-problems corresponding to p0.\\nFor least-to-most prompting, we decompose p0 into {p1, ..., pn}, given by\\n{p1, ..., pn}\\n=\\nG(p0)\\n(3.2)\\nwhere G(·) denotes the function of sub-problem generation. Then, we solve the sub-problems\\n{p1, ..., pn} sequentially, resulting in a sequence of answers {a1, ..., an}. For answering the i-th\\nsub-problem pi, we include both the original problem p0 and all previously-seen problem-answer\\npairs in the context for prediction. The answer ai is given by\\nai\\n=\\nSi(pi, {p0, p<i, a<i})\\n(3.3)\\nwhere p<i = {p1, ..., pi−1} and a<i = {a1, ..., ai−1}. Si(·) denotes the function that solves the\\nsub-problem pi given the context {p0, p<i, a<i}. The last step is to generate the answer to the\\noriginal problem p0, which can be expressed in a similar manner to Eq. (3.3).\\na0\\n=\\nS0(p0, {p≤n, a≤n})\\n(3.4)\\nOne way to reﬁne this model is to modify the G(·) function so that the model can dynamically\\ngenerate answers. Instead of generating all sub-problems at one time, we can generate each of\\nthem during problem-solving [Dua et al., 2022]. To do this, we can replace Eq. (3.2) with\\npi\\n=\\nGi(p0, {p<i, a<i})\\n(3.5)\\nHence we obtain a sub-problem generation model that operates in a step-by-step manner. At each\\nstep i, we ﬁrst generate the sub-problem pi by prompting an LLM with the original problem p0\\nand the problem-solving history {p<i, a<i}. We then generate the answer ai for this sub-problem\\nusing the same or a different LLM, based on the same contextual information (see Eq. (3.3)). This\\nmethod effectively expands the reasoning capacity of LLMs by allowing them to dynamically\\ngenerate and solve sub-problems in intermediate reasoning steps. As a result, the reasoning paths\\nare not ﬁxed in advance, and the models can choose and adapt their reasoning strategies during\\nproblem-solving.\\nAnother way to improve the above model is to focus on developing better sub-problem solvers.\\nIn our previous discussion, we restricted Si(·) to LLMs that are prompted to solve the sub-problem\\npi. In fact, we can expand this function to any system that is capable of addressing the sub-\\nproblem. For example, Si(·) could make calls to IR systems, thereby allowing us to access a\\nbroader range of data for problem-solving. Another example is using Si(·) as a calculator to\\naccurately compute results in mathematical problem-solving. If the sub-problem pi is complex\\nand requires multiple intermediate problem-solving steps, it is also possible to further decompose\\npi into smaller sub-problems. For example, Si(·) can be deﬁned as a recursive program that\\ngenerates and solves sub-problems. This incorporates recursion into problem-solving and allows\\nus to address problems by iteratively decomposing them. As a result, we can deﬁne a hierarchical\\nstructure for problem-solving [Khot et al., 2023].\\nIf we generalize the above formulation a bit further, we can consider it as a reinforcement\\nlearning problem. A typical method is to model a problem-solving process as a decision making\\nprocess. In each step of this process, an action is taken based on the current state. These actions'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 130}, page_content='124\\nPrompting\\ncan include all functions for sub-problem generation and solving (i.e., Gi(·) and Si(·)). Thus,\\nthe action sequence corresponds to a problem-solving path. Since the discussion of reinforcement\\nlearning problems is beyond the scope of this chapter, we skip the precise description of this\\nlearning task. Nevertheless, developing an agent or controller to determine when and how to\\ngenerate and solve a sub-problem is also a natural choice.\\nIn NLP, problem decomposition is related to a long line of research on multi-hop question\\nanswering [Mavi et al., 2024]. This task requires the system to gather and combine information\\nfrom multiple pieces of text to provide an accurate answer to a complex question. For example,\\nto answer the question “What is the capital of the country where Albert Einstein was born?”, we\\nneed to know “Where Albert Einstein was born?” and “What’s the capital of Germany?”. Earlier\\nwork in this area and related ones has investigated the issue of problem decomposition, though the\\nmethods might not be based on LLMs. For example, a popular method is to develop an additional\\nneural model to generate simpler questions that address different aspects of the original question\\n[Andreas et al., 2016; Talmor and Berant, 2018; Min et al., 2019]. This question generator can\\ncreate questions in a batch or sequential manner.\\nBroadly speaking, problem decomposition is also related to the compositionality issue in NLP\\n[Drozdov et al., 2022; Press et al., 2023]. For example, in semantic parsing, we map natural lan-\\nguage sentences into structured meaning representations by breaking them down into constituent\\nparts and understanding the sentences based on the meanings of these parts and the rules used to\\ncombine them. In early studies of this ﬁeld, highly compositional sentences were considered easier\\nfor testing systems, as it is relatively straightforward to decompose such sentences and compose\\nthe meanings of their parts. However, the task becomes much more difﬁcult when more gener-\\nalization is required for modeling compositionality in new data. In this case, we want systems\\nto have improved abilities of compositional generalization. In more recent research on LLMs,\\nthis issue has been frequently discussed in compositional reasoning tasks, such as SCAN7, as it\\nis considered an important aspect of testing the language understanding and reasoning abilities\\nof LLMs. This also presents new tasks for developing and examining problem decomposition\\nmethods.\\nIn LLMs, one interesting application of problem decomposition is tool use. In some cases,\\nit is necessary to integrate external tools into LLMs to access accurate data not available during\\ntraining or ﬁne-tuning. For example, LLMs can integrate with APIs to fetch real-time data such\\nas weather updates, stock market prices, or news feeds, enabling them to provide up-to-date re-\\nsponses to user queries. When using tools, LLM predictions might include markers that indicate\\nwhere and how to call external APIs. This requires decomposing the problem into sub-problems,\\nwith some handled by the LLMs and others by external tools. More detailed discussions on this\\ntopic will be presented in Section 3.2.5.\\n3.2.3\\nSelf-reﬁnement\\nIn many cases, predictions of LLMs can be inaccurate or incorrect. Given that current LLMs can\\nperform tasks like reﬁnement and correction, it makes sense to explore methods for these models\\nto self-reﬁne their outputs. Self-reﬁnement is a common phenomenon in human psychological\\n7The SCAN tasks (Simpliﬁed versions of the CommAI Navigation tasks) are designed to evaluate the ability of\\nLLMs to perform compositional generalization [Lake and Baroni, 2018]. They involve translating natural language\\ncommands into a sequence of actions. For example, a command “jump opposite left and walk thrice” can be translated\\ninto the action sequence “LTURN LTURN JUMP WALK WALK WALK”.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 131}, page_content='3.2 Advanced Prompting Methods\\n125\\nactivities and daily behavior. For example, when designing a product, a designer might ﬁrst create\\na basic prototype, then reﬁne the design after evaluation and testing to enhance user experience\\nand functionality. The reﬁnement can be iterated several times until the design is satisfactory. The\\nidea of predict-then-reﬁne can also be found in NLP. One early example is Brill’s tagger [Brill,\\n1992], where an initial POS tagging result of a sentence can be iteratively reﬁned using a rule-\\nbased system. In the era of deep learning, a good deal of work on sequence-to-sequence problems,\\nsuch as grammar correction and text rewriting, can also be seen as examples on this theme.\\nWe can prompt LLMs to do self-reﬁnement. Consider a simple example where we ﬁrst use\\nan LLM to translate a text from Chinese to English and then use the same model to reﬁne the\\ntranslation. Here is how we can prompt the model to translate:\\nTranslate the following text from Chinese to English:\\nChinese:\\n一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。最新公\\n布的研究成果认为，大约从距今5800年开始，中华大地上各个区域相继出\\n现较为明显的社会分化，进入文明起源的加速阶段。\\nEnglish:\\nA series of discoveries have cemented the signiﬁcant role of the Hongshan\\nculture in studies on the origins of Chinese civilization.\\nThe latest research\\nﬁndings suggests that, starting from around 5800 years ago, many regions\\nacross China began to experience noticeable social differentiations, entering an\\naccelerated phase in the origin of civilization.\\nThen, we can prompt it to reﬁne the translation:\\nNiuTrans'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 132}, page_content='126\\nPrompting\\nYou are provided with a Chinese text along with its English translation. Please\\nreﬁne the translation to improve its accuracy and ﬂuency if necessary.\\nChinese:\\n一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。最新公\\n布的研究成果认为，大约从距今5800年开始，中华大地上各个区域相继出\\n现较为明显的社会分化，进入文明起源的加速阶段。\\nEnglish:\\nA series of discoveries have cemented the signiﬁcant role of the Hongshan cul-\\nture in studies on the origins of Chinese civilization. The latest research ﬁndings\\nsuggests that, starting from around 5800 years ago, many regions across China\\nbegan to experience noticeable social differentiations, entering an accelerated\\nphase in the origin of civilization.\\nReﬁnement:\\nA series of archaeological discoveries have cemented the signiﬁcant role\\nof the Hongshan culture in studies on the origins of Chinese civilization.\\nThe latest research ﬁndings suggest that, starting from around 5800 years\\nago, various regions across China began to experience noticeable social\\ndifferentiations, entering an accelerated phase in the origin of civilization.\\nHere modiﬁcations to the input translation are highlighted in green. This process can be iterative:\\nat each step, we use the translation from the previous step as input, and prompt the LLM to produce\\na potentially better translation.\\nIn the above example, the reﬁnement of the initial model output was achieved by prompting the\\nLLM with a simple instruction like “Please reﬁne it!”. However, the reﬁnement is based solely on\\nthe instruction-following ability of the LLM, and there is no guidance or supervision on how and\\nwhere to improve the model output. A more effective approach would be to consider feedback on\\nspeciﬁc aspects that need reﬁnement. For example, we can prompt the LLM with “Please correct\\nall the grammatical errors in the translation”, so that the model can focus more on grammatical\\nerror correction during reﬁnement.\\nA general framework of self-reﬁnement with LLMs involves three steps [Madaan et al., 2024].\\n• Prediction. We use an LLM to produce the initial model output.\\n• Feedback Collection. We obtain feedback on the model output.\\n• Reﬁnement. We use the LLM to reﬁne the model output based on the feedback.\\nThe last two steps can be repeated multiple times, which leads to an iterative self-reﬁnement\\nprocess. In this framework, receiving accurate and detailed feedback is critical as it guides the sub-\\nsequent reﬁnement step. In general, feedback can be obtained in various ways [Fernandes et al.,\\n2023; Pan et al., 2024]. We can manually examine the model’s predictions to identify issues. Also,\\nwe can use a feedback model to automatically generate feedback. For instance, a reward model\\ntrained on some labeled data can help evaluate the model output and return a score indicating its\\nquality.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 133}, page_content='3.2 Advanced Prompting Methods\\n127\\nSince the focus of this chapter is on LLM prompting, we consider use LLMs to provide feed-\\nback. To illustrate, let us consider an example of generating good responses to user questions:\\n• First, we prompt an LLM to answer the input question, as usual.\\nGenerate a response to the question: “What are some major environmental\\nconcerns today?”\\nResponse:\\nMajor environmental concerns today include climate change, air and water\\npollution, deforestation, loss of biodiversity, and ocean degradation.\\n• Then, using the following prompt, we prompt the same LLM again to get feedback on the\\ngenerated response.\\nYou will be provided with a question and its corresponding response. Eval-\\nuate the accuracy and depth of this response, and identify any factual in-\\naccuracies or areas where additional details could enhance understanding.\\nQuestion:\\nWhat are some major environmental concerns today?\\nResponse:\\nMajor environmental concerns today include climate change, air and water\\npollution, deforestation, loss of biodiversity, and ocean degradation.\\nFeedback:\\nThe response provided is accurate and highlights key environmental\\nconcerns.\\nHowever, it would beneﬁt from more speciﬁc and detailed\\ndescriptions of these concerns. Here are suggestions for improvement:\\n1. Include speciﬁc examples of environmental issues.\\n2. Add information about major pollutants.\\n3. Specify the regions most affected.\\n• Given the feedback the LLM produces, we prompt it to reﬁne the response.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 134}, page_content='128\\nPrompting\\nYou will be provided with a question, its corresponding response, and\\nsome feedback on this response.\\nQuestion:\\nWhat are some major environmental concerns today?\\nResponse:\\nMajor environmental concerns today include climate change, air and water\\npollution, deforestation, loss of biodiversity, and ocean degradation.\\nFeedback:\\nThe response provided is accurate and highlights key environmental con-\\ncerns. However, it would beneﬁt from more speciﬁc and detailed descrip-\\ntions of these concerns. Here are suggestions for improvement:\\n1. Include speciﬁc examples of environmental issues.\\n2. Add information about major pollutants.\\n3. Specify the regions most affected.\\nNow reﬁne the response based on the provided feedback.\\nReﬁned Response:\\nKey environmental concerns include climate change, evidenced by rising\\nglobal temperatures and severe weather patterns; signiﬁcant air and\\nwater pollution in urban and industrial areas; rampant deforestation in\\nregions like the Amazon, causing biodiversity loss; and ocean degradation,\\nhighlighted by coral reef bleaching and widespread overﬁshing.\\nIdeally, if a strong LLM is adopted, we would like to have it perform all three steps without\\nextra training. On the other hand, if we have enough labeled data for the task of interest, we can\\nenhance the performance of the LLM using supervised learning. For example, we can ﬁne-tune\\nthe LLM to better adapt it to reﬁnement tasks, or alternatively, use task-speciﬁc models, which\\nmay not necessarily be based on LLMs [Welleck et al., 2023; Schick et al., 2023]. In a broader\\nsense, improving LLMs for self-reﬁnement tasks can be seen as an alignment issue. For example,\\nit has been found that some self-correction abilities can be activated through RLHF [Ganguli et al.,\\n2023]. However, discussing these issues is beyond the scope of this chapter. Further discussion\\ncan be found in Chapter 4.\\nIn LLMs, self-reﬁnement is related to several concepts that reveal the psychological aspects\\nof these models, such as the ability to self-reﬂect. A view is that if LLMs are capable of self-\\nreﬂection, their predictions can become more accurate and even possess self-correcting capabili-\\nties. This self-reﬂection can be activated in various ways, for example, by prompting these LLMs\\nto engage in more in-depth and careful thinking, or by providing examples from which the models\\ncan learn and reﬂect. To illustrate, we consider here the deliberate-then-generate (DTG) method\\npresented in Li et al. [2023a]’s work, where LLMs are prompted to deliberate. In DTG, we are\\ngiven an initial model output which may contain errors. LLMs are then prompted to identify the\\nerror types of this model output and provide an improved output. Below is a template of DTG\\nprompting for Chinse-to-English translation tasks.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 135}, page_content='3.2 Advanced Prompting Methods\\n129\\nGiven the Chinese sentence: {∗source∗}\\nThe English translation is: {∗target∗}\\nPlease ﬁrst detect the type of error, and then reﬁne the translation.\\nError Type:\\nWe aim to ﬁrst predict the error type (red), and then produce a reﬁned translation (blue). This\\nprocess of deliberation is guided by the instruction “Please ﬁrst detect the type of error, and then\\nreﬁne the translation”. It encourages LLMs to initially engage in thoughtful analysis and then give\\nbetter results. Since error type prediction and reﬁnement are performed in a single run of LLMs,\\nthis method incorporates both steps of feedback and reﬁnement into one process.\\nIn the above prompts, we assume that the LLM we use is able to review the input translation\\nand correctly identify its error types. However, this raises new difﬁculties as the model may not\\nbe good at ﬁnding errors in translations. This will in turn result in extra ﬁne-tuning or prompt-\\ning engineering efforts. So a simpler method is to reduce the burden of error identiﬁcation and\\nuse LLMs for deliberation only. To do this, we can replace the input translation with a random\\ntranslation and assign a default error type. An example of such a prompt is shown below.\\nGiven the Chinese sentence:\\n一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。\\nThe English translation is:\\nA variety of innovative techniques have redeﬁned the importance of modern art\\nin contemporary cultural studies.\\nPlease ﬁrst detect the type of error, and then reﬁne the translation.\\nError Type: Incorrect Translation\\nIn this example, the input translation is not generated by LLMs but is instead randomly sam-\\npled from the dataset. So it is simply an incorrect translation for the source sentence, and we can\\nset the error type accordingly. The LLMs then generate a new translation by taking both the source\\nsentence and the incorrect translation as input. The design of this prompt can also be considered as\\nactivating the learning capabilities of LLMs through “negative evidence” [Marcus, 1993], thereby\\nenabling them to reﬂect and produce better outcomes through contrastive analysis. Nevertheless,\\nthis method does not rely on any feedback and can enhance the performance of a single LLM\\nprediction via simple prompting.\\nNote that while DTG is non-iterative, iterative learning and reﬁnement are commonly used\\nin NLP. An advantage of these iterative approaches are that they mimics human learning and\\nproblem-solving, where continuous feedback and adjustments lead to progressively improved out-\\ncomes. Iterative methods can be applied to a range of LLM prompting problems. For example, in\\nproblem decomposition, one can incorporate new sub-problems and their solutions into the con-\\ntext at each step, and thus LLMs can progressively approach the solution of the original problem.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 136}, page_content='130\\nPrompting\\nOn the other hand, iterative methods raise several issues that are absent in non-iterative meth-\\nods, for example, errors in earlier steps may negatively impact subsequent problem-solving, and\\ndetermining when to stop iterating often requires additional engineering effort.\\n3.2.4\\nEnsembling\\nModel ensembling for text generation has been extensively discussed in the NLP literature. The\\nidea is to combine the predictions of two or more models to generate a better prediction. This\\ntechnique can be directly applicable to LLMs. For example, we can collect a set of LLMs and run\\neach of them on the same input. The ﬁnal output is a combined prediction from these models.\\nFor LLM prompting, it is also possible to improve performance by combining predictions\\nbased on different prompts. Suppose we have an LLM and a collection of prompts that address\\nthe same task. We can run this LLM with each of the prompts and then combine the predictions.\\nFor example, below are three different prompt templates for text simpliﬁcation.\\nMake this text simpler.\\n{∗text∗}\\nCondense and simplify this text.\\n{∗text∗}\\nRewrite for easy reading.\\n{∗text∗}\\nEach of these prompts will lead to a different prediction, and we can consider all three predictions\\nto generate the ﬁnal one.\\nFormally, let {x1, ..., xK} be K prompts for performing the same task. Given an LLM Pr(·|·),\\nwe can ﬁnd the best prediction for each xi using ˆyi = arg maxyi Pr(yi|xi). These predictions\\ncan be combined to form a “new” prediction:\\nˆy\\n=\\nCombine(ˆy1, ..., ˆyK)\\n(3.6)\\nHere Combine(·) is the combination model, which can be designed in several different ways. For\\nexample, we can select the best prediction by voting or by identifying the one that overlaps the\\nmost with others. Another method for model combination is to perform model averaging during\\ntoken prediction. Let ˆyj be the predicted token at the j-th step for model combination. The'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 137}, page_content='3.2 Advanced Prompting Methods\\n131\\nprobability of predicting ˆyj is given by\\nˆyj\\n=\\narg max\\nyj\\nK\\nX\\nk=1\\nlog Pr(yj|xk, ˆy1, ..., ˆyj−1)\\n(3.7)\\nIn ensembling for LLM prompting, it is generally advantageous to use diverse prompts so that\\nthe combination can capture a broader range of potential responses. This practice is common in\\nensemble learning, as diversity helps average out biases and errors that may be speciﬁc to any\\nsingle model or conﬁguration. From the Bayesian viewpoint, we can treat the prompt x as a latent\\nvariable, given the problem of interest, p. This allows the predictive distribution of y given p to\\nbe written as the distribution Pr(y|x) marginalized over all possible prompts\\nPr(y|p)\\n=\\nZ\\nPr(y|x) Pr(x|p)dx\\n(3.8)\\nThe integral computes the total probability of y by considering all possible values of x, weighted\\nby their likelihoods given p. Here Pr(y|x) is given by the LLM, and Pr(x|p) is the prior distri-\\nbution of prompts for the problem. This is a good model because the integral effectively accounts\\nfor the uncertainty in the choice of x, ensuring that the ﬁnal predictive distribution Pr(y|p) is\\nrobust and encompasses all potential variations and biases in the prompts. However, computing\\nthis integral directly can be computationally infeasible due to the potentially inﬁnite space of x.\\nOne approach to addressing this issue is to employ methods like Monte Carlo sampling, which\\napproximate the integral using a manageable, ﬁnite number of prompts.\\nWhile the Bayesian treatment is mathematically well-deﬁned, it is common practice in NLP\\nto assume a non-informative or uniform prior and focus instead on constructing a set of diverse\\nprompts. Consequently, the output can be computed using a straightforward combination model,\\nas described in Eq. (3.6). The issue of creating high-quality, diverse prompts has been studied in\\nCoT and other in-context learning areas. Most of the research focuses on incorporating a variety\\nof demonstration examples across different prompts. Here, we list some of these methods.\\n• Given a problem, we manually create a number of demonstrations and use different ones\\nfor different prompts.\\n• Given a problem, we use LLMs to automatically generate demonstrations and prompts.\\n• Given a prompt, we create different prompts by changing the order of demonstrations in the\\nprompt.\\n• Given a prompt, we use LLMs to generate a number of similar prompts.\\n• Given a prompt, we transform it into other forms, e.g., translating it into other languages.\\nOf course, in practice, we can combine these methods to achieve greater diversity. An underly-\\ning assumption here is that diverse prompts can lead to diverse model outputs. This is particularly\\nthe case when the problem we deal with is relatively new and difﬁcult. For stronger and more ro-\\nbust LLMs, the variance in output for similar prompts might not be large. In this case, the beneﬁt\\nof involving multiple prompts can be modest.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 138}, page_content='132\\nPrompting\\nIn addition to providing diverse prompts for LLMs, another approach is to make use of the\\ninherent variance in the outputs of LLMs. One simple way to generate multiple outputs is to\\nsample outputs from the hypothesis space. This is straightforward for LLMs because their search\\nalgorithms are naturally suited to producing multiple results. For example, if beam search is\\nused, we can easily collect all the complete hypotheses in the beam to generate multiple output\\ncandidates. Also, we can modify the search algorithms to implement more powerful sampling\\nmethods and explore a wider range of high-quality hypotheses.\\nThe next question is how to combine diverse model predictions. This leads us back to the\\nmodel combination issue as described in Eq. (3.6). Here we consider the self-consistency method,\\nwhich outputs not the prediction with the highest probability, but rather the one that best aligns\\nwith other predictions [Wang et al., 2022a; 2023b]. First, an LLM is prompted with CoT as\\nusual and generates multiple reasoning paths by sampling. Then, the frequency of each answer is\\ncounted across these reasoning paths. The ﬁnal output is the answer that has the highest count.\\nBelow shows three different predictions by prompting an LLM with CoT.\\nPrediction 1 (Correct):\\nQ: Three friends play a game where they ﬂip a fair coin. Each friend ﬂips the\\ncoin once. What is the probability that exactly one of them ﬂips heads?\\nA: Let’s think step by step.\\nEach coin ﬂip is an independent event with a 50% chance of landing heads\\n(H) and 50% tails (T). Consider the possible outcomes for three ﬂips: HHH,\\nHHT, HTH, HTT, THH, THT, TTH, TTT. Out of these, only HHH, HHT, HTH,\\nand THH are relevant. We are only interested in scenarios with exactly one\\nhead: HTT, THT, and TTH. There are 3 such outcomes out of 8 total, giving a\\nprobability of 3/8.\\nPrediction 2 (Correct):\\nQ: Three friends play a game where they ﬂip a fair coin. Each friend ﬂips the\\ncoin once. What is the probability that exactly one of them ﬂips heads?\\nA: Let’s think step by step.\\nUsing a binomial probability formula, where p (success or heads) is 0.5, and\\neach of the 3 trials (ﬂips) is independent, we look for exactly one success\\n(head).\\nThe formula for exactly k successes in n trials is given by the\\nbinomial coefﬁcient formula\\n\\x00n\\nk\\n\\x01pk(1 −p)n−k. Plugging in the values, we get\\n\\x003\\n1\\n\\x01(0.5)1(0.5)3−1 = 3 · 0.5 · 0.25 = 0.375.\\nPrediction 3 (Wrong):'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 139}, page_content='3.2 Advanced Prompting Methods\\n133\\nQ: Three friends play a game where they ﬂip a fair coin. Each friend ﬂips the\\ncoin once. What is the probability that exactly one of them ﬂips heads?\\nA: Let’s think step by step.\\nSince each coin ﬂip has a 50% chance of landing heads and a 50% chance of\\nlanding tails, and we are looking for the case where only one of the three ﬂips\\nresults in heads, we might initially think this is simply a matter of picking which\\nﬂip is the heads. There are three ﬂips, so one might incorrectly assume that the\\nchance of picking one speciﬁc outcome like this would be 1 out of 3. Thus, they\\nmight conclude that the probability of exactly one head is 1/3 = 33.3%.\\nPredictions 1 and 2 correctly identify the three cases where exactly one head is ﬂipped, both\\nobtaining a probability of 37.5%. The reasoning in Prediction 3 fails to account for the total num-\\nber of outcomes possible with three coin ﬂips, thus giving a wrong answer of 33.3%. Therefore,\\nwe select 37.5% as the ﬁnal answer because it is the consensus.\\nSelf-consistency provides a criterion for determining the best prediction in a pool of can-\\ndidates. Since the prompt and the model are ﬁxed in this method, it is not strictly a prompt\\nensembling method. Instead, it can be seen as an instance of output ensembling methods, also\\nknown as hypothesis selection methods, which have long been explored in NLP, particularly for\\ntext generation problems [Xiao et al., 2013]. In these methods, multiple outputs are generated by\\nvarying model architectures or parameters. Each output is then assigned a score by some criterion,\\nand the outputs are re-ranked based on these scores. There are various ways to deﬁne the scoring\\nfunction, such as measuring the agreement between an output and others, and using a stronger\\nmodel to rescore each output8. Figure 3.2 shows a comparison of different ensembling methods\\nfor LLMs.\\nNow, let us brieﬂy review the methods we have discussed so far in this section, such as problem\\ndecomposition and self-reﬁnement. It is apparent that these methods enhance decision-making by\\nintroducing more “choices” into the reasoning process. To some extent, they all involve evalu-\\nating and providing feedback on the results of LLMs. For example, in self-reﬁnement, we need\\nto offer suggestions for improving the prediction of LLMs, and in output ensembling, we select\\nthe optimal output from a pool of candidates. In this sense, these methods fall under the broader\\ncategory of predict-then-verify approaches, where predictions are initially made, then veriﬁed and\\nreﬁned. The fundamental problem here involves verifying and evaluating the reasoning results\\nor intermediate steps. This issue is somewhat related to the problem of training reward models\\nin RLHF, although RLHF addresses a different aspect. In fact, the development of veriﬁers has\\nbeen explored and implemented in reasoning with LLMs. Most work, rather than developing\\nheuristic-based inference-time algorithms, focuses on learning veriﬁers in a supervised manner. A\\nstraightforward method is to train veriﬁers as binary classiﬁers, such as classifying an answer as\\n8An interpretation of self-consistency is to view it as a minimum Bayes risk search process. It searches for the best\\noutput by minimizing the Bayes risk. More speciﬁcally, a risk function R(y, yr) is deﬁned on each pair of outputs\\n(denoted by (y, yr)), representing the cost of replacing y with yr. Given a set of outputs Ω, the risk of an output\\ny ∈Ωis given by\\nRisk(y)\\n=\\nEyr∼Pr(yr|x)R(y, yr)\\n=\\nX\\nyr∈Ω\\nR(y, yr) · Pr(yr|x)\\n(3.9)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 140}, page_content='134\\nPrompting\\nLLM2\\nLLM1\\nLLM2\\nPrompt\\nPrediction2\\nPrediction1\\nPrediction3\\nCombine/Select\\nFinal\\nPrediction\\n(a) Model Ensembling\\nLLM\\nPrompt2\\nPrompt1\\nPrompt3\\nPrediction2\\nPrediction1\\nPrediction3\\nCombine/Select\\nFinal\\nPrediction\\n(b) Prompt Ensembling\\nLLM\\nPrompt\\nPrediction2\\nPrediction1\\nPrediction3\\nCombine/Select\\nFinal\\nPrediction\\nSample\\n(c) Output Ensembling\\nFig. 3.2: Ensembling methods for LLMs. In standard model ensembling (a), multiple LLMs varying in architectures or\\nparameters are used. Each LLM receives the same prompt and produces a prediction. These predictions are combined\\nto generate the ﬁnal prediction. In prompt ensembling (b), we have one LLM and multiple prompts. The LLM produces\\na prediction for each prompt, and these predictions are combined as usual. In output ensembling (c), the LLM samples\\nmultiple predictions over the prediction space given a prompt. It can be seen as a method to boost the performance\\nof the LLM itself. Note that these ensembling methods can be combined to increase the diversity of predictions. For\\nexample, we can use both prompt ensembling and output ensembling to obtain more diverse predictions.\\ncorrect or incorrect, although these veriﬁers are typically used as scoring models. Given a reason-\\ning path for a problem, the veriﬁers can be used to score either the entire path (called outcome-\\nbased approaches) [Cobbe et al., 2021], or each individual reasoning step (called process-based\\napproaches) [Uesato et al., 2022; Lightman et al., 2024].\\n3.2.5\\nRAG and Tool Use\\nRAG is generally employed when standard LLMs, which rely solely on pre-trained knowledge,\\nlack accuracy and depth in the generated text. By drawing from external databases and documents,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 141}, page_content='3.2 Advanced Prompting Methods\\n135\\nRAG can signiﬁcantly improve the quality of responses, ensuring they are both contextually rel-\\nevant and factually correct. Such an approach is particularly useful in scenarios that require high\\nfactual accuracy and up-to-date information, such as complex question answering.\\nThe concept of RAG has been mentioned several times in the previous sections and chapters.\\nFor completeness, we outline the key steps involved in RAG here.\\n• We prepare a collection of texts which are treated as an additional source of knowledge we\\ncan access.\\n• We retrieve relevant texts for a given query.\\n• We input both the retrieved texts and the query into an LLM, which is then prompted to\\nproduce the ﬁnal prediction.\\nSteps 1 and 2 can be implemented by using an external information retrieval system. For\\nexample, we can store the collection of texts in a vector database and then retrieve the most similar\\ntexts through vector-based search techniques. Since information retrieval is not the focus of this\\nchapter, we will assume that such systems are available off-the-shelf and use them directly.\\nHere we present how to prompt LLMs to make use of retrieved texts. To illustrate, consider\\nan example of using LLMs to answer the following question.\\nWhere will the 2028 Olympics be held?\\nWe can simply input this question into an online search engine. It will then return the relevant\\npieces of text found on the internet, for example,\\n(Wikipedia)\\nThe 2028 Summer Olympics, ofﬁcially the Games of the XXXIV Olympiad and com-\\nmonly known as Los Angeles 2028 or LA28, is an upcoming international multi-sport\\nevent scheduled to take place from July 14-30, 2028, in the United States. ...\\n(The Sporting News)\\nIn 2028, Los Angeles will become the third city, following London and Paris respectively,\\nto host three Olympics after hosting the Summer Games in 1932 and 1984. It will also\\nbe the ﬁrst time the United States has hosted an Olympic Games since the 2002 Winter\\nGames in Salt Lake City. ...\\n...\\nWe can use these retrieved texts as additional context, and prompt an LLM to generate a\\nresponse based on these texts. Below is an example RAG prompt.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 142}, page_content='136\\nPrompting\\nYour task is to answer the following question. To help you with this, relevant\\ntexts are provided. Please base your answer on these texts.\\nQuestion:\\nWhere will the 2028 Olympics be held?\\nRelevant Text 1:\\nThe 2028 Summer Olympics, ofﬁcially the Games of the XXXIV Olympiad and\\ncommonly known as Los Angeles 2028 or LA28 ...\\nRelevant Text 2:\\nIn 2028, Los Angeles will become the third city, following London and Paris\\nrespectively, to host three Olympics after ...\\n...\\nThe 2028 Olympics will be held in Los Angeles.\\nThis prompt assumes that the provided texts are relevant to the question and expects the LLM\\nto generate a faithful response using these texts. However, the information retrieval system may\\nsometimes provide irrelevant or incorrect texts, which may lead the LLM to produce an incorrect\\nanswer. One straightforward way to address this issue is to improve the accuracy of the informa-\\ntion retrieval system. Nevertheless, as with most AI systems, errors may still occur. Therefore, it\\nis also necessary to enhance the robustness of the LLM, so that it can make reasonable predictions\\neven when the input is inaccurate. Below is a new prompt that enables the LLM to be more faith-\\nful to the facts, and allows it to choose not to answer questions when the information provided is\\ninaccurate.\\nYour task is to answer the following question. To help you with this, relevant\\ntexts are provided. Please base your answer on these texts.\\nPlease note that your answers need to be as accurate as possible and faithful to\\nthe facts. If the information provided is insufﬁcient for an accurate response, you\\nmay simply output \"No answer!\".\\nQuestion:\\nWhere will the 2028 Olympics be held?\\nRelevant Text 1:\\nThe 2024 Summer Olympics, ofﬁcially the Games of the XXXIII Olympiad and\\nbranded as Paris 2024, were an international multi-sport event ...\\n...\\nNo answer!\\nIn this example, the LLM refuses to answer because the provided information is insufﬁcient and\\nirrelevant to the question.\\nBoth RAG and ﬁne-tuning are common methods for adapting LLMs using task-speciﬁc data.\\nStandard RAG is training-free and can be directly applied to LLMs. To further improve RAG, it'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 143}, page_content='3.2 Advanced Prompting Methods\\n137\\nis also possible to ﬁne-tune LLMs, though this will require some training effort. For example, we\\ncan ﬁne-tune LLMs using human-labelled data to supervise them in learning to refuse to answer.\\nNote that, while the examples shown above seem simple, RAG is not trivial. From the prompt\\nengineering perspective, different use cases may require different prompts, though our somewhat\\n“greedy” goal is to develop a universal prompting strategy that can adapt to different tasks. In\\nmany cases, we need to control how much we depend on the retrieved context to make predictions.\\nSometimes, LLMs must derive responses strictly from the provided texts, while at other times,\\nthey may need to generate responses using their pre-trained knowledge if the provided texts are\\ninsufﬁcient. There are many aspects of RAG, such as improvements to the retrieval systems, that\\ncannot be covered in this chapter. Interested readers can refer to surveys of RAG techniques for\\nmore information [Li et al., 2022; Gao et al., 2023c].\\nOne reason we discuss RAG here is that it can be broadly regarded as an instance of the\\ngeneral problem decomposition framework (see Section 3.2.2). RAG divides problem-solving\\ninto two steps. In the ﬁrst step, we collect relevant and supporting information for a given query\\nfrom various knowledge sources. In the second step, we use LLMs to generate responses based\\non the collected information. If we extend the concept of problem decomposition further, we\\nwill ﬁnd that many tasks requiring the use of external systems or tools can be treated as similar\\nproblems. One such example is tool use in LLMs. In many applications, LLMs need to employ\\nexternal databases, APIs, and even simulation tools to generate accurate responses. For example,\\nLLMs can access real-time data from ﬁnancial markets to provide up-to-date investment advice or\\nintegrate with healthcare databases to offer personalized medical insights. This integration extends\\nthe capabilities of LLMs by allowing them to interact with, and in some contexts, inﬂuence or\\ncontrol external systems. Consequently, LLMs function more as autonomous agents rather than\\nmere text generators [Franklin and Graesser, 1996].\\nThe issue of tool use is broad and vast. Here we narrow our discussion to tasks that can be fa-\\ncilitated by calling external APIs to solve some of the sub-problems [Parisi et al., 2022; Gao et al.,\\n2023b]. Consider again the example of asking an LLM to answer “Where will the 2028 Olympics\\nbe held?”. Suppose the LLM can access a web search tool. We can then prompt the LLM to\\nanswer the question with web search, like this\\nYour task is to answer the following question. You may use external tools, such\\nas web search, to assist you.\\nQuestion:\\nWhere will the 2028 Olympics be held?\\nThe information regarding this question is given as follows:\\n{tool: web-search, query: \"2028 Olympics\"}\\nSo the answer is: Los Angeles\\nHere {tool:\\nweb-search, query:\\n\"2028 Olympics\"} indicates a request to the web search\\nsystem using the query “2028 Olympics”. When the LLM sees this string, it executes a web search\\nand uses the result to replace the string. Then, in subsequent steps of prediction, the LLM uses\\nthis web search result as context to produce the correct answer.\\nConsider another example where we ask the LLM to solve a mathematical problem.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 144}, page_content='138\\nPrompting\\nProblem:\\nA swimming pool needs to be ﬁlled with water. The pool measures 10 meters\\nin length, 4 meters in width, and 2 meters in depth. Calculate the volume of the\\npool in cubic meters and then determine how many liters of water are needed to\\nﬁll it (considering 1 cubic meter equals 1000 liters).\\nSolution:\\nTo solve this problem, the LLM needs to ﬁrst calculate the volume of the\\npool by using the formula for the volume of a rectangular prism: Length×\\nWidth × Depth. Therefore, The volume is 10 m × 4 m × 2 m = {tool:\\ncalculator, expression: 10 * 4 * 2} m3. Next, to ﬁnd out how\\nmany liters of water are needed, the LLM multiplies the volume in cubic\\nmeters by 1000 (since 1 cubic meter equals 1000 liters). Thus, 80 ×1000\\n= {tool: calculator, expression: 80*1000} liters.\\nHere the string {tool:\\ncalculator, expression:\\n10 * 4 * 2} triggers the invocation\\nof a mathematical interpreter to calculate the result of the expression. Note that the result (i.e.,\\n80) will replace {tool:\\ncalculator, expression:\\n10 * 4 * 2} and can be referred to\\nin the following token predictions. For example, in the last step of problem-solving, 80 is used\\ninstead of {tool:\\ncalculator, expression:\\n10 * 4 * 2}.\\nA key difference between the tool use examples here and the previously discussed RAG ex-\\namples is that in tool use, external functions can be called during inference. In contrast, in RAG,\\nthe retrieved texts are provided before the prediction process begins. However, from the language\\nmodeling perspective, they are actually doing the same thing: before generating the ﬁnal result,\\nwe use external tools, either manually or automatically, to obtain sufﬁcient and relevant context. A\\nhigh-level interpretation of these approaches is that they both rely on an “agent” that can determine\\nwhere and how to call external functions to generate the context necessary for prediction.\\nAn issue with tool use is that the original LLMs are not trained to generate the necessary\\nmarkers for tool use. Therefore, we need to ﬁne-tune the LLMs to adapt them for these tasks\\n[Schick et al., 2024]. As this chapter focuses on prompting, we will not present the details of this\\nﬁne-tuning process. To put it simply, we ﬁrst need to annotate data. For each ﬁne-tuning example,\\nwe replace parts of the output that require the use of external tools with predeﬁned commands\\nor markers. Then, we use this labeled data to ﬁne-tune the parameters of the LLM as usual. As\\na result, the LLM can gain the ability to generate commands for calling external tools. During\\ninference, we can execute these tool use commands in the model outputs to get assistance from\\nexternal tools.\\n3.3\\nLearning to Prompt\\nSo far in this chapter, we have considered several basic prompting strategies and various reﬁne-\\nments to them. However, all the prompts we have discussed were designed manually. This leads\\nto a number of problems: First, designing high-quality prompts is inherently difﬁcult and requires\\nsubstantial manual effort. For example, extensive experimentation with different prompts is often\\nneeded to identify the most effective ones. Since different LLMs may respond better to certain'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 145}, page_content='3.3 Learning to Prompt\\n139\\ntypes of prompts, developing universally effective prompts can be even more resource-intensive.\\nSecond, manual prompt design relies heavily on human expertise, which can limit the diversity\\nof approaches and overlook potentially effective prompts that are not immediately obvious to hu-\\nmans. Third, prompts created by humans can be complex and redundant, leading to longer inputs\\nfor LLMs and higher computational costs.\\nIn this section, we discuss techniques for automated prompting. These methods aim to auto-\\nmatically create, optimize, and represent prompts so that the downstream tasks can be addressed\\nmore effectively and efﬁciently. In particular, we consider three issues here.\\n• How can we automate the process of designing and optimizing prompts for LLMs?\\n• Are there other forms of representing prompts beyond strings, and how can we learn such\\nrepresentations?\\n• How can we make prompts more concise and compact, thereby reducing their complexity\\nand length?\\nNote that there are many settings in which we can investigate these issues. For example, we\\nmight specify that prompts are developed speciﬁcally for a particular LLM, or that the develop-\\nment is independent of the LLM used. These settings can lead to different methods and application\\nscenarios, but these methods may overlap in some ways. In the following discussion, we will cover\\nseveral different scenarios and discuss the connections between various methods.\\n3.3.1\\nPrompt Optimization\\nGiven that prompt design is difﬁcult and labor-intensive, it is desirable to use machine learning\\nmodels to discover the optimal prompt for a speciﬁc task (call it automatic prompt design or\\nprompt optimization). This approach can broadly be regarded as an instance of automated ma-\\nchine learning (AutoML), which aims to reduce or eliminate the need for expert-driven manual\\ndesign of machine learning models. Although our focus here is on the design of prompts, prompts\\nthemselves are discrete structures. Therefore, designing prompts is very similar to designing ma-\\nchine learning models, such as discrete model architectures. Perhaps one of the most related ﬁelds\\nis neural architecture search (NAS), where the most optimal neural networks are identiﬁed by\\nexploring a space of possible neural networks [Zoph and Le, 2016; Elsken et al., 2019]. If we con-\\nsider prompt optimization as a search process, then we can describe a general prompt optimization\\nframework involving the following components:\\n• Prompt Search Space. This deﬁnes all possible prompts that the algorithms can explore.\\nFor example, one can edit some seed prompts to generate a set of diverse candidate prompts.\\n• Performance Estimation. Once an prompt is chosen, it needs to be evaluated. For example,\\na straightforward way is to input it to an LLM and measure its performance on a validation\\nset.\\n• Search Strategy. The search process is generally the same as that used in many AI sys-\\ntems. At each step, the system explores a set of promising prompts in the search space and'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 146}, page_content='140\\nPrompting\\nevaluates them. This process continues as more prompts are explored. The outcome of the\\nsearch is the best-performing prompt observed until the search stops.\\nThis is a very general framework, and different prompt optimization systems can vary in their\\ndesign of each component. A widely-used approach is to use LLMs as the basis to develop these\\ncomponents. Initially, a few prompts are provided. Then, the following process is iterated until\\na stopping criterion is met: 1) the prompts are evaluated on a validation set; 2) a candidate pool\\nis maintained by keeping only the most promising prompts; and 3) new prompts are created by\\nemploying LLMs to infer similar prompts from this candidate pool. One beneﬁt of this approach is\\nthat it allows us to use off-the-shelf LLMs to perform the tasks mentioned above without the need\\nfor substantial system development. To achieve this, we can prompt or ﬁne-tune LLMs to adapt\\nthem to these tasks. Here we consider Zhou et al. [2023c]’s method for illustrating LLM-based\\nprompt optimization. It involves the following steps.\\n• Initialization. Let C represent the pool of the candidate prompts we intend to explore. The\\nﬁrst step is to add initial prompts into C. We can do this in several ways. A simple method is\\nto create such prompts by hand for a given task. However, in many cases where humans have\\nlimited knowledge about how to write effective prompts for the task, developing prompts\\nbecomes challenging.\\nIn these cases, it is desirable to use LLMs to generate prompts.\\nFor example, we can directly instruct LLMs to produce prompts, providing them with a\\ndescription of the task.\\nYou are given a task to complete using LLMs. Please write a prompt to\\nguide the LLMs.\\n{∗task-descripion∗}\\nThis method is straightforward, but it still requires a human-provided description of the task.\\nAn alternative method is to use LLMs to generate prompts given examples of the input and\\noutput of the task. Here is a prompt template.\\nYou are provided with several input-output pairs for a task. Please write\\nan instruction for performing this task.\\nInput: {∗input1∗} Output: {∗output1∗}\\nInput: {∗input2∗} Output: {∗output2∗}\\n...\\nAs such, LLMs can infer the corresponding instruction for the task from the provided inputs\\nand outputs.\\n• Evaluation. Once we obtain the candidate pool C, we need to evaluate the prompts in C.\\nOne method is to feed each prompt into an LLM and assess the results on the downstream'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 147}, page_content='3.3 Learning to Prompt\\n141\\ntask. For example, we can evaluate the output of the LLM given an input using a pre-deﬁned\\nmetric, or alternatively, use the log-likelihood of the output as a measure of the quality of\\nthe prompt.\\n• Pruning. If C contains a large number of prompts, it is reasonable to prune the unpromising\\nprompts within it, thus reducing the computational burden in subsequent steps. This is a\\nstandard pruning problem. Given the evaluation score for each prompt, a simple method is\\nto keep only a certain percentage of the prompts and discard the rest.\\n• Expansion. Expansion is a key operation in search algorithms used to explore different\\nstates in the search space. The expansion operation here can be deﬁned as a function\\nC′\\n=\\nExpand(C, f)\\n(3.10)\\nwhere C′ is the set of new prompts generated from C using the model f. If we consider f\\nas an LLM, we can perform the expansion operation by instructing f to generate new and\\nrelevant prompts based on C. Below is an example.\\nBelow is a prompt for an LLM. Please provide some new prompts to per-\\nform the same task.\\nInput: {∗prompt∗}\\nThen, we replace C with C′. The steps of evaluation, pruning and expansion can be re-\\npeated, and so we can gradually explore a wider range of prompts.\\nIn prompt optimization, the expansion step plays a key role, as it deﬁnes how we explore\\nthe search space, and our goal is to ﬁnd optimal results with minimal effort. One improvement\\nto this step is to treat the problem as a paraphrasing task. A simple method is to apply off-the-\\nshelf paraphrasing systems, either based on LLMs or other models, to transform input prompts\\ninto semantically equivalent forms [Jiang et al., 2020]. Alternatively, we can deﬁne speciﬁc edit\\noperations, such as insertions and modiﬁcations, for each token. A given prompt can be edited\\ninto new prompts by applying these operations [Prasad et al., 2023]. Also, further evaluation and\\npruning can be applied to ﬁlter out low-quality prompts. In addition to framing prompt generation\\nas a paraphrasing problem, we can improve the quality of prompts during expansion by learning\\nfrom feedback [Pryzant et al., 2023]. This approach is somewhat related to the self-reﬁnement\\nissue discussed in Section 3.2.3. An LLM can be used to generate feedback on an input prompt,\\nwhich is then revised based on this feedback. This feedback-and-revision cycle can be repeated\\nmultiple times until the result converges or the desired outcome is achieved.\\nAnother approach to prompt optimization is to apply classic optimization techniques. For\\nexample, the problem can be framed as an evolutionary computation problem, where prompts\\nare treated as candidates that evolve generation by generation as the optimization progresses\\n[Guo et al., 2024]. Since many powerful optimization algorithms have been developed in related\\nﬁelds, they can be directly applied to this problem.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 148}, page_content='142\\nPrompting\\nIn practice, we might be tempted to use existing LLM APIs to implement the steps described\\nabove. Such an approach, however, would be strongly dependent on the inference and in-context\\nlearning abilities of the LLMs. If these LLMs are not strong and lack adaptation to the tasks, they\\nmay introduce errors into search, for example, generating incorrect prompts during expansion. In\\nsuch cases, it is preferable to train models that are better suited to the tasks. One approach in\\nthis research direction appeals to reinforcement learning, which has been widely used in solving\\ndiscrete decision making and optimization problems. For example, Deng et al. [2022] developed\\na prompt generator by integrating an FFN-based adaptor into an LLM. The prompt generator is\\ntrained as a typical policy network, but only the parameters of the adaptor are updated while the\\nremaining parameters of the model are kept unchanged. During training, the reward is obtained by\\ntesting the generated prompts using another LLM, similar to the evaluation method as discussed\\nabove. Once the training is complete, the prompt generator is then employed to generate new\\nprompts.\\nNote that, in our discussion here, prompts are simply seen as sequences of tokens, and the out-\\nput of prompt optimization is such a sequence. However, in a strict sense, prompts have complex\\nstructures and include different ﬁelds such as user input, instruction, and demonstration. While\\nour discussed approaches are mostly general, much work in prompt optimization has focused on\\nlearning better instructions for prompting. Speciﬁcally, the goal is to generate instructions that\\neffectively guide LLMs based on a given task. Of course, the concept of prompt optimization\\ncan also be extended to learning other parts of prompts. For example, there has been substan-\\ntial research interest in learning to select or generate demonstrations in CoT [Liu et al., 2022;\\nRubin et al., 2022; Zhang et al., 2023b]. One of the differences between learning instructions\\nand learning demonstrations is that generating high-quality demonstrations using LLMs is rela-\\ntively easy and the focus of learning demonstrations is typically on how to sample appropriate\\ndemonstrations from a pool of candidates. In contrast, the difﬁculty in learning instructions is\\npartly because pre-trained LLMs are not suited to predict the quality of instructions, and testing\\nthese instructions on downstream tasks is computationally expensive. This makes the optimization\\nmethods costly to apply, and exploring a wide variety of instructions poses signiﬁcant challenges.\\n3.3.2\\nSoft Prompts\\nAlthough developing natural language prompts, either manually or automatically, is a straight-\\nforward and widely applied approach, it presents some problems. One problem is that natural\\nlanguage prompts can be complex and lengthy, resulting in signiﬁcant computational burdens\\nwhen processed via LLMs. In many applications, users may need to perform a task repeatedly,\\nand inputting the same long prompt into the LLMs a large number of times is clearly inefﬁcient.\\nAnother problem is that while prompts are typically represented as discrete token sequences (call\\nthem hard prompts) in regular LLM input, the LLMs encode them as low-dimensional real-\\nvalued vectors. This raises the question of whether there are more compact and efﬁcient ways to\\nrepresent prompts.\\nIn this subsection, we introduce the concept of soft prompts, which can be viewed as hidden,\\ndistributed representations of prompts. When prompting LLMs, we are concerned with commu-\\nnicating tasks or questions to elicit the desired responses. We can deﬁne hard prompts as explicit,\\npredeﬁned text sequences that users input directly into LLMs to guide the responses. In contrast,\\nwe can think of soft prompts as implicit, adaptable prompting patterns embedded within LLMs.\\nUnlike hard prompts, which are expressed in natural language and should be understandable for'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 149}, page_content='3.3 Learning to Prompt\\n143\\n...\\nTranslate this\\ninto Chinese\\n.\\nI\\nhave\\na\\ncat\\n.\\n...\\nTransformer\\n...\\nhj\\nhj+1 hj+2 hj+3 hj+4 hj+5 hj+6 hj+7 hj+8 hj+9\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\nHard Prompt (Instruction)\\nSoft Prompt\\nFig. 3.3: Illustration of hard and soft prompts. Here the hard prompt is the instruction we input to the LLM for\\nperforming the task. The LLM encodes this instruction as usual, and the intermediate representations corresponding to\\nthe instruction can be viewed as some sort of soft prompt.\\nhumans, soft prompts are encoded in a format that is more comprehensible to the model rather\\nthan to humans. To illustrate, consider a simple prompt\\nTranslate the sentence into Chinese.\\nConsider it done!\\nHere, the instruction “Translate the sentence into Chinese” can be seen as a hard prompt, denoted\\nby the token sequence c1...c5. By feeding these tokens into an LLM, they are transformed into a\\nsequence of real-valued vectors h1...h5, each corresponding to a token. We can roughly think of\\nh1...h5 as a soft prompt, as illustrated in Figure 3.3.\\nWhile the above example shows that soft prompts can be generated by transforming hard\\nprompts, there is not necessarily a direct correspondence between them. In fact, we do not even\\nneed to interpret soft prompts using meaningful text. They are instead simply hidden states in\\nLLMs and can be learned as standard parameters of the models through continuous optimiza-\\ntion. Such a treatment allows us to explore prompting methods beyond text. As another beneﬁt,\\nsoft prompts provide dense, low-dimensional, and learnable representations for encoding how\\nwe guide LLMs to generate speciﬁc outputs. The training and application of these representa-\\ntions require signiﬁcantly lower computational costs than those required for processing long hard\\nprompts. This approach would be of great practical value in LLM inference applications where\\nthe same prompt is repeatedly used.\\n3.3.2.1\\nAdapting LLMs with Less Prompting\\nOne obvious way to adapt an LLM for a particular task is to simply ﬁne-tune the model using\\nlabeled data. This leads to a variety of LLM alignment methods, such as supervised ﬁne-tuning,\\nwhich update the model parameters by aligning the responses to given prompts with supervision\\nsignals. Fine-tuned LLMs embed task-related information in model parameters, and thus these'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 150}, page_content='144\\nPrompting\\nmodels can response correctly when dealing with similar prompts with those in ﬁne-tuning.\\nIf we take this idea further, we can expect LLMs to absorb the knowledge about prompting\\nof a task as much as possible during ﬁne-tuning. Consequently, the prompting information is\\npartially captured in the model parameters, and the ﬁne-tuned LLMs can perform the task with\\nless prompting. Here we consider a simple form of prompt, where only an instruction (denoted by\\nc) and an user input (denoted by z) are included. A prompt can be expressed using the following\\ntuple\\nx\\n=\\n(c, z)\\n(3.11)\\nGiven a set of prompt-response pairs D = {(x, y)}, the objective of ﬁne-tuning is to minimize\\nthe total loss incurred over this set. A popular method is to minimize the negative log-likelihood\\n(i.e., maximize the log-likelihood) with respect to the model parameters θ:\\nˆθ\\n=\\narg max\\nθ\\nX\\n(x,y)∈D\\nlog Prθ(y|x)\\n=\\narg max\\nθ\\nX\\n(x,y)∈D\\nlog Prθ(y|c, z)\\n(3.12)\\nwhere Prθ(·|·) is the probability predicted by an LLM with the parameters θ9.\\nIn general, the instruction in each ﬁne-tuning example should follow the guideline of prompt\\ndesign, for example, a good instruction should be as clear as possible and provide a detailed\\ndescription of the task. However, the method described in the above equation does not restrict the\\ninstruction to any particular form. This ﬂexibility allows us to instruct LLMs in any way that we\\nwant. Consider an example where we intend to instruct LLMs to translate an English sentence\\ninto Chinese. Of course, as mentioned earlier in this chapter, we can prompt LLMs using the\\ninstruction\\nTranslate the following sentence from English to Chinese.\\nIf we want the instruction to be simpler, we may rephrase it into a simpler form\\nTranslate this into Chinese.\\nEven, we can deﬁne the instruction as a single phrase\\nTranslate!\\nWith certain ﬁne-tuning effort, we can adapt LLMs to follow any of these instructions. From\\na efﬁcient prompting perspective, there are computational advantages in simplifying instructions\\nin prompting. For example, we can use simple instructions like “Translate!” to perform tasks\\nthat would typically require more complex and detailed instructions. This can make subsequent\\n9In practice, we initialize θ with the parameters obtained from pre-training, and then adjust θ moderately to ensure\\nthat the results after ﬁne-tuning do not deviate too much from the pre-trained results.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 151}, page_content='3.3 Learning to Prompt\\n145\\nFull Context\\nUser Input\\n+\\nPrt(y|c, z)\\nc\\nz\\ny\\nTeacher Model:\\nSimpliﬁed Context\\nUser Input\\n+\\nPrs(y|c′, z)\\nc′\\nz\\ny\\nStudent Model:\\nLoss\\nFig. 3.4: Illustration of context distillation [Snell et al., 2022]. The teacher model is a standard LLM, which takes both\\nthe context and the user input as model input and produces a prediction as model output. Then, we simplify the context\\n(e.g., simplifying the instruction in prompting) and use the student model to make predictions based on the simpliﬁed\\ncontext and the user input. The student model is trained by minimizing the loss between the predictions produced by\\nthe two models.\\nprompting during inference much easier. On the other hand, ﬁne-tuning LLMs with overly simpli-\\nﬁed instructions may be harmful to the generalization of the models. Since simpliﬁed instructions\\ncan lead to a loss of information, it is more likely that the LLMs will overﬁt the ﬁne-tuning data\\nand fail to generalize beyond those instructions. In scenarios involving both complex and simpli-\\nﬁed instructions for ﬁne-tuning, this problem is more severe because the labeled data available for\\nﬁne-tuning is usually limited, and accommodating a variety of instructions is costly.\\nAn alternative way to adapt LLMs for simpliﬁed instructions is through knowledge distillation.\\nAs an example, we consider the context distillation method [Snell et al., 2022]. The goal of this\\nmethod is to learn a student model that can make use of simpliﬁed instructions from a well-trained\\ninstruction-following teacher model. Figure 3.4 shows an illustration of this approach. Building\\nthe teacher model follows a standard ﬁne-tuning process: we ﬁrst collect a certain amount of\\ndata that includes instructions, user inputs, and correct responses, and then we continue to train a\\npre-trained model with this dataset. For building the student model, we need to construct a new\\ndataset D′ where each sample is a tuple consisting of an instruction, a corresponding simpliﬁed\\ninstruction, and a user input, denoted by x′ = (c, c′, z). Knowledge distillation is performed by\\nminimizing a loss function deﬁned on the outputs of the teacher and student models\\nˆθ\\n=\\narg min\\nθ\\nX\\nx′∈D′\\nLoss(Prt(·|·), Prs\\nθ(·|·), x′)\\n(3.13)\\nwhere Prt(·|·) denotes the pre-trained teacher model, and Prs\\nθ(·|·) denotes the student model with\\nthe parameters θ. To keep the notation simple we will write Loss(Prt(·|·), Prs\\nθ(·|·), x) as Loss\\nfor short. A commonly-used loss is the sequence-level loss, which has the basic form:\\nLoss\\n=\\nX\\ny\\nPrt(y|c, z) log Prs\\nθ(y|c′, z)\\n(3.14)\\nBut this function is computationally infeasible because it requires summing over an exponen-\\ntially large number of outputs. A variant of this method is to train the student model using outputs\\ngenerated by the teacher model. For each sample, we use the teacher model to produce an output'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 152}, page_content='146\\nPrompting\\nˆy = arg maxy log Prt(y|c, z). Then we consider ˆy as the target for learning, and the loss function\\nis given by\\nLoss\\n=\\nlog Prs\\nθ(ˆy|c′, z)\\n(3.15)\\nAlternatively, we can minimize the distances between the probability distributions outputted\\nby the two models [Askell et al., 2021]. For example, the loss function can be deﬁned as the KL\\ndivergence between the two output distributions\\nLoss\\n=\\nKL(Pt || Ps\\nθ)\\n(3.16)\\nwhere\\nPt\\n=\\nPrt(·|c, z)\\n(3.17)\\nPs\\nθ\\n=\\nPrs\\nθ(·|c′, z)\\n(3.18)\\nAlthough we have restricted ourselves to knowledge distillation for instructions, the approaches\\ndiscussed here are general. By learning from the outputs of the teacher model, the knowledge in\\nprompting can be distilled into the parameters of the student model. Therefore, the distilled model\\ncan be considered as encoding some sort of soft prompt. This method can be applied to many\\nother problems in prompt learning, such as compressing long contexts and learning soft prompts\\nas speciﬁc components of LLMs.\\n3.3.2.2\\nLearning Soft Prompts for Parameter-efﬁcient Fine-tuning\\nUpdating all parameters is a common method for adapting LLMs to tasks of interest. Although\\nﬁne-tuning is considered computationally cheaper than pre-training, it is still costly to apply in\\npractice. This issue motivates the development of parameter-efﬁcient ﬁne-tuning methods, which\\naim to minimize the number of parameters that need to be updated.\\nOne approach, known as preﬁx ﬁne-tuning, is to append a series of trainable vectors, or\\npreﬁxes, at the beginning of the input of each Transformer layer [Li and Liang, 2021]. These\\npreﬁxes can be thought of as soft prompts that serve as additional context to guide the behavior\\nof the model under speciﬁc tasks. During ﬁne-tuning, we need only to learn the preﬁxes for\\nembedding task-speciﬁc knowledge. Thus, this method is efﬁcient because it only modiﬁes a\\nsmall part of the model rather than adjusting the entire set of model parameters.\\nSpeciﬁcally, let the input of a layer at depth l be denoted by Hl = hl\\n0hl\\n1...hl\\nm. The output of\\nthe layer can be expressed as\\nHl+1\\n=\\nLayer(Hl)\\n(3.19)\\nIn preﬁx ﬁne-tuning, we extend the sequence hl\\n0hl\\n1...hl\\nm by adding a few vectors at beginning,\\nwhich we denote as pl\\n0pl\\n1...pl\\nn. Hence Hl can be written in the form\\nHl = pl\\n0 pl\\n1 ... pl\\nn\\n|\\n{z\\n}\\ntrainable\\nhl\\n0 hl\\n1 ... hl\\nm\\n|\\n{z\\n}\\nprevious layer output\\n(3.20)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 153}, page_content='3.3 Learning to Prompt\\n147\\nThe output of the layer is the last m + 1 representations.\\nHl+1\\n=\\nLayer(Hl)[−m −1 :]\\n=\\nhl+1\\n0\\nhl+1\\n1\\n... hl+1\\nm\\n(3.21)\\nwhere [−m−1 :] denotes the slicing operation that extracts the last m+1 elements of a sequence.\\nGiven Hl+1, the input of the next layer can be expressed in the same form of Eq. (3.20):\\nHl+1\\n=\\npl+1\\n0\\npl+1\\n1\\n... pl+1\\nn\\nHl+1\\n=\\npl+1\\n0\\npl+1\\n1\\n... pl+1\\nn\\nhl+1\\n0\\nhl+1\\n1\\n... hl+1\\nm\\n(3.22)\\nHere each pi ∈Rd can be seen as a learnable parameter. During training, pl\\n0pl\\n1...pl\\nn are trained\\nas usual, and the parameters of the original Transformer model are kept ﬁxed.\\nFigure 3.5 shows an illustration of preﬁx ﬁne-tuning for a translation task. Here, only the preﬁx\\nvectors pl\\n0 and pl\\n1 are updated by receiving the error gradients from the output (i.e., the Chinese\\ntranslation). By adjusting these vectors for the translation task, the model adapts accordingly. This\\nmakes pl\\n0 and pl\\n1 serve as prompts which activate the LLM to perform the task without needing\\nexplicit input prompts like “Translate the following sentence from English to Chinese”. At test\\ntime, we prepend the optimized pl\\n0 and pl\\n1 to the layer, and the LLM will then translate the input\\nsentence. Note that preﬁx ﬁne-tuning introduces additional L × n × d parameters, where L is the\\nnumber of layers, n is the number of preﬁxes, and d is the dimensionality of each preﬁx. However,\\nthis number is much smaller compared to the total number of parameters in the LLM, making the\\nﬁne-tuning process highly efﬁcient.\\nWhile preﬁx ﬁne-tuning is simple, it still requires modiﬁcations to LLMs. Alternatively, sep-\\narating soft prompts from the LLMs allows us to preserve the original model architecture, making\\nit more efﬁcient for deployment across different tasks without the need to adjust the core model.\\nOne such method is prompt tuning [Lester et al., 2021]. Like preﬁx ﬁne-tuning, prompt tuning\\nincorporates trainable vectors so that LLMs can adapt to given tasks by adjusting these vectors.\\nHowever, prompt tuning differs in that it modiﬁes only the embedding layer.\\nRecall that in LLMs each input token zi is represented by an embedding ei. These embeddings\\nare generally learned through a token embedding model and are then used as the real inputs to the\\nLLMs, replacing the symbolically represented tokens. In prompt tuning, a number of pseudo\\nembeddings p0...pn are added at the beginning of the token embedding sequence. So the actual\\ninput to the LLMs can be expressed as\\np0 p1 ... pn\\n|\\n{z\\n}\\ntrainable\\ne0 e1 ... em\\n|\\n{z\\n}\\ntoken embeddings\\nNote that a pseudo embedding needs not to correspond to any token in natural language. Instead\\nthese embeddings can be seen as “soft prompt embeddings” that serve to condition the LLMs.\\nBy training soft prompt embeddings on task-speciﬁc data, they learn to interact adaptively with\\nthe token embeddings e0...em and guide the behavior of LLMs. Since prompt tuning does not\\nchange the underlying parameters of pre-trained LLMs, it is considered a lightweight and efﬁcient\\nmethod of ﬁne-tuning, improving task-speciﬁc performance while maintaining their generalization\\ncapabilities. See Figure 3.6 for an illustration of prompt tuning.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 154}, page_content='148\\nPrompting\\npl\\n0\\npl\\n1\\nhl\\n0\\nhl\\n1\\nhl\\n2\\nhl\\n3\\nhl\\n4\\nLayer l\\npl−1\\n0\\npl−1\\n1\\nhl−1\\n0\\nhl−1\\n1\\nhl−1\\n3\\nhl−1\\n4\\nhl−1\\n5\\nLayer l −1\\npl+1\\n0\\npl+1\\n1\\nhl+1\\n0\\nhl+1\\n1\\nhl+1\\n3\\nhl+1\\n4\\nhl+1\\n5\\nLayer l + 1\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\nLoss\\nLoss\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\nLook\\nout\\n!\\n小心\\n!\\ntrainable preﬁxes\\nUser Input\\nLLM Prediction\\nSoft Prompt\\nFig. 3.5: Illustration of preﬁx ﬁne-tuning for a translation task (Look out! →小心!). For each layer, we add two\\npreﬁxes pl\\n0 and pl\\n1 at the beginning. The LLM is trained to minimize the loss on the predictions given the input.\\nDuring this process, only the preﬁxes are optimized while the rest of the parameters remain ﬁxed. Therefore, the model\\ncan adapt to the given task in a very efﬁcient manner. At inference time, the LLM works with optimized preﬁxes, and\\ncan perform the task without the need of explicit hard prompts.\\nSince p0 p1 ... pn is itself a sequence, we can employ sequence models to better represent\\nit. For example, a Transformer model can encode this sequence, and the resulting representation\\ncan then be used as the input to the LLM. In other words, we can develop an additional model\\nfor encoding soft prompts. Another way to improve prompting is by combining soft and hard\\nprompts, thereby taking advantage of both types [Liu et al., 2023b]. In the embedding sequence,\\nwe can arrange or intersperse these prompts. This would result in different prompt patterns. For\\nexample, a simple pattern that uses both two types of prompt is\\np0\\np1\\n· · ·\\npn\\nq0\\nq1\\n· · ·\\nqm′\\ne0\\ne1\\n· · ·\\nem\\nc0\\nc1\\n· · ·\\ncm′\\nz0\\nz1\\n· · ·\\nzm\\nSoft Prompt\\nHard Prompt\\nUser Input and Response\\nwhere c0...cm′ denotes the hard prompt and q0...qm′ denotes the corresponding embedding se-\\nquence.\\nHere we have considered methods for inserting soft prompts in LLMs. But we skip the details\\nof training these soft prompts and assume that the reader is familiar with the standard supervised\\nlearning process, that is, maximizing the likelihood of the correct model output given the model'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 155}, page_content='3.3 Learning to Prompt\\n149\\nLayer l −1\\nLayer l\\nLayer l + 1\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\np0\\np1\\ne0\\ne1\\ne2\\ne3\\ne4\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\nLoss\\nLoss\\nLook\\nout\\n!\\n小心\\n!\\ntrainable prompt\\nembeddings\\nUser Input\\nLLM Prediction\\nSoft Prompt\\nFig. 3.6: Illustration of prompt tuning for a translation task (Look out! →小心!). Instead of using ﬁxed textual\\nprompts, soft prompts are learnable embeddings that are added at the beginning of the embedding sequence. During\\nﬁne-tuning, only these prompt embeddings are optimized to efﬁciently adapt the LLM to the given task. Once opti-\\nmized, the prompt embeddings are used to instruct the LLM to perform the task as new data arrives.\\ninput. In fact, learning soft prompts can be related to many issues in LLM ﬁne-tuning. For exam-\\nple, if we consider it as a context compression problem, we can apply the knowledge distillation\\nmethods described previously. In Mu et al. [2024]’s work, prompts are compressed and repre-\\nsented as a few pseudo tokens, which are appended to each input sequence. The embeddings of\\nthese pseudo tokens are optimized to mimic the predictions of a standard-prompted model. In\\nother words, the prompting knowledge is distilled from a teacher model into the pseudo tokens.\\nBroadly speaking, many parameter-efﬁcient ﬁne-tuning methods can be thought of as learning\\nsome sort of soft prompt [Lialin et al., 2023]. When we ﬁne-tune a part of an LLM for a task, this\\nprocess can essentially be seen as injecting task-related prompting information into that speciﬁc\\npart of the model. Another widely-used approach to parameter-efﬁcient ﬁne-tuning is to add an\\nadaptor layer between the existing model layers. This approach allows us to ﬁne-tune only the\\nadaptor layer on speciﬁc tasks without altering the underlying architecture or retraining the entire\\nmodel. In this sense, adaptor layers can be viewed as soft prompts that encode prompting and task-\\nrelated information and interact with the original LLM to help it adapt. To summarize, Figure 3.7\\nshows a comparison of different methods of using soft prompts in LLMs.\\n3.3.2.3\\nLearning Soft Prompts with Compression\\nAnother approach to learning soft prompts is from the perspective of compression. As a simple\\nexample, consider the problem of approximating a long context using a continuous representation\\n[Wingate et al., 2022]. Suppose we have a user input z and its context c (such as long instructions\\nand demonstrations). Now we want to develop a compressed representation of the context, denoted'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 156}, page_content='150\\nPrompting\\nLLM\\n(a) Soft Prompts as Preﬁxes\\nLLM\\n(b) Soft Prompts as Inputs (Embeddings)\\nLLM\\nLayer\\n(c) Fine-tuning Parts of the Model\\nLLM\\nAdaptor\\n(d) Fine-tuning the Adaptor\\nFig. 3.7: Illustrations of using soft prompts in LLMs. Here tunable soft prompts are shown in blue, and components\\nwhose parameters are ﬁxed during ﬁne-tuning are shown in gray. In sub-ﬁgure (a), soft prompts are preﬁxes appended\\nto each layer of the LLM. In sub-ﬁgure (b), soft prompts are used as input embeddings for the LLM. In sub-ﬁgures (c)\\nand (d), soft prompts are broadly treated as components of the model that are ﬁne-tuned for task adaptation.\\nby σ, such that the prediction based on z and σ is as close as possible to the prediction based on z\\nand c. This goal can be expressed in the form\\nˆσ\\n=\\narg min\\nσ\\ns(ˆy, ˆyσ)\\n(3.23)\\nwhere ˆy = arg maxy Pr(y|c, z) and ˆyσ = arg maxyσ Pr(y|σ, z) are the LLM predictions given\\nthe full context and the compressed context, respectively. The function s(·, ·) typically represents\\na loss or similarity measure, aiming to minimize the difference in predictions between the two\\ncontext representations.\\nOne general framework for achieving this is knowledge distillation, where ˆy and ˆyσ can be\\nseen as the predictions of the teacher model and the student model, respectively. This formal-\\nization links our discussion to the context distillation problem discussed earlier. The training\\nobjective can be obtained by analogy with Eqs. (3.15) and (3.16). For example, a simple training\\nobjective is given by\\nˆσ\\n=\\narg max\\nσ\\nlog Pr(ˆy|σ, z)\\n(3.24)\\nAlternatively, we can minimize the KL divergence between the output distributions, giving\\nˆσ\\n=\\narg min\\nσ\\nKL(Pr(·|c, z) || Pr(·|σ, z))\\n(3.25)\\nThe difference with the models in Eqs. (3.15) and (3.16) is that here the compressed context is\\nrepresented as real-valued vectors (call them prompt embeddings), rather than as normal tokens.\\nBy applying the above methods, we distill the context from the token sequence c into the embed-\\ndings σ. Note that the teacher model Pr(·|c, z) and the student model Pr(·|σ, z) may not share\\nthe same architecture or model settings. In practice, we generally wish for the teacher model to be'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 157}, page_content='3.3 Learning to Prompt\\n151\\nσ<i\\n1\\nσ<i\\n2\\nei\\n1\\nei\\n2\\nei\\n3\\nei\\n4\\n⟨e1⟩\\n⟨e2⟩\\nzi\\n1\\nzi\\n2\\nzi\\n3\\nzi\\n4\\n⟨g1⟩\\n⟨g2⟩\\nTransformer Layers\\nh<i\\n1\\nh<i\\n1\\nh1\\nh2\\nh3\\nh4\\nσ<i+1\\n1\\nσ<i+1\\n2\\nSoft Prompts\\nat Step i −1\\nSoft Prompts\\nat the Current Step\\nFig. 3.8: Illustration of compressing a context segment into soft prompts (κ = 2 and mi = 4). The input to the\\nLLM includes the soft prompts from the previous step (σ<i\\n1\\nand σ<i\\n2 ), the tokens of the segment (z1, z2, z3, and z4),\\nand the summary tokens (⟨g1⟩and ⟨g2⟩). Given these, the LLM operates as usual. We then extract the outputs at the\\nlast Transformer layer that correspond to the summary tokens. These outputs can be viewed as the soft prompts that\\naccumulated up to this segment.\\nstronger, while the student model should be smaller and more efﬁcient.\\nWhile compressing full context into continuous representations is a straightforward approach\\nto learning soft prompts, it requires a teacher model that can deal with long input sequences. In\\nmany cases, however, the context is so long that applying an LLM is too costly or infeasible.\\nModeling long input sequences can fall under the broad family of efﬁcient methods for long-\\ncontext LLMs. Many techniques have been developed to address this issue. For example, one can\\nuse a ﬁxed-size KV cache to store the past information at each step during inference. Efﬁcient\\nTransformer architectures and long-context LLMs have been intensively discussed in this book.\\nFor more detailed discussions of these topics, interested readers can refer to Chapter 2.\\nThere are also methods speciﬁcally designed to compress long context into soft prompts. Here\\nwe consider Chevalier et al. [2023]’s method as an example. The basic idea is that we learn\\nsoft prompts gradually by accumulating the ﬁxed-size context representation over the context\\nsequence. Given a long context, we ﬁrst divide it into a number of segments z1, ..., zK. We\\nthen process these segments in sequence, each time generating a representation of the context we\\nhave processed so far, denoted by σ<i+1. To do this, a few summary tokens ⟨g1⟩, ..., ⟨gκ⟩are\\nintroduced. At each step, we take a segment zi = zi\\n1...zi\\nmi, along with the previous context rep-\\nresentation σ<i and the summary tokens ⟨g1⟩, ..., ⟨gκ⟩as input, and use an LLM to produce the\\ncorresponding hidden representation sequence at the last Transformer layer. An example of this\\nprocess is illustrated in Figure 3.8.\\nHere σ<i is essentially a memory. The model operates in an RNN fashion. Each time we take\\na segment and update this memory by encoding both the previous memory state and the segment.\\nTherefore, the σ<i produced at the last segment is a representation of the entire context sequence.\\nThe Transformer model for learning these representations can be a standard LLM but we need to\\nﬁne-tune it to adapt to this context representation task.\\nNote that here we simply consider prompt and context as similar terms, even though they are\\nnot the same. Although we are somewhat “misusing” the concept prompt, we can often view it as\\na type of context. From this perspective, the methods discussed here can be applied to general text\\ncompression problems.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 158}, page_content='152\\nPrompting\\n3.3.3\\nPrompt Length Reduction\\nWhile soft prompts provide dense, hidden representations, they are not directly interpretable. The\\nlack of interpretability can be a signiﬁcant barrier for users trying to understand how their inputs\\ninﬂuence LLM outputs. Moreover, although soft prompts are efﬁcient for ﬁne-tuning and de-\\nployment, they are inﬂexible and do not allow for easy adjustments without extensive ﬁne-tuning\\nor modiﬁcation. This inﬂexibility can limit their utility in dynamic environments where prompt\\nchanges are frequently needed.\\nOne alternative way to develop efﬁcient prompts is to simplify the text used for prompting.\\nFor example, below is a prompt for answering questions on healthcare and ﬁnance.\\nThe task involves developing a language model capable of understanding and\\nresponding to user inquiries across various domains, with a particular emphasis\\non healthcare and ﬁnance. Considering the broad range of potential queries,\\nfrom the speciﬁcs of medical diagnoses to the nuances of ﬁnancial regulations,\\nthe model must ensure a comprehensive understanding and accurate responses.\\nQuestion:\\nWhat are the best practices for using artiﬁcial intelligence in diagnosing cardio-\\nvascular diseases?\\nWe can simplify the task description by deleting the unimportant parts.\\nThe task involves developing a language model capable of understanding\\nand responding to user inquiries across various domains, with a particular\\nemphasis on healthcare and ﬁnance. Considering the broad range of potential\\nqueries, from the speciﬁcs of medical diagnoses to the nuances of ﬁnancial\\nregulations, The model must ensure a comprehensive understanding and ac-\\ncurate responses.\\nWe can also paraphrase it as a shorter text.\\nThe task involves developing a language model focused on healthcare and\\nﬁnance, capable of understanding and accurately responding to a wide range\\nof user inquiries.\\nThis problem can be viewed as a classic NLP issue — text simpliﬁcation. So the methods\\nused can be general and not restricted to the problem of simplifying prompts. There are many\\nways to achieve this. One simple method is to deﬁne some heuristics and identify redundant\\nwords that can be eliminated without losing essential information. For example, we can examine\\neach token in a sequence in terms of its contribution to the overall meaning and remove those that\\nprovide minimal value [Li et al., 2023c; Jiang et al., 2023b]. Another method involves framing\\nthe problem as a sequence-to-sequence task. With labeled data for text simpliﬁcation, we can\\ntrain an encoder-decoder model to transform each input text into its simpliﬁed form. In addition,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 159}, page_content='3.4 Summary\\n153\\ngiven that many LLMs have been ﬁne-tuned and aligned to perform text simpliﬁcation tasks, it is\\nstraightforward to use these models to simplify prompts. For example, we can prompt an LLM to\\nsimplify a text under certain constraints, such as limiting the length of the simpliﬁed text.\\n3.4\\nSummary\\nIn this chapter, we have discussed a variety of issues related to LLM prompting. Our discussion\\nhas focused mainly on two aspects:\\n• How to design basic prompts to guide the predictions of LLMs and reﬁne these prompts for\\nmore effective and efﬁcient problem-solving?\\n• How to automate the design and representation of prompts?\\nSolutions to these issues involve both general prompt designs and more advanced techniques, such\\nas CoT and prompt learning, which have been explored extensively in recent research.\\nIn NLP, prompting can be viewed as a technology that has evolved along with LLMs, and\\nin a sense, it has opened the door to the practical application of these models in an impressive\\nrange of problem domains. In fact, if we expand the concept of prompts to some extent, it can be\\ntraced back to the early days of machine learning and NLP. For example, many NLP systems use\\nhand-crafted features and templates to “prompt” speciﬁc tasks. Imagine developing a feature to\\nindicate whether a text is formal or informal. We can feed this feature into a machine translation\\nsystem to condition the translation on the type of the input text.\\nThe widespread use of the modern concept of prompts began with the rise of large pre-trained\\nmodels in the ﬁeld of NLP. Initially, these models, such as BERT, were adapted to speciﬁc down-\\nstream tasks mainly through ﬁne-tuning. However, researchers soon discovered that by designing\\nspeciﬁc \"prompts\" — adding certain words or sentences to the input — the models could be\\ntriggered to respond to speciﬁc tasks without extensive ﬁne-tuning. This motivated the NLP com-\\nmunity to develop and apply universal foundation models that can be prompted to address various\\ntasks without changing the underlying architecture and the pre-training procedure.\\nPrompting approaches were ﬁrst experimented with smaller models and later demonstrated\\nimpressive capabilities with large models like GPT-3, which could generate high-quality text in\\nresponse to simple prompts across various tasks. As prompting technology evolved, prompt en-\\ngineering emerged as a critical area of research. As discussed in this chapter, it broadly involves\\ndesigning effective prompts to maximize model performance, encompassing both hand-crafted\\nand automatically generated prompts. More recent research has explored how to enhance the\\neffectiveness of prompting through techniques like few-shot learning, zero-shot learning, and\\nCoT reasoning, enabling LLMs to work effectively across a wide range of scenarios. A gen-\\neral discussion of prompting can be very broad, and we cannot cover all details in this chapter.\\nFor more advanced techniques of prompting, the reader can refer to recent surveys. Topics in-\\nclude in-context learning [Li, 2023; Dong et al., 2022], CoT [Chu et al., 2023; Yu et al., 2023;\\nZhang et al., 2023a], efﬁcient prompting [Chang et al., 2024], and general prompt engineering\\n[Liu et al., 2023c; Chen et al., 2023a].'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 160}, page_content='154\\nPrompting\\nNote that although we would ideally like to develop general prompting methods without ad-\\njusting model architectures and parameters, the results of prompting generally depend heavily\\non the quality and size of the given LLMs. For stronger models, such as commercialized online\\nLLMs, simple prompts may be sufﬁcient to instruct these models to perform tasks correctly. In\\nthis case, prompt engineering is relatively easy, though we still need certain efforts to make LLMs\\nwork properly. By contrast, if the LLMs are not powerful enough, we may need to carefully design\\nthe prompts to achieve the desired results. In many cases, ﬁne-tuning is still necessary to adapt\\nthe models to sophisticated prompting strategies.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 161}, page_content='CHAPTER 4\\nAlignment\\nAlignment is not a new concept in NLP, but its meaning varies across different domains and over\\ntime. In traditional NLP, the term alignment typically refers to the tasks that link corresponding\\nelements in two sets, such as aligning words between a Chinese sentence and an English sentence.\\nAs LLMs become increasingly important in NLP research, this term is more broadly used to refer\\nto aligning model outputs with human expectations. The problem that alignment addresses is\\nthat the output of a model may not align with the speciﬁc goals or contexts intended by users.\\nFor example, pre-trained LLMs may not be able to follow user instructions because they were\\nnot trained to do so. Another example is that LLMs may generate harmful content or perpetuate\\nbiases inherent in their training data. This poses new challenges in ensuring that LLM outputs are\\nnot only accurate and relevant, but also ethically sound and non-discriminatory.\\nSimply pre-training LLMs can result in a variety of alignment problems. Our ultimate goal\\nis to resolve or mitigate all these problems to ensure LLMs are both accurate and safe. There\\nis an interesting issue here: since large language models are trained on vast amounts of data,\\nwe have reason to believe that if we have sufﬁcient data covering a variety of tasks and aligned\\nwith human preferences, pre-training could make LLMs accurate and safe enough, perhaps even\\neliminating the need for alignment. However, the reality is that it is nearly impossible to gather\\ndata that encompasses all tasks or adequately represents human preferences. This makes it difﬁcult\\nto achieve model alignment through pre-training alone, or at least, at this stage, alignment remains\\na very necessary and critical step in the development of LLMs.\\nIn this chapter, we will focus on alignment methods for LLMs. We will begin by discussing the\\ngeneral alignment tasks. Then we will consider two widely-used approaches, known as instruc-\\ntion alignment and human preference alignment, respectively. The former resorts to supervised\\nﬁne-tuning techniques and guides the LLMs to generate outputs that adhere closely to user instruc-\\ntions. On the other hand, the latter typically relies on reinforcement learning techniques, where\\nthe LLMs are trained based on feedback from humans. While these methods are motivated by\\ndifferent goals, they are commonly used together to develop well-aligned LLMs.\\n4.1\\nAn Overview of LLM Alignment\\nAlignment can be achieved in several different ways. We need different methods for LLM align-\\nment because this problem is itself complicated and multifaceted, requiring a blend of technical\\nconsiderations. Here we consider three widely-used approaches to aligning LLMs.\\nThe ﬁrst approach is to ﬁne-tune LLMs with labeled data. This approach is straightforward\\nas it simply extends the pre-existing training of a pre-trained LLM to adapt it to speciﬁc tasks.\\nAn example of this is supervised ﬁne-tuning (SFT), in which the LLM is further trained on a\\ndataset comprising task-speciﬁc instructions paired with their expected outputs. The SFT dataset\\nis generally much smaller compared to the original training set, but this data is highly specialized.\\nThe result of SFT is that the LLM can learn to execute tasks based on user instructions. For\\nexample, by ﬁne-tuning the LLM with a set of question-answer pairs, the model can respond to\\nspeciﬁc questions, even if not directly covered in the SFT dataset. This method proves particularly\\n155'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 162}, page_content='156\\nAlignment\\nPre-training\\nPrompting\\nInstruction\\nAlignment\\n(e.g., SFT)\\nHuman Preference\\nAlignment\\n(e.g., RLHF)\\nPre-training\\nStage\\nAlignment\\nStage\\nTraining & Fine-tuning\\nInference\\nFig. 4.1: Schematic illustration of the pre-train-then-align method for developing LLMs. In the pre-training stage, we\\ntrain an LLM on vast amounts of data using next token prediction. Then, in the alignment stage, we align the LLM\\nto user instructions, intents, and preferences. This includes instruction alignment, human preference alignment, and\\nprompting.\\nuseful when it is relatively easy to describe the input-output relationships and straightforward to\\nannotate the data.\\nThe second approach is to ﬁne-tune LLMs using reward models. One difﬁculty in alignment\\nis that human values and expectations are complex and hard to describe. In many cases, even for\\nhumans themselves, articulating what is ethically correct or culturally appropriate can be challeng-\\ning. As a result, collecting or annotating ﬁne-tuning data is not as straightforward as it is with SFT.\\nMoreover, aligning LLMs is not just a task of ﬁtting data, or in other words, the limited samples\\nannotated by humans are often insufﬁcient to comprehensively describe these behaviors. What we\\nreally need here is to teach the model how to determine which outputs are more in line with human\\npreferences, for example, we not only want the outputs to be technically accurate but also to align\\nwith human expectations and values. One idea is to develop a reward model analogous to a human\\nexpert. This reward model would work by rewarding the LLM whenever it generates responses\\nthat align more closely with human preferences, much like how a teacher provides feedback to a\\nstudent. To obtain such a reward model, we can train a scoring function from human preference\\ndata. The trained reward model is then used as a guide to adjust and reﬁne the LLM. This frames\\nthe LLM alignment task as a reinforcement learning task. The resulting methods, such as rein-\\nforcement learning from human feedback (RLHF), have been demonstrated to be particularly\\nsuccessful in adapting LLMs to follow the subtleties of human behavior and social norms.\\nThe third approach is to perform alignment during inference rather than during training or\\nﬁne-tuning. From this perspective, prompting in LLMs can also be seen as a form of alignment,\\nbut it does not involve training or ﬁne-funing. So we can dynamically adapt an LLM to various\\ntasks at minimal cost. Another method to do alignment at inference time is to rescore the outputs\\nof an LLM. For example, we could develop a scoring system to simulate human feedback on the\\noutputs of the LLM (like a reward model) and prioritize those that receive more positive feedback.\\nThe three methods mentioned above are typically used in sequence once the pre-training is\\ncomplete: we ﬁrst perform SFT, then RLHF, and then prompt the LLM in some way during\\ninference. This roughly divides the development of LLMs into two stages — the pre-training stage\\nand the alignment stage. Figure 4.1 shows an illustration of this. Since prompting techniques have\\nbeen intensively discussed in the previous chapter, we will focus on ﬁne-tuning-based alignment\\nmethods in the rest of this chapter.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 163}, page_content='4.2 Instruction Alignment\\n157\\n4.2\\nInstruction Alignment\\nOne feature of LLMs is that they can follow the prompts provided by users to perform various\\ntasks. In many applications, a prompt consists of a simple instruction and user input, and we want\\nthe LLM to follow this instruction to perform the task correctly. This ability of LLMs is also\\ncalled the instruction-following ability. For example, below is a prompt where we want the LLM\\nto extract key points and provide a concise summary for a lengthy article.\\nInstruction\\nSummarize this text in three sentences.\\nInput\\nDaylight Savings Time (DST) - the process of moving clocks forward\\nby one hour in the summer - was started in Germany in 1916 ...\\nOutput\\nThis task requires the LLM to understand the instruction “Summarize this text in three sentences”\\nand perform the summarization accordingly. However, LLMs are typically trained for next-token\\nprediction rather than for generating outputs that follow instructions. Applying a pre-trained LLM\\nto the above example would likely result in the model continuing to write the input article instead\\nof summarizing the main points. The goal of instruction alignment (or instruction ﬁne-tuning) is\\nto tune the LLM to accurately respond to user instructions and intentions. The rest of this section\\nwill discuss some issues related to instruction alignment, including ﬁne-tuning LLMs to follow\\ninstructions, generating or collecting instruction data, and generalizing instruction alignment.\\n4.2.1\\nSupervised Fine-tuning\\nOne straightforward approach to adapting LLMs to follow instructions is to ﬁne-tune these models\\nusing annotated input-output pairs [Ouyang et al., 2022; Wei et al., 2022a]. Unlike standard lan-\\nguage model training, here we do not wish to maximize the probability of generating a complete\\nsequence, but rather maximizing the probability of generating the rest of the sequence given its\\npreﬁx. This approach makes instruction ﬁne-tuning a bit different from pre-training. The SFT\\ndata is a collection of such input-output pairs (denoted by S), where each output is the correct\\nresponse for the corresponding input instruction. For example, below is an SFT dataset\\nx (instruction + user input)\\ny (output)\\nSummarize the following article.\\n{∗summary∗}\\nArticle: In recent years, solar energy has seen\\nunprecedented growth, becoming the fastest-growing ...\\nExtract the main ﬁnancial ﬁgures from the following\\nRevenue: $10 million,\\nearnings report.\\nProﬁt Margin: 15%\\nReport: The company reported a revenue of $10 million\\nin the ﬁrst quarter with a proﬁt margin of 15% ...\\nClassify the following email as spam or not spam.\\nSpam\\nText: Congratulations! You’ve won a $500 gift card.\\nClick here to claim now.\\nProvide a solution to the following technical issue.\\nFirst, check for ...\\nIssue: my computer is running slow and often freezes.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 164}, page_content='158\\nAlignment\\nwhere the instructions are highlighted. This dataset contains instructions and the corresponding\\noutputs for several different NLP problems, and so we can ﬁne-tune an LLM to handle multiple\\ntasks simultaneously.\\nLet x = x0...xm be an input sequence (e.g., instruction + user input) and y = y1...yn be the\\ncorresponding output sequence. In SFT, we aim to maximize the probability of the output y given\\nthe input x. Consider an LLM with pre-trained parameters ˆθ. The ﬁne-tuning objective can then\\nbe formulated as:\\n˜θ\\n=\\narg max\\nˆθ+\\nX\\n(x,y)∈D\\nlog Prˆθ+(y|x)\\n(4.1)\\nwhere ˜θ denotes the parameters optimized via ﬁne-tuning, and ˆθ+ represents an adjustment to ˆθ.\\nHere we will omit the superscript + and use θ to represent ˆθ+ to keep the notation uncluttered. But\\nthe reader should keep in mind that the ﬁne-tuning starts from the pre-trained parameters rather\\nthan randomly initialized parameters.\\nThe objective function log Prθ(yi|x, y<i) is computed by summing the log-probabilities of\\nthe tokens in y, conditional on the input x and all the previous tokens y<i:\\nlog Prθ(y|x)\\n=\\nn\\nX\\ni=1\\nlog Prθ(yi|x, y<i)\\n(4.2)\\nThis formulation is equivalent to minimizing the cross-entropy loss.\\nNote that minimizing the conditional log-probability log Prθ(y|x) is not a standard language\\nmodel training problem. If we concatenate x and y as a single sequence, a more general form of\\nlanguage modeling is based on the joint log-probability log Prθ(x, y), that is, we minimize the\\nloss over all tokens of the sequence seqx,y = [x, y]. We can write the probability of this sequence\\nusing the chain rule\\nlog Prθ(seqx,y)\\n=\\nlog Prθ(x, y)\\n=\\nlog Prθ(x)\\n|\\n{z\\n}\\nset to 0\\n+ log Prθ(y|x)\\n|\\n{z\\n}\\nloss computation\\n(4.3)\\nThere are two terms on the right-hand side of the equation. We can simply set the ﬁrst term\\nlog Prθ(x) to 0, focusing solely on the second term log Prθ(y|x) for loss computation. As a\\nresult, the training can be implemented using standard LLMs. For the sequence seqx,y, we ﬁrst\\nrun the forward pass as usual. Then, during the backward pass, we force the loss corresponding\\nto x to be zero. Figure 4.2 shows an illustration of this process.\\nBy taking log Prθ(seqx,y) as the objective function, we can describe SFT using a regular form\\nof language model training:\\n˜θ\\n=\\narg max\\nθ\\nX\\n(x,y)∈D\\nlog Prθ(seqx,y)\\n(4.4)\\nThe problem we considered above is fundamentally a single-round prediction problem, where\\nthe LLM generates a response based on a single input without any further interaction or feedback\\nfrom the user. The input is processed, and the output is generated in one go. This is typical in'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 165}, page_content='4.2 Instruction Alignment\\n159\\nx0\\nx1\\nx2\\nx3\\ny1\\nx1\\nx2\\nx3\\ny1\\ny2\\nInput\\nOutput\\n(a) Forward Pass\\nx0\\nx1\\nx2\\nx3\\ny1\\nx1\\nx2\\nx3\\ny1\\ny2\\nLoss = 0\\nLoss ̸= 0\\n(b) Backward Pass\\nFig. 4.2: Illustration of supervised ﬁne-tuning for LLMs. We concatenate the input and the output into a single se-\\nquence. During the forward pass, we run the LLM as usual. During the backward pass, we compute the loss only for\\nthe output part and simply set the loss for the input part to 0.\\nscenarios where a single question is asked, and a single answer is provided, with no follow-up\\nquestions or clariﬁcations. However, in practice, we sometimes have to handle multi-round pre-\\ndiction problems, for example, an LLM engages in a dialogue over multiple turns. In this setting,\\nthe LLM not only generates responses based on the initial input but also incorporates subsequent\\ninputs that might reﬁne or expand on earlier interactions. For example, we can use the LLM to act\\nas a healthcare assistance chatbot and have a conversation with the user, like this\\nUser\\nI’ve been feeling very tired lately.\\nChatbot\\nI’m sorry to hear that. Besides feeling tired, have you noticed any\\nother symptoms?\\nUser\\nYes, I’m also experiencing headaches frequently.\\nChatbot\\nHow long have these symptoms been going on?\\nUser\\nAbout a week now.\\nChatbot\\nIt might be good to check in with a healthcare professional. Would\\nyou like help setting up an appointment?\\nUser\\nYes, please. Can it be after work hours?\\nChatbot\\nSure, I can arrange that. There are slots available next Wednesday\\nand Thursday after 5 PM. Which day works better for you?\\n...\\nIn this task, there are several rounds of conversation, each involving the generation of a re-\\nsponse based on the user’s request or question and the conversational history. Suppose we have\\nK rounds of conversation, denoted by {x1, y1, x2, y2, ..., xK, yK}. Here xk and yk denote the\\nuser request and the response, respectively, for each round k. The log-probability of generating\\nthe response can be written as log Prθ(yk|x1, y1, ..., xk). Our goal is then to maximize the sum\\nof these log-probabilities\\n˜θ\\n=\\narg max\\nθ\\nK\\nX\\nk=1\\nlog Prθ(yk|x1, y1, ..., xk)\\n(4.5)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 166}, page_content='160\\nAlignment\\nA straightforward implementation of this involves calculating the conditional probability for\\neach k. However, it requires running the LLM K times, each time with an increased conversa-\\ntional history to make predictions. A more efﬁcient method is to perform loss computation of all\\nresponses in a single run of the LLM. To do this, we represent the conversation as a sequence\\nseqx1,y1,...,xK,yK = [x1, y1, ..., xK, yK] (or seq for short). The log-probability of this sequence\\nis given by\\nlog Prθ(seq)\\n=\\nlog Prθ(x1, y1, ..., xK, yK)\\n=\\nlog Prθ(x1)\\n|\\n{z\\n}\\nset to 0\\n+ log Prθ(y1|x1)\\n|\\n{z\\n}\\nloss computation\\n+ · · · +\\nlog Prθ(xK|x1, y1, ..., yK−1)\\n|\\n{z\\n}\\nset to 0\\n+\\nlog Prθ(yK|x1, y1, ..., xK)\\n|\\n{z\\n}\\nloss computation\\n(4.6)\\nThe trick here is that we ignore the loss for generating user inputs, as illustrated in Figure 4.3.\\nHence we only compute the probabilities of generating the responses given their conversational\\nhistories, in other words, the value on the right-hand side of Eq. (4.6) is actually equal to the value\\non the right-hand side of Eq. (4.5). As with Eq. (4.4), the training of this multi-round prediction\\nmodel can be achieved by maximizing the log likelihood over a training dataset D:\\n˜θ\\n=\\narg max\\nθ\\nX\\nseq∈D\\nlog Prθ(seq)\\n(4.7)\\nWhile implementing the SFT methods introduced above seems trivial as they are fundamen-\\ntally the same as regular language model training, there are still issues that need to be considered\\nin practice. For example,\\n• SFT requires labeled data. This makes SFT quite different from pre-training, where raw text\\nis used as training data and is readily available. As in other supervised machine learning\\nproblems, data annotation and selection in SFT are not simple tasks. In general, we wish\\nto develop SFT data that is both substantial in quantity and high in quality, and this data\\nshould be highly relevant to the tasks the LLM will perform. On the other hand, there is\\na need to ﬁne-tune LLMs with less data to minimize computational and data construction\\ncosts. Often, the quality of LLMs is highly dependent on the data used in SFT. Thus, such\\ndata must be carefully developed and examined. As we will see in later subsections, SFT\\ncan be more efﬁcient and effective through more advanced techniques for data construction.\\n• SFT is still computationally expensive for LLMs due to their big size. As a result, main-\\ntaining and updating such models is resource-intensive. For example, applying gradient up-\\ndates to billions of parameters within an LLM requires signiﬁcant computational power and\\nmemory. This often requires high-performance computing environments, which are costly\\nto operate. To address these challenges, various optimization strategies, such as pruning,\\nquantization, and the use of more efﬁcient training algorithms, have been explored. In par-\\nticular, there has been signiﬁcant interest in parameter-efﬁcient ﬁne-tuning methods which\\nare designed to maintain state-of-the-art performance without the need for extensive compu-\\ntational resources. We have seen in Chapter 3 that applying techniques like soft prompts can'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 167}, page_content='4.2 Instruction Alignment\\n161\\nUser: I’ve been feeling very tired lately.\\nChatbot: I’m sorry to hear that. Besides feeling tired,\\nhave you noticed any other symptoms?\\nUser: Yes, I’m also experiencing headaches frequently.\\nChatbot: How long have these symptoms been going on?\\n...\\nx1\\ny1\\nx2\\ny2\\n· · ·\\nPrθ(x1)\\nPrθ(y1|x1)\\nPrθ(x2|x1, y1)\\nPrθ(y2|x1, y1, x2)\\nLoss = 0\\nLoss ̸= 0\\nLoss = 0\\nLoss ̸= 0\\nFig. 4.3: Illustration of supervised ﬁne-tuning for conversational models. Here the LLM acts as a chatbot to respond to\\neach request based on the conversational history. The conversation progresses by alternating between the user and the\\nchatbot. In SFT, we treat the entire conversation as a sequence, just like in standard LLMs, but compute the loss only\\nfor the responses of the LLM.\\nmake the ﬁne-tuning process more efﬁcient. For further discussion on parameter-efﬁcient\\nmethods, the reader can refer to related papers on this issue [Houlsby et al., 2019; Hu et al.,\\n2022; Han et al., 2024].\\n• SFT can be regarded as a post-training step following pre-training. It is a separate training\\nphase designed to preserve the advantages of the initial pre-training while incorporating new\\nadjustments. This may seem paradoxical because updating a pre-trained LLM with further\\ndata potentially causes the model to forget some of its prior knowledge. Imagine a scenario\\nwhere we have a large amount of SFT data and extensively ﬁne-tune the LLM. In this\\ncase, the LLM could overﬁt the data, which in turn may reduce generalization performance\\nor cause catastrophic forgetting. A common strategy to mitigate this issue is to employ\\nregularization and early stopping techniques. Another practical approach is to use a smaller\\nlearning rate to gently adjust the weights of the LLM. In addition, ﬁne-tuning with data from\\ndiverse sources and problem domains can also be beneﬁcial. Nevertheless, in practice, the\\nSFT step is often carefully examined and requires substantial engineering and experimental\\nefforts to optimize.\\n4.2.2\\nFine-tuning Data Acquisition\\nFine-tuning data is so important that much recent work in LLM has focused on developing various\\ndatasets for instruction ﬁne-tuning. As with most work in machine learning, there are generally\\ntwo approaches to data acquisition — manual data generation and automatic data generation.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 168}, page_content='162\\nAlignment\\n4.2.2.1\\nManually Generated Data\\nOne straightforward method is to recruit human annotators to create input-output pairs for the\\ntasks of interest. Unlike data annotation in conventional NLP, such as text classiﬁcation, where\\nannotators simply assign labels to collected texts according to guidelines, creating ﬁne-tuning data\\nfor LLMs requires more steps and effort, making it thus more challenging. Suppose we want to\\nobtain ﬁne-tuning data for the English-to-Chinese machine translation task. The ﬁrst step is to\\nwrite a prompt template to describe the task and format the problem clearly. For example,\\nInstruction\\nTranslate the text from English to Chinese.\\nUser Input\\n{∗text∗}\\nOutput\\n{∗translation∗}\\nThen, we collect pairs of source and target texts (i.e., Chinese texts and the corresponding\\ntranslations), and replace the variables {∗text∗} and {∗translation∗} to generate the ﬁne-tuning\\nsamples. For example, given a pair of English and Chinese sentences\\nHow’s the weather today?\\n→\\n今天天气怎么样？\\n{∗text∗}\\n{∗translation∗}\\nwe can generate a ﬁne-tuning sample using the prompt template, like this\\nInstruction\\nTranslate the text from English to Chinese.\\nUser Input\\nHow’s the weather today?\\nOutput\\n今天天气怎么样？\\nThat is,\\nx\\n=\\nTranslate the text from English to Chinese.\\\\n How’s the weather today?\\ny\\n=\\n今天天气怎么样？\\nWe can use this (x, y) pair to ﬁne-tune the LLM, as described in the previous subsection.\\nOne difﬁculty here is that there are many, many different ways to write prompt templates\\nfor the same task, and different people may produce prompt templates with varying qualities\\nand complexities. Sometimes, we may write prompt templates with overly complex or verbose\\ninstructions. Sometimes, we may not even know exactly what the target task is and how to de-\\nscribe it. A widely-adopted strategy is to create prompt templates for existing NLP tasks, given\\nthat there have been so many well-established NLP problems and benchmarks [Bach et al., 2022;\\nWang et al., 2022b; Mishra et al., 2022]. In this case, annotators can be given the original task\\ndescription and many examples. Then, they can use their own ways to express how to prompt the\\nLLM to perform the tasks. Note that, while such a method can ease the process of creating and\\nwriting prompts, we still need annotation frameworks and crowdsourcing systems to manage the\\nwork and conduct quality control. For example, we generally need to design annotation guidelines'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 169}, page_content='4.2 Instruction Alignment\\n163\\nand a uniﬁed format for writing prompt templates, especially when many annotators are contribut-\\ning to the same task. One advantage of inducing prompts from existing NLP tasks is that, once the\\nprompt templates have been developed, it is easy to generate prompts using the annotated samples\\nin the original tasks. For example, given a bilingual dataset for English-to-Chinese translation, we\\ncan easily create a number of ﬁne-tuning examples by ﬁlling the slots in the above template with\\nthe sentence pairs in this dataset.\\nAnother approach is to directly use the naturally existing data available on the internet. A\\ncommon example is by collecting question-and-answer pairs from QA websites to ﬁne-tune LLMs\\nfor open-domain QA tasks [Joshi et al., 2017]. Many benchmarks in QA are built in this way\\nbecause there are so many types of questions that it is impossible to think of them all by a small\\ngroup of people. Instead, using data from those websites can ensure that the LLM ﬁne-tuning data\\nis at a good or acceptable level in terms of quantity and quality.\\nIn addition to employing existing resources, another straightforward way to develop a ﬁne-\\ntuning dataset is to crowdsource the data. A simple approach is to allow users to input any ques-\\ntion, after which responses are either manually given or automatically generated by an LLM and\\nthen manually annotated and corrected. It is thus possible to capture real user behavior and conse-\\nquently gather inputs and outputs for a large number of “new” problems that traditional NLP tasks\\ndo not cover.\\nAn issue related to the construction of the ﬁne-tuning datasets is that we usually want the\\ndata to be as diverse as possible. Many studies have found that increasing the diversity of ﬁne-\\ntuning data can improve the robustness and generalization ability of LLMs. For this reason, there\\nhas been considerable interest in involving more diverse prompts and tasks in LLM ﬁne-tunining\\ndatasets. We will give further discussions on the generalization of ﬁne-tuning in Section 4.2.4.\\n4.2.2.2\\nAutomatically Generated Data\\nOne limitation of manual data generation is that the quality and diversity largely depend on human\\nexperience and creativity. Therefore, if we want LLMs to handle a broad range of tasks, that\\nis, to effectively execute any instruction, relying on human-annotated data for LLM ﬁne-tuning\\nis often inefﬁcient. Moreover, the coverage of such data can be limited, and the data may even\\ncontain biases introduced by the annotators themselves. An alternative approach is to generate data\\nautomatically. For example, we can collect a number of questions through crowdsourcing, and\\nemploy a well-tuned LLM to generate answers to the questions. These question-answer pairs are\\nthen used as ﬁne-tuning samples as usual. This method, though very simple, has been extensively\\napplied to generate large-scale ﬁne-tuning data for LLMs.\\nThe above way of producing synthetic ﬁne-tuning data is similar to those used in data aug-\\nmentation for NLP. If we have an LLM, we can produce a prediction in response to any input.\\nRepeating this process for different inputs allows us to create a sufﬁcient number of ﬁne-tuning\\nsamples. Such a method is particularly useful for ﬁne-tuning new LLMs using a well-tuned LLM.\\nHowever, one disadvantage of this approach is that it relies on human-crafted or collected inputs\\nfor data generation, which may turn out to be inappropriate for generalizing LLMs. In many LLM\\napplications, a signiﬁcant challenge arises from the broad range of users’ questions and requests,\\nmany of which are not covered in existing NLP tasks and datasets. In these cases, it becomes\\nnecessary to generate not only the predictions but also the inputs themselves.\\nHere we consider self-instruct as an example to illustrate how to generate LLM ﬁne-tuning'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 170}, page_content='164\\nAlignment\\nInitialization\\nInitialize the task pool with a number of instructions\\nand corresponding input-output samples.\\nSample 1:\\n(Instruction, User-input, Output)\\nSample 2:\\n(Instruction, User-input, Output)\\n· · ·\\nTask Pool\\nSampling\\nDraw a few instructions from the pool\\nInstructiona\\nInstructionb\\nInstructionc\\nTask Pool\\nsampling\\nInstruction\\nGeneration\\nPrompt the LLM to generate a new instruction based on\\nthe drawn instructions.\\nYou are provided several different instructions for performing\\nsome tasks.\\nPlease generate an instruction based on these.\\nTask 1: Instructiona\\nTask 2: Instructionb\\nTask 3: Instructionc\\nNew Task: Instructionnew\\nSample\\nGeneration\\nGiven the newly-generated instruction and a few\\ninput-output samples, generate a new sample.\\nYou are provided with a set of input-output samples tasks,\\neach composed of an instruction, a user input, and an output.\\nPlease generate a new sample based on these.\\nSample 1: Samplea\\nSample 2: Sampleb\\nNew Sample: Instructionnew User-inputnew Outputnew\\nFiltering\\nFilter out invalid and low-quality samples.\\nAdd the remaining samples into the pool.\\nFig. 4.4: Illustration of self-instruct [Wang et al., 2023b]. This method maintains a pool of instructions and correspond-\\ning input-output samples. Initially, the pool contains a number of hand-crafted instructions and samples. Each time,\\nwe draw a few instructions from the pool. An LLM is then prompted to generate new instructions and samples based\\non those drawn. Finally, the newly-generated instructions and samples are ﬁltered and added to the pool.\\nsamples [Wang et al., 2023d; Honovich et al., 2023]. The idea is that we can prompt an LLM to\\ncreate a new instruction by learning from other instructions. Given this instruction, the LLM can\\nthen ﬁll in other ﬁelds (such as the user input) and produce the predictions. Figure 4.4 shows a\\nschematic illustration of self-instruct. Here we give a brief outline of the key steps involved.\\n• The self-instruct algorithm maintains a pool of tasks. Initially it contains a number of seed\\nhand-crafted tasks, each with an instruction and input-output sample. As the algorithm\\nproceeds, LLM-generated instructions and samples will be added to this pool.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 171}, page_content='4.2 Instruction Alignment\\n165\\n• At each step, a small number of instructions are drawn from the instruction pool. For ex-\\nample, we can randomly select a few human-written instructions and a few LLM-generated\\ninstructions to ensure diversity.\\n• The selected instructions are then used as demonstration examples. Thus, the LLM can\\nin-context learn from these examples and produce a new instruction. Below is an example\\ntemplate for prompting the LLM.\\nYou are provided several different instructions for performing some tasks.\\nPlease generate an instruction based on these.\\nTask 1: {instruction1}\\nTask 2: {instruction2}\\nTask 3: {instruction3}\\nTask 4: {instruction4}\\nNew Task:\\n• Given the generated instruction, the LLM is then prompted to complete the sample by ﬁlling\\nin the remaining input ﬁelds and generating the corresponding output. Below is a prompt\\ntemplate.\\nYou are provided with a set of input-output samples, each composed of\\nan instruction, a user input, and an output. Please generate a new sample\\nbased on these.\\nSample 1: {instruction1}\\nInput: {user-input1}\\nOutput: {output1}\\nSample 2: {instruction2}\\nInput: {user-input2}\\nOutput: {output2}\\nNew Sample: {new-instruction}\\n• This newly-generated sample is examined by some heuristic rules (such as ﬁltering out\\nsamples or instructions that are similar to those already in the pool). If it passes, the sample\\nand instruction are added to the pool.\\nThis generation process can be repeated many times to obtain a sufﬁcient number of ﬁne-\\ntuning samples. Note that, above, we just show simple prompt templates for generating instruction\\nand ﬁne-tuning samples. Of course, we can develop better templates to generate more diverse and\\naccurate instruction and ﬁne-tuning samples. For example, for certain tasks like text classiﬁcation,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 172}, page_content='166\\nAlignment\\nthe LLM may tend to produce biased predictions, for example, most generated samples belong to\\na single class. In such cases, we can adjust the order of generation of different ﬁelds. More\\nspeciﬁcally, we can specify the output (i.e., the class) with some prior, and prompt the LLM\\nto generate user input given both the instruction and the output. This method resembles input\\ninversion, where the LLM generates the input based on the speciﬁed output [Longpre et al., 2023].\\nUsing LLM-generated instructions and ﬁne-tuning samples has been a common method for\\ndeveloping LLMs, especially given that manually developing such data is so expensive that most\\nresearch groups cannot afford it. In several well-tuned LLMs, their ﬁne-tuning datasets include\\na certain amount of synthetic data, which has proved useful [Ouyang et al., 2022; Taori et al.,\\n2023; Chiang et al., 2023]. There have been further studies on synthetic data generation for LLM\\nﬁne-tuning. For example, one can generate more diverse instructions by introducing evolutionary\\nalgorithms [Xu et al., 2024], or use synthetic data as supervision signals in a more advanced ﬁne-\\ntuning process [Chen et al., 2024b]. More recently, there has also been considerable interest in\\nusing synthetic data in the pre-training stage [Gunasekar et al., 2023; Allal et al., 2024].\\nIn many applications, a real-world scenario is that, given a task, we can collect or annotate a\\nrelatively small amount of ﬁne-tuning data, for example, we can recruit experts to create questions\\nfor QA tasks in a speciﬁc domain. But the quantity and diversity of this data are in general not\\nsufﬁcient. In this case, we can use self-instruct techniques to generate more diverse question-\\nanswer pairs, and thus augment the ﬁne-tuning data. This provides a way of bootstrapping the\\nLLM starting from a seed set of ﬁne-tuning samples. Note that using self-generated data is a com-\\nmon practice and has long been applied in NLP. For example, this approach has been successfully\\nused in parsing and machine translation [Charniak, 1997; Sennrich et al., 2016].\\n4.2.3\\nFine-tuning with Less Data\\nWith the increasing prominence of instruction ﬁne-tuning, there has been a surge in demand for\\nlarge-scale, high-quality ﬁne-tuning data. For example, the FLAN ﬁne-tuning dataset, which\\nis compiled from 1,836 tasks, contains 15 million samples [Longpre et al., 2023]. Fine-tuning\\nLLMs with such large datasets is typically a computationally expensive task, especially given\\nthat updating the large number of parameters in LLMs is resource-intensive. One approach for\\nmitigating this issue is to explore efﬁcient model training methods, for example, one can use\\nparameter-efﬁcient methods to update only a small portion of the model. However, many ﬁne-\\ntuning datasets contain a large amount of synthetic data, where errors and biases are still inevitable.\\nAnother approach to efﬁcient ﬁne-tuning is to consider only the most relevant and impactful\\nexamples for ﬁne-tuning. We can thus reduce the amount of data that needs to be processed while\\nstill maintaining the quality of the model updates. There are several methods to achieve this. For\\nexample, Zhou et al. [2023a] built an instruction-following dataset containing only 1,000 sam-\\nples by carefully crafting the prompts and collecting samples from a variety of NLP tasks. They\\nshowed that the LLaMa 65B model ﬁne-tuned with this dataset could be competitive with or even\\nbetter than models with much more ﬁne-tuning effort. This suggests that LLMs can be adapted\\nto respond to diverse tasks without necessarily needing ﬁne-tuning on all types of instruction-\\nfollowing data. Chen et al. [2024a] developed a system based on the GPT-3.5 model to assess\\nthe quality of each instruction-following sample. Therefore, they could select high-quality sam-\\nples from existing datasets, showing better ﬁne-tuning performance with fewer ﬁne-tuning sam-\\nples. Researchers have also developed methods to either select or ﬁlter out data using heuristics'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 173}, page_content='4.2 Instruction Alignment\\n167\\n[Zhao et al., 2024; Ge et al., 2024], or to prioritize data that more signiﬁcantly inﬂuences the ﬁne-\\ntuning process [Xia et al., 2024]. In fact, most of these methods can be seen as instances of larger\\nfamilies of data selection and ﬁltering methods. And it is often the case that using higher quality\\n(but maybe less) data is beneﬁcial for training NLP models.\\nThe discoveries in instruction ﬁne-tuning somewhat differ from traditional views in NLP: the\\nability of models to handle complex problems can be activated with a small amount of annotated\\ndata, rather than requiring massive amounts of supervised data for extensive training. One pos-\\nsible explanation is that the ability of generating correct responses given instructions has been\\nlearned during pre-training, but such instruction-response mappings are not with high probabil-\\nities during inference. Fine-tuning can slightly adjust the models to get them to follow instruc-\\ntions, requiring signiﬁcantly less training effort than pre-training. This is closely related to what\\nis known as the superﬁcial alignment hypothesis, which suggests that learning primarily occurs\\nduring pre-training, and the subsequent ﬁne-tuning or alignment phase does not signiﬁcantly con-\\ntribute to the underlying knowledge base of an LLM [Zhou et al., 2023a]. Since the core abilities\\nand knowledge of the model are already established from pre-training, effective ﬁne-tuning for\\nalignment with user needs can be achieved with relatively small training ﬁne-tuning effort. This\\nimplies the possibility of ﬁne-tuning LLMs with very little data. In another direction, it may not\\nbe necessary to restrict ﬁne-tuning to paired instruction-response data. For example, Hewitt et al.\\n[2024] found that instruction-following can be implicitly achieved by ﬁne-tuning LLMs only on\\nresponses, without corresponding instructions.\\nA concept related to the discussion here is sample efﬁciency. A machine learning method is\\ncalled sample efﬁcient if it can learn effectively from a small number of training examples. In this\\nsense, instruction ﬁne-tuning is sample efﬁcient compared with pre-training. From the perspective\\nof machine learning, sample-efﬁcient methods can be seen as efﬁcient ways to sample the space\\nof data, and are advantageous as they make optimal use of scarce data. Therefore, sampling-based\\nlearning techniques, such as many reinforcement learning algorithms, can beneﬁt from these sam-\\nple efﬁcient approaches. For example, in human preference alignment, we can either efﬁciently\\nsample preference data via reward models [Liu et al., 2024b] or improve sampling efﬁciency in\\npolicy learning [Wang et al., 2024].\\n4.2.4\\nInstruction Generalization\\nIn many machine learning and NLP problems, training a model to generalize is a fundamental\\ngoal. For example, in text classiﬁcation, we expect our model to correctly classify new texts that\\nwere not seen during training. However, generalization poses additional challenges in instruction\\nﬁne-tuning. We expect instruction-ﬁne-tuned LLMs to not only generate appropriate responses for\\ndifferent inputs within a task but also to accurately perform various tasks as described by different\\ninstructions. To illustrate this issue, consider an LLM Pr(y|c, z), where c is an instruction, z\\nis a user input, and y is the corresponding model output (i.e., the response). Suppose that the\\nperformance of this model is evaluated in terms of a metric, written as Performance(Pr(y|c, z))\\nor P(c, z, y) for short. Informally, when we say this model can generalize within a given task\\n(indicated by the instruction c∗), we mean that there may be a value ǫ such that the average\\nperformance on new inputs is above this value:\\n1\\n|Z|\\nX\\nz′∈Z\\nP(c∗, z′, y′) > ǫ\\n(4.8)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 174}, page_content='168\\nAlignment\\nwhere Z is the set of new inputs, and z′ and y′ are an input in this set and the corresponding\\noutput, respectively.\\nLikewise, we can say that this model can generalize across tasks if the average performance\\nover all instruction-input pairs is above some ǫ:\\n1\\n|D|\\nX\\n(c′,z′)∈D\\nP(c′, z′, y′) > ǫ\\n(4.9)\\nwhere D is the set of new instruction-input pairs.\\nHere, we need to deal with variations in two dimensions: instruction and user input. This\\nmakes the generalization problem very complex, because, intuitively, a model needs to learn from\\na vast number of tasks and different input-output pairs associated with each task to achieve good\\ngeneralization. As we have discussed several times in this book, achieving such generalization\\nincurs much lower cost than pre-training. In general, ﬁne-tuning LLMs with instruction-response\\ndata to some extent can lead to models yielding instruction following on new tasks. Nevertheless,\\nit is typically believed that certain efforts are still needed to adapt LLMs to make them understand\\nand execute instructions broadly.\\nOne way to generalize instruction ﬁne-tuning is to increase the diversity of the ﬁne-tuning\\ndata. In earlier studies on instruction ﬁne-tuning, researchers developed many datasets, covering a\\nwide variety of NLP tasks and different instructions for each task [Wang et al., 2022b; Sanh et al.,\\n2022; Longpre et al., 2023]. By transforming these tasks into a uniﬁed format, one can ﬁne-\\ntune an LLM with a sufﬁciently large number of samples, for example, there have been several\\ninstruction ﬁne-tuning datasets that involve over 100 NLP tasks and 1M samples. However, these\\nearly datasets mostly focus on existing academic problems, but not those that users want to deal\\nwith in real-world applications. Much recent work has shifted focus to addressing new and more\\npractical problems. For example, there has been considerable interest in constructing datasets\\nthat contain large and complicated demonstrations and responses from SOTA models to real user\\nqueries [Wang et al., 2023c; Teknium, 2023].\\nPerhaps the use of large and diverse ﬁne-tuning datasets has its origins in attempts to scale\\nLLMs in different dimensions. Indeed, scaling laws have been used broadly to motivate the de-\\nvelopment of a wide range of different instruction-ﬁne-tuned LLMs. And it is reasonable to scale\\ninstruction ﬁne-tuning to make an LLM follow broad instructions. From the perspective of LLM\\nalignment, however, scaling instruction ﬁne-tuning might not be efﬁcient to achieve generaliza-\\ntion.\\nOne problem is that instruction ﬁne-tuning relies on supervised learning that learns to gener-\\nalize and perform tasks based on instruction-response mappings. However, such an approach does\\nnot capture subtle or complex human preferences (e.g., tone, style, or subjective quality) because\\nthese are hard to encode as explicit instruction-response data. Moreover, the generalization per-\\nformance is bounded by the diversity and quality of the instruction-response dataset. Given these\\nlimitations, we would instead like to employ preference models as an additional ﬁne-tuning step\\nfollowing instruction ﬁne-tuning, so the LLMs can generalize further (see Section 4.3).\\nAnother view is that some instruction-response mappings may already be learned during pre-\\ntraining, and so the pre-trained LLMs have encoded such mappings. However, since we often do\\nnot know exactly what data is used in the pre-training, it is hard to judge whether we need to learn\\nsuch mappings in the ﬁne-tuning. A related question is whether out-of-distribution generalization'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 175}, page_content='4.2 Instruction Alignment\\n169\\nis primarily achieved during pre-training or ﬁne-tuning. While directly answering this question is\\nbeyond the scope of this chapter, it has been shown that pre-training on large and diverse datasets\\nis effective in improving out-of-distribution performance [Hendrycks et al., 2020; Radford et al.,\\n2021; Gunasekar et al., 2023]. This raises an interesting problem: if an LLM has been well pre-\\ntrained at scale, ﬁne-tuning may not be as essential for out-of-distribution generalization, since the\\nmodel may have already encountered sufﬁcient distributional variation. This prompts researchers\\nto ﬁne-tune LLMs with modest effort or to explore new methods to achieve instruction-following.\\nAs discussed in the previous sub-section, for example, instruction following can be yielded by\\nﬁne-tuning on a small number of carefully selected instruction-response pairs [Zhou et al., 2023a],\\nor even by using methods that are not explicitly designed to do so [Kung and Peng, 2023].\\nThe above discussion provides two different strategies: one requires scaling up ﬁne-tuning\\ndatasets for larger diversity, the other requires small but necessary ﬁne-tuning datasets for efﬁcient\\nLLM adaptation. However, in practice, involving diverse instructions often helps. In many cases,\\nwe need to adapt our LLM for speciﬁc purposes. But the LLM, which has possibly encoded broad\\ninstruction-following mappings during pre-training, might tend to behave as a general-purpose\\ninstruction executor even with modest ﬁne-tuning. An interesting phenomenon is that when ﬁne-\\ntuning on math data, the resulting LLM might not specialize in math outputs. Instead, this model\\nmight respond normally to general instructions, for example, it could generate poetry if instructed\\nto do so [Hewitt, 2024]. This is not a bad thing, but it shows that LLMs may not easily change their\\nnature of following general instructions. In this case, additional adaptations with more diverse\\ndata may help adjust the way the LLM follows instructions, particularly for those tasks we aim to\\naddress.\\n4.2.5\\nUsing Weak Models to Improve Strong Models\\nSo far we have explored a variety of instruction ﬁne-tuning methods based on labeled data. One\\nof the limitations of many such methods is that they require the data to be annotated by humans or\\ngenerated by strong LLMs, which can provide accurate supervision signals in ﬁne-tuning. How-\\never, in many cases, the LLM we have in hand is already strong (or at least is advantageous in\\nspeciﬁc aspects of problem solving), and thus it is not easy to ﬁnd a superior model for supervi-\\nsion. Even for human experts, when the problem becomes complex, providing correct and detailed\\nanswers might be difﬁcult, or sometimes infeasible. For example, when faced with an extremely\\nlong document, the experts would ﬁnd it challenging to identify any inconsistencies, subtle biases,\\nor missing key points without conducting an exhaustive and time-consuming review.\\nOne may ask at this point: can we use weak LLMs to supervise strong LLMs? This seems\\nto be a signiﬁcant challenge, but it may reﬂect a future scenario where we need to supervise AI\\nsystems that are smarter than humans or any other AI systems [Burns et al., 2023b]. The problem\\nof using smaller, less complex models to improve the training of larger, more complex models\\nis also called the weak-to-strong generalization problem. While there have not been mature\\napproaches to weak-to-strong generalization, using smaller models to assist stronger models has\\nindeed proven useful in several areas of LLMs.\\nFor instruction ﬁne-tuning, one of the simplest ways of applying weak LLMs is to use these\\nmodels to generate synthetic ﬁne-tuning data. Suppose we have a collection of inputs X, where\\neach input includes an instruction and a user input if necessary. For each x ∈X, we use a weak\\nLLM Prw(·) to generate a prediction ˆy = arg maxy Prw(y|x). Then, the strong LLM Prs\\nθ(·) can'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 176}, page_content='170\\nAlignment\\nbe trained on these generated predictions (see Eq. (4.1)):\\n˜θ\\n=\\narg max\\nθ\\nX\\nx∈X\\nlog Prs\\nθ(ˆy|x)\\n(4.10)\\nwhere θ is the model parameters.\\nThe above form transforms the ﬁne-tuning problem into a knowledge distillation problem, in\\nother words, we distill knowledge from the weak model to the strong model. Consequently, we\\ncan employ various knowledge distillation methods to achieve this goal. However, explaining\\nweak-to-strong ﬁne-tuning from the perspective of knowledge distillation is not straightforward.\\nA major concern is that the strong model may merely imitate or overﬁt the errors of the weak\\nmodel and fail to generalize. For example, the ﬁne-tuned strong model still cannot solve difﬁcult\\nproblems that the weak model cannot accurately predict. Fortunately, preliminary experiments in\\nthis line of research have shown positive and promising results. For example, Burns et al. [2023a]\\nfound that ﬁne-tuning the strong pre-trained GPT-4 model with GPT-2-level supervision could\\nimprove generalization across several NLP tasks. To measure how the weak model improves the\\ngeneralization of the strong model, we deﬁne the following terms:\\n• Weak Performance (Pweak). This is the test-set performance of the weak model, which\\ncan be regarded as the baseline performance.\\n• Weak-to-strong Performance(Pweak→strong). This is the test-set performance of the strong\\nmodel that is ﬁne-tuned with the weak model.\\n• Strong Ceiling Performance (Pceiling). This is the test-set performance of the strong model\\nthat is ﬁne-tuned with ground truth data. For example, we ﬁne-tune the strong model with\\nhuman-annotated predictions and take the resulting model as a ceiling.\\nThen, the performance gap recovered (PGR) can be deﬁned as\\nPGR\\n=\\nmax\\nn\\n0, Pweak→strong −Pweak\\nPceiling −Pweak\\no\\n(4.11)\\nThis metric measures how much of the performance gap between the ceiling model and the\\nweak model can be recovered by the weak-to-strong model. A PGR of 1 indicates that the weak-\\nto-strong ﬁne-tuning can completely closes the performance gap, whereas a PGR of 0 indicates\\nno improvement. In Burns et al. [2023a]’s work, it is shown that PGR can be around 0.8 on 22\\nNLP classiﬁcation tasks. It should be noted that, while the potential of weak-to-strong ﬁne-tuning\\nis promising, achieving substantial weak-to-strong generalization remains a challenging goal that\\nneeds further investigation [Aschenbrenner, 2024].\\nFine-tuning LLMs with weak supervision is just one choice for using small models to improve\\nlarge models. Although this section primarily focuses on ﬁne-tuning LLMs, we also mention\\nother methods here to give a more complete discussion (see Figure 4.5 for illustrations of these\\nmethods).\\n• Instead of using small models to generate synthetic data, it is also straightforward to in-\\ncorporate knowledge distillation loss based on these models. For example, a simple loss'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 177}, page_content='4.2 Instruction Alignment\\n171\\nLarge Model\\nx\\nˆy\\nCompute Loss & Train\\nDataset\\n=⇒\\nx ˆy\\nSmall Model\\nInput\\nPredict\\n(a) Fine-tuning on data generated by a small model\\n(weak-to-strong generalization)\\nLarge Model\\nx\\ny\\nCompute Loss & Train\\nDataset\\n=⇒\\nx y\\nSmall Model\\nKD Loss\\nLM Loss\\n(b) Fine-tuning with KD Loss from a small model\\n(weak-to-strong generalization)\\nLarge Model\\nx\\ny\\nCompute Loss & Train\\nDataset\\n=⇒\\nx y\\nDataset\\nSmall Model\\nData\\nSelection\\n(c) Data selection with a small model\\nSmall Model 2\\nSmall Model 1\\nSmall Model 3\\nx\\nx\\nx\\nCombination Model\\ny\\n(d) Ensemble of multiple small models\\nLarge Model\\nx\\ny2\\nSmall Model\\n(e) Cascading (at inference time)\\nx\\ny1\\nStep 2\\n(expensive)\\nStep 1\\n(cheap)\\nIf Step 1 is not satisfactory, go to Step 2\\nFig. 4.5: Illustrations of using small models to improve large models in LLMs. One approach involves using smaller\\nmodels for the ﬁne-tuning or pre-training of larger models. This includes generating synthetic data (a), incorporating\\nauxiliary loss (b), and selecting appropriate data (c). Another approach involves combining small models and large\\nmodels. This includes learning a strong model by aggregating multiple small models (d), and cascading small models\\nwith large models (e).\\nfunction that measures the difference between the small and large models can be deﬁned as:\\nLosskd\\n=\\nKL(Prw(·|x) || Prs\\nθ(·|x))\\n(4.12)\\nThen, we can add this loss to the original loss of language modeling, and yield the following\\ntraining objective\\n˜θ\\n=\\narg max\\nθ\\nX\\n(x,y)∈D\\nlog Prs\\nθ(y|x) −λ · Losskd\\n(4.13)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 178}, page_content='172\\nAlignment\\nwhere D is the set of input and output pairs, and λ is the coefﬁcient of the interpolation. This\\nmethod can be employed in either the pre-training or ﬁne-tuning phase. We can adjust λ to\\ncontrol how much the small model inﬂuences the training. For example, we can gradually\\ndecrease λ to make the training rely more on the original language modeling loss as the\\nlarge model becomes more capable.\\n• Another approach to involving small models in LLM pre-training and ﬁne-tuning is to use\\nthem to do data selection or ﬁltering. Given a sequence, we can compute the likelihood\\nor cross-entropy using a small model. These quantities can then be used as criteria for\\nselecting or ﬁltering data. For example, sequences with low likelihood or high cross-entropy\\nmight be excluded from the training set, as they are less aligned with the small model’s\\nlearned distribution. Conversely, sequences with high likelihood or low cross-entropy can\\nbe prioritized, ensuring that the training focuses on more relevant or high-quality data.\\n• Ensemble learning is a simple and effective way to build a strong model by combining mul-\\ntiple weak models. Applying this technique to LLMs is straightforward. We can aggregate\\ndistributions predicted by multiple small models or specialized submodels, and derive the\\nﬁnal prediction from the aggregated results. This aggregation can be done using methods\\nsuch as majority voting, weighted averaging, or stacking.\\n• Small models can also be employed at inference time to improve overall efﬁciency. Suppose\\nwe have a large model that is slow but more accurate, and a small model that is fast but\\nless accurate. In model cascading, the small model ﬁrst processes the input data, quickly\\ngenerating preliminary results. If these results meet certain pre-deﬁned criteria, they can be\\ndirectly used. However, if the initial results are not sufﬁciently good, the input is then passed\\nto the larger, more accurate model to produce a better result. This approach signiﬁcantly\\nreduces computational costs and latency, as the small model can effectively handle many\\ninputs without access to the large model.\\n4.3\\nHuman Preference Alignment: RLHF\\nSo far in this chapter, we have focused on ﬁne-tuning LLMs using input-output paired labeled data.\\nThis approach allows us to adapt LLMs for instruction-following via supervised learning. In many\\napplications, however, LLMs are required not only to follow instructions but also to act in ways\\nthat are more aligned with human values and preferences. Consider a scenario where a user asks an\\nLLM how to hack into a computer system. If the LLM is not appropriately aligned, it may respond\\nby providing details on how to perform this illegal activity. Instead, a more desirable response\\nmight be to advise the user against engaging in illegal activities and offer a general overview of\\nthe consequences of such actions. The difﬁculty in achieving this is that the ethical nuances and\\ncontextual considerations required for an LLM to respond appropriately in such scenarios are not\\nalways straightforward to encode into a ﬁne-tuning dataset. What’s even more challenging is that,\\noften, humans themselves cannot precisely express their own preferences.\\nIn this section, we discuss an alternative LLM ﬁne-tuning method, called reinforcement learn-\\ning from human feedback or RLHF for short [Christiano et al., 2017; Stiennon et al., 2020]. The\\nbasic idea behind RLHF is that LLMs can learn from comparisons of model outputs using reward'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 179}, page_content='4.3 Human Preference Alignment: RLHF\\n173\\nLLM\\nx\\ny\\nPredicted Token Distributions\\n(n token distributions)\\nGold-standard Predictions\\n(n one-hot distributions)\\nObjective (MLE):\\nmax Pr(y|x)\\nwhere\\nx: input\\ny: gold-standard output\\n(a) Supervised ﬁne-tuning (maximizing the prediction probability given the input)\\nLLM\\nx\\ny1\\ny2\\nGenerate multiple\\noutputs via sampling\\nPrediction y2\\nPrediction y1\\nObjective (RL Loss Minimization):\\nmin L(x, {y1, y2}, r)\\nReward Model\\nHuman preference data\\ntrain\\nwhere\\nL(·): loss function\\nr(·): reward model\\n(b) Reinforcement Learning from Human Feedback\\nFig. 4.6: Supervised ﬁne-tuning vs. reinforcement learning from human feedback. In supervised ﬁne-tuning, we\\noptimize the LLM by maximizing the probability of the prediction given the input. In reinforcement learning from\\nhuman feedback, we ﬁrst train a reward model on human preference data (on each pair of predictions, evaluators are\\nasked to choose which one they prefer). Then, we use this reward model to supervise the LLM during ﬁne-tuning.\\nmodels (see Figure 4.6). To do this, we can recruit human experts who indicate their preferences\\nbetween pairs of outputs generated by the LLM. This preference data is used to train a reward\\nmodel that can predict the perceived quality of LLM outputs. Once trained, the reward model\\nprovides feedback by assigning scores to new outputs that the LLM generates in response to the\\ninputs. The LLM uses these scores to update its parameters through reinforcement learning algo-\\nrithms. In the rest of this section, we will ﬁrst introduce the basic knowledge of reinforcement\\nlearning to facilitate the discussion, and then discuss methods for training reward models and\\naligning LLMs with these models.\\n4.3.1\\nBasics of Reinforcement Learning\\nWe begin by looking at some basic concepts of reinforcement learning. Note that the notation used\\nhere slightly differs from that used in the previous sections and chapters because we want to make\\nour description more consistent with those in the reinforcement learning literature. Nevertheless,\\nwe will show how this notation corresponds to the language modeling notation. The reader who\\nis already familiar with reinforcement learning techniques may skip or skim this subsection.\\nA general reinforcement learning framework describes how an agent interacts with a dynamic\\nenvironment. This interaction is modeled as a sequence of actions taken by the agent in response\\nto the state of the environment. At each time step, the agent observes the current state, chooses an\\naction based on its policy, performs the action, and then receives feedback from the environment\\nin the form of a reward and a new state. This sequence of observe-act-receive feedback is repeated'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 180}, page_content='174\\nAlignment\\nuntil the agent achieves its goal.\\nA reinforcement learning system involves several components\\n• Agent. This is the learner or decision-maker in reinforcement learning. In the context of\\nLLMs, it can be seen as the LLM itself.\\n• Environment. This includes everything external to the agent with which the agent interacts.\\nBut the environment in LLMs is less about a physical or virtual space and more about the\\nframework within which the agent (e.g., an LLM) receives feedback and learns.\\n• State (s). A state represents the current situation of the environment. Given a sequence of\\ntokens for language modeling, a state at a time step can be viewed as the tokens we observed\\nso far, that is, the context tokens we take to predict the next token. For example, we can\\ndeﬁne (x, y<t) as the state when predicting the next token at the time step t.\\n• Action (a). Actions represent possible decisions the agent can make. We can see them as\\npossible predicted tokens in the vocabulary.\\n• Reward (R). The reward is the feedback from the environment that evaluates the success\\nof an action. For example, r(s, a, s′) denotes the reward the agent receives for taking the\\naction a at the state s and moving to the next state s′. If the state-action sequence is given,\\nwe can denote the reward at the time step t as rt = r(st, at, st+1). Also note that if the\\ndecision-making process is deterministic, we can omit st+1 because it is can be determined\\nby st and at. In such cases, we can use r(st, at) as shorthand for r(st, at, st+1).\\n• Policy (π). For an LLM, a policy is deﬁned as the probability distribution over the tokens\\nthat the LLM predicts, given the preceding context tokens. Formally, this can be expressed\\nas\\nπ(a|s) = Pr(yt|x, y<t)\\n(4.14)\\nwhere a corresponds to the token y, and s corresponds to the context (x, y<t). Figure 4.7\\nillustrates how an LLM can be treated as a policy in the reinforcement learning framework.\\n• Value Function (V and Q). A state-value function (or value function, for short) assesses\\nthe expected discounted return (i.e., accumulated rewards) for an agent starting from a par-\\nticular state s and following a speciﬁc policy π. It is deﬁned as:\\nV (s)\\n=\\nE\\nh\\nr(s0, a0, s1) + γr(s1, a1, s2) + γ2r(s2, a2, s3) + · · ·\\n\\x0c\\x0c s0 = s, π\\ni\\n=\\nE\\nh\\nr0 + γr1 + γ2r2 + · · ·\\n\\x0c\\x0c s0 = s, π\\ni\\n=\\nE\\nh ∞\\nX\\nt=0\\nγtrt\\n\\x0c\\x0c s0 = s, π\\ni\\n(4.15)\\nwhere γ ∈[0, 1] is the discount factor that adjusts the importance of future rewards, s0 = s\\nindicates that the agent starts with the state s, and the expectation E is performed over all\\npossible trajectories (i.e., state-action sequences). Similarly, an action-value function (or'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 181}, page_content='4.3 Human Preference Alignment: RLHF\\n175\\nx0\\nx1\\n...\\nxm\\ny1\\n...\\nyt−1\\nPolicy (LLM)\\ny1\\ny2\\n...\\nyt\\nState st (x and y<t)\\nAction at\\nReward Model\\nR( st, at )\\nValue Functions\\nV ( st ) and Q( st, at )\\nFeedback\\nFig. 4.7: LLM as policy in reinforcement learning. At each step t, the LLM predicts a token yt given the model\\ninput x and the previously-generated tokens y<t. This process can be framed as a reinforcement learning problem,\\nwhere yt serves as the action, (x, y<t) as the state, and the predicted distribution Pr(yt|x, y<t) as the policy. Once\\nyt is predicted, the LLM inputs both (x, y<t) and yt to the reward model, which evaluates how effectively the chosen\\ntoken contributes to achieving the desired textual outcome. This evaluation generates reward scores which are used to\\ncompute the value functions V (st) and Q(st, at). These functions then provide feedback to the LLM and guide the\\npolicy training.\\nQ-value function) measures the expected return starting from a state s taking an action a\\nand thereafter following a policy π, given by\\nQ(s, a)\\n=\\nE\\nh ∞\\nX\\nt=0\\nγtrt\\n\\x0c\\x0c s0 = s, a0 = a, π\\ni\\n(4.16)\\nwhere a0 = a indicates that the action taken at the initial state is a.\\nThe goal of reinforcement learning is to learn a policy that maximizes the cumulative re-\\nward (or return) the agent receives over the long run.\\nGiven a state-action sequence τ =\\n{(s1, a1), ..., (sT , aT )}1, the cumulative reward over this sequence can be written as\\nR(τ)\\n=\\nT\\nX\\nt=1\\nrt\\n(4.17)\\nThe expectation of this cumulative reward over a space of state-action sequences is given in\\nthe form\\nJ(θ)\\n=\\nE\\nτ∼D\\nh\\nR(τ)\\n\\x0c\\x0cπθ\\ni\\n=\\nX\\nτ∈D\\nPrθ(τ)R(τ)\\n=\\nX\\nτ∈D\\nPrθ(τ)\\nT\\nX\\nt=1\\nrt\\n(4.18)\\n1We assume the state-action sequence begins with s1 and a1, rather than s0 and a0, to align with the notation\\ncommonly used in this chapter, where the prediction y typically starts from y1. Of course, it is also common to denote\\na state-action sequence as {(s0, a0), ..., (sT , aT )} or {(s0, a0), ..., (sT −1, aT −1)} in the literature. But this variation\\nin notation does not affect the discussion of the models presented here.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 182}, page_content='176\\nAlignment\\nwhere τ ∼D indicates that τ is drawn from the state-action sequence space D, and the subscript\\nθ indicates the parameters of the policy. J(θ) is also called the performance function.\\nThen the training objective is to maximize J(θ):\\n˜θ\\n=\\narg max\\nθ\\nJ(θ)\\n(4.19)\\nNow, we have a simple reinforcement learning approach: 1) we sample a number of state-\\naction sequences; then, 2) we evaluate each sequence using the performance function; then, 3) we\\nupdate the model to maximize this performance function. If we take Eq. (4.18) and use gradient\\ndescent to optimize the policy, this approach would constitutes a form of policy gradient methods\\n[Williams, 1992].\\nNote that in many NLP problems, such as machine translation, rewards are typically sparse.\\nFor instance, a reward is only received at the end of a complete sentence. This means that rt = 0\\nfor all t < T, and rt is non-zero only when t = T. Ideally, one might prefer feedback to\\nbe immediate and frequent (dense), and thus the training of the policy can be easier and more\\nefﬁcient. While several methods have been proposed to address sparse rewards, such as reward\\nshaping, we will continue in our discussion to assume a sparse reward setup, where the reward is\\navailable only upon completing the prediction.\\nThe model described in Eqs. (4.17-4.19) establishes a basic form of reinforcement learning,\\nand many variants and improvements of this model have been developed. Before showing those\\nmore sophisticated models, let us take a moment to interpret the objective function J(θ) from the\\nperspective of policy gradient. In gradient descent, we need to compute the gradient of J(θ) with\\nrespect to θ:\\n∂J(θ)\\n∂θ\\n=\\n∂P\\nτ∈D Prθ(τ)R(τ)\\n∂θ\\n=\\nX\\nτ∈D\\n∂Prθ(τ)\\n∂θ\\nR(τ)\\n=\\nX\\nτ∈D\\nPrθ(τ)∂Prθ(τ)/∂θ\\nPrθ(τ)\\nR(τ)\\n=\\nX\\nτ∈D\\nPrθ(τ)∂log Prθ(τ)\\n∂θ\\nR(τ)\\n(4.20)\\nIn some cases, we will assume that every sequence in D is equally probable (i.e., Prθ(τ) =\\n1/|D|). In this case we can simplify Eq. (4.20) and need only consider the terms ∂log Prθ(τ)\\n∂θ\\nand\\nR(τ):\\n∂J(θ)\\n∂θ\\n=\\n1\\nm\\nX\\nτ∈D\\n∂log Prθ(τ)\\n∂θ\\nR(τ)\\n(4.21)\\nOne advantage of this result is that R(τ) does not need to be differentiable, which means that we\\ncan use any type of reward function in reinforcement learning.\\nBy treating the generation of the sequence τ as a Markov decision process, we can further'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 183}, page_content='4.3 Human Preference Alignment: RLHF\\n177\\nderive ∂log Prθ(τ)\\n∂θ\\n, and obtain:\\n∂log Prθ(τ)\\n∂θ\\n=\\n∂\\n∂θ log\\nT\\nY\\nt=1\\nπθ(at|st) Pr(st+1|st, at)\\n=\\n∂\\n∂θ\\nT\\nX\\nt=1\\nlog πθ(at|st)\\n|\\n{z\\n}\\npolicy\\n+ ∂\\n∂θ\\nT\\nX\\nt=1\\nlog Pr(st+1|st, at)\\n|\\n{z\\n}\\ndynamics\\n(4.22)\\nwhere the gradient is decomposed into two parts: the policy gradient and the dynamics gradient.\\nThe policy component, log πθ(at|st), determines the log-probability of taking action at given\\nstate st, and it is parameterized by θ. The dynamics component, log Pr(st+1|st, at), represents\\nthe log-probability of transitioning to state st+1 from state st after taking action at. In typical\\nreinforcement learning settings, the dynamics are not directly inﬂuenced by the policy parameters\\nθ, and thus, their derivatives are often zero. In this case, therefore, Eq. (4.22) can be simpliﬁed to:\\n∂log Prθ(τ)\\n∂θ\\n=\\n∂\\n∂θ\\nT\\nX\\nt=1\\nlog πθ(at|st)\\n(4.23)\\nIn other words, we only concentrate on optimizing the policy without concerning ourselves with\\nthe underlying dynamics.\\nSubstituting Eq. (4.23) into Eq. (4.21), and expanding R(τ), we then obtain\\n∂J(θ)\\n∂θ\\n=\\n1\\n|D|\\nX\\nτ∈D\\n∂\\n∂θ\\n\\x10 T\\nX\\nt=1\\nlog πθ(at|st)\\nT\\nX\\nt=1\\nrt\\n\\x11\\n(4.24)\\nWhile this policy gradient approach is straightforward, it suffers from the problem that the\\nvariance of the estimated gradients can be very high, making the learning process noisy and inef-\\nﬁcient. One reason for this high variance problem is that rewards can vary greatly across different\\nsteps or scenarios. Imagine that in a sequence of action decisions, the reward model tends to assign\\nsmall rewards to good actions (e.g., Rt = 2) and large penalties to poor actions (e.g., Rt = −50).\\nSuch varying reward scales for good and poor actions can result in a very low total reward for the\\nentire sequence, even if it includes good actions.\\nOne simple method for reducing the variance of the gradient is to set a baseline b and subtract\\nit from PT\\nt=1 rt, resulting in PT\\nt=1 rt −b.2 Here, the baseline can be interpreted as a reference\\npoint. By centering the rewards around this baseline, we remove systematic biases in the reward\\nsignal, making the updates more stable and less sensitive to extreme ﬂuctuations in individual\\nrewards.\\n2In fact, the use of a baseline b does not change the variance of the total rewards PT\\nt=1 rt. However, it is important\\nto note that while introducing a baseline does not alter the overall variance of the rewards, it helps reduce the variance\\nof the gradient estimates. This is because subtracting the baseline from the total rewards effectively reduces ﬂuctuations\\naround their mean, which makes the gradient estimates more stable. In general, the operation PT\\nt=1 rt −b centers the\\nrewards around zero (e.g., b is deﬁned as the expected value of PT\\nt=1 rt), which can lead to reduced variance in the\\nproduct PT\\nt=1 log πθ(at|st)(PT\\nt=1 rt −b).'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 184}, page_content='178\\nAlignment\\nThis policy gradient model with a baseline can be given by\\n∂J(θ)\\n∂θ\\n=\\n1\\n|D|\\nX\\nτ∈D\\n∂\\n∂θ\\n\\x10 T\\nX\\nt=1\\nlog πθ(at|st)\\n\\x11\\x10 T\\nX\\nt=1\\nrt −b\\n\\x11\\n=\\n1\\n|D|\\nX\\nτ∈D\\n∂\\n∂θ\\nh T\\nX\\nt=1\\nlog πθ(at|st)\\n\\x10\\nT\\nX\\nk=1\\nrk −b\\n\\x11i\\n=\\n1\\n|D|\\nX\\nτ∈D\\n∂\\n∂θ\\nh T\\nX\\nt=1\\nlog πθ(at|st)\\n\\x10 t−1\\nX\\nk=1\\nrk +\\nT\\nX\\nk=t\\nrk −b\\n\\x11i\\n(4.25)\\nHere we write PT\\nk=1 rk as the sum of two terms Pt−1\\nk=1 rk and PT\\nk=t rk to distinguish between the\\nrewards accrued before and after the action at time step t. Note that in Markov decision processes,\\nthe future is independent of the past given the present. Therefore, the action taken at time step t\\ncannot inﬂuence the rewards received before t, or in other words, the rewards prior to t are already\\n“ﬁxed” by the time the action at t is chosen. The term Pt−1\\nk=1 rk does not contribute to the gradient\\nand can be omitted, leading to a simpliﬁed version of Eq. (4.25)\\n∂J(θ)\\n∂θ\\n=\\n1\\n|D|\\nX\\nτ∈D\\n∂\\n∂θ\\nh T\\nX\\nt=1\\nlog πθ(at|st)\\n\\x10 T\\nX\\nk=t\\nrk −b\\n\\x11i\\n(4.26)\\nAlso note that removing PT\\nk=t rk can further reduce the variance of the gradient.\\nThere are many ways to deﬁne the baseline b. Here we consider the value function of the state\\nst, that is, the estimated value of being in state st: V (st) = E(rt + rt+1 + · · · + rT ). Hence we\\nhave\\nA(st, at)\\n=\\nT\\nX\\nk=t\\nrk −b\\n=\\nT\\nX\\nk=t\\nrk −V (st)\\n(4.27)\\nwhere PT\\nk=t rk represents the actual return received, and V (st) represents the expected return.\\nA(st, at) (or At for short) is called the advantage at time step t, which quantiﬁes the relative\\nbeneﬁt of the action at compared to the expected value of following the policy from the state st\\nonward.\\nBy using the advantage function A(st, at), the gradient of J(θ) can be written in the form\\n∂J(θ)\\n∂θ\\n=\\n1\\n|D|\\nX\\nτ∈D\\n∂\\n∂θ\\n\\x10 T\\nX\\nt=1\\nlog πθ(at|st)A(st, at)\\n\\x11\\n(4.28)\\nThis optimization objective corresponds to the advantage actor-critic (A2C) method in re-\\ninforcement learning [Mnih et al., 2016]. In this method, the actor aims at learning a policy. It\\nupdates the policy parameters using Eq. (4.28) to help focus more on actions that are likely to\\nimprove performance. The critic, on the other hand, updates its estimation of the value function,\\nwhich is used to calculate the advantage function A(st, at), thus serving as the evaluator of the'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 185}, page_content='4.3 Human Preference Alignment: RLHF\\n179\\npolicy being learned by the actor.\\nIn the A2C method, A(st, at) is typically expressed as the difference of the action-value func-\\ntion Q(st, at) and the state-value function V (st)\\nA(st, at)\\n=\\nQ(st, at) −V (st)\\n(4.29)\\nAt ﬁrst glance, this model may seem challenging to develop because it requires two separate sub-\\nmodels to calculate Q(st, at) and V (st) respectively. Fortunately, considering that Q(st, at) can\\nbe deﬁned as the return rt + V (st+1), we can rewrite Eq. (4.29) as\\nA(st, at)\\n=\\nrt + V (st+1) −V (st)\\n(4.30)\\nor alternatively, introduce the discount factor γ to obtain a more general form\\nA(st, at)\\n=\\nrt + γV (st+1) −V (st)\\n(4.31)\\nA(st, at) = rt + γV (st+1) −V (st) is also called the temporal difference (TD) error. What\\nwe need is to train a critic network for the value function V (st), and then use it to compute the\\nadvantage function3.\\nUp to this point, we have spent considerable space discussing the basics of reinforcement\\nlearning, especially on how to derive the optimization objective for the A2C method. However,\\nreinforcement learning is a vast ﬁeld, and many technical details cannot be covered here. The in-\\nterested reader can refer to reinforcement learning books for more details [Sutton and Barto, 2018;\\nSzepesvári, 2010]. Nevertheless, we now have the necessary knowledge to further discuss RLHF.\\nIn the subsequent subsections, we will return to the discussion on LLM alignment, demonstrating\\nhow to use the A2C method for aligning with human preferences.\\n4.3.2\\nTraining Reward Models\\nWe have shown that reward models play a very important role in the general reinforcement learn-\\ning framework and form the basis for computing value functions. We now consider the problem\\nof training these reward models.\\nIn RLHF, a reward model is a neural network that maps a pair of input and output token\\nsequences to a scalar. Given an input x and an output y, the reward can be expressed as\\nr\\n=\\nReward(x, y)\\n(4.33)\\nwhere Reward(·) is the reward model. r can be interpreted as a measure of how well the output y\\naligns with the desired behavior given the input x. As discussed in the previous subsection, both x\\n3The training loss for the value network (or critic network) in A2C is generally formulated as the mean squared\\nerror between the computed return rt + γV (st+1) and the predicted state value V (st). Suppose that the value network\\nis parameterized by ω. The loss function is given by\\nLv(ω)\\n=\\n1\\nM\\nX \\x00rt + γVω(st+1) −Vω(st)\\x012\\n(4.32)\\nwhere M is the number of training samples, for example, for a sequence of T tokens, we can set M = T.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 186}, page_content='180\\nAlignment\\nx0\\nx1\\nx2\\n· · ·\\nxm\\ny1\\ny2\\n· · ·\\nyn\\n(Last Token ⟨EOS⟩)\\nhx0\\nhx1\\nhx2\\n· · ·\\nhxm\\nhy1\\nhy2\\n· · ·\\nhlast\\nTransformer Decoder (LLM)\\nRepresentation\\nat Each Position\\nReward (Scalar)\\nWr\\nLinear Map\\nFig. 4.8: Architecture of the reward model based on Transformer. The main component of this model is still an LLM.\\nWe use the Transformer decoder as the sequence representation model. We extract the representation of the last position\\nof the decoder as the representation of the entire sequence [x, y]. We then map this representation to a scalar through a\\nlinear transformation, which serves as the reward score for [x, y].\\nand y are assumed to complete texts. This means that the reward model evaluates the relationship\\nbetween inputs and outputs that provide full semantic content. For example, when applying the\\nreward model, it assigns a value of 0 (or another predetermined value) at each position t in the\\noutput sequence y = y1...yn. Only at the ﬁnal position, when t = n, does the reward model\\ngenerate the actual reward score. To keep the notation uncluttered, we will use r(x, y) to denote\\nthe reward model Reward(x, y) from here on.\\nThere are many ways to implement the reward model. One simple approach is to build the\\nreward model based on a pre-trained LLM. More speciﬁcally, we can concatenate x and y to form\\na single token sequence seqx,y = [x, y]. We run a pre-trained LLM on this sequence, as usual,\\nand at each position, we obtain a representation from the top-most Transformer layer. Then, we\\ntake the representation at the last position (denoted by hlast) and map it to a scalar via linear\\ntransformation:\\nr(x, y)\\n=\\nhlastWr\\n(4.34)\\nwhere hlast is a d-dimensional vector, and Wr is a d × 1 linear mapping matrix. This architecture\\nof the reward model is illustrated in Figure 4.8.\\nTo train the reward model, the ﬁrst step is to collect human feedback on a set of generated\\noutputs. Given an input x, we use the LLM to produce multiple candidate outputs {y1, ..., yN}.\\nHuman feedback can be obtained in several ways:\\n• Pairwise Comparison (Pairwise Ranking). Given two different outputs, human experts\\nselect which one is better.\\n• Rating. Human experts provide a score or rating to each output. This score is often a\\ncontinuous or discrete numerical value, such as a score on a scale (e.g., 1-5 stars, or 1-10\\npoints). In some cases, the rating might be binary, indicating a “yes/no” or “positive/nega-\\ntive” preference.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 187}, page_content='4.3 Human Preference Alignment: RLHF\\n181\\n• Listwise Ranking. Human experts are asked to rank or order the given set of possible\\noutputs.\\nHere we consider pairwise comparison feedback as it is one of the simplest and most com-\\nmon forms of human feedback used in RLHF. In this setting, each time, two outputs (ya, yb) are\\nrandonly drawn from the candidate pool {y1, ..., yN}. Human experts are then presented with\\nthese pairs and asked to decide which output they prefer based on speciﬁc criteria, such as clarity,\\nrelevance, and accuracy. The human feedback can be encoded as a binary label, ya ≻yb for a\\npreference for ya, and yb ≻ya for a preference for yb.\\nOne simple and widely used model for describing such pairwise comparisons is the Bradley-\\nTerry model [Bradley and Terry, 1952]. It is a probabilistic model that estimates the probability\\nthat one item is preferred over another. Adapting this model to the notation used here, we can\\nwrite the probability that ya is preferred over yb in the form\\nPr(ya ≻yb|x)\\n=\\ner(x,ya)\\ner(x,ya) + er(x,yb)\\n=\\ner(x,ya)−r(x,yb)\\ner(x,ya)−r(x,yb) + 1\\n=\\nSigmoid(r(x, ya) −r(x, yb))\\n(4.35)\\nWhen training the reward model, we want to maximize this preference probability. A loss\\nfunction based on the Bradley-Terry model is given by\\nLr(φ)\\n=\\n−E(x,ya,yb)∼Dr\\n\\x02 log Prφ(ya ≻yb|x)\\n\\x03\\n(4.36)\\nwhere (x, ya, yb) is drawn from a human-annotated dataset Dr consisting of preference pairs of\\noutputs and their corresponding inputs. φ represents the parameters of the reward model, which\\nincludes both the parameters of the Transformer decoder and the linear mapping matrix Wr. In\\npractice, assuming (x, ya, yb) is uniformly sampled from Dr, we can replace the expectation with\\na summation\\nLr(φ)\\n=\\n−1\\n|Dr|\\nX\\n(x,ya,yb)∈Dr\\nlog Prφ(ya ≻yb|x)\\n(4.37)\\nThe goal of training the reward model is to ﬁnd the optimal parameters ˆφ that minimize this\\nloss function, given by\\nˆφ\\n=\\narg min\\nφ\\nLr(φ)\\n(4.38)\\nSince the reward model itself is also an LLM, we can directly reuse the Transformer training\\nprocedure to optimize the reward model. The difference from training a standard LLM is that we\\nonly need to replace the cross-entropy loss with the pairwise comparison loss as described in Eq.\\n(4.37). After the training of the reward model, we can apply the trained reward model rˆφ(·) to\\nsupervise the target LLM for alignment.\\nIt is worth noting that although we train the reward model to perform pairwise ranking, we\\napply it to score each input-output pair independently during the alignment process. The pairwise'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 188}, page_content='182\\nAlignment\\nranking objective ensures that the reward model is sensitive to subtle differences between outputs,\\nbut we rely on the continuous scores produced by the reward model to guide the optimization of\\nthe LLM. An advantage of this approach is that we can choose from or combine various ranking\\nloss functions, and still apply the resulting reward models in the same way as we have done in this\\nsubsection. This consistency ensures a uniﬁed framework for aligning the LLM, regardless of the\\nspeciﬁc ranking loss used during reward model training.\\n4.3.3\\nTraining LLMs\\nHaving obtained the reward model, we then train the policy (i.e., the LLM) via the A2C method.\\nRecall from Section 4.3.1 that a state-action sequence or trajectory τ can be evaluated by the utility\\nfunction\\nU(τ; θ)\\n=\\nT\\nX\\nt=1\\nlog πθ(at|st)A(st, at)\\n(4.39)\\nwhere A(st, at) is the advantage of taking the action at given the state st. An estimate of A(st, at)\\nis deﬁned as the TD error rt + γV (st+1) −V (st), where the value function V (st) is trained with\\nthe reward model.\\nGiven this utility function, the A2C-based loss function can be written in the form\\nL(θ)\\n=\\n−Eτ∼D\\n\\x02U(τ; θ)\\n\\x03\\n=\\n−Eτ∼D\\n\\x02 T\\nX\\nt=1\\nlog πθ(at|st)A(st, at)\\n\\x03\\n(4.40)\\nwhere D is a space of state-action sequences. As usual, the goal of training the policy is to\\nminimize this loss function\\n˜θ\\n=\\narg min\\nθ\\nL(θ)\\n(4.41)\\nIf we map the problem back to the language modeling problem and adopt the notation from\\nLLMs, the loss function can be written as:\\nL(θ)\\n=\\n−E(x,y)∼D\\n\\x02U(x, y; θ)\\n\\x03\\n(4.42)\\nwhere\\nU(x, y; θ)\\n=\\nT\\nX\\nt=1\\nlog πθ(yt|x, y<t)A(x, y<t, yt)\\n(4.43)\\nHere πθ(yt|x, y<t) = Prθ(yt|x, y<t) is the LLM parameterized by θ.\\nIn general, we do not have a human annotated input-output dataset D in RLHF, but a dataset\\ncontaining inputs only. The outputs, in this case, are typically the predictions made by the LLM.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 189}, page_content='4.3 Human Preference Alignment: RLHF\\n183\\nThe loss function is then deﬁned as\\nL(θ)\\n=\\n−Ex∼DEy∼πθ(·|x)\\n\\x02U(x, y; θ)\\n\\x03\\n(4.44)\\nwhere D denotes the input-only dataset, and y ∼πθ(·|x) denotes that the output y is sampled by\\nthe policy πθ(·|x).\\nThe above formulation provides a basic form of the A2C method for LLMs. Improved versions\\nof this model are more commonly used in RLHF. In the following discussion, we will still use\\nthe reinforcement learning notation to simplify the presentation and will get back the language\\nmodeling notation later.\\nOne common improvement of policy gradient methods is to use importance sampling to\\nreﬁne the estimation of U(τ; θ). This can be written as\\nU(τ; θ)\\n=\\nT\\nX\\nt=1\\nπθ(at|st)\\nπθref(at|st)A(st, at)\\n(4.45)\\nHere we replace the log-probability log πθ(at|st) with the ratio\\nπθ(at|st)\\nπθref (at|st). θref denotes the pa-\\nrameters of the previous policy (such as an initial model from which we start the training). So\\nπθ(at|st)\\nπθref (at|st), also called the ratio function, can be interpreted as the log-probability ratio between\\nthe current policy πθ and the previous policy πθref (call it the reference policy). By using the\\nratio function we reweight the observed rewards based on the likelihood of the actions under the\\ncurrent policy versus the reference policy. When\\nπθ(at|st)\\nπθref (at|st) > 1, the action at is more favored by\\nthe current policy compared to the reference policy. By contrast, when\\nπθ(at|st)\\nπθref (at|st) < 1, the action\\nat is less favored by the current policy4.\\n4Consider a more general case where we wish to evaluate the policy using its expected reward (also see Eq. (4.18))\\nJ(θ)\\n=\\nEτ∼πθ\\nh\\nR(τ)\\ni\\n(4.46)\\nHere τ ∼πθ means that the sequence τ is generated by the policy πθ. Alternatively, we can write J(θ) in another form\\nJ(θ)\\n=\\nEτ∼πθref\\nh Prθ(τ)\\nPrθref (τ)R(τ)\\ni\\n(4.47)\\nIt is not difﬁcult to ﬁnd that the right-hand sides of these equations are essentially the same since\\nEτ∼πθref\\nh\\nPrθ(τ)\\nPrθref (τ)R(τ)\\ni\\n= P\\nτ Prθref (τ)\\nPrθ(τ)\\nPrθref (τ)R(τ) = P\\nτ Prθ(τ)R(τ) = Eτ∼πθ\\nh\\nR(τ)\\ni\\nNote that this equivalence holds only when the expectation is performed over the entire sequence space. In practice,\\nhowever, we often only sample a relatively small number of sequences using a policy in policy learning. As a result,\\nthe sampling method itself matters. Eq. (4.47) offers an interesting manner to separate the sampling and reward\\ncomputation processes: we ﬁrst use a baseline policy (with θref) to sample a number of sequences, and then use the\\ntarget policy (with θ) to compute the expected reward. In this way, we separate the policy used for collecting the data,\\nand the policy used for computing the gradient. This approach avoids the need to directly sample from the policy we are\\nevaluating, which can be beneﬁcial in cases where generating sequences from the target policy is expensive or difﬁcult.\\nIn reinforcement learning, Eτ∼πθref\\nh\\nPrθ(τ)\\nPrθref (τ)R(τ)\\ni\\nis often called a surrogate objective.\\nEq. (4.47) can also be interpreted from a policy gradient perspective. For Eτ∼πθref\\nh\\nPrθ(τ)\\nPrθref (τ)R(τ)\\ni\\n, the gradient at\\nθ = θref is given by\\n∂\\n∂θ Eτ∼πθref\\nh Prθ(τ)\\nPrθref (τ)R(τ)\\ni\\x0c\\x0c\\x0c\\nθ=θref\\n=\\nEτ∼πθref\\nh∂Prθ(τ)|θ=θref\\n∂θ\\nR(τ)\\ni\\n(4.48)\\nThe right-hand side is a standard form used in policy gradient methods, meaning that we compute the direction of'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 190}, page_content='184\\nAlignment\\nA problem with the model presented in Eq. (4.47) (as well as in Eq. (4.39)) is that the\\nvariance in the gradient estimates is often high, making the learning process unstable. To mitigate\\nthis issue, techniques such as clipping are often employed to bound the importance weights and\\nprevent large updates. A clipped version of the utility function (also called the clipped surrogate\\nobjective function) is given by\\nUclip(τ; θ)\\n=\\nT\\nX\\nt=1\\nClip\\n\\x10 πθ(at|st)\\nπθref(at|st)\\n\\x11\\nA(st, at)\\n(4.49)\\nClip\\n\\x10 πθ(at|st)\\nπθref(at|st)\\n\\x11\\n=\\nmin\\n\\x10 πθ(at|st)\\nπθref(at|st), bound\\n\\x00 πθ(at|st)\\nπθref(at|st), 1 −ǫ, 1 + ǫ\\n\\x01\\x11\\n(4.50)\\nHere the function bound( πθ(at|st)\\nπθref (at|st), 1 −ǫ, 1 + ǫ) constrains the ratio function to the range [1 −\\nǫ, 1 + ǫ].\\nA further improvement to the above model is to consider trust regions in optimization [Schulman et al.,\\n2015]. In reinforcement learning, a large update to the policy can lead to instability, where the\\nagent may start performing worse after an update. A reasonable idea is to optimize the model in\\nthe trust region, which refers to a region around the current parameter estimate where the model\\nis well-behaved. One approach to incorporating trust regions is to impose a constraint on the size\\nof the policy update, ensuring that the current policy does not deviate too signiﬁcantly from the\\nreference policy. This can be achieved by adding a penalty based on some form of divergence\\nbetween the current and reference policies to the objective function. A simple form of such a\\npenalty is given by the difference in the log-probability of the sequence τ under the current policy\\nversus the reference policy:\\nPanalty\\n=\\nlog πθ(τ) −log πθref(τ)\\n(4.51)\\nIn practice, this penalty can be approximated by considering only the policy probabilities and\\nignoring the dynamics. This gives\\nPenalty\\n=\\nT\\nX\\nt=1\\nlog πθ(at|st) −\\nT\\nX\\nt=1\\nlog πθref(at|st)\\n(4.52)\\nBy including this penalty in the optimization objective, we encourage the current policy to remain\\nclose to the reference policy, limiting very large updates that could destabilize learning.\\nWe can incorporate this penalty into the clipped surrogate objective function, and obtain\\nUppo-clip(τ; θ)\\n=\\nUclip(τ; θ) −βPenalty\\n(4.53)\\nwhere β is the weight of the penalty. This training method is called proximal policy optimization\\n(PPO), which is one of the most popular reinforcement learning methods used in LLMs and many\\nother ﬁelds [Schulman et al., 2017].\\nNow we can write the objective of training LLMs in the form of PPO.\\nthe parameter update at the point θ = θref on the optimization surface.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 191}, page_content='4.3 Human Preference Alignment: RLHF\\n185\\nU(x, y; θ)\\n=\\nUppo-clip(x, y; θ) −βPenalty\\n(4.54)\\nwhere\\nUppo-clip(x, y; θ)\\n=\\nT\\nX\\nt=1\\nClip\\n\\x10 πθ(yt|x, y<t)\\nπθref(yt|x, y<t)\\n\\x11\\nA(x, y<t, yt)\\n(4.55)\\nPenalty\\n=\\nlog Prθ(y|x) −log Prθref(y|x)\\n=\\nT\\nX\\nt=1\\nlog Prθ(yt|x, y<t) −\\nT\\nX\\nt=1\\nlog Prθref(yt|x, y<t)\\n(4.56)\\nAlthough the notation here appears a bit tedious, the idea of PPO is simple: we develop an\\nobjective by combining the clipped likelihood ratio of the target and reference policies with an\\nadvantage function, and then impose a penalty that ensures policy updates are not too large. The\\nPPO-based RLHF is illustrated in Figure 4.9.\\nTo summarize, implementing RLHF requires building four models, all based on the Trans-\\nformer decoder architecture.\\n• Reward Model (rφ(·) where φ denotes the parameters). The reward model learns from\\nhuman preference data to predict the reward for each pair of input and output token se-\\nquences. It is a Transformer decoder followed by a linear layer that maps a sequence (the\\nconcatenation of the input and output) to a real-valued reward score.\\n• Value Model or Value Function (Vω(·) where ω denotes the parameters). The value func-\\ntion receives reward scores from the reward model and is trained to predict the expected\\nsum of rewards that can be obtained starting from a state. It is generally based on the same\\narchitecture as the reward model.\\n• Reference Model (πθref(·) = Prθref(·) where θref denotes the parameters). The reference\\nmodel is the baseline LLM that serves as a starting point for policy training. In RLHF, it\\nrepresents the previous version of the model or a model trained without human feedback. It\\nis used to perform sampling over the space of outputs and contribute to the loss computation\\nfor policy training.\\n• Target Model or Policy (πθ(·) = Prθ(·) where θ denotes the parameters). This policy\\ngoverns how the LLM decides the most appropriate next token given its context. It is trained\\nunder the supervision of both the reward model and the value model.\\nIn practice, these models need to be trained in a certain order. First, we need to initialize them\\nusing some other models. For example, the reward model and the value model can be initialized\\nwith a pre-trained LLM, while the reference model and the target model can be initialized with a\\nmodel that has been instruction ﬁne-tuned. Note that, at this point, the reference model is ready for\\nuse and will not be further updated. Second, we need to collect human preference data and train the\\nreward model on this data. Third, both the value model and the policy are trained simultaneously\\nusing the reward model. At each position in an output token sequence, we update the value model'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 192}, page_content='186\\nAlignment\\nReward Model\\nTraining\\nPolicy Training\\nReward Model\\nrφ(x, y)\\nTo Learn\\nLLM Policy\\nPrθ(yt|x, y<t)\\nTo Learn\\nValue Function\\nVω(x, y<t)\\nTo Learn\\nRef Model\\nPrθold(yt|x, y<t)\\nFixed\\nMinimizing the loss based on\\nthe Bradley-Terry model\\nmin\\nφ −\\n1\\n|Dr|\\nP\\n(x,ya,yb)∈Dr\\nlog σ(rφ(x, ya) −rφ(x, yb))\\nHuman preference data Dr = {(x, ya, yb)}\\nInput-only data D = {x}\\nx0\\nx1\\n· · ·\\nxm\\ny1\\n· · ·\\nyt−1\\nState (x, y<t)\\nx1\\nx2\\n· · ·\\ny1\\ny2\\n· · ·\\nyt\\nAction yt\\n(sampled with Prθold )\\nEvaluate the state-action pair using the advantage\\nfunction or the TD error (based on the reward\\nmodel and the value function)\\nLLM Policy\\nLLM Policy\\nMinimizing the clipped PPO loss\\nwith the penalty\\nmin\\nθ\\n−P\\nx∈D,y∼Prθold (·|x)\\nPT\\nt=1\\nh\\nClip\\x00Prθ(yt|x,y<t)\\nPrθold (yt|x,y<t)\\n\\x01\\nAt−\\nβ · \\x00log Prθ(yt|x, y<t)−\\nlog Prθold(yt|x, y<t)\\x01i\\nValue Function\\nMinimizing the MSE between the\\ncomputed return and the predicted\\nstate value\\nmin\\nω\\n1\\nM\\nP\\nx∈D\\nPT\\nt=1\\n\\x02\\nrt + γVω(x, y<t+1) −Vω(x, y<t)\\x032\\n∗∗rt = r(x, y<t+1) denotes the reward received as step t.\\n∗∗At denotes the advantage at step t, and can be deﬁned as rt + γVω(x, y<t+1) −Vω(x, y<t)\\nFig. 4.9: Illustration of RLHF. The ﬁrst step is to collect human preference data and train the reward model using this\\ndata. Once the reward model is optimized, along with the reference model, we proceed to train both the policy and\\nthe value function. At each prediction step, we compute the sum of the PPO-based loss and update the parameters of\\nthe policy. This requires access to the reward model, the reference model, and the value function at hand. At the same\\ntime, we update the parameters of the value function by minimizing the MSE loss.\\nby minimizing the MSE error of value predition, and the policy is updated by minimizing the PPO\\nloss.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 193}, page_content='4.4 Improved Human Preference Alignment\\n187\\n4.4\\nImproved Human Preference Alignment\\nIn the previous section, we reviewed the basic concepts of reinforcement learning and the general\\nframework of RLHF. In this section, we will discuss some reﬁnements of RLHF and alternative\\nmethods to achieve human preference alignment.\\n4.4.1\\nBetter Reward Modeling\\nIn Section 4.3.2, we highlighted the task of learning from human preferences as well as the use\\nof pairwise ranking loss for training reward models. Here we consider more methods for reward\\nmodeling. Our discussion will be relatively general, and since the reward model is widely used in\\nmany reinforcement learning problems, it will be easy for us to apply the methods discussed here\\nto RLHF and related applications.\\n4.4.1.1\\nSupervision Signals\\nThe training of reward models can broadly be seen as a ranking problem, where the model learns\\nto assign scores to outputs so that their order reﬂects the preferences indicated by humans. There\\nare several methods to train a reward model from the perspective of ranking.\\nOne approach is to extend pairwise ranking to listwise ranking. For each sample in a dataset,\\nwe can use the LLM to generate multiple outputs, and ask human experts to order these outputs.\\nFor example, given a set of four outputs {y1, y2, y3, y4}, one possible order of them can be\\ny2 ≻y3 ≻y1 ≻y4. A very simple method to model the ordering of the list is to accumulate the\\npairwise comparison loss. For example, we can deﬁne the listwise loss by accumulating the loss\\nover all pairs of outputs:\\nLlist\\n=\\n−E(x,Y )∼Dr\\nh\\n1\\nN(N −1)\\nX\\nya∈Y,yb∈Y\\nya̸=yb\\nlog Pr(ya ≻yb|x)\\ni\\n(4.57)\\nwhere Y is a list of outputs, and N is the number of outputs in the list. Pr(ya ≻yb|x) can be\\ndeﬁned using the Bradley-Terry model, that is, Pr(ya ≻yb|x) = Sigmoid(r(x, ya) −r(x, yb)).\\nHere we omit the φ superscript on the Pr(·) to keep the notation uncluttered.\\nAn extension to the Bradley-Terry model for listwise ranking could involve a ranking mecha-\\nnism that takes into account the entire list of outputs rather than just pairwise comparisons. One\\nsuch model is the Plackett-Luce model, which generalizes the Bradley-Terry model to handle\\nmultiple items in a ranking [Plackett, 1975]. In the Plackett-Luce model, for each item in a list,\\nwe deﬁne a “worth” for this item that reﬂects its relative strength of being chosen over other items.\\nFor the reward modeling problem here, the worth of y in the list Y can be deﬁned as\\nα(y)\\n=\\nexp(r(x, y))\\n(4.58)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 194}, page_content='188\\nAlignment\\nThen the probability of selecting y from Y is given by\\nPr(y is selected|x, Y )\\n=\\nα(y)\\nP\\ny′∈Y α(y′)\\n=\\nexp(r(x, y))\\nP\\ny′∈Y exp(r(x, y′))\\n(4.59)\\nSuppose ˚Y is an ordered list yj1 ≻yj2 ≻· · · ≻yjN . The overall log-probability of this\\nordered list can be deﬁned as the sum of the conditional log-probabilities at each stage of selection,\\ngiven by\\nlog Pr(˚Y |x)\\n=\\nlog Pr(yj1 ≻yj2 ≻· · · ≻yjN |x)\\n=\\nlog Pr(yj1|x, {yj1, yj2, ..., yjN }) +\\nlog Pr(yj2|x, {yj2, ..., yjN }) +\\n· · · +\\nlog Pr(yjN|x, {yjN })\\n=\\nN\\nX\\nk=1\\nlog Pr(yjk|x, ˚Y≥k)\\n(4.60)\\nwhere ˚Y≥k represents the subset of the list of outputs that remain unselected at the k-th stage, i.e.,\\n˚Y≥k = {yjk, ..., yjN }. Given the log-probability log Pr(˚Y |x), we can deﬁne the loss function\\nbased on the Plackett-Luce model by\\nLpl\\n=\\n−E(x,˚\\nY )∼Dr\\n\\x02 log Pr(˚Y |x)\\n\\x03\\n(4.61)\\nThere are also many other pairwise and listwise methods for modeling rankings, such as\\nRankNet [Burges et al., 2005] and ListNet [Cao et al., 2007]. All these methods can be cate-\\ngorized into a large family of learning-to-rank approaches, and most of them are applicable to the\\nproblem of modeling human preferences. However, discussing these methods is beyond the scope\\nof this chapter. Interested readers can refer to books on this topic for more details [Liu, 2009; Li,\\n2011].\\nIn addition to pairwise and listwise ranking, using pointwise methods to train reward models\\noffers an alternative way to capture human preferences. Unlike methods that focus on the relative\\nrankings between different outputs, pointwise methods treat each output independently. For ex-\\nample, human experts might assign a score to an individual output, such as a rating on a ﬁve-point\\nscale. The objective is to adjust the reward model so that its outputs align with these scores. A\\nsimple way to achieve pointwise training is through regression techniques where the reward of\\neach output is treated as a target variable. Let ϕ(x, y) be the score assigned to y given x by\\nhumans. Pointwise reward models can be trained by minimizing a loss function, often based on\\nmean squared error or other regression losses, between the predicted reward r(x, y) and the actual\\nhuman feedback ϕ(x, y). For example, the loss function could be\\nLpoint\\n=\\n−E\\n\\x02ϕ(x, y) −r(x, y)\\n\\x032\\n(4.62)\\nWhile pointwise methods are conceptually simpler and can directly guide the reward model to'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 195}, page_content='4.4 Improved Human Preference Alignment\\n189\\npredict scores, they might not always be the best choice in RLHF. A problem is that these methods\\nmay struggle with high variance in human feedback, especially when different experts provide\\ninconsistent scores for similar outputs. Because they focus on ﬁtting to absolute scores rather than\\nrelative differences, inconsistencies in scoring can lead to poor model performance. Moreover,\\nﬁtting to speciﬁc scored outputs might discourage generalization, particularly given that training\\ndata is often very limited in RLHF. In contrast, methods that consider relative preferences can\\npromote the learning of more generalized patterns of success and failure. Nevertheless, there are\\nscenarios where pointwise methods might still be suitable. For example, in tasks where training\\ndata is abundant and the costs of obtaining accurate, consistent annotations are low, pointwise\\nmethods can prove effective.\\nIn fact, to make the supervision signal for training the reward model more robust, we can also\\nintroduce additional regularization terms into training. For example, if we consider the ﬁrst term\\nUppo-clip(x, y; θ) in Eq. (4.54) as a type of generalized reward, then the second term (i.e., the\\npenalty term) can be viewed as a form of regularization for the reward model, except that here the\\ngoal is to train the policy rather than the reward model. Another example is that Eisenstein et al.\\n[2023] develop a regularization term based on the squared sum of rewards, and add it to the\\npairwise comparison loss in RLHF:\\nLreg\\n=\\nLpair + (−E(x,ya,yb)∼Dr\\n\\x02r(x, ya) + r(x, yb)\\n\\x032)\\n=\\n−E(x,ya,yb)∼Dr\\n\\x02 log Prφ(ya ≻yb|x)\\n\\x03\\n−E(x,ya,yb)∼Dr\\n\\x02r(x, ya) + r(x, yb)\\n\\x032\\n(4.63)\\nOptimizing with this regularization term can help mitigate the underdetermination of reward mod-\\nels5.\\n4.4.1.2\\nSparse Rewards vs. Dense Rewards\\nAs discussed in Section 4.3, the rewards in RLHF are very sparse: they are observed only at the\\nend of sequences, rather than continuously throughout the generation process. Dealing with sparse\\nrewards has long been a concern in reinforcement learning, and has been one of the challenges in\\nmany practical applications. For example, in robotics, it often needs to shape the reward function\\nto ease optimization rather than relying solely on end-of-sequence rewards. Various methods\\nhave been developed to address this issue. One common approach is reward shaping, where the\\noriginal function is modiﬁed to include intermediate rewards, thereby providing more immediate\\nfeedback. Also, one can adopt curriculum learning to sequentially structure tasks in a way that the\\ncomplexity gradually increases. This can help models to master simpler tasks ﬁrst, which prepares\\nthem for more complex challenges as their skills develop. There are many such methods that can\\nmitigate the impact of sparse rewards, such as Monte Carlo methods and intrinsic motivation. Most\\nof these methods are general and the discussion of them can be found in the broader literature on\\nreinforcement learning, such as Sutton and Barto [2018]’s book.\\nAlthough we do not discuss methods for mitigating sparse rewards in detail here, an interesting\\nquestion arises: why are sparse rewards so successful in RLHF? Recall from Section 4.3.1 that\\nthe supervision signal received at each time step t is not the reward for the current action, but\\n5A model is called underdetermined if there are multiple alternative sets of parameters that can achieve the same\\nobjective.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 196}, page_content='190\\nAlignment\\nrather some form of the accumulated rewards from t until the last time step. Such supervision\\nsignals are dense over the sequence, because the reward obtained at the end of the sequence can\\nbe transferred back to that time step, regardless of which time step it is. In other words, the sparse\\nrewards are transformed into the dense supervision signals. Furthermore, from the perspective of\\nreward shaping, Ng et al. [1999] show that the reward at t can be deﬁned as\\nr′(st, at, st+1)\\n=\\nr(st, at, st+1) + f(st, at, st+1)\\n(4.64)\\nwhere r′(·) is the transformed reward function, r(·) is the original reward function, and f(·) is\\nthe shaping reward function. To ensure the optimality of the policy under the transformed reward\\nfunction, the shaping reward function can be given in the form\\nf(st, at, st+1)\\n=\\nγΦ(st+1) −Φ(st)\\n(4.65)\\nwhere Φ(s) is called the potential value of the state s. If we deﬁne Φ(s) as the common value\\nfunction as in Eq. (4.15) and substitute Eq. (4.65) into Eq. (4.64), we obtain\\nr′(st, at, st+1)\\n=\\nr(st, at, st+1) + γV (st+1) −V (st)\\n(4.66)\\nIt is interesting to see that this function is exactly the same as the advantage function used in PPO.\\nThis relates advantage-based methods to reward shaping: the advantage is essentially a shaped\\nreward.\\nOn the other hand, one of the reasons for adopting end-of-sequence rewards lies in the nature\\nof the RLHF tasks. Unlike traditional reinforcement learning environments where the agent in-\\nteracts with a dynamic environment, RLHF tasks often involve complex decision-making based\\non linguistic or other high-level cognitive processes. These processes do not lend themselves eas-\\nily to frequent and meaningful intermediate rewards because the quality and appropriateness of\\nthe actions can only be fully evaluated after observing their impact in the larger context of the\\nentire sequence or task. In this case, the reward signals based on human feedback, though very\\nsparse, are typically very informative and accurate. Consequently, this sparsity, together with the\\nhigh informativeness and accuracy of the human feedback, can make the learning both robust and\\nefﬁcient.\\n4.4.1.3\\nFine-grained Rewards\\nFor many applications, our objective will be more complex than merely evaluating an entire text.\\nFor example, in sentiment analysis, we often do not just determine the sentiment of a text, but need\\nto analyze the sentiment in more detail by associating it with speciﬁc aspects of a topic discussed\\nin the text. Consider the sentence \"The camera of the phone is excellent, but the battery life is\\ndisappointing.\" In this example, we would need to separately analyze the sentiments expressed\\nabout the camera and the battery. Such analysis, known as aspect-based sentiment analysis, helps\\nprovide a ﬁner-grained understanding of the customer review compared to general sentiment anal-\\nysis.\\nFor the problem of reward modeling, we often need to model different parts of a sequence as\\nwell. A simple and straightforward way to do this is to divide a sequence into different segments\\nand then compute the reward for each segment [Wu et al., 2023]. Suppose that an output token'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 197}, page_content='4.4 Improved Human Preference Alignment\\n191\\nsequence y can be divided into ns segments {¯y1, ..., ¯yns} by some criterion. We can use the\\nreward model to evaluate each of these segments. By taking x, y and ¯yk as input to the reward\\nmodel, the reward score for the k-th segment is given by\\nrk\\n=\\nr(x, y, ¯yk)\\n(4.67)\\nThen the reward score for the entire output sequence is given by\\nr(x, y)\\n=\\nns\\nX\\nk=1\\nr(x, y, ¯yk)\\n(4.68)\\nHere r(x, y) can be used to train the policy as usual.\\nA problem with this model is that training reward models at the segment level is not as straight-\\nforward as learning from human preferences on entire texts, as it is difﬁcult to obtain segment-level\\nhuman preference data. For rating-like problems (e.g., we rate a segment according to its level\\nof misinformation), one simple approach is to assign a rating score to each segment and train the\\nreward model using pointwise methods. For example, we can use a strong LLM to rate the se-\\nquences ¯y1...¯yk−1 and ¯y1...¯yk, and obtain the scores s(¯y1...¯yk−1) and s(¯y1...¯yk). We can then\\ndeﬁne the score of the segment ¯yk as the difference between s(¯y1...¯yk) and s(¯y1...¯yk−1)\\ns(¯yk)\\n=\\ns(¯y1...¯yk) −s(¯y1...¯yk−1)\\n(4.69)\\nUsing these segment-level scores, we can train the reward model with a regression loss func-\\ntion\\nLrating\\n=\\n−E¯yk\\n\\x02s(¯yk) −r(x, y, ¯yk)\\n\\x032\\n(4.70)\\nSometimes, alignment can be treated as a classiﬁcation problem, for example, we assess\\nwhether a segment has ethical issues.\\nIn this case, the segment can be labeled as ethical or\\nunethical, either by humans or using additional classiﬁers. Given the label of the segment, we\\ncan train the reward model using some classiﬁcation loss function. For example, suppose that\\nr(x, y, ¯yk) = 1 if the segment is classiﬁed as unethical, and r(x, y, ¯yk) = −1 otherwise6. The\\nhinge loss of training binary classiﬁcation models is given by\\nLhinge\\n=\\nmax(0, 1 −r(x, y, ¯yk) · ˆr)\\n(4.71)\\nwhere ˆr ∈{1, −1} denotes the ground truth label.\\nThe remaining issue here is how to split y into segments. One approach is to deﬁne a ﬁxed-\\nlength segmentation, where y is divided into equal-length chunks. However, this may not always\\nbe ideal, as the content of the sequence may not align well with ﬁxed boundaries. An alternative\\napproach is to segment y based on speciﬁc linguistic or semantic cues, such as sentence bound-\\naries, topic shifts, or other meaningful structures in the text. Such a segmentation can be achieved\\nby using linguistic segmentation systems or prompting LLMs to identify natural breaks in the se-\\nquence. Another approach is to use dynamic segmentation methods based on the complexity of\\n6To allow the reward model to output categories, we can replace the linear layer described in Section 4.3.2 with a\\nSoftmax layer.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 198}, page_content='192\\nAlignment\\nthe sequence. For example, segments could be deﬁned where there is a signiﬁcant change in the\\nreward score, which might correspond to shifts in the task being modeled.\\n4.4.1.4\\nCombination of Reward Models\\nA reward model can be viewed as a proxy for the environment. Since the true environment is often\\ntoo complex or unknown, developing a perfect proxy for the environment is generally not possible.\\nAs a result, over-aligning LLMs with this imperfect proxy might lead to decreased performance,\\nknown as the overoptimization problem [Stiennon et al., 2020; Gao et al., 2023a]7. We can also\\nexplain this through Goodhart’s law, which states: when a measure becomes a target, it ceases to\\nbe a good measure [Goodhart, 1984].\\nAddressing the overoptimization problem is not easy, and there is no mature solution yet. The\\nideal approach might be to develop an oracle reward model that can perfectly capture the true\\nobjectives of the task and prevent the agent from “tricking”. However, creating such a model is\\nextremely difﬁcult due to the complexity of the real-world environment, as well as the challenge\\nof deﬁning all the relevant factors that contribute to the desired outcome. Instead, a more practical\\napproach is to combine multiple reward models, thereby alleviating the misalignment between\\nthe training objective and the true objective that arises from using a single, speciﬁc reward model\\n[Coste et al., 2024].\\nGiven a set of reward models, combining them is straightforward, and in some cases, we can\\nsimply treat this problem as an ensemble learning problem. A simple yet common approach is to\\naverage the outputs of these models to obtain a more precise reward estimation:\\nrcombine\\n=\\n1\\nN\\nK\\nX\\nk=1\\nwk · rk(x, y)\\n(4.72)\\nwhere rk(·) is the k-th reward model in the ensemble, wk is the weight of rk(·), and K is the\\nnumber of reward models. This combined reward can then be used to supervise the training of\\na policy. In fact, there are many ways to combine different models, for example, one can make\\npredictions using Bayesian model averaging or develop a fusion network to learn to combine the\\npredictions from different models. Alternatively, one can frame this task as a multi-objective\\noptimization problem, and use multiple reward models to train the policy simultaneously. These\\nmethods have been intensively discussed in the literature on optimization and machine learning\\n[Miettinen, 1999; Bishop, 2006].\\nIn addition to model combination methods, another important issue is how to collect or con-\\nstruct multiple different reward models. One of the simplest approaches is to employ ensemble\\nlearning techniques, such as developing diverse reward models from different subsets of a given\\ndataset or from various data sources. For RLHF, it is also possible to construct reward models\\nbased on considerations of different aspects of alignment. For example, we can develop a reward\\nmodel to evaluate the factual accuracy of the output and another reward model to evaluate the\\n7This problem is also called reward hacking or reward gaming [Krakovna et al., 2020; Skalse et al., 2022;\\nPan et al., 2022], which refers to the phenomenon where the agent attempts to trick the reward model but fails to\\nalign its actions with the true intended objectives of the task. Imagine a student who is assigned homework and is re-\\nwarded with points or praise for completing it. The student might then ﬁnd ways to ﬁnish the homework with minimal\\neffort to maximize the reward, such as copying and pasting solutions from the internet or previous assignments, rather\\nthan solving the problems themselves.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 199}, page_content='4.4 Improved Human Preference Alignment\\n193\\nya ≻yb\\nPreference\\nData\\nReward Model\\nValue Function\\nPolicy\\ntraining with MLE\\nTraining\\nwith PPO\\n(a) RLHF (PPO)\\nya ≻yb\\nPreference\\nData\\nPolicy\\ntraining with MLE\\n(b) DPO\\nFig. 4.10: Standard RLHF (PPO) vs. DPO. In RLHF, the human preference data is used to train a reward model, which\\nis then employed in training the policy as well as the value function. In DPO, the use of human preference data is more\\ndirect, and the policy is trained on this data without the need for reward model training.\\ncompleteness of the output. These two models are complementary to each other, and can be com-\\nbined to improve the overall evaluation of the output. Another approach is to employ different\\noff-the-shelf LLMs as reward models. This approach is simple and practical, as there have been\\na lot of well-developed LLMs and we just need to use them with no or little modiﬁcation. An\\ninteresting issue, though not closely related to the discussion here, arises: can an LLM that aligns\\nwith other LLMs outperform those LLMs? Probably not at ﬁrst glance. In part, this is because\\nthe target LLM merely imitates other LLMs based on limited supervision and thus cannot capture\\nwell the nuances of the behaviors of these supervisors. However, given the strong generalization\\nability of LLMs, this approach can, in fact, be quite beneﬁcial. For example, using open-sourced\\nor commercial LLMs as reward models has demonstrated strong performance in aligning LLMs,\\neven achieving state-of-the-art results on several popular tasks [Lambert et al., 2024].\\n4.4.2\\nDirect Preference Optimization\\nAlthough learning reward models is a standard step in reinforcement learning, it makes the entire\\ntraining process much more complex than supervised training. Training a reliable reward model\\nis itself not an easy task, and a poorly trained reward model can greatly affect the outcome of\\npolicy learning. We now consider an alternative alignment method, called direct preference op-\\ntimization (DPO), which simpliﬁes the training framework by eliminating the need to explicitly\\nmodel rewards [Rafailov et al., 2024]. This method directly optimizes the policy based on user\\npreferences, rather than developing a separate reward model. As a result, we can achieve human\\npreference alignment in a supervised learning-like fashion. Figure 4.10 shows a comparison of\\nthe standard RLHF method and the DPO method.\\nBefore deriving the DPO objective, let us ﬁrst review the objective of policy training used in\\nRLHF. As discussed in Section 4.3.3, the policy is typically trained by optimizing a loss function'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 200}, page_content='194\\nAlignment\\nwith a penalty term. The DPO method assumes a simple loss function where the quality of the\\noutput y given the input x is evaluated by the reward model r(x, y). The training objective is thus\\ngiven by\\n˜θ\\n=\\narg min\\nθ\\nEx∼DEy∼πθ(·|x)\\n\\x02 −r(x, y)\\n|\\n{z\\n}\\nloss\\n+β (log πθ(y|x) −log πθref(y|x))\\n|\\n{z\\n}\\npenalty\\n\\x03\\n(4.73)\\nNote that in this optimization problem, only the term πθ(y|x) depends on the target policy πθ(·).\\nBoth the reward model r(x, y) and the reference model πθref(y|x) are assumed to be ﬁxed given\\nx and y. This is a strong assumption compared with PPO, but as will be shown later, it simpliﬁes\\nthe problem and crucial for deriving the DPO objective.\\nSince θ is the variable we want to optimize, we rearrange the right-hand side of Eq. (4.73) to\\nisolate πθ(y|x) as an independent term:\\n˜θ\\n=\\narg min\\nθ\\nEx∼DEy∼πθ(·|x)\\n\\x02β log πθ(y|x) −β log πθref(y|x) −r(x, y)\\n\\x03\\n=\\narg min\\nθ\\nEx∼DEy∼πθ(·|x)\\n\\x02 log πθ(y|x) −\\n\\x00 log πθref(y|x) + 1\\nβ r(x, y)\\n\\x01\\x03\\n=\\narg min\\nθ\\nEx∼DEy∼πθ(·|x)\\n\\x02 log πθ(y|x)\\n|\\n{z\\n}\\ndependent on θ\\n−log πθref(y|x) exp\\n\\x00 1\\nβ r(x, y)\\n\\x01\\n|\\n{z\\n}\\nnot dependent on θ\\n\\x03\\n(4.74)\\nThis equation deﬁnes the objective function as the difference between the log-probability dis-\\ntribution function of y and another function of y. This form of the objective function seems not\\n“ideal”, as we usually prefer to see the difference between two distributions, so that we can in-\\nterpret this difference as some kind of divergence between the distributions. A simple idea is\\nto convert the second term (i.e., log πθref(y|x) exp( 1\\nβ r(x, y))) into a log-probability distribution\\nover the domain of y. If we treat πθref(y|x) exp( 1\\nβ r(x, y)) as an unnormalized probability of y,\\nwe can convert it into a normalized probability by dividing it by a normalization factor:\\nZ(x)\\n=\\nX\\ny\\nπθref(y|x) exp\\n\\x00 1\\nβ r(x, y)\\n\\x01\\n(4.75)\\nHence we can deﬁne a probability distribution by\\nπ∗(y|x)\\n=\\nπθref(y|x) exp\\n\\x00 1\\nβr(x, y)\\n\\x01\\nZ(x)\\n(4.76)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 201}, page_content='4.4 Improved Human Preference Alignment\\n195\\nWe then rewrite Eq. (4.74) as\\n˜θ\\n=\\narg min\\nθ\\nEx∼DEy∼πθ(·|x)\\nh\\nlog πθ(y|x) −log\\nπθref(y|x) exp\\n\\x00 1\\nβr(x, y)\\n\\x01\\nZ(x\\n\\x01\\n−log Z(x)\\ni\\n=\\narg min\\nθ\\nEx∼DEy∼πθ(·|x)\\nh\\nlog πθ(y|x) −log π∗(y|x) −log Z(x)\\ni\\n=\\narg min\\nθ\\nEx∼D\\n\\x14\\nEy∼πθ(·|x)\\nh\\nlog πθ(y|x) −log π∗(y|x)\\ni\\n−Ey∼πθ(·|x)\\n\\x02 log Z(x)\\n\\x03\\x15\\n=\\narg min\\nθ\\nEx∼D\\nh\\nKL\\n\\x00πθ(·|x) || π∗(·|x)\\n\\x01\\n|\\n{z\\n}\\nKL divergence\\n−log Z(x)\\n|\\n{z\\n}\\nconstant wrt. θ\\ni\\n(4.77)\\nSince log Z(x) is independent of θ, it does not affect the result of the arg minθ operation,\\nand can be removed from the objective. Now we obtain a new training objective which ﬁnds the\\noptimal policy πθ by minimizing the KL divergence between πθ(·|x) and π∗(·|x)\\n˜θ\\n=\\narg min\\nθ\\nEx∼D\\nh\\nKL\\n\\x00πθ(·|x) || π∗(·|x)\\n\\x01i\\n(4.78)\\nClearly, the solution to this optimization problem is given by\\nπθ(y|x)\\n=\\nπ∗(y|x)\\n=\\nπθref(y|x) exp\\n\\x00 1\\nβr(x, y))\\nZ(x\\n\\x01\\n(4.79)\\nGiven this equation, we can express the reward r(x, y) using the target model πθ(y|x), the\\nreference model πθref(y|x), and the normalization factor Z(x):\\nr(x, y)\\n=\\nβ\\n \\nlog πθ(y|x)\\nπθref(y|x) + log Z(x)\\n!\\n(4.80)\\nThis is interesting because we initially seek to learn the policy πθ(·) using the reward model\\nr(x, y), but eventually obtain a representation of the reward model based on the policy. Given the\\nreward model deﬁned in Eq. (4.80), we can apply it to the Bradley-Terry model to calculate the\\npreference probability (also see Section 4.3.2):\\nPrθ(ya ≻yb|x)\\n=\\nSigmoid(r(x, ya) −r(x, yb))\\n=\\nSigmoid\\n\\x12\\nβ\\n\\x10\\nlog πθ(ya|x)\\nπθref(ya|x) + log Z(x)\\n\\x11\\n−\\nβ\\n\\x10\\nlog πθ(yb|x)\\nπθref(yb|x) + log Z(x)\\n\\x11\\x13\\n=\\nSigmoid\\n\\x12\\nβ log πθ(ya|x)\\nπθref(ya|x) −β log πθ(yb|x)\\nπθref(yb|x)\\n\\x13\\n(4.81)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 202}, page_content='196\\nAlignment\\nThis formula is elegant because it converts the difference in rewards into the difference in\\nratio functions, and we do not need to calculate the value of Z(x). A direct result is that we no\\nlonger need a reward model, but only need the target policy and reference model to calculate the\\nprobability of preferences. Finally, we can train the target policy by minimizing the following\\nDPO loss function\\nLdpo(θ)\\n=\\n−E(x,ya,yb)∼Dr\\n\\x02 log Prθ(ya ≻yb|x)\\n\\x03\\n(4.82)\\nThe form of this loss function is very similar to that used in training reward models in RLHF (see\\nEq. (4.36)). But it should be noted that the loss function here depends on the parameters of the\\npolicy (i.e., θ) rather than the parameters of the reward model (i.e., φ).\\nThe main advantage of DPO lies in its simplicity and efﬁciency. The DPO objective is very\\nstraightforward — it directly optimizes for preference-based feedback, rather than relying on sep-\\narately developed reward models. Moreover, DPO is generally more sample-efﬁcient, as it learns\\nfrom a ﬁxed dataset without the need for the computationally expensive sampling process used\\nin PPO. This makes DPO a popular method for human preference alignment, especially when\\ndeveloping and applying reward models via reinforcement learning is challenging.\\nDPO can broadly be viewed as an ofﬂine reinforcement learning method, where the training\\ndata is pre-collected and ﬁxed, and there is no exploration. In contrast, online reinforcement learn-\\ning methods like PPO, which require exploring new states through interaction with the environ-\\nment (using the reward model as a proxy), also have their unique advantages. One of the beneﬁts\\nof online reinforcement learning is that it allows the agent to continuously adapt to changes in\\nthe environment by learning from real-time feedback. This means that, unlike ofﬂine methods,\\nonline methods are not constrained by the static nature of pre-collected data and can discover\\nnew problem-solving strategies. In addition, exploration can help the agent cover a wider range of\\nstate-action pairs, thus improving generalization. This could be an important advantage for LLMs,\\nas generalization is considered a critical aspect in applying such large models.\\n4.4.3\\nAutomatic Preference Data Generation\\nAlthough learning from human preferences is an effective and popular method for aligning LLMs,\\nannotating preference data is costly. Using human feedback does not only faces the problem of\\nlimited scalability, but it may also introduce bias because human feedback is inherently subjective.\\nAs a result, one can turn to AI feedback methods to address these scalability and consistency issues\\nwithout the limitations associated with human annotators.\\nAs with data generation for instruction ﬁne-tuning, generating preference data using LLMs is\\nstraightforward. Given a set of inputs, we ﬁrst use an LLM to generate pairs of outputs. Then, we\\nprompt the LLM to label the preference between each pair of outputs, along with its corresponding\\ninput. Below is an example of prompting the LLM to generate a preference label for a pair of\\nconsumer service responses.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 203}, page_content='4.4 Improved Human Preference Alignment\\n197\\nConsider a customer service scenario where a customer poses a request. You\\nwill review two responses to this request. Please indicate which response is\\npreferred. Note that a good response should be courteous, clear, and concise. It\\nshould address the customer’s concern directly, provide helpful information or a\\nsolution, and maintain a positive tone.\\nRequest:\\nHello, I noticed that my order hasn’t arrived yet, though it was scheduled to\\narrive several days ago. Could you please update me on its status? Thank you!\\nResponse A:\\nI’m very sorry for the delay and understand how disappointing this can be. We’re\\ndoing our best to sort this out quickly for you.\\nResponse B:\\nHey, stuff happens! Your package will get there when it gets there, no need to\\nstress.\\nResponse A is preferred.\\nOnce we collect such preference labels, we can use them, along with the output pair and input,\\nto train the reward model. Of course, we can consider demonstrating a few examples or using\\nadvanced prompting techniques, such as CoT, to improve labeling performance. For example, we\\ncan include in the prompt an example showing how and why one of the two responses is preferred\\nbased on a CoT rationale.\\nIn addition to preference labels, we can also obtain the probability associated with each label\\n[Lee et al., 2023]. A simple method is to extract the probabilities for the label tokens, such as “A”\\nand “B”, from the probabilities output by the LLM. We can then use the Softmax function or other\\nnormalization techniques to re-normalize these probabilities into a distribution over the labels.\\nThese probabilities of preferred labels can serve as pointwise supervision signals for training the\\nreward model, as discussed in Section 4.4.1.\\nFor data generation, although it is easy to scale up, it is often necessary to ensure the data is\\naccurate and diverse. Here, the data quality and diversity issues involve not only the labeling of\\npreferences but also the inputs and outputs of the model. Therefore, we often need to use a variety\\nof techniques to obtain large-scale, high-quality data. For example, one can generate diverse\\nmodel outputs and annotations by using different LLMs, prompts, in-context demonstrations, and\\nso on [Cui et al., 2024]. Dubois et al. [2024] report that the variability in pairwise preference data\\nis important for training LLMs from either human or AI feedback.\\nWhile learning from AI feedback is highly scalable and generally objective, this method is\\nmore suited to well-deﬁned tasks where objective performance metrics are available. By contrast,\\nlearning from human feedback is more advantageous when aligning AI systems with human val-\\nues, preferences, and complex real-world tasks that require understanding of subtle or subjective\\ncontext. These methods can be combined to train LLMs that beneﬁt from both human insights\\nand the scalability of AI feedback.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 204}, page_content='198\\nAlignment\\n4.4.4\\nStep-by-step Alignment\\nSo far, our discussion of alignment has primarily focused on the use of reward models for evalu-\\nating entire input-output sequence pairs. These methods can be easily adapted to scenarios where\\nthe correctness of an output can be examined by checking whether the desired result is included.\\nFor example, in the task of calculating a mathematical expression, a reward model can provide\\npositive feedback if the answer is correct, and negative feedback if the answer is wrong. How-\\never, in many problems that require complex reasoning, simply examining the correctness of the\\nﬁnal result is insufﬁcient for learning. Imagine a student who is only given the ﬁnal answer to\\na challenging math problem. Knowing whether the ﬁnal answer is right or wrong does not help\\nthe student ﬁgure out where they went wrong and how to calculate the correct answer. A better\\napproach would be to guide the student with a step-by-step breakdown of the problem-solving\\nprocess and encourage understanding of the underlying concepts and logic behind these steps.\\nIn Chapter 3, we studied CoT methods to prompt LLMs to explicitly write out intermediate\\nsteps or the reasoning process needed to reach a conclusion or solve a problem. We saw that\\nbreaking down a problem into smaller parts could make it easier to understand the solution path\\nand increase the accuracy of the output. These methods can be naturally extended to the alignment\\nof LLMs, that is, we supervise the model during the intermediate steps of reasoning. Consider a\\nreasoning task where an LLM produces a sequence of reasoning steps y = {¯y1, ..., ¯yns} for the\\ngiven input. The result of the reasoning is assumed to be included in the last step ¯yns, and can\\nbe easily veriﬁed. For this reasoning problem, Uesato et al. [2022] categorize LLM ﬁne-tuning\\napproaches into two classes:\\n• Outcome-based Approaches. Supervision occurs only when the end result is veriﬁed. This\\nis a standard method for learning from human feedback we have discussed in this chapter.\\nFor example, the LLM is optimized to maximize some form of the reward r(x, y).\\n• Process-based Approaches. Supervision is involved in all intermediate steps in addition to\\nthe last step. To do this, we need to develop a model to give a supervision signal at each\\nstep, and develop loss functions that can make use of such supervision signals.\\nFigure 4.11 shows two LLM outputs for an example math problem. Although the LLM gives\\nthe correct ﬁnal answer in both cases, it makes mistakes during the problem-solving process in the\\nsecond output. Outcome-based approaches overlook these mistakes and give positive feedback for\\nthe entire solution. By contrast, process-based approaches can take these mistakes into account\\nand provide additional guidance on the detailed reasoning steps.\\nAn important issue for process-based approaches is that we need to get step-level feedback\\nduring a (potentially) long reasoning path. We can collect or generate reasoning paths correspond-\\ning to problems from existing datasets. Human experts then annotate each step in these paths for\\ncorrectness. These annotations can be used to directly train LLMS or as rewards in reward mod-\\neling. However, in practice, richer annotations are often introduced [Lightman et al., 2024]. In\\naddition to the correct and incorrect labels, a step can also be labeled as neutral to indicate that\\nwhile the step may be technically correct, it might still be problematic within the overall reason-\\ning process. Furthermore, to improve the efﬁciency of data annotation, techniques such as active\\nlearning can be employed. Identifying obvious errors usually does not signiﬁcantly contribute to'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 205}, page_content='4.4 Improved Human Preference Alignment\\n199\\nQuestion (from The International Junior Math Olympiad):\\nJessica has a lot of apps on her phone. The number of apps she has is divisible by 5. If she\\ndownloaded 10 more apps, she would still have less than 50 apps. What is the maximum\\noriginal number of apps Jessica could have?\\nResponse 1:\\nx must be a multiple of 5, i.e.,\\nx ≡0 (mod 5).\\nx\\n¯y1:\\nAfter downloading 10 apps, the total\\ntotal number of apps should still be less\\nthan 50: x + 10 < 50. Solving this\\ninequality: x < 40.\\nx\\n¯y2:\\nSince x must be a multiple of 5 and\\nless than 40, the largest value of x is 35.x\\n¯y3:\\nVerify the result. If x = 35, then after\\ndownloading 10 apps, Jessica would\\nhave: 35 + 10 = 45 which satisfy\\n45 < 50.\\nx\\n¯y4:\\nThe maximum original number of\\napps is 35.\\nx\\n¯y5:\\ncorrect\\nResponse 2:\\nThe number of apps is divisible by 5,\\nso x must be a multiple of 5.\\nx\\n¯y1:\\nIf she downloads 10 more apps, her\\nnew total will be x + 10, and this must\\nbe less than 50. Therefore, x + 10 < 50.\\nSimplifying this gives x < 40.\\nx\\n¯y2:\\nx must be less than or equal to\\nthe largest multiple of 5, which is 40. x\\n¯y3:\\nBut x + 10 should not be more\\nthan or equal to 50. So we need to\\nsubtract 5 from 40.\\nx\\n¯y4:\\nTherefore, the ﬁnal result is 35.\\nx\\n¯y5:\\ncorrect\\nproblematic\\nproblematic\\nFig. 4.11: Two LLM responses to a math problem. In response 1, both the ﬁnal result and all the reasoning steps are\\ncorrect. In response 2, the ﬁnal result is correct, but there are mistakes in the reasoning process (highlighted in red).\\nFor outcome-based approaches, both responses are considered correct. For process-based approaches, the mistakes in\\nresponse 2 can be considered in reward modeling.\\nlearning from reasoning mistakes. Instead, annotating steps that the model conﬁdently considers\\ncorrect but are actually problematic is often more effective.\\nGiven a set of step-level annotated reasoning paths and corresponding inputs, we can train\\na reward model to provide feedback for supervising policy learning. The reward model can be\\ntreated as a classiﬁcation model, and so its architecture can be a Transformer decoder with a\\nSoftmax layer stacked on top. At step k, the reward model takes both the problem description\\n(denoted by x) and the reasoning steps generated so far (denoted by ¯y≤k) as input and outputs\\na probability distribution over the label set {correct, incorrect} or {correct, incorrect, neutral}.\\nThen the learned reward model is used to evaluate reasoning paths by assessing the correctness of\\neach step. A simple method to model correctness is to count the number of steps that are classiﬁed\\nas correct, given by\\nr(x, y)\\n=\\nns\\nX\\nk=1\\nδ(correct, C(x, ¯y≤k))\\n(4.83)\\nwhere C(x, ¯y≤k) denotes the label with the maximum probability. We can also use log-probabilities'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 206}, page_content='200\\nAlignment\\nof classiﬁcation to deﬁne the reward of the entire path\\nr(x, y)\\n=\\nns\\nX\\nk=1\\nlog Pr(correct|x, ¯y≤k)\\n(4.84)\\nwhere Pr(correct|x, ¯y≤k) denotes the probability of the correct label generated by the reward\\nmodel. The reward score r(x, y) can then be used to train the policy in RLHF as usual.\\nWhile we restrict our discussion to math problems, the approaches described here are general\\nand can be applied to a wide variety of tasks that involve multi-step reasoning and decision-\\nmaking. Moreover, we can consider various aspects when assessing the quality of a step, rather\\nthan just its correctness. For example, in dialogue systems, responses must not only be accurate\\nbut also contextually appropriate across multiple turns of conversation. If a model provides a\\ncorrect response but fails to maintain coherence in the context of the ongoing dialogue, step-\\nlevel feedback could help the model identify and correct such discrepancies. Also note that the\\nprocess-based approaches are related to the ﬁne-grained reward modeling approaches discussed\\nin Section 4.4.1.3. All these approaches essentially aim to provide more detailed supervision to\\nLLMs by breaking their outputs into smaller, more manageable steps. However, process-based\\nfeedback focuses more on evaluating the correctness of a step based on its preceding steps, while\\nthe approaches in Section 4.4.1.3 emphasize evaluating each step independently.\\nThe idea of aligning LLMs step by step has great application potential, especially considering\\nthe recent shift towards more complex reasoning tasks in the use of LLMs. For example, both\\nthe GPT-o1 and GPT-o3 models are designed with more advanced reasoning techniques (such\\nas long internal CoT) to solve challenging problems like scientiﬁc and mathematical reasoning\\n[OpenAI, 2024]. These tasks often rely on long and complex reasoning paths, and therefore, it\\nseems essential to introduce detailed supervision signals in the reasoning process. Moreover, from\\na practical perspective, effective supervision on long reasoning paths not only improves reasoning\\nperformance, but it also helps the model eliminate redundant or unnecessary reasoning steps,\\nthereby reducing reasoning complexity and improving efﬁciency.\\n4.4.5\\nInference-time Alignment\\nIn this section we explored a variety of methods to align models with human preferences and an-\\nnotations. However, one of the signiﬁcant limitations of many such methods is that LLMs must\\nbe ﬁne-tuned. For RLHF and its variants, training LLMs with reward models can be computa-\\ntionally expensive and unstable, leading to increased complexity and costs when applying these\\napproaches. In this case, we can consider aligning models at inference time, thus avoiding the\\nadditional complexity and effort involved.\\nOne simple way to achieve inference-time alignment is to use the reward model to select\\nthe best one from N alternative outputs generated by the LLM, a method known as Best-of-N\\nsampling (BoN sampling). We can consider BoN sampling as a form of reranking. In fact,\\nreranking methods have been widely used in NLP tasks, such as machine translation, for a long\\ntime. They are typically applied in situations where training complex models is costly. In such\\ncases, directly reranking the outputs allows for the incorporation of these complex models at a\\nvery low cost8.\\n8Reranking methods can also help us explore what are known as model errors and search errors, although these'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 207}, page_content='4.5 Summary\\n201\\nIn the BoN sampling process, the LLM takes the input sequence x and generates N different\\noutput sequences {ˆy1, ..., ˆyN}:\\n{ˆy1, ..., ˆyN}\\n=\\nargTopN\\ny\\n[Pr(y|x)]\\n(4.85)\\nwhere the argTopN operation returns the top-N outputs that maximize the function Pr(y|x).\\nThese outputs can be generated in a variety of ways, depending on the search algorithm used by\\nthe model (e.g., sampling or beam search). Once the N-best output candidates are generated, the\\nreward model is used to evaluate and select the best one:\\nˆybest\\n=\\nmax{r(x, ˆy1), ..., r(x, ˆyN)}\\n(4.86)\\nIt is worth noting that the result of BoN sampling is also inﬂuenced by the diversity of the\\nN-best list. This is a common issue with most reranking methods. Typically, we wish the N-best\\noutput candidates to have relatively high quality but be sufﬁciently different from each other. In\\nmany text generation systems, the N-best outputs are very similar, often differing by just one\\nor two words. The diversity issue is even more challenging in LLMs, as the N-best outputs\\ngenerated by an LLM can be different in their wordings, yet their semantic meanings are often\\nquite similar. In practice, one can adjust the model hyperparameters and/or adopt different LLMs\\nto generate more diverse output candidates for reranking. Nevertheless, as with many practical\\nsystems, we need to make a trade-off between selecting high-quality candidates and ensuring\\nsufﬁcient variation in the generated outputs.\\nBoN sampling can be used for training LLMs as well. A closely related method is rejection\\nsampling. In this method, we ﬁrst select the “best” outputs from the N-best lists via the reward\\nmodel, and then take these selected outputs to ﬁne-tune the LLM. In this way, we can introduce\\nhuman preferences into the training of LLMs via a much simpler approach compared to RLHF.\\nMany LLMs have adopted rejection sampling for ﬁne-tuning [Nakano et al., 2021; Touvron et al.,\\n2023b].\\n4.5\\nSummary\\nIn this chapter, we have explored a range of techniques for aligning LLMs. In particular, we\\nhave discussed ﬁne-tuning methods that enable LLMs to follow instructions and align them with\\nhuman preferences. One of the beneﬁts of ﬁne-tuning LLMs is computation efﬁciency. Unlike\\npre-training based on large-scale neural network optimization, ﬁne-tuning is a post-training step\\nand so is less computationally expensive. Moreover, it is better suited to address problems that are\\nnot easily solved in pre-training, such as human value alignment. The widespread attention to the\\nalignment issue has also led to a surge of research papers on this topic, which has posed challenges\\nin writing this chapter, as it is difﬁcult to cover all the latest techniques. However, we have tried\\nto provide a relatively detailed introduction to the fundamental approaches to alignment, such as\\nissues are not often discussed in the context of LLMs. For example, suppose we have an old model and a new, more\\npowerful model. We can use the new model to select the best output from the N-best list of the old model as the oracle\\noutput. The performance difference between the oracle output and the top-1 output of the original N-best list reﬂects\\nthe performance gain brought by the new model. If the performance gain is signiﬁcant, we can say that the old model\\nhas more model errors. If the gain is small, it may indicate that the issue lies in search errors, as the best candidates\\nwere not found.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 208}, page_content='202\\nAlignment\\ninstruction ﬁne-tuning and RLHF.\\nWhile we have focused on LLM alignment techniques in this chapter, the term AI alignment\\nis a wide-ranging concept. It generally refers to the process of ensuring that the behavior of an AI\\nsystem aligns with human values, goals, and expectations. The idea of AI alignment can be traced\\nback to the early days of AI. A widely cited description of AI alignment comes from an article by\\nthe mathematician and computer scientist Norbert Wiener [Wiener, 1960]. The quote is as follows\\nIf we use, to achieve our purposes, a mechanical agency with whose operation\\nwe cannot efﬁciently interfere ... we had better be quite sure that the purpose\\nput into the machine is the purpose which we really desire.\\nAt that time, AI alignment was a distant concern for researchers. But today, it greatly inﬂu-\\nences the design of various AI systems. For example, in robotics, alignment is critical to ensur-\\ning that autonomous robots safely interact with humans and their environments. In autonomous\\ndriving, cars must not only follow trafﬁc laws but also make complex, real-time decisions that\\nprioritize human safety, avoid accidents, and navigate ethical dilemmas.\\nIn current AI research, alignment is usually achieved by developing a surrogate objective that\\nis analogous to the real goal and steering the AI system towards this objective. However, designing\\nthe objective of AI alignment is very difﬁcult. One reason is that human values are diverse and\\noften context-dependent, making it difﬁcult to distill them into a single, universally applicable\\nobjective function. Also, the complexity of real-world environments, where values and goals often\\nconﬂict or evolve over time, further complicates alignment efforts. Even if we could deﬁne an\\nappropriate objective, AI systems may ﬁnd unintended ways to achieve it, leading to “misaligned”\\noutcomes that still technically satisfy the objective but in a harmful or counterproductive way.\\nThese challenges have motivated and are motivating AI research towards more aligned sys-\\ntems, either through developing new mechanisms for perceiving the world or more efﬁcient and\\ngeneralizable methods to adapt these systems to given tasks. More importantly, as AI systems\\nbecome more powerful and intelligent, especially given that recent advances in LLMs have shown\\nremarkable capabilities in dealing with many challenging problems, the need for AI alignment\\nhas become more urgent. Researchers have started to be concerned with AI safety and warn the\\ncommunity that they need to develop and release AI systems with great caution to prevent these\\nsystems from being misaligned [Russell, 2019; Bengio et al., 2024].'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 209}, page_content='Bibliography\\n[Ainslie et al., 2020] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher,\\nPhilip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. Etc: Encoding long and\\nstructured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in\\nNatural Language Processing (EMNLP), pages 268–284, 2020.\\n[Ainslie et al., 2023] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico\\nLebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-\\nhead checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language\\nProcessing, pages 4895–4901, 2023.\\n[Akyürek et al., 2023] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.\\nWhat learning algorithm is in-context learning? investigations with linear models. In Proceedings of\\nThe Eleventh International Conference on Learning Representations, 2023.\\n[Alabdulmohsin et al., 2022] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisit-\\ning neural scaling laws in language and vision. Advances in Neural Information Processing Systems, 35:\\n22300–22312, 2022.\\n[Allal et al., 2024] Loubna Ben Allal, Anton Lozhkov, and Daniel van Strien. cosmopedia: how to create\\nlarge-scale synthetic data for pre-training. https://huggingface.co/blog/cosmopedia, 2024.\\n[Almazrouei et al., 2023] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cap-\\npelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Gofﬁnet, Daniel Hesslow, Julien Launay, Quentin\\nMalartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon\\nseries of open language models. arXiv preprint arXiv:2311.16867, 2023.\\n[Andreas et al., 2016] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module\\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\\n39–48, 2016.\\n[Arjovsky et al., 2016] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent\\nneural networks. In International conference on machine learning, pages 1120–1128, 2016.\\n[Aschenbrenner, 2024] Leopold Aschenbrenner. Situational awareness: The decade ahead, 2024. URL\\nhttps://situational-awareness.ai/.\\n[Askell et al., 2021] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan,\\nAndy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma, Nelson Elhage, Zac Hatﬁeld-Dodds,\\nDanny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom B. Brown,\\nJack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a laboratory\\nfor alignment. arXiv preprint arXiv:2112.00861, 2021.\\n[Bach et al., 2022] Stephen H. Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V.\\nNayak, Abheesht Sharma, Taewoon Kim, M. Saiful Bari, Thibault Févry, Zaid Alyafeai, Manan Dey,\\nAndrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan\\nFries, Maged Saeed AlShaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang,\\nDragomir R. Radev, Mike Tian-Jian Jiang, and Alexander M. Rush. Promptsource: An integrated de-\\nvelopment environment and repository for natural language prompts. In Proceedings of the 60th Annual\\nMeeting of the Association for Computational Linguistics: System Demonstrations, pages 93–104, 2022.\\n[Bengio et al., 2003] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural\\nprobabilistic language model. Journal of Machine Learning Research, 3:1137–1155, 2003.\\n[Bengio et al., 2006] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-\\nwise training of deep networks. Advances in neural information processing systems, 19, 2006.\\n[Bengio et al., 2024] Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Trevor\\nDarrell, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian K. Hadﬁeld, Jeff\\n203'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 210}, page_content='204\\nAlignment\\nClune, Tegan Maharaj, Frank Hutter, Atilim Gunes Baydin, Sheila A. McIlraith, Qiqi Gao, Ashwin\\nAcharya, David Krueger, Anca Dragan, Philip Torr, Stuart Russell, Daniel Kahneman, Jan Markus\\nBrauner, and Sören Mindermann. Managing extreme ai risks amid rapid progress. Science, 384(6698):\\n842–845, 2024.\\n[Bentivogli and Giampiccolo, 2011] Luisa Bentivogli and Danilo Giampiccolo. Pascal recognizing textual\\nentailment challenge (rte-7) at tac 2011. https://tac.nist.gov/2011/RTE/, 2011.\\n[Besta et al., 2024] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski,\\nLukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten\\nHoeﬂer. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of\\nthe AAAI Conference on Artiﬁcial Intelligence, volume 38, pages 17682–17690, 2024.\\n[Biderman et al., 2021] Stella Biderman, Sid Black, Charles Foster, Leo Gao, Eric Hallahan, Horace He,\\nBen Wang, and Phil Wang. Rotary embeddings: A relative revolution. https://blog.eleuther.ai/\\nrotary-embeddings/, 2021.\\n[Bishop, 2006] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.\\n[Blum and Mitchell, 1998] Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with\\nco-training. In Proceedings of the eleventh annual conference on Computational learning theory, pages\\n92–100, 1998.\\n[Bradley and Terry, 1952] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block\\ndesigns: I. the method of paired comparisons. Biometrika, 39(3/4):324–345, 1952.\\n[Brandon et al., 2024] William Brandon, Mayank Mishra, Aniruddha Nrusimha, Rameswar Panda, and\\nJonathan Ragan Kelly. Reducing transformer key-value cache size with cross-layer attention. arXiv\\npreprint arXiv:2405.12981, 2024.\\n[Brill, 1992] Eric Brill. A simple rule-based part of speech tagger. In Speech and Natural Language:\\nProceedings of a Workshop Held at Harriman, New York, February 23-26, 1992, 1992.\\n[Brown et al., 1993] Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer.\\nThe mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,\\n19(2):263–311, 1993.\\n[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey\\nWu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\\nLanguage models are few-shot learners. Advances in neural information processing systems, 33:1877–\\n1901, 2020.\\n[Bubeck et al., 2023] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid\\nPalangi, Marco Túlio Ribeiro, and Yi Zhang. Sparks of artiﬁcial general intelligence: Early experiments\\nwith gpt-4. arXiv preprint arXiv:2303.12712, 2023.\\n[Bulatov et al., 2022] Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer.\\nAdvances in Neural Information Processing Systems, 35:11079–11091, 2022.\\n[Burges et al., 2005] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton,\\nand Greg Hullender. Learning to rank using gradient descent. In Proceedings of the 22nd international\\nconference on Machine learning, pages 89–96, 2005.\\n[Burns et al., 2023] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold\\nAschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeff Wu.\\nWeak-to-strong generalization: Eliciting strong capabilities with weak supervision.\\narXiv preprint\\narXiv:2312.09390, 2023a.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 211}, page_content='4.5 Summary\\n205\\n[Burns et al., 2023] Collin Burns, Jan Leike, Leopold Aschenbrenner, Jeffrey Wu, Pavel Izmailov, Leo\\nGao, Bowen Baker, and Jan Hendrik Kirchner. Weak-to-strong generalization, 2023b. URL https://\\nhttps://openai.com/index/weak-to-strong-generalization.\\n[Caballero et al., 2023] Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural\\nscaling laws. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation\\nModels, 2023.\\n[Cao et al., 2007] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: from\\npairwise approach to listwise approach. In Proceedings of the 24th international conference on Machine\\nlearning, pages 129–136, 2007.\\n[Chang et al., 2024] Kaiyan Chang, Songcheng Xu, Chenglong Wang, Yingfeng Luo, Tong Xiao, and\\nJingbo Zhu.\\nEfﬁcient prompting methods for large language models: A survey.\\narXiv preprint\\narXiv:2404.01077, 2024.\\n[Charniak, 1997] Eugene Charniak. Statistical parsing with a context-free grammar and word statistics.\\nAAAI/IAAI, 2005(598-603):18, 1997.\\n[Chen et al., 2023] Banghao Chen, Zhaofeng Zhang, Nicolas Langrené, and Shengxin Zhu. Unleashing\\nthe potential of prompt engineering in large language models: a comprehensive review. arXiv preprint\\narXiv:2310.14735, 2023a.\\n[Chen et al., 2023] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng\\nTang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. Alpagasus: Training a better alpaca\\nwith fewer data. arXiv preprint arXiv:2307.08701, 2023b.\\n[Chen et al., 2024] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng\\nTang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. Alpagasus: Training a better alpaca\\nwith fewer data. In The Twelfth International Conference on Learning Representations, 2024a.\\n[Chen et al., 2023] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending\\ncontext window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595,\\n2023c.\\n[Chen et al., 2020] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang\\nWang, and Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks. Advances in\\nneural information processing systems, 33:15834–15846, 2020.\\n[Chen et al., 2024] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play ﬁne-\\ntuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335,\\n2024b.\\n[Chevalier et al., 2023] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting\\nlanguage models to compress contexts. In Proceedings of the 2023 Conference on Empirical Methods\\nin Natural Language Processing, pages 3829–3846, 2023.\\n[Chi et al., 2022] Ta-Chung Chi, Ting-Han Fan, Peter J Ramadge, and Alexander Rudnicky. Kerple:\\nKernelized relative positional embedding for length extrapolation. Advances in Neural Information Pro-\\ncessing Systems, 35:8386–8399, 2022.\\n[Chi et al., 2023] Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting\\ntransformer length extrapolation via the lens of receptive ﬁeld analysis. In Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13522–13537,\\n2023.\\n[Chiang et al., 2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lian-\\nmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://\\nlmsys.org/blog/2023-03-30-vicuna/.\\n[Chowdhery et al., 2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 212}, page_content='206\\nAlignment\\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker\\nSchuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam\\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury,\\nJacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\\nmawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus,\\nDenny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan\\nSepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankara-\\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov,\\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\\nJason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling\\nlanguage modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\\n[Christiano et al., 2017] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario\\nAmodei. Deep reinforcement learning from human preferences. Advances in neural information pro-\\ncessing systems, 30, 2017.\\n[Chu et al., 2023] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang,\\nWeihua Peng, Ming Liu, Bing Qin, and Ting Liu. A survey of chain of thought reasoning: Advances,\\nfrontiers and future. arXiv preprint arXiv:2309.15402, 2023.\\n[Chung et al., 2022] Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus,\\nEric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gau-\\nrav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov,\\nEd Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.\\nScaling instruction-ﬁnetuned language models. arXiv preprint arXiv:2210.11416, 2022.\\n[Clark et al., 2019] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra:\\nPre-training text encoders as discriminators rather than generators.\\nIn Proceedings of International\\nConference on Learning Representations, 2019.\\n[Cobbe et al., 2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz\\nKaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\\nSchulman. Training veriﬁers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n[Conneau et al., 2020] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guil-\\nlaume Wenzek, Francisco Guzmán, Édouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.\\nUnsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting\\nof the Association for Computational Linguistics, pages 8440–8451, 2020.\\n[Coste et al., 2024] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensem-\\nbles help mitigate overoptimization. In The Twelfth International Conference on Learning Representa-\\ntions, 2024.\\n[Cui et al., 2024] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan\\nNi, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong Sun. ULTRAFEEDBACK:\\nBoosting language models with scaled AI feedback. In Proceedings of the 41st International Conference\\non Machine Learning, volume 235, pages 9722–9744, 2024.\\n[Dai et al., 2023] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei.\\nWhy can gpt learn in-context? language models secretly perform gradient descent as meta-optimizers.\\nIn Findings of the Association for Computational Linguistics: ACL 2023, pages 4005–4019, 2023.\\n[Dai et al., 2019] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan\\nSalakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. In Proceed-\\nings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978–2988,\\n2019.\\n[Dao et al., 2022] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast\\nand memory-efﬁcient exact attention with io-awareness. Advances in Neural Information Processing'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 213}, page_content='4.5 Summary\\n207\\nSystems, 35:16344–16359, 2022.\\n[Dehghani et al., 2018] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz\\nKaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.\\n[Deletang et al., 2024] Gregoire Deletang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim\\nGenewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent\\nOrseau, Marcus Hutter, and Joel Veness. Language modeling is compression. In The Twelfth Interna-\\ntional Conference on Learning Representations, 2024.\\n[Deng et al., 2022] Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu,\\nMeng Song, Eric Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement\\nlearning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,\\npages 3369–3391, 2022.\\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\\ntraining of deep bidirectional transformers for language understanding. In Proceedings of the 2019\\nConference of the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019.\\n[Ding et al., 2024] Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang\\nXu, Fan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens. arXiv\\npreprint arXiv:2402.13753, 2024.\\n[Dolan and Brockett, 2005] Bill Dolan and Chris Brockett. Automatically constructing a corpus of senten-\\ntial paraphrases. In Proceedings of Third International Workshop on Paraphrasing (IWP2005), 2005.\\n[Dong et al., 2019] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng\\nGao, Ming Zhou, and Hsiao-Wuen Hon.\\nUniﬁed language model pre-training for natural language\\nunderstanding and generation. Advances in neural information processing systems, 32, 2019.\\n[Dong et al., 2022] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun,\\nJingjing Xu, and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.\\n[Dong et al., 2021] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you\\nneed: Pure attention loses rank doubly exponentially with depth. In International Conference on Machine\\nLearning, pages 2793–2803. PMLR, 2021.\\n[Drozdov et al., 2022] Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song,\\nXinyun Chen, Olivier Bousquet, and Denny Zhou. Compositional semantic parsing with large language\\nmodels. In Proceedings of The Eleventh International Conference on Learning Representations, 2022.\\n[Dua et al., 2022] Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. Successive prompting\\nfor decomposing complex questions. In Proceedings of the 2022 Conference on Empirical Methods in\\nNatural Language Processing, pages 1251–1265, 2022.\\n[Dubey et al., 2024] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-\\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of\\nmodels. arXiv preprint arXiv:2407.21783, 2024.\\n[Dubois et al., 2024] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy\\nBa, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework\\nfor methods that learn from human feedback. Advances in Neural Information Processing Systems, 36,\\n2024.\\n[Eisenstein et al., 2023] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D’Amour,\\nDJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, and Peter Shaw.\\nHelping or herding? reward model ensembles mitigate but do not eliminate reward hacking. arXiv\\npreprint arXiv:2312.09244, 2023.\\n[Elsken et al., 2019] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search:\\nA survey. Journal of Machine Learning Research, 20(55):1–21, 2019.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 214}, page_content='208\\nAlignment\\n[Erhan et al., 2010] Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. Why does\\nunsupervised pre-training help deep learning? In Proceedings of the thirteenth international conference\\non artiﬁcial intelligence and statistics, pages 201–208, 2010.\\n[Fan et al., 2019] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand\\nwith structured dropout. In Proceedings of International Conference on Learning Representations, 2019.\\n[Fedus et al., 2022] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to\\ntrillion parameter models with simple and efﬁcient sparsity. The Journal of Machine Learning Research,\\n23(1):5232–5270, 2022.\\n[Fernandes et al., 2023] Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique\\nMartins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, and\\nAndré F. T. Martins. Bridging the gap: A survey on integrating (human) feedback for natural language\\ngeneration. Transactions of the Association for Computational Linguistics, 11:1643–1668, 2023.\\n[Franklin and Graesser, 1996] Stan Franklin and Art Graesser. Is it an agent, or just a program?: A taxon-\\nomy for autonomous agents. In International workshop on agent theories, architectures, and languages,\\npages 21–35. Springer, 1996.\\n[Frensch and Funke, 2014] Peter A Frensch and Joachim Funke. Complex problem solving: The European\\nperspective. Psychology Press, 2014.\\n[Gale et al., 2019] Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks.\\narXiv preprint arXiv:1902.09574, 2019.\\n[Ganguli et al., 2023] Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamile Luko-\\nsiute, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain,\\nDustin Li, Eli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau,\\nKamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemí Mercado, Nova\\nDasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kada-\\nvath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom\\nHenighan, Tristan Hume, Yuntao Bai, Zac Hatﬁeld-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph,\\nSam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, and Jared Kaplan.\\nThe capacity for moral self-correction in large language models. arXiv preprint arXiv:2302.07459,2023.\\n[Gao et al., 2023] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overopti-\\nmization. In International Conference on Machine Learning, pages 10835–10866. PMLR, 2023a.\\n[Gao et al., 2023] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie\\nCallan, and Graham Neubig. Pal: Program-aided language models. In International Conference on\\nMachine Learning, pages 10764–10799. PMLR, 2023b.\\n[Gao et al., 2023] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei\\nSun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv\\npreprint arXiv:2312.10997, 2023c.\\n[Garg et al., 2022] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can trans-\\nformers learn in-context? a case study of simple function classes. Advances in Neural Information\\nProcessing Systems, 35:30583–30598, 2022.\\n[Ge et al., 2024] Yuan Ge, Yilun Liu, Chi Hu, Weibin Meng, Shimin Tao, Xiaofeng Zhao, Hongxia\\nMa, Li Zhang, Boxing Chen, Hao Yang, Bei Li, Tong Xiao, and Jingbo Zhu. Clustering and rank-\\ning: Diversity-preserved instruction selection through expert-aligned quality estimation. arXiv preprint\\narXiv:2402.18191, 2024.\\n[Gemma Team, 2024] Google DeepMind Gemma Team. Gemma: Open Models Based on Gemini Re-\\nsearch and Technology, 2024.\\n[Goodhart, 1984] Charles AE Goodhart. Problems of monetary management: the UK experience. Springer,\\n1984.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 215}, page_content='4.5 Summary\\n209\\n[Gordon et al., 2021] Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws\\nfor neural machine translation. In Proceedings of the 2021 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 5915–5922, 2021.\\n[Gu and Dao, 2023] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state\\nspaces. arXiv preprint arXiv:2312.00752, 2023.\\n[Gunasekar et al., 2023] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del\\nGiorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil\\nSalim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman\\nKalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023.\\n[Guo et al., 2024] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang\\nBian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful\\nprompt optimizers. In The Twelfth International Conference on Learning Representations, 2024.\\n[Gupta and Berant, 2020] Ankit Gupta and Jonathan Berant. Gmat: Global memory augmentation for\\ntransformers. arXiv preprint arXiv:2006.03274, 2020.\\n[Gupta et al., 2021] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-\\nefﬁcient transformers via top-k attention. In Proceedings of the Second Workshop on Simple and Efﬁcient\\nNatural Language Processing, pages 39–52, 2021.\\n[Han et al., 2021] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu,\\nLiang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan, Yang Liu, Zhiyuan Liu, Zhiwu Lu,\\nXipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan, Wayne Xin Zhao, and Jun Zhu. Pre-\\ntrained models: Past, present and future. AI Open, 2:225–250, 2021.\\n[Han et al., 2024] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efﬁcient\\nﬁne-tuning for large models: A comprehensive survey. arXiv preprint arXiv:2403.14608, 2024.\\n[Harlap et al., 2018] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Deva-\\nnur, Greg Ganger, and Phil Gibbons. Pipedream: Fast and efﬁcient pipeline parallel dnn training. arXiv\\npreprint arXiv:1806.03377, 2018.\\n[He et al., 2019] Kaiming He, Ross Girshick, and Piotr Dollár. Rethinking imagenet pre-training. In\\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 4918–4927, 2019.\\n[He et al., 2021] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-\\nenhanced bert with disentangled attention. In Proceedings of International Conference on Learning\\nRepresentations, 2021.\\n[Hendrycks and Gimpel, 2016] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus).\\narXiv preprint arXiv:1606.08415, 2016.\\n[Hendrycks et al., 2020] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan,\\nand Dawn Song. Pretrained transformers improve out-of-distribution robustness. In Proceedings of the\\n58th Annual Meeting of the Association for Computational Linguistics, pages 2744–2751, 2020.\\n[Hendrycks et al., 2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn\\nSong, and Jacob Steinhardt. Measuring massive multitask language understanding. In Proceedings of\\nInternational Conference on Learning Representations, 2021.\\n[Hestness et al., 2017] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,\\nHassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is\\npredictable, empirically. arXiv preprint arXiv:1712.00409, 2017.\\n[Hewitt, 2024] John Hewitt. Instruction following without instruction tuning, 2024. URL https://nlp.\\nstanford.edu/~johnhew/instruction-following.html.\\n[Hewitt et al., 2024] John Hewitt, Nelson F Liu, Percy Liang, and Christopher D Manning. Instruction\\nfollowing without instruction tuning. arXiv preprint arXiv:2409.14254, 2024.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 216}, page_content='210\\nAlignment\\n[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory.\\nNeural computation, 9(8):1735–1780, 1997.\\n[Hoffmann et al., 2022] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor\\nCai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom\\nHennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Si-\\nmon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training\\ncompute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\\n[Honovich et al., 2023] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instruc-\\ntions: Tuning language models with (almost) no human labor. In Proceedings of the 61st Annual Meeting\\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 14409–14428, 2023.\\n[Houlsby et al., 2019] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin\\nDe Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer\\nlearning for NLP. In Proceedings of the 36th International Conference on Machine Learning, pages\\n2790–2799. PMLR, 2019.\\n[Hu et al., 2022] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\\nLu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International\\nConference on Learning Representations, 2022.\\n[Huang, 2009] Liang Huang. Dynamic programming-based search algorithms in NLP. In Proceedings\\nof Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the\\nAssociation for Computational Linguistics, Companion Volume: Tutorial Abstracts, 2009.\\n[Huang et al., 2019] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao\\nChen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efﬁcient\\ntraining of giant neural networks using pipeline parallelism. Advances in neural information processing\\nsystems, 32, 2019.\\n[Hutchins et al., 2022] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam\\nNeyshabur.\\nBlock-recurrent transformers.\\nAdvances in neural information processing systems, 35:\\n33248–33261, 2022.\\n[Jelinek, 1998] Frederick Jelinek. Statistical methods for speech recognition. MIT Press, 1998.\\n[Jiang et al., 2023] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Deven-\\ndra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\\nSaulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,\\nThomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. arXiv preprint arXiv:2310.06825,\\n2023a.\\n[Jiang et al., 2023] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua:\\nCompressing prompts for accelerated inference of large language models. In Proceedings of the 2023\\nConference on Empirical Methods in Natural Language Processing, pages 13358–13376, 2023b.\\n[Jiang et al., 2020] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what\\nlanguage models know?\\nTransactions of the Association for Computational Linguistics, 8:423–438,\\n2020.\\n[Jiao et al., 2020] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,\\nand Qun Liu. Tinybert: Distilling bert for natural language understanding. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2020, pages 4163–4174, 2020.\\n[Joshi et al., 2017] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large\\nscale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th\\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–\\n1611, 2017.\\n[Joshi et al., 2020] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer\\nLevy.\\nSpanbert: Improving pre-training by representing and predicting spans.\\nTransactions of the'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 217}, page_content='4.5 Summary\\n211\\nassociation for computational linguistics, 8:64–77, 2020.\\n[Jurafsky and Martin, 2008] Dan Jurafsky and James H. Martin. Speech and Language Processing (2nd\\ned.). Prentice Hall, 2008.\\n[Kahneman, 2011] Daniel Kahneman. Thinking, fast and slow. macmillan, 2011.\\n[Kaplan et al., 2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Re-\\nwon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\\nmodels. arXiv preprint arXiv:2001.08361, 2020.\\n[Katharopouloset al., 2020] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret.\\nTransformers are rnns: Fast autoregressive transformers with linear attention. In International conference\\non machine learning, pages 5156–5165. PMLR, 2020.\\n[Khandelwal et al., 2020] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike\\nLewis.\\nGeneralization through memorization: Nearest neighbor language models. In International\\nConference on Learning Representations, 2020.\\n[Khot et al., 2023] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark,\\nand Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. In\\nProceedings of The Eleventh International Conference on Learning Representations, 2023.\\n[Kim et al., 2023] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan\\nYan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Yakun Sophia\\nShao, and Amir Gholami. Full stack optimization of transformer inference: a survey. arXiv preprint\\narXiv:2302.14017, 2023.\\n[Kirkpatrick et al., 2017] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume\\nDesjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska,\\nDemis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic\\nforgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526,\\n2017.\\n[Koehn, 2010] Philipp Koehn. Statistical Machine Translation. Cambridge University Press, 2010.\\n[Kojima et al., 2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\\nIwasawa. Large language models are zero-shot reasoners. Advances in neural information processing\\nsystems, 35:22199–22213, 2022.\\n[Korthikanti et al., 2023] Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee,\\nMichael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in\\nlarge transformer models. Proceedings of Machine Learning and Systems, 5, 2023.\\n[Krakovna\\net\\nal.,\\n2020]\\nVictoria\\nKrakovna,\\nJonathan\\nUesato,\\nVladimir\\nMikulik,\\nMatthew\\nRahtz,\\nTom Everitt, Ramana Kumar, Zac Kenton, Jan Leike, and Shane Legg.\\nSpeciﬁ-\\ncation gaming:\\nthe ﬂip side of ai ingenuity.\\nhttps://deepmind.google/discover/blog/\\nspecification-gaming-the-flip-side-of-ai-ingenuity, 2020.\\n[Kung and Peng, 2023] Po-Nien Kung and Nanyun Peng. Do models really learn to follow instructions?\\nan empirical study of instruction tuning. arXiv preprint arXiv:2305.11383, 2023.\\n[Kwon et al., 2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao\\nYu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. Efﬁcient memory management for large language\\nmodel serving with pagedattention. arXiv preprint arXiv:2309.06180, 2023.\\n[Lake and Baroni, 2018] Brenden Lake and Marco Baroni. Generalization without systematicity: On\\nthe compositional skills of sequence-to-sequence recurrent networks. In International conference on\\nmachine learning, pages 2873–2882. PMLR, 2018.\\n[Lambert et al., 2024] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen\\nLin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Han-\\nnaneh Hajishirzi.\\nRewardbench: Evaluating reward models for language modeling. arXiv preprint'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 218}, page_content='212\\nAlignment\\narXiv:2403.13787, 2024.\\n[Lample and Conneau, 2019] Guillaume Lample and Alexis Conneau. Cross-lingual language model\\npretraining. arXiv preprint arXiv:1901.07291, 2019.\\n[Lan et al., 2020] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and\\nRadu Soricut. Albert: A lite bert for self-supervised learning of language representations. In Proceedings\\nof International Conference on Learning Representations, 2020.\\n[Lee et al., 2023] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Ren Lu, Thomas Mesnard, Johan\\nFerret, Colton Bishop, Ethan Hall, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement\\nlearning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.\\n[Lester et al., 2021] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-\\nefﬁcient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-\\nguage Processing, pages 3045–3059, 2021.\\n[Lewis et al., 2020] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mo-\\nhamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence\\npre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th\\nAnnual Meeting of the Association for Computational Linguistics, pages 7871–7880, 2020.\\n[Li et al., 2023] Bei Li, Rui Wang, Junliang Guo, Kaitao Song, Xu Tan, Hany Hassan, Arul Menezes, Tong\\nXiao, Jiang Bian, and JingBo Zhu. Deliberate then generate: Enhanced prompting framework for text\\ngeneration. arXiv preprint arXiv:2305.19835, 2023a.\\n[Li, 2011] Hang Li. Learning to Rank for Information Retrieval and Natural Language Processing. Online\\naccess: Morgan & Claypool Synthesis Collection Five. Morgan & Claypool Publishers, 2011. ISBN\\n9781608457076.\\n[Li et al., 2022] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. A survey on retrieval-\\naugmented text generation. arXiv preprint arXiv:2202.01110, 2022.\\n[Li et al., 2024] Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil\\nZaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. Functional interpolation\\nfor relative positions improves long context transformers. In The Twelfth International Conference on\\nLearning Representations, 2024.\\n[Li et al., 2023] Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. Sequence\\nparallelism: Long sequence training from system perspective. In Proceedings of the 61st Annual Meeting\\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 2391–2404, 2023b.\\n[Li and Liang, 2021] Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for\\ngeneration. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\\nand the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),\\npages 4582–4597, 2021.\\n[Li, 2023] Yinheng Li. A practical survey on zero-shot prompt design for in-context learning. In Proceed-\\nings of the 14th International Conference on Recent Advances in Natural Language Processing, pages\\n641–647, 2023.\\n[Li et al., 2023] Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance\\ninference efﬁciency of large language models. In Proceedings of the 2023 Conference on Empirical\\nMethods in Natural Language Processing, pages 6342–6353, 2023c.\\n[Lialin et al., 2023] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling down to scale up:\\nA guide to parameter-efﬁcient ﬁne-tuning. arXiv preprint arXiv:2303.15647, 2023.\\n[Lightman et al., 2024] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker,\\nTeddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In The\\nTwelfth International Conference on Learning Representations, 2024.\\n[Liu et al., 2024] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 219}, page_content='4.5 Summary\\n213\\nZhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint\\narXiv:2412.19437, 2024a.\\n[Liu et al., 2022] Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and\\nWeizhu Chen. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside\\nOut (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning\\nArchitectures, pages 100–114, 2022.\\n[Liu et al., 2023] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham\\nNeubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language\\nprocessing. ACM Computing Surveys, 55(9):1–35, 2023a.\\n[Liu et al., 2024] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and\\nJialu Liu. Statistical rejection sampling improves preference optimization. In The Twelfth International\\nConference on Learning Representations, 2024b.\\n[Liu, 2009] Tie-Yan Liu. Learning to rank for information retrieval. Foundations and Trends® in Informa-\\ntion Retrieval, 3(3):225–331, 2009.\\n[Liu et al., 2023] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie\\nTang. Gpt understands, too. AI Open, 2023b.\\n[Liu et al., 2023] Xiaoxia Liu, Jingyi Wang, Jun Sun, Xiaohan Yuan, Guoliang Dong, Peng Di, Wenhai\\nWang, and Dongxia Wang. Prompting frameworks for large language models: A survey. arXiv preprint\\narXiv:2311.12785, 2023c.\\n[Liu et al., 2024] Xinyu Liu, Runsong Zhao, Pengcheng Huang, Chunyang Xiao, Bei Li, Jingang Wang,\\nTong Xiao, and Jingbo Zhu. Forgetting curve: A reliable method for evaluating memorization capability\\nfor long-context models. In Proceedings of the 2024 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 4667–4682, 2024c.\\n[Liu et al., 2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\\napproach. arXiv preprint arXiv:1907.11692, 2019.\\n[Longpre et al., 2023] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay,\\nDenny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The ﬂan collection: Designing\\ndata and methods for effective instruction tuning. In International Conference on Machine Learning,\\npages 22631–22648. PMLR, 2023.\\n[Ma et al., 2023] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig,\\nJonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. In The Eleventh\\nInternational Conference on Learning Representations, 2023.\\n[Ma et al., 2024] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan\\nMay, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efﬁcient llm pretraining and\\ninference with unlimited context length. arXiv preprint arXiv:2404.08801, 2024.\\n[Madaan et al., 2024] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao,\\nSarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bod-\\nhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.\\nSelf-reﬁne: Iterative reﬁnement with self-feedback. Advances in Neural Information Processing Sys-\\ntems, 36, 2024.\\n[Manning, 2022] Christopher D Manning. Human language understanding & reasoning. Daedalus, 151\\n(2):127–138, 2022.\\n[Marcus, 1993] Gary F Marcus. Negative evidence in language acquisition. Cognition, 46(1):53–85, 1993.\\n[Martins et al., 2022] Pedro Henrique Martins, Zita Marinho, and André FT Martins. ∞-former: Inﬁnite\\nmemory transformer-former: Inﬁnite memory transformer. In Proceedings of the 60th Annual Meeting\\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 5468–5485, 2022.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 220}, page_content='214\\nAlignment\\n[Mavi et al., 2024] Vaibhav Mavi, Anubhav Jangra, and Adam Jatowt. Multi-hop question answering.\\nFoundations and Trends® in Information Retrieval, 17(5):457–586, 2024.\\n[Michel et al., 2019] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than\\none? Advances in neural information processing systems, 32, 2019.\\n[Micikevicius et al., 2018] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich\\nElsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao\\nWu. Mixed precision training. In Proceedings of International Conference on Learning Representations,\\n2018.\\n[Miettinen, 1999] Kaisa Miettinen. Nonlinear multiobjective optimization, volume 12. Springer Science\\n& Business Media, 1999.\\n[Mikolov et al., 2013] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation\\nof word representations in vector space. In Proceedings of the International Conference on Learning\\nRepresentations (ICLR 2013), 2013a.\\n[Mikolov et al., 2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Dis-\\ntributed representations of words and phrases and their compositionality. In Proceedings of the 26th In-\\nternational Conference on Neural Information Processing Systems - Volume 2, pages 3111–3119, 2013b.\\n[Min et al., 2019] Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. Multi-hop read-\\ning comprehension through question decomposition and rescoring. In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics, pages 6097–6109, 2019.\\n[Minaee et al., 2024] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard\\nSocher, Xavier Amatriain, and Jianfeng Gao.\\nLarge language models: A survey.\\narXiv preprint\\narXiv:2402.06196, 2024.\\n[Mishra et al., 2022] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-\\ntask generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470–3487,\\n2022.\\n[Mnih et al., 2016] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Tim Harley,\\nTimothy P Lillicrap, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce-\\nment learning. In Proceedings of the 33rd International Conference on International Conference on\\nMachine Learning, pages 1928–1937, 2016.\\n[Mohtashami and Jaggi, 2024] Amirkeivan Mohtashami and Martin Jaggi. Random-access inﬁnite context\\nlength for transformers. Advances in Neural Information Processing Systems, 36, 2024.\\n[Mu et al., 2024] Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens.\\nAdvances in Neural Information Processing Systems, 36, 2024.\\n[Munkhdalaiet al., 2024] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context\\nbehind: Efﬁcient inﬁnite context transformers with inﬁni-attention. arXiv preprint arXiv:2404.07143,\\n2024.\\n[Nakano et al., 2021] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina\\nKim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna\\nEloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman.\\nWebgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332,\\n2021.\\n[Narayanan et al., 2021] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley,\\nMostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan\\nCatanzaro, Amar Phanishayee, and Matei Zaharia. Efﬁcient large-scale language model training on\\ngpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance\\nComputing, Networking, Storage and Analysis, pages 1–15, 2021.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 221}, page_content='4.5 Summary\\n215\\n[Ng et al., 1999] Andrew Y Ng, Daishi Harada, and Stuart J Russell. Policy invariance under reward\\ntransformations: Theory and application to reward shaping. In Proceedings of the Sixteenth International\\nConference on Machine Learning, pages 278–287, 1999.\\n[OpenAI, 2024] OpenAI. Learning to reason with llms, September 2024. URL https://openai.com/\\nindex/learning-to-reason-with-llms/.\\n[Ouyang et al., 2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan\\nLeike, and Ryan Lowe. Training language models to follow instructions with human feedback. Advances\\nin Neural Information Processing Systems, 35:27730–27744, 2022.\\n[Pal et al., 2023] Koyena Pal, Jiuding Sun, Andrew Yuan, Byron C Wallace, and David Bau. Future lens:\\nAnticipating subsequent tokens from a single hidden state. In Proceedings of the 27th Conference on\\nComputational Natural Language Learning (CoNLL), pages 548–560, 2023.\\n[Pan et al., 2022] Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspeciﬁca-\\ntion: Mapping and mitigating misaligned models. In International Conference on Learning Representa-\\ntions, 2022.\\n[Pan et al., 2024] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and\\nWilliam Yang Wang.\\nAutomatically correcting large language models: Surveying the landscape of\\ndiverse automated correction strategies. Transactions of the Association for Computational Linguistics,\\n12:484–506, 2024.\\n[Parisi et al., 2022] Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models.\\narXiv preprint arXiv:2205.12255, 2022.\\n[Parisi et al., 2019] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter.\\nContinual lifelong learning with neural networks: A review. Neural networks, 113:54–71, 2019.\\n[Parmar et al., 2018] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer,\\nAlexander Ku, and Dustin Tran. Image transformer. In International conference on machine learn-\\ning, pages 4055–4064. PMLR, 2018.\\n[Penedo et al., 2023] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessan-\\ndro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The reﬁned-\\nweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv\\npreprint arXiv:2306.01116, 2023.\\n[Peng et al., 2024] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efﬁcient con-\\ntext window extension of large language models. In The Twelfth International Conference on Learning\\nRepresentations, 2024.\\n[Pennington et al., 2014] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global\\nvectors for word representation. In Proceedings of Empirical Methods in Natural Language Processing\\n(EMNLP), pages 1532–1543, 2014.\\n[Peters et al., 2018] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,\\nKenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the\\n2018 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-\\nman Language Technologies, Volume 1 (Long Papers), 2018.\\n[Plackett, 1975] Robin L Plackett. The analysis of permutations. Journal of the Royal Statistical Society\\nSeries C: Applied Statistics, 24(2):193–202, 1975.\\n[Prasad et al., 2023] Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-\\nbased instruction search for prompting large language models. In Proceedings of the 17th Conference of\\nthe European Chapter of the Association for Computational Linguistics, pages 3845–3864, 2023.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 222}, page_content='216\\nAlignment\\n[Press et al., 2022] Oﬁr Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with lin-\\near biases enables input length extrapolation. In Proceedings of International Conference on Learning\\nRepresentations, 2022.\\n[Press et al., 2023] Oﬁr Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis.\\nMeasuring and narrowing the compositionality gap in language models. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2023, pages 5687–5711, 2023.\\n[Pryzant et al., 2023] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng.\\nAutomatic prompt optimization with \"gradient descent\" and beam search. In The 2023 Conference on\\nEmpirical Methods in Natural Language Processing, 2023.\\n[Qiu et al., 2020] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang.\\nPre-trained models for natural language processing: A survey. Science China Technological Sciences,\\n63(10):1872–1897, 2020.\\n[Radford et al., 2018] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving\\nlanguage understanding by generative pre-training. OpenAI, 2018.\\n[Radford et al., 2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\\nSutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8), 2019.\\n[Radford et al., 2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-\\nhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\\nSutskever. Learning transferable visual models from natural language supervision. In International\\nconference on machine learning, pages 8748–8763. PMLR, 2021.\\n[Rae et al., 2019] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P\\nLillicrap. Compressive transformers for long-range sequence modelling. In International Conference on\\nLearning Representations, 2019.\\n[Rafailov et al., 2024] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano\\nErmon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward\\nmodel. Advances in Neural Information Processing Systems, 36, 2024.\\n[Raffel et al., 2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\\ntext-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.\\n[Ramachandran et al., 2017] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation\\nfunctions. arXiv preprint arXiv:1710.05941, 2017.\\n[Rolnick et al., 2019] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory\\nWayne. Experience replay for continual learning. Advances in Neural Information Processing Systems,\\n32, 2019.\\n[Rosenfeld et al., 2020] Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A con-\\nstructive prediction of the generalization error across scales. In Proceedings of International Conference\\non Learning Representations, 2020.\\n[Ruan et al., 2024] Junhao Ruan, Long Meng, Weiqiao Shan, Tong Xiao, and Jingbo Zhu. A survey of llm\\nsurveys. https://github.com/NiuTrans/ABigSurveyOfLLMs, 2024.\\n[Rubin et al., 2022] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts\\nfor in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies, pages 2655–2671, 2022.\\n[Russell, 2019] Stuart Russell. Human Compatible: Artiﬁcial Intelligence and the Problem of Controls.\\nViking, 2019.\\n[Sanh et al., 2020] Victor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity\\nby ﬁne-tuning. Advances in Neural Information Processing Systems, 33:20378–20389, 2020.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 223}, page_content='4.5 Summary\\n217\\n[Sanh et al., 2022] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid\\nAlyafeai, Antoine Chafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish\\nThakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak,\\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,\\nZheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht\\nSharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Bider-\\nman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables zero-shot\\ntask generalization. In Proceedings of International Conference on Learning Representations, 2022.\\n[Schick et al., 2023] Timo Schick, Jane A. Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izac-\\nard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. PEER: A collaborative\\nlanguage model. In Proceedings of The Eleventh International Conference on Learning Representations,\\n2023.\\n[Schick et al., 2024] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric\\nHambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can\\nteach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024.\\n[Schmidhuber, 2015] Jürgen Schmidhuber. Deep learning in neural networks: An overview. Neural net-\\nworks, 61:85–117, 2015.\\n[Schulman et al., 2015] John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel.\\nTrust region policy optimization. In Proceedings of the 32nd International Conference on International\\nConference on Machine Learning-Volume 37, pages 1889–1897, 2015.\\n[Schulman et al., 2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\\nProximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\\n[Sennrich et al., 2016] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine\\ntranslation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers), pages 86–96, 2016.\\n[Seo et al., 2017] Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional\\nattention ﬂow for machine comprehension. In Proceedings of International Conference on Learning\\nRepresentations, 2017.\\n[Shannon, 1951] Claude E Shannon. Prediction and entropy of printed english. Bell system technical\\njournal, 30(1):50–64, 1951.\\n[Shaw et al., 2018] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position\\nrepresentations. In Proceedings of the 2018 Conference of the North American Chapter of the Associ-\\nation for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages\\n464–468, 2018.\\n[Shazeer, 2019] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint\\narXiv:1911.02150, 2019.\\n[Shazeer, 2020] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202,2020.\\n[Shen et al., 2020] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W\\nMahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceed-\\nings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 8815–8821, 2020.\\n[Shoeybi et al., 2019] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,\\nand Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model\\nparallelism. arXiv preprint arXiv:1909.08053, 2019.\\n[Skalse et al., 2022] Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Deﬁning\\nand characterizing reward gaming. Advances in Neural Information Processing Systems, 35:9460–9471,\\n2022.\\n[Snell et al., 2022] Charlie Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context. arXiv'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 224}, page_content='218\\nAlignment\\npreprint arXiv:2209.15189, 2022.\\n[Socher et al., 2013] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,\\nAndrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a\\nsentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language\\nprocessing, pages 1631–1642, 2013.\\n[Song et al., 2019] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence\\nto sequence pre-training for language generation. In International Conference on Machine Learning,\\npages 5926–5936. PMLR, 2019.\\n[Stiennon et al., 2020] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea\\nVoss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback.\\nAdvances in Neural Information Processing Systems, 33:3008–3021, 2020.\\n[Su et al., 2024] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Ro-\\nformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\\n[Sun et al., 2020] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.\\nMobilebert: a compact task-agnostic bert for resource-limited devices. In Proceedings of the 58th Annual\\nMeeting of the Association for Computational Linguistics, pages 2158–2170, 2020.\\n[Sutskever et al., 2014] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with\\nneural networks. Advances in neural information processing systems, 27, 2014.\\n[Sutton and Barto, 2018] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduc-\\ntion (2nd ed.). The MIT Press, 2018.\\n[Szepesvári, 2010] Csaba Szepesvári. Algorithms for reinforcement learning. Synthesis Lectures on Arti-\\nﬁcial Intelligence and Machine Learning, 4(1):1–103, 2010.\\n[Talmor and Berant, 2018] Alon Talmor and Jonathan Berant. The web as a knowledge-base for answering\\ncomplex questions. arXiv preprint arXiv:1803.06643, 2018.\\n[Taori et al., 2023] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos\\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama\\nmodel. https://github.com/tatsu-lab/stanford_alpaca, 2023.\\n[Tay et al., 2020] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A\\nsurvey. CoRR, abs/2009.06732, 2020.\\n[Team et al., 2024] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin,\\nSurya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al.\\nGemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118, 2024.\\n[Teknium, 2023] Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants,\\n2023. URL https://huggingface.co/datasets/teknium/OpenHermes-2.5.\\n[Touvron et al., 2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Ro-\\ndriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efﬁcient foundation\\nlanguage models. arXiv preprint arXiv:2302.13971, 2023a.\\n[Touvron et al., 2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,\\nYasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel,\\nLukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernan-\\ndes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony\\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Is-\\nabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor\\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schel-\\nten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 225}, page_content='4.5 Summary\\n219\\nAdina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\\nMelanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas\\nScialom. Llama 2: Open foundation and ﬁne-tuned chat models. arXiv preprint arXiv:2307.09288,\\n2023b.\\n[Uesato et al., 2022] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa\\nWang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-\\nand outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.\\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\\nGomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of Advances in\\nNeural Information Processing Systems, volume 30, 2017.\\n[Von Oswald et al., 2023] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento,\\nAlexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by\\ngradient descent.\\nIn Proceedings of International Conference on Machine Learning, pages 35151–\\n35174. PMLR, 2023.\\n[Wang et al., 2024] Chenglong Wang, Hang Zhou, Yimin Hu, Yifu Huo, Bei Li, Tongran Liu, Tong Xiao,\\nand Jingbo Zhu. Esrl: Efﬁcient sampling-based reinforcement learning for sequence generation. In\\nProceedings of the AAAI Conference on Artiﬁcial Intelligence, pages 19107–19115, 2024.\\n[Wang et al., 2023] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of\\ncontinual learning: Theory, method and application. arXiv preprint arXiv:2302.00487, 2023a.\\n[Wang et al., 2019] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and\\nLidia S Chao. Learning deep transformer models for machine translation. In Proceedings of the 57th\\nAnnual Meeting of the Association for Computational Linguistics, pages 1810–1822, 2019.\\n[Wang et al., 2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou.\\nRationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022a.\\n[Wang et al., 2023] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang,\\nAakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in lan-\\nguage models. In Proceedings of The Eleventh International Conference on Learning Representations,\\n2023b.\\n[Wang et al., 2022] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza\\nMirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Es-\\nhaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson,\\nKirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mi-\\nrali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia,\\nSavan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit,\\nand Xudong Shen. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp\\ntasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,\\npages 5085–5109, 2022b.\\n[Wang et al., 2023] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khy-\\nathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh\\nHajishirzi. How far can camels go? exploring the state of instruction tuning on open resources. Ad-\\nvances in Neural Information Processing Systems, 36:74764–74786, 2023c.\\n[Wang et al., 2023] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel\\nKhashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated in-\\nstructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pages 13484–13508, 2023d.\\n[Wang et al., 2023] Zhenyi Wang, Enneng Yang, Li Shen, and Heng Huang. A comprehensive survey of\\nforgetting in deep learning beyond continual learning. arXiv preprint arXiv:2307.09218, 2023e.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 226}, page_content='220\\nAlignment\\n[Warstadt et al., 2019] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network accept-\\nability judgments. Transactions of the Association for Computational Linguistics, 7:625–641, 2019.\\n[Wei et al., 2022] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Proceedings\\nof International Conference on Learning Representations, 2022a.\\n[Wei et al., 2022] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud,\\nDani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol\\nVinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. arXiv\\npreprint arXiv:2206.07682, 2022b.\\n[Wei et al., 2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,\\nEd H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\\nmodels. Advances in Neural Information Processing Systems, 35:24824–24837, 2022c.\\n[Welleck et al., 2023] Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel\\nKhashabi, and Yejin Choi. Generating sequences by learning to self-correct. In Proceedings of The\\nEleventh International Conference on Learning Representations, 2023.\\n[Weng, 2021] Lilian Weng. How to train really large models on many gpus? lilianweng.github.io, Sep\\n2021. URL https://lilianweng.github.io/posts/2021-09-25-train-large/.\\n[Wiener, 1960] Norbert Wiener. Some moral and technical consequences of automation: As machines\\nlearn they may develop unforeseen strategies at rates that bafﬂe their programmers. Science, 131(3410):\\n1355–1358, 1960.\\n[Williams et al., 2018] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge\\ncorpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies,\\nVolume 1 (Long Papers), pages 1112–1122, 2018.\\n[Williams, 1992] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist\\nreinforcement learning. Machine learning, 8:229–256, 1992.\\n[Wingate et al., 2022] David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression\\nand contrastive conditioning for controllability and toxicity reduction in language models. In Findings\\nof the Association for Computational Linguistics: EMNLP 2022, pages 5621–5634, 2022.\\n[Wu et al., 2024] Wilson Wu, John X Morris, and Lionel Levine. Do language models plan for future\\ntokens? arXiv preprint arXiv:2404.00859, 2024.\\n[Wu et al., 2021] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memo-\\nrizing transformers. In Proceedings of International Conference on Learning Representations, 2021.\\n[Wu et al., 2023] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu,\\nNoah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better\\nrewards for language model training. In Thirty-seventh Conference on Neural Information Processing\\nSystems, 2023.\\n[Xia et al., 2024] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen.\\nLess: Selecting inﬂuential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333, 2024.\\n[Xiao et al., 2024] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efﬁcient\\nstreaming language models with attention sinks. In Proceedings of The Twelfth International Conference\\non Learning Representations, 2024.\\n[Xiao and Zhu, 2023] Tong Xiao and Jingbo Zhu. Introduction to transformers: an nlp perspective. arXiv\\npreprint arXiv:2311.17633, 2023.\\n[Xiao et al., 2013] Tong Xiao, Jingbo Zhu, and Tongran Liu. Bagging and boosting statistical machine\\ntranslation systems. Artiﬁcial Intelligence, 195:496–527, 2013.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 227}, page_content='4.5 Summary\\n221\\n[Xiao et al., 2019] Tong Xiao, Yinqiao Li, Jingbo Zhu, Zhengtao Yu, and Tongran Liu. Sharing attention\\nweights for fast transformer. In Proceedings of the Twenty-Eighth International Joint Conference on\\nArtiﬁcial Intelligence (IJCAI-19), pages 5292–5298, 2019.\\n[Xie et al., 2022] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation\\nof in-context learning as implicit bayesian inference. In Proceedings of International Conference on\\nLearning Representations, 2022.\\n[Xin et al., 2020] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early\\nexiting for accelerating bert inference. In Proceedings of the 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 2246–2251, 2020.\\n[Xu et al., 2024] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao,\\nQingwei Lin, and Daxin Jiang. Wizardlm: Empowering large pre-trained language models to follow\\ncomplex instructions. In The Twelfth International Conference on Learning Representations, 2024.\\n[Yang et al., 2024] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint\\narXiv:2412.15115, 2024.\\n[Yang et al., 2019] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and\\nQuoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in\\nneural information processing systems, 32, 2019.\\n[Yao et al., 2024] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Grifﬁths, Yuan Cao, and Karthik\\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in\\nNeural Information Processing Systems, 36, 2024.\\n[Yarowsky, 1995] David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods.\\nIn Proceedings of the 33rd annual meeting of the association for computational linguistics, pages 189–\\n196, 1995.\\n[Yu et al., 2023] Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, and Jiajun Chen. Towards better chain-of-\\nthought prompting strategies: A survey. arXiv preprint arXiv:2310.04959, 2023.\\n[Zaheer et al., 2020] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, C. Alberti,\\nS. Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, L. Yang, and A. Ahmed. Big bird: Transformers\\nfor longer sequences. Advances in neural information processing systems, 33:17283–17297, 2020.\\n[Zellers et al., 2018] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale\\nadversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing, pages 93–104, 2018.\\n[Zhang and Sennrich, 2019] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Ad-\\nvances in Neural Information Processing Systems, 32, 2019.\\n[Zhang et al., 2023] Zhuosheng Zhang, Yao Yao, Aston Zhang, Xiangru Tang, Xinbei Ma, Zhiwei He,\\nYiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu, and Hai Zhao.\\nIgniting language intelli-\\ngence: The hitchhiker’s guide from chain-of-thought reasoning to language agents.\\narXiv preprint\\narXiv:2311.11797, 2023a.\\n[Zhang et al., 2023] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought\\nprompting in large language models. In The Eleventh International Conference on Learning Represen-\\ntations, 2023b.\\n[Zhao et al., 2024] Hao Zhao, Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Long\\nis more for alignment: A simple but tough-to-beat baseline for instruction ﬁne-tuning. arXiv preprint\\narXiv:2402.04833, 2024.\\n[Zhao et al., 2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Z. Chen,\\nJinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jianyun Nie, and Ji rong Wen.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 228}, page_content='222\\nAlignment\\nA survey of large language models. arXiv preprint arXiv:2303.18223, 2023.\\n[Zhou et al., 2023] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma,\\nAvia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer\\nLevy. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023a.\\n[Zhou et al., 2023] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale\\nSchuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. Least-to-most prompting enables\\ncomplex reasoning in large language models. In Proceedings of The Eleventh International Conference\\non Learning Representations, 2023b.\\n[Zhou et al., 2020] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, and Furu Wei. Bert\\nloses patience: Fast and robust inference with early exit. Advances in Neural Information Processing\\nSystems, 33:18330–18341, 2020.\\n[Zhou et al., 2023] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris\\nChan, and Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh\\nInternational Conference on Learning Representations, 2023c.\\n[Zoph and Le, 2016] Barret Zoph and Quoc Le. Neural architecture search with reinforcement learning. In\\nProceedings of International Conference on Learning Representations, 2016.\\n[Zoph et al., 2020] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk,\\nand Quoc Le. Rethinking pre-training and self-training. Advances in neural information processing\\nsystems, 33:3833–3845, 2020.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 229}, page_content='Index\\nk-NN, 74\\nk-NN LM, 76\\nk-NN language modeling, 76\\nk-nearest neighbors, 74\\nA2C, 175\\naction-value function, 171\\nadvantage, 175\\nadvantage actor-critic, 175\\nAgent, 47\\nALiBi, 85\\nalignment, 46\\nattention with linear biases, 85\\nautomated machine learning, 137\\nautomatic prompt design, 137\\nAutoML, 137\\nautonomous agents, 134\\nBART, 19\\nBERT, 1\\nBest-of-N sampling, 197\\nBoN sampling, 197\\nBradley-Terry model, 178\\ncalculation annotation, 113\\ncatastrophic forgetting, 34\\ncausal language modeling, 9\\nchain of thought, 113\\nchain-of-thought prompting, 53\\ncompletion, 6\\ncompositional generalization, 122\\nCoT, 113\\nCOT prompting, 53\\ncross-lingual language models, 28\\ncumulative reward, 172\\ndeliberate-then-generate, 126\\ndemonstrations, 6\\ndirect preference optimization, 190\\nDocument Rotation, 20\\nDPO, 190\\nDTG, 126\\nemergent abilities, 63\\nexternal memories, 74\\nExtrapolation, 81\\nfew-shot COT prompting, 54\\ngated linear unit, 58\\ngaussian error linear unit, 58\\nGeLU, 58\\nGLU, 58\\nGPT, 1\\nGQA, 80\\nGrouped query attention, 80\\nhard prompts, 140\\nhuman preference alignment, 152\\nICL, 53\\nICT, 6\\nimportance sampling, 180\\nin-context learning, 6, 53, 95\\ninput inversion, 163\\ninstruction alignment, 152\\ninstruction ﬁne-tuning, 43, 154\\ninterference, 30\\ninternal memories, 74\\nInterpolation, 82\\nirreducible error, 63\\nkey-value cache, 68\\nKV cache, 68\\nlabel mapping, 105\\nLearning from Human Feedback, 47\\nleast-to-most prompting, 119\\nlong-context LLMs, 66\\nmasked language modeling, 1, 9\\nmBERT, 28\\nmemory-based methods, 74\\nMQA, 79\\nmulti-lingual BERT, 28\\nmulti-query attention, 79\\nNAS, 137\\nneural architecture search, 137\\nnext sentence prediction, 13\\nNSP, 13\\nofﬂine reinforcement learning, 193\\none-shot COT prompting, 54\\nOutcome-based Approaches, 195\\n223'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'source': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Foundations of Large Language Models.pdf', 'total_pages': 231, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-16T20:13:48-05:00', 'trapped': '', 'modDate': \"D:20250116201348-05'00'\", 'creationDate': \"D:20250116201348-05'00'\", 'page': 230}, page_content='224\\nAlignment\\noveroptimization problem, 189\\nPerformance Estimation, 137\\nperformance function, 172\\nperformance gap recovered, 167\\npermuted language modeling, 11\\nPGR, 167\\nPlackett-Luce model, 184\\nPPO, 50, 181\\npreﬁx ﬁne-tuning, 144\\npreﬁx language modeling, 16\\nproblem decomposition, 116\\nProcess-based Approaches, 195\\nprompt embeddings, 148\\nprompt engineering, 95\\nprompt optimization, 137\\nPrompt Search Space, 137\\nprompting engineering, 51\\nproximal policy optimization, 50, 181\\nQ-value function, 171\\nRAG, 76\\nratio function, 180\\nrectiﬁed linear unit, 58\\nreinforcement learning from human feedback,\\n47, 153\\nrejection sampling, 198\\nrelation extraction, 108\\nReLU, 58\\nretrieval-augmented generation, 76\\nreturn, 172\\nreward gaming, 189\\nreward hacking, 189\\nReward Model, 47\\nRLHF, 47, 153\\nRoBERTa, 26\\nsample efﬁcient, 164\\nscaling laws, 63\\nself-consistency, 129\\nself-instruct, 160\\nself-supervised learning, 3\\nself-training, 3\\nSentence Reordering, 19\\nSequence Encoding Models, 3\\nSequence Generation Models, 3\\nSFT, 47, 152\\nsingle-round prediction, 155\\nsoft prompts, 140\\nSpan Masking, 19\\nstate-value function, 171\\nStrong Ceiling Performance, 167\\nSub-problem Generation, 118\\nSub-problem Solving, 118\\nsuperﬁcial alignment hypothesis, 164\\nSupervised Fine-tuning, 47\\nsupervised ﬁne-tuning, 152\\nsupervised learning, 2\\nsurrogate objective, 180\\nT5, 15\\nTD, 176\\ntemporal difference, 176\\ntext completion, 109\\ntext transformation, 109\\nToken Deletion, 19\\nToken Masking, 19\\nTransformers, 1\\ntranslation language modeling, 29\\ntrust regions, 181\\nunsupervised learning, 2\\nWeak Performance, 167\\nweak-to-strong generalization, 166\\nWeak-to-strong Performance, 167\\nXLMs, 28\\nzero-shot COT, 54\\nzero-shot learning, 45'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 0}, page_content='Second \\nEdition\\nDavid Foster\\nForeword by Karl Friston\\nGenerative\\nDeep Learning\\nTeaching Machines to Paint, Write, \\nCompose, and Play'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 1}, page_content='AI / DEEP LEARNING \\n“Generative Deep \\nLearning is an \\naccessible introduction \\nto the deep learning \\ntoolkit for generative \\nmodeling. If you are a \\ncreative practitioner \\nwho loves to tinker \\nwith code and want to \\napply deep learning \\nto your work, then this \\nis the book for you.”\\n—David Ha\\nHead of Strategy, Stability AI\\n“An excellent book that \\ndives right into all of \\nthe major techniques \\nbehind state-of-the-\\nart generative deep \\nlearning. An exciting \\nexploration of one of \\nthe most fascinating \\ndomains in AI!” \\n—François Chollet\\nCreator of Keras\\nGenerative Deep Learning\\nTwitter: @oreillymedia\\nlinkedin.com/company/oreilly-media\\nyoutube.com/oreillymedia \\nGenerative AI is the hottest topic in tech. This practical book \\nteaches machine learning engineers and data scientists how \\nto use TensorFlow and Keras to create impressive generative \\ndeep learning models from scratch, including variational \\nautoencoders (VAEs), generative adversarial networks \\n(GANs), Transformers, normalizing flows, energy-based \\nmodels, and denoising diffusion models.\\nThe book starts with the basics of deep learning and \\nprogresses to cutting-edge architectures. Through tips \\nand tricks, you’ll understand how to make your models \\nlearn more efficiently and become more creative.\\n•\\t Discover how VAEs can change facial expressions in photos\\n•\\t Train GANs to generate images based on your own dataset\\n•\\t Build diffusion models to produce new varieties of flowers\\n•\\t Train your own GPT for text generation\\n•\\t Learn how large language models like ChatGPT are trained\\n•\\t Explore state-of-the-art architectures such as StyleGAN2 \\nand ViT-VQGAN\\n•\\t Compose polyphonic music using Transformers \\nand MuseGAN\\n•\\t Understand how generative world models can solve \\nreinforcement learning tasks\\n•\\t Dive into multimodal models such as DALL.E 2, Imagen, \\nand Stable Diffusion\\nThis book also explores the future of generative AI and \\nhow individuals and companies can proactively begin \\nto leverage this remarkable new technology to create \\ncompetitive advantage.\\nDavid Foster is the cofounder of \\nApplied Data Science Partners.\\nUS $79.99\\t  CAN $99.99\\nISBN: 978-1-098-13418-1'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 2}, page_content='Praise for Generative Deep Learning\\nGenerative Deep Learning is an accessible introduction to the deep learning toolkit for\\ngenerative modeling. If you are a creative practitioner who loves to tinker with code and\\nwant to apply deep learning to your work, then this is the book for you.\\n—David Ha, Head of Strategy, Stability AI\\nAn excellent book that dives right into all of the major techniques behind state-of-the-art\\ngenerative deep learning. You’ll find intuitive explanations and clever analogies—\\nbacked by didactic, highly readable code examples. An exciting\\nexploration of one of the most fascinating domains in AI!\\n—François Chollet, Creator of Keras\\nDavid Foster’s explanations of complex concepts are clear and concise,\\nenriched with intuitive visuals, code examples, and exercises.\\nAn excellent resource for students and practitioners!\\n—Suzana Ilić, Principal Program Manager Responsible AI,\\nMicrosoft Azure OpenAI\\nGenerative AI is the next revolutionary step in AI technology that will have a massive\\nimpact on the world. This book provides a great introduction to this field\\nand its incredible potential and potential risks.\\n—Connor Leahy, CEO at Conjecture\\nand Cofounder of EleutherAI\\nPredicting the world means understanding the world—in all modalities. In that sense,\\ngenerative AI is solving the very core of intelligence.\\n—Jonas Andrulis, Founder & CEO Aleph Alpha'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 3}, page_content='Generative AI is reshaping countless industries and powering a new generation of\\ncreative tools. This book is the perfect way to get going with generative modeling and\\nstart building with this revolutionary technology yourself.\\n—Ed Newton-Rex, VP Audio at Stability AI and composer\\nDavid taught me everything I know about machine learning and has a\\nknack for explaining the underlying concepts. Generative Deep Learning is my go-to\\nresource for generative AI and lives on a shelf next to my work desk amongst\\nmy small collection of favorite technical books.\\n—Zack Thoutt, CPO at AutoSalesVelocity\\nGenerative AI is likely to have a profound impact on society. This book gives an\\nintroduction to the field that is accessible without skimping on technical detail.\\n—Raza Habib, Cofounder of Humanloop\\nWhen people ask me how to get started with generative AI, I always recommend David’s\\nbook. The second edition is awesome because it covers the strongest models,\\nsuch as diffusion models and Transformers. Definitely a must-have for\\nanyone interested in computational creativity!\\n—Dr. Tristan Behrens, AI Expert and AI Music Artist in\\nResidence at KI Salon Heilbronn\\nDense in tech knowledge, this is my number one go-to literature when I have ideas\\naround generative AI. It should be on every data scientist’s bookshelf.\\n—Martin Musiol, Founder of generativeAI.net\\nThe book covers the full taxonomy of generative models in excellent detail. One of the\\nbest things I found about the book is that it covers the important theory behind the\\nmodels as well as solidifying the reader’s understanding with practical examples.\\nI must point out that the chapter on GANs is one of the best explanations I have read\\nand provides intuitive means to fine-tune your models. The book covers a\\nwide range of generative AI modalities including text, image, and music.\\nA great resource for anyone getting started with GenAI.\\n—Aishwarya Srinivasan, Data Scientist, Google Cloud'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 4}, page_content='David Foster\\nForeword by Karl Friston\\nGenerative Deep Learning\\nTeaching Machines to Paint, Write,\\nCompose, and Play\\nSECOND EDITION\\nBoston\\nFarnham\\nSebastopol\\nTokyo\\nBeijing\\nBoston\\nFarnham\\nSebastopol\\nTokyo\\nBeijing'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 5}, page_content='978-1-098-13418-1\\n[LSI]\\nGenerative Deep Learning\\nby David Foster\\nCopyright © 2023 Applied Data Science Partners Ltd. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com.\\nAcquisitions Editor: Nicole Butterfield\\nDevelopment Editor: Michele Cronin\\nProduction Editor: Christopher Faucher\\nCopyeditor: Charles Roumeliotis\\nProofreader: Rachel Head\\nIndexer: Judith McConville\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Kate Dullea\\nJuly 2019:\\n First Edition\\nMay 2023:\\n Second Edition\\nRevision History for the Second Edition\\n2023-04-28: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098134181 for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Generative Deep Learning, the cover\\nimage, and related trade dress are trademarks of O’Reilly Media, Inc.\\nThe views expressed in this work are those of the author and do not represent the publisher’s views. While\\nthe publisher and the author have used good faith efforts to ensure that the information and instructions\\ncontained in this work are accurate, the publisher and the author disclaim all responsibility for errors or\\nomissions, including without limitation responsibility for damages resulting from the use of or reliance\\non this work. Use of the information and instructions contained in this work is at your own risk. If any\\ncode samples or other technology this work contains or describes is subject to open source licenses or the\\nintellectual property rights of others, it is your responsibility to ensure that your use thereof complies\\nwith such licenses and/or rights.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 6}, page_content='For Alina, the loveliest noise vector of them all.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 7}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 8}, page_content='Table of Contents\\nForeword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xv\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xvii\\nPart I. \\nIntroduction to Generative Deep Learning\\n1. Generative Modeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\\nWhat Is Generative Modeling?                                                                                       4\\nGenerative Versus Discriminative Modeling                                                           5\\nThe Rise of Generative Modeling                                                                              6\\nGenerative Modeling and AI                                                                                      8\\nOur First Generative Model                                                                                           9\\nHello World!                                                                                                                  9\\nThe Generative Modeling Framework                                                                    10\\nRepresentation Learning                                                                                           12\\nCore Probability Theory                                                                                               15\\nGenerative Model Taxonomy                                                                                       18\\nThe Generative Deep Learning Codebase                                                                  20\\nCloning the Repository                                                                                             20\\nUsing Docker                                                                                                              21\\nRunning on a GPU                                                                                                     21\\nSummary                                                                                                                         21\\n2. Deep Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  23\\nData for Deep Learning                                                                                                24\\nDeep Neural Networks                                                                                                  25\\nvii'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 9}, page_content='What Is a Neural Network?                                                                                       25\\nLearning High-Level Features                                                                                  26\\nTensorFlow and Keras                                                                                               27\\nMultilayer Perceptron (MLP)                                                                                       28\\nPreparing the Data                                                                                                     28\\nBuilding the Model                                                                                                    30\\nCompiling the Model                                                                                                 35\\nTraining the Model                                                                                                    37\\nEvaluating the Model                                                                                                 38\\nConvolutional Neural Network (CNN)                                                                      40\\nConvolutional Layers                                                                                                 41\\nBatch Normalization                                                                                                  46\\nDropout                                                                                                                       49\\nBuilding the CNN                                                                                                      51\\nTraining and Evaluating the CNN                                                                           53\\nSummary                                                                                                                         54\\nPart II. \\nMethods\\n3. Variational Autoencoders. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  59\\nIntroduction                                                                                                                   60\\nAutoencoders                                                                                                                  61\\nThe Fashion-MNIST Dataset                                                                                   62\\nThe Autoencoder Architecture                                                                                63\\nThe Encoder                                                                                                                64\\nThe Decoder                                                                                                                65\\nJoining the Encoder to the Decoder                                                                        67\\nReconstructing Images                                                                                              69\\nVisualizing the Latent Space                                                                                     70\\nGenerating New Images                                                                                            71\\nVariational Autoencoders                                                                                             74\\nThe Encoder                                                                                                                75\\nThe Loss Function                                                                                                      80\\nTraining the Variational Autoencoder                                                                    82\\nAnalysis of the Variational Autoencoder                                                                84\\nExploring the Latent Space                                                                                           85\\nThe CelebA Dataset                                                                                                   85\\nTraining the Variational Autoencoder                                                                    87\\nAnalysis of the Variational Autoencoder                                                                89\\nGenerating New Faces                                                                                               90\\nviii \\n| \\nTable of Contents'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 10}, page_content='Latent Space Arithmetic                                                                                            91\\nMorphing Between Faces                                                                                          92\\nSummary                                                                                                                         93\\n4. Generative Adversarial Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  95\\nIntroduction                                                                                                                   96\\nDeep Convolutional GAN (DCGAN)                                                                        97\\nThe Bricks Dataset                                                                                                     98\\nThe Discriminator                                                                                                      99\\nThe Generator                                                                                                          101\\nTraining the DCGAN                                                                                              104\\nAnalysis of the DCGAN                                                                                          109\\nGAN Training: Tips and Tricks                                                                              110\\nWasserstein GAN with Gradient Penalty (WGAN-GP)                                        113\\nWasserstein Loss                                                                                                       114\\nThe Lipschitz Constraint                                                                                        115\\nEnforcing the Lipschitz Constraint                                                                       116\\nThe Gradient Penalty Loss                                                                                      117\\nTraining the WGAN-GP                                                                                         119\\nAnalysis of the WGAN-GP                                                                                     121\\nConditional GAN (CGAN)                                                                                        122\\nCGAN Architecture                                                                                                 123\\nTraining the CGAN                                                                                                 124\\nAnalysis of the CGAN                                                                                             126\\nSummary                                                                                                                       127\\n5. Autoregressive Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  129\\nIntroduction                                                                                                                 130\\nLong Short-Term Memory Network (LSTM)                                                         131\\nThe Recipes Dataset                                                                                                 132\\nWorking with Text Data                                                                                          133\\nTokenization                                                                                                             134\\nCreating the Training Set                                                                                        137\\nThe LSTM Architecture                                                                                          138\\nThe Embedding Layer                                                                                             138\\nThe LSTM Layer                                                                                                       140\\nThe LSTM Cell                                                                                                         142\\nTraining the LSTM                                                                                                   144\\nAnalysis of the LSTM                                                                                              146\\nRecurrent Neural Network (RNN) Extensions                                                       149\\nStacked Recurrent Networks                                                                                  149\\nTable of Contents \\n| \\nix'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 11}, page_content='Gated Recurrent Units                                                                                             151\\nBidirectional Cells                                                                                                    153\\nPixelCNN                                                                                                                      153\\nMasked Convolutional Layers                                                                                154\\nResidual Blocks                                                                                                         156\\nTraining the PixelCNN                                                                                           158\\nAnalysis of the PixelCNN                                                                                       159\\nMixture Distributions                                                                                             162\\nSummary                                                                                                                       164\\n6. Normalizing Flow Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  167\\nIntroduction                                                                                                                 168\\nNormalizing Flows                                                                                                      169\\nChange of Variables                                                                                                 170\\nThe Jacobian Determinant                                                                                      172\\nThe Change of Variables Equation                                                                        173\\nRealNVP                                                                                                                        174\\nThe Two Moons Dataset                                                                                         174\\nCoupling Layers                                                                                                       175\\nTraining the RealNVP Model                                                                                 181\\nAnalysis of the RealNVP Model                                                                            184\\nOther Normalizing Flow Models                                                                              186\\nGLOW                                                                                                                       186\\nFFJORD                                                                                                                     187\\nSummary                                                                                                                       188\\n7. Energy-Based Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  189\\nIntroduction                                                                                                                 189\\nEnergy-Based Models                                                                                                  191\\nThe MNIST Dataset                                                                                                 192\\nThe Energy Function                                                                                               193\\nSampling Using Langevin Dynamics                                                                    194\\nTraining with Contrastive Divergence                                                                  197\\nAnalysis of the Energy-Based Model                                                                    201\\nOther Energy-Based Models                                                                                  202\\nSummary                                                                                                                       203\\n8. Diffusion Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  205\\nIntroduction                                                                                                                 206\\nDenoising Diffusion Models (DDM)                                                                        208\\nThe Flowers Dataset                                                                                                208\\nx \\n| \\nTable of Contents'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 12}, page_content='The Forward Diffusion Process                                                                             209\\nThe Reparameterization Trick                                                                               210\\nDiffusion Schedules                                                                                                 211\\nThe Reverse Diffusion Process                                                                               214\\nThe U-Net Denoising Model                                                                                  217\\nTraining the Diffusion Model                                                                                224\\nSampling from the Denoising Diffusion Model                                                  225\\nAnalysis of the Diffusion Model                                                                            228\\nSummary                                                                                                                       231\\nPart III. \\nApplications\\n9. Transformers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  235\\nIntroduction                                                                                                                 236\\nGPT                                                                                                                                236\\nThe Wine Reviews Dataset                                                                                     237\\nAttention                                                                                                                   238\\nQueries, Keys, and Values                                                                                       239\\nMultihead Attention                                                                                                241\\nCausal Masking                                                                                                        242\\nThe Transformer Block                                                                                           245\\nPositional Encoding                                                                                                 248\\nTraining GPT                                                                                                            250\\nAnalysis of GPT                                                                                                        252\\nOther Transformers                                                                                                     255\\nT5                                                                                                                                256\\nGPT-3 and GPT-4                                                                                                    259\\nChatGPT                                                                                                                    260\\nSummary                                                                                                                       264\\n10. Advanced GANs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  267\\nIntroduction                                                                                                                 268\\nProGAN                                                                                                                        269\\nProgressive Training                                                                                                269\\nOutputs                                                                                                                      276\\nStyleGAN                                                                                                                      277\\nThe Mapping Network                                                                                            278\\nThe Synthesis Network                                                                                            279\\nOutputs from StyleGAN                                                                                         280\\nStyleGAN2                                                                                                                    281\\nTable of Contents \\n| \\nxi'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 13}, page_content='Weight Modulation and Demodulation                                                                282\\nPath Length Regularization                                                                                    283\\nNo Progressive Growing                                                                                         284\\nOutputs from StyleGAN2                                                                                       286\\nOther Important GANs                                                                                              286\\nSelf-Attention GAN (SAGAN)                                                                               286\\nBigGAN                                                                                                                     288\\nVQ-GAN                                                                                                                   289\\nViT VQ-GAN                                                                                                           292\\nSummary                                                                                                                       294\\n11. Music Generation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  297\\nIntroduction                                                                                                                 298\\nTransformers for Music Generation                                                                         299\\nThe Bach Cello Suite Dataset                                                                                 300\\nParsing MIDI Files                                                                                                   300\\nTokenization                                                                                                             303\\nCreating the Training Set                                                                                        304\\nSine Position Encoding                                                                                           305\\nMultiple Inputs and Outputs                                                                                  307\\nAnalysis of the Music-Generating Transformer                                                  309\\nTokenization of Polyphonic Music                                                                        313\\nMuseGAN                                                                                                                     317\\nThe Bach Chorale Dataset                                                                                      317\\nThe MuseGAN Generator                                                                                      320\\nThe MuseGAN Critic                                                                                              326\\nAnalysis of the MuseGAN                                                                                      327\\nSummary                                                                                                                       329\\n12. World Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  331\\nIntroduction                                                                                                                 331\\nReinforcement Learning                                                                                             332\\nThe CarRacing Environment                                                                                 334\\nWorld Model Overview                                                                                              336\\nArchitecture                                                                                                              336\\nTraining                                                                                                                     338\\nCollecting Random Rollout Data                                                                              339\\nTraining the VAE                                                                                                         340\\nThe VAE Architecture                                                                                             341\\nExploring the VAE                                                                                                   343\\nCollecting Data to Train the MDN-RNN                                                                 346\\nxii \\n| \\nTable of Contents'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 14}, page_content='Training the MDN-RNN                                                                                            346\\nThe MDN-RNN Architecture                                                                                347\\nSampling from the MDN-RNN                                                                             348\\nTraining the Controller                                                                                               348\\nThe Controller Architecture                                                                                   349\\nCMA-ES                                                                                                                     349\\nParallelizing CMA-ES                                                                                              351\\nIn-Dream Training                                                                                                      353\\nSummary                                                                                                                       356\\n13. Multimodal Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  359\\nIntroduction                                                                                                                 360\\nDALL.E 2                                                                                                                       361\\nArchitecture                                                                                                              362\\nThe Text Encoder                                                                                                     362\\nCLIP                                                                                                                           362\\nThe Prior                                                                                                                   367\\nThe Decoder                                                                                                             369\\nExamples from DALL.E 2                                                                                       373\\nImagen                                                                                                                           377\\nArchitecture                                                                                                              377\\nDrawBench                                                                                                               378\\nExamples from Imagen                                                                                           379\\nStable Diffusion                                                                                                            380\\nArchitecture                                                                                                              380\\nExamples from Stable Diffusion                                                                            381\\nFlamingo                                                                                                                       381\\nArchitecture                                                                                                              382\\nThe Vision Encoder                                                                                                 382\\nThe Perceiver Resampler                                                                                         383\\nThe Language Model                                                                                               385\\nExamples from Flamingo                                                                                        388\\nSummary                                                                                                                       389\\n14. Conclusion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  391\\nTimeline of Generative AI                                                                                          392\\n2014–2017: The VAE and GAN Era                                                                      394\\n2018–2019: The Transformer Era                                                                          394\\n2020–2022: The Big Model Era                                                                              395\\nThe Current State of Generative AI                                                                          396\\nLarge Language Models                                                                                           396\\nTable of Contents \\n| \\nxiii'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 15}, page_content='Text-to-Code Models                                                                                               400\\nText-to-Image Models                                                                                             402\\nOther Applications                                                                                                   405\\nThe Future of Generative AI                                                                                      407\\nGenerative AI in Everyday Life                                                                              407\\nGenerative AI in the Workplace                                                                            409\\nGenerative AI in Education                                                                                    410\\nGenerative AI Ethics and Challenges                                                                    411\\nFinal Thoughts                                                                                                             413\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  417\\nxiv \\n| \\nTable of Contents'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 16}, page_content='Foreword\\nThis book is becoming part of my life. On finding a copy in my living room I asked\\nmy son, “When did you get this?” He replied, “When you gave it to me,” bemused by\\nmy senior moment. Going through various sections together, I came to regard Gener‐\\native Deep Learning as the Gray’s Anatomy of Generative AI.\\nThe author dissects the anatomy of generative AI with an incredible clarity and reas‐\\nsuring authority. He offers a truly remarkable account of a fast-moving field, under‐\\nwritten with pragmatic examples, engaging narratives, and references that are so\\ncurrent, it reads like a living history.\\nThroughout his deconstructions, the author maintains a sense of wonder and excite‐\\nment about the potential of generative AI—especially evident in the book’s compel‐\\nling dénouement. Having laid bare the technology, he reminds us that we are at the\\ndawn of a new age of intelligence, an age in which generative AI holds a mirror up to\\nour language, our art, our creativity; reflecting not just what we have created, but\\nwhat we could create—what we can create—limited only by “your own imagination.”\\nThe central theme of generative models in artificial intelligence resonates deeply with\\nme, because I see exactly the same themes emerging in the natural sciences; namely, a\\nview of ourselves as generative models of our lived world. I suspect in the next edition\\nof this book we will read about the confluence of artificial and natural intelligence.\\nUntil that time, I will keep this edition next to my copy of Gray’s Anatomy, and other\\ntreasures on my bookshelf.\\n— Karl Friston, FRS\\nProfessor of Neuroscience\\nUniversity College London\\nxv'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 17}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 18}, page_content='Preface\\nWhat I cannot create, I do not understand.\\n—Richard Feynman\\nGenerative AI is one of the most revolutionary technologies of our time, transform‐\\ning the way we interact with machines. Its potential to revolutionize the way we live,\\nwork, and play has been the subject of countless conversations, debates, and predic‐\\ntions. But what if there was an even greater potential to this powerful technology?\\nWhat if the possibilities of generative AI extend beyond our current imagination?\\nThe future of generative AI may be more exciting than we ever thought possible…\\nSince our earliest days, we have sought opportunities to generate original and beauti‐\\nful creations. For early humans, this took the form of cave paintings depicting wild\\nanimals and abstract patterns, created with pigments placed carefully and methodi‐\\ncally onto rock. The Romantic Era gave us the mastery of Tchaikovsky symphonies,\\nwith their ability to inspire feelings of triumph and tragedy through sound waves,\\nwoven together to form beautiful melodies and harmonies. And in recent times, we\\nhave found ourselves rushing to bookshops at midnight to buy stories about a fic‐\\ntional wizard, because the combination of letters creates a narrative that wills us to\\nturn the page and find out what happens to our hero.\\nIt is therefore not surprising that humanity has started to ask the ultimate question of\\ncreativity: can we create something that is in itself creative?\\nThis is the question that generative AI aims to answer. With recent advances in meth‐\\nodology and technology, we are now able to build machines that can paint original\\nartwork in a given style, write coherent blocks of text with long-term structure, com‐\\npose music that is pleasant to listen to, and develop winning strategies for complex\\ngames by generating imaginary future scenarios. This is just the start of a generative\\nrevolution that will leave us with no choice but to find answers to some of the biggest\\nquestions about the mechanics of creativity, and ultimately, what it means to be\\nhuman.\\nxvii'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 19}, page_content='In short, there has never been a better time to learn about generative AI—so let’s get\\nstarted!\\nObjective and Approach\\nThis book assumes no prior knowledge of generative AI. We will build up all of the\\nkey concepts from scratch in a way that is intuitive and easy to follow, so don’t worry\\nif you have no experience with generative AI. You have come to the right place!\\nRather than only covering the techniques that are currently in vogue, this book serves\\nas a complete guide to generative modeling that covers a broad range of model fami‐\\nlies. There is no one technique that is objectively better or worse than any other—in\\nfact, many state-of-the-art models now mix together ideas from across the broad\\nspectrum of approaches to generative modeling. For this reason, it is important to\\nkeep abreast of developments across all areas of generative AI, rather than focusing\\non one particular kind of technique. One thing is certain: the field of generative AI is\\nmoving fast, and you never know where the next groundbreaking idea will come\\nfrom!\\nWith this in mind, the approach I will take is to show you how to train your own\\ngenerative models on your own data, rather than relying on pre-trained off-the-shelf\\nmodels. While there are now many impressive open source generative models that\\ncan be downloaded and run in a few lines of code, the aim of this book is to dig\\ndeeper into their architecture and design from first principles, so that you gain a\\ncomplete understanding of how they work and can code up examples of each techni‐\\nque from scratch using Python and Keras.\\nIn summary, this book can be thought of as a map of the current generative AI land‐\\nscape that covers both theory and practical applications, including full working\\nexamples of key models from the literature. We will walk through the code for each\\nstep by step, with clear signposts that show how the code implements the theory\\nunderpinning each technique. This book can be read cover to cover or used as a ref‐\\nerence book that you can dip into. Above all, I hope you find it a useful and enjoyable\\nread!\\nThroughout the book, you will find short, allegorical stories that\\nhelp explain the mechanics of some of the models we will be build‐\\ning. I believe that one of the best ways to teach a new abstract\\ntheory is to first convert it into something that isn’t quite so\\nabstract, such as a story, before diving into the technical explana‐\\ntion. The story and the model explanation are just the same\\nmechanics explained in two different domains—you might there‐\\nfore find it useful to refer back to the relevant story while learning\\nabout the technical details of each model!\\nxviii \\n| \\nPreface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 20}, page_content='Prerequisites\\nThis book assumes that you have experience coding in Python. If you are not familiar\\nwith Python, the best place to start is through LearnPython.org. There are many free\\nresources online that will allow you to develop enough Python knowledge to work\\nwith the examples in this book.\\nAlso, since some of the models are described using mathematical notation, it will be\\nuseful to have a solid understanding of linear algebra (for example, matrix multiplica‐\\ntion) and general probability theory. A useful resource is Deisenroth et al.’s book\\nMathematics for Machine Learning (Cambridge University Press), which is freely\\navailable.\\nThe book assumes no prior knowledge of generative modeling (we will examine the\\nkey concepts in Chapter 1) or TensorFlow and Keras (these libraries will be intro‐\\nduced in Chapter 2).\\nRoadmap\\nThis book is divided into three parts.\\nPart I is a general introduction to generative modeling and deep learning, where we\\nexplore the core concepts that underpin all of the techniques in later parts of the\\nbook:\\n• In Chapter 1, “Generative Modeling”, we define generative modeling and con‐\\nsider a toy example that we can use to understand some of the key concepts that\\nare important to all generative models. We also lay out the taxonomy of genera‐\\ntive model families that we will explore in Part II of this book.\\n• In Chapter 2, “Deep Learning”, we begin our exploration of deep learning and\\nneural networks by building our first example of a multilayer perceptron (MLP)\\nusing Keras. We then adapt this to include convolutional layers and other\\nimprovements, to observe the difference in performance.\\nPart II walks through the six key techniques that we will be using to build generative\\nmodels, with practical examples for each:\\n• In Chapter 3, “Variational Autoencoders”, we consider the variational autoen‐\\ncoder (VAE) and see how it can be used to generate images of faces and morph\\nbetween faces in the model’s latent space.\\n• In Chapter 4, “Generative Adversarial Networks”, we explore generative adversa‐\\nrial networks (GANs) for image generation, including deep convolutional GANs,\\nconditional GANs, and improvements such as the Wasserstein GAN that make\\nthe training process more stable.\\nPreface \\n| \\nxix'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 21}, page_content='• In Chapter 5, “Autoregressive Models”, we turn our attention to autoregressive\\nmodels, starting with an introduction to recurrent neural networks such as long\\nshort-term memory networks (LSTMs) for text generation and PixelCNN for\\nimage generation.\\n• In Chapter 6, “Normalizing Flow Models”, we focus on normalizing flows,\\nincluding an intuitive theoretical exploration of the technique and a practical\\nexample of how to build a RealNVP model to generate images.\\n• In Chapter 7, “Energy-Based Models”, we cover energy-based models, including\\nimportant methods such as how to train using contrastive divergence and sample\\nusing Langevin dynamics.\\n• In Chapter 8, “Diffusion Models”, we dive into a practical guide to building diffu‐\\nsion models, which drive many state-of-the-art image generation models such as\\nDALL.E 2 and Stable Diffusion.\\nFinally, in Part III we build on these foundations to explore the inner workings of\\nstate-of-the-art models for image generation, writing, composing music, and model-\\nbased reinforcement learning:\\n• In Chapter 9, “Transformers”, we explore the lineage and technical details of the\\nStyleGAN models, as well as other state-of-the-art GANs for image generation\\nsuch as VQ-GAN.\\n• In Chapter 10, “Advanced GANs”, we consider the Transformer architecture,\\nincluding a practical walkthrough for building your own version of GPT for text\\ngeneration.\\n• In Chapter 11, “Music Generation”, we turn our attention to music generation,\\nincluding a guide to working with music data and application of techniques such\\nas Transformers and MuseGAN.\\n• In Chapter 12, “World Models”, we see how generative models can be used in the\\ncontext of reinforcement learning, with the application of world models and\\nTransformer-based methods.\\n• In Chapter 13, “Multimodal Models”, we explain the inner workings of four\\nstate-of-the-art multimodal models that incorporate more than one type of data,\\nincluding DALL.E 2, Imagen, and Stable Diffusion for text-to-image generation\\nand Flamingo, a visual language model.\\n• In Chapter 14, “Conclusion”, we recap the key milestones of generative AI to date\\nand discuss the ways in which generative AI will revolutionize our daily lives in\\nyears to come.\\nxx \\n| \\nPreface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 22}, page_content='Changes in the Second Edition\\nThank you to everyone who read the first edition of this book—I am really pleased\\nthat so many of you have found it a useful resource and provided feedback on things\\nthat you would like to see in the second edition. The field of generative deep learning\\nhas progressed significantly since the first edition was published in 2019, so as well as\\nrefreshing the existing content I have added several new chapters to bring the mate‐\\nrial in line with the current state of the art.\\nThe following is a summary of the main updates, in terms of the individual chapters\\nand general book improvements:\\n• Chapter 1 now includes a section on the different families of generative models\\nand a taxonomy of how they are related.\\n• Chapter 2 contains improved diagrams and more detailed explanations of key\\nconcepts.\\n• Chapter 3 is refreshed with a new worked example and accompanying\\nexplanations.\\n• Chapter 4 now includes an explanation of conditional GAN architectures.\\n• Chapter 5 now includes a section on autoregressive models for images (e.g.,\\nPixelCNN).\\n• Chapter 6 is an entirely new chapter, describing the RealNVP model.\\n• Chapter 7 is also a new chapter, focusing on techniques such as Langevin dynam‐\\nics and contrastive divergence.\\n• Chapter 8 is a newly written chapter on denoising the diffusion models that\\npower many of today’s state-of-the-art applications.\\n• Chapter 9 is an expansion of the material provided in the conclusion of the first\\nedition, with deeper focus on architectures of the various StyleGAN models and\\nnew material on VQ-GAN.\\n• Chapter 10 is a new chapter that explores the Transformer architecture in detail.\\n• Chapter 11 includes modern Transformer architectures, replacing the LSTM\\nmodels from the first edition.\\n• Chapter 12 includes updated diagrams and descriptions, with a section on how\\nthis approach is informing state-of-the-art reinforcement learning today.\\n• Chapter 13 is a new chapter that explains in detail how impressive models like\\nDALL.E 2, Imagen, Stable Diffusion, and Flamingo work.\\n• Chapter 14 is updated to reflect the outstanding progress in the field since the\\nfirst edition and give a more complete and detailed view of where generative AI is\\nheading in the future.\\nPreface \\n| \\nxxi'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 23}, page_content='• All comments given as feedback to the first edition and typos identified have\\nbeen addressed (to the best of my knowledge!).\\n• Chapter goals have been added at the start of each chapter, so that you can see the\\nkey topics covered in the chapter before you start reading.\\n• Some of the allegorical stories have been rewritten to be more concise and\\nclear—I am pleased that so many readers have said that the stories have helped\\nthem to better understand the key concepts!\\n• The headings and subheadings of each chapter have been aligned so that is it\\nclear which parts of the chapter are focused on explanation and which are\\nfocused on building your own models.\\nOther Resources\\nI highly recommend the following books as general introductions to machine learn‐\\ning and deep learning:\\n• Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts,\\nTools, and Techniques to Build Intelligent Systems by Aurélien Géron (O’Reilly)\\n• Deep Learning with Python by Francois Chollet (Manning)\\nMost of the papers in this book are sourced through arXiv, a free repository of scien‐\\ntific research papers. It is now common for authors to post papers to arXiv before\\nthey are fully peer-reviewed. Reviewing the recent submissions is a great way to keep\\non top of the most cutting-edge developments in the field.\\nI also highly recommend the website Papers with Code, where you can find the latest\\nstate-of-the-art results in a variety of machine learning tasks, alongside links to the\\npapers and official GitHub repositories. It is an excellent resource for anyone wanting\\nto quickly understand which techniques are currently achieving the highest scores in\\na range of tasks and has certainly helped me to decide which techniques to include in\\nthis book.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for commands and program listings, as well as within paragraphs to refer to\\nprogram elements such as variable or function names.\\nxxii \\n| \\nPreface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 24}, page_content='Constant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nThis element signifies a tip or suggestion.\\nThis element signifies a general note.\\nThis element signifies a warning or caution.\\nCodebase\\nThe code examples in this book can be found in a GitHub repository. I have deliber‐\\nately ensured that none of the models require prohibitively large amounts of compu‐\\ntational resources to train, so that you can start training your own models without\\nhaving to spend lots of time or money on expensive hardware. There is a comprehen‐\\nsive guide in the repository on how to get started with Docker and set up cloud\\nresources with GPUs on Google Cloud if required.\\nThe following changes have been made to the codebase since the first edition:\\n• All examples are now runnable from within a single notebook, instead of some\\ncode being imported from modules across the codebase. This is so that you can\\nrun each example cell by cell and delve into exactly how each model is built, piece\\nby piece.\\n• The sections of each notebook are now broadly aligned between examples.\\n• Many of the examples in this book now utilize code snippets from the amazing\\nopen source Keras repository—this is to avoid creating a completely detached\\nopen source repository of Keras generative AI examples, when there already exist\\nexcellent implementations available through the Keras website. I have added ref‐\\nerences and links to the original authors of code that I have utilized from the\\nKeras website throughout this book and in the repository.\\nPreface \\n| \\nxxiii'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 25}, page_content='• I have added new data sources and improved the data collection process from the\\nfirst edition—now, there is a script that can be easily run to collect data from the\\nrequired sources in order to train the examples in the book, using tools such as\\nthe Kaggle API.\\nUsing Code Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition.\\nIf you have a technical question or a problem using the code examples, please send\\nemail to bookquestions@oreilly.com.\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. You do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing examples from O’Reilly\\nbooks does require permission. Answering a question by citing this book and quoting\\nexample code does not require permission. Incorporating a significant amount of\\nexample code from this book into your product’s documentation does require\\npermission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “Generative Deep Learning, 2nd edi‐\\ntion, by David Foster (O’Reilly). Copyright 2023 Applied Data Science Partners Ltd.,\\n978-1-098-13418-1.”\\nIf you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at permissions@oreilly.com.\\nO’Reilly Online Learning\\nFor more than 40 years, O’Reilly Media has provided technol‐\\nogy and business training, knowledge, and insight to help\\ncompanies succeed.\\nOur unique network of experts and innovators share their knowledge and expertise\\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\\nplatform gives you on-demand access to live training courses, in-depth learning\\npaths, interactive coding environments, and a vast collection of text and video from\\nO’Reilly and 200+ other publishers. For more information, visit https://oreilly.com.\\nxxiv \\n| \\nPreface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 26}, page_content='How to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. You can access this page at https://oreil.ly/generative-dl.\\nEmail bookquestions@oreilly.com to comment or ask technical questions about this\\nbook.\\nFor news and information about our books and courses, visit https://oreilly.com.\\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media\\nFollow us on Twitter: https://twitter.com/oreillymedia\\nWatch us on YouTube: https://youtube.com/oreillymedia\\nAcknowledgments\\nThere are so many people I would like to thank for helping me write this book.\\nFirst, I would like to thank everyone who has taken time to technically review the\\nbook—in particular Vishwesh Ravi Shrimali, Lipi Deepaakshi Patnaik, Luba Elliot,\\nand Lorna Barclay. Thanks also to Samir Bico for helping to review and test the code‐\\nbase that accompanies this book. Your input has been invaluable.\\nAlso, a huge thanks to my colleagues at Applied Data Science Partners, Ross Witeszc‐\\nzak, Amy Bull, Ali Parandeh, Zine Eddine, Joe Rowe, Gerta Salillari, Aleshia Parkes,\\nEvelina Kireilyte, Riccardo Tolli, Mai Do, Khaleel Syed, and Will Holmes. Your\\npatience with me while I have taken time to finish the book is hugely appreciated, and\\nI am greatly looking forward to all the machine learning projects we will complete\\ntogether in the future! Particular thanks to Ross—had we not decided to start a busi‐\\nness together, this book might never have taken shape, so thank you for believing in\\nme as your business partner!\\nI also want to thank anyone who has ever taught me anything mathematical—I was\\nextremely fortunate to have fantastic math teachers at school, who developed my\\ninterest in the subject and encouraged me to pursue it further at university. I would\\nPreface \\n| \\nxxv'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 27}, page_content='like to thank you for your commitment and for going out of your way to share your\\nknowledge of the subject with me.\\nA huge thank you goes to the staff at O’Reilly for guiding me through the process of\\nwriting this book. A special thanks goes to Michele Cronin, who has been there at\\neach step, providing useful feedback and sending me friendly reminders to keep com‐\\npleting chapters! Also to Nicole Butterfield, Christopher Faucher, Charles Roumelio‐\\ntis, and Suzanne Huston for getting the book into production, and Mike Loukides for\\nfirst reaching out to ask if I’d be interested in writing a book. You have all been so\\nsupportive of this project from the start, and I want to thank you for providing me\\nwith a platform on which to write about something that I love.\\nThroughout the writing process, my family has been a constant source of encourage‐\\nment and support. A huge thank you goes to my mum, Gillian Foster, for checking\\nevery single line of text for typos and for teaching me how to add up in the first place!\\nYour attention to detail has been extremely helpful while proofreading this book, and\\nI’m really grateful for all the opportunities that both you and dad have given me. My\\ndad, Clive Foster, originally taught me how to program a computer—this book is full\\nof practical examples, and that’s thanks to his early patience while I fumbled around\\nin BASIC trying to make football games as a teenager. My brother, Rob Foster, is the\\nmost modest genius you will ever find, particularly within linguistics—chatting with\\nhim about AI and the future of text-based machine learning has been amazingly\\nhelpful. Last, I would like to thank my Nana, who was always a constant source of\\ninspiration and fun for all of us. Her love of literature was one of the reasons I first\\ndecided that writing a book would be an exciting thing to do.\\nI would also like to thank my wife, Lorna Barclay. As well as providing me with end‐\\nless support and cups of tea throughout the writing process, you have rigorously\\nchecked every word of this book in meticulous detail. I couldn’t have done it without\\nyou. Thank you for always being there for me, and for making this journey so much\\nmore enjoyable. I promise I won’t talk about generative AI at the dinner table for at\\nleast a few days after the book is published.\\nLastly, I would like to thank our beautiful baby daughter Alina for providing endless\\nentertainment during the long nights of book-writing. Your adorable giggles have\\nbeen the perfect background music to my typing. Thanks for being my inspiration\\nand for always keeping me on my toes. You’re the real brains behind this operation.\\nxxvi \\n| \\nPreface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 28}, page_content='PART I\\nIntroduction to Generative\\nDeep Learning\\nPart I is a general introduction to generative modeling and deep learning—the two\\nfields that we need to understand in order to get started with generative deep\\nlearning!\\nIn Chapter 1 we will define generative modeling and consider a toy example that we\\ncan use to understand some of the key concepts that are important to all generative\\nmodels. We will also lay out the taxonomy of generative model families that we will\\nexplore in Part II of this book.\\nChapter 2 provides a guide to the deep learning tools and techniques that we will\\nneed to start building more complex generative models. In particular, we will build\\nour first example of a deep neural network—a multilayer perceptron (MLP)—using\\nKeras. We will then adapt this to include convolutional layers and other improve‐\\nments, to observe the difference in performance.\\nBy the end of Part I you will have a good understanding of the core concepts that\\nunderpin all of the techniques in later parts of the book.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 29}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 30}, page_content='CHAPTER 1\\nGenerative Modeling\\nChapter Goals\\nIn this chapter you will:\\n• Learn the key differences between generative and discriminative models.\\n• Understand the desirable properties of a generative model through a simple\\nexample.\\n• Learn about the core probabilistic concepts that underpin generative models.\\n• Explore the different families of generative models.\\n• Clone the codebase that accompanies this book, so that you can get started build‐\\ning generative models!\\nThis chapter is a general introduction to the field of generative modeling.\\nWe will start with a gentle theoretical introduction to generative modeling and see\\nhow it is the natural counterpart to the more widely studied discriminative modeling.\\nWe will then establish a framework that describes the desirable properties that a good\\ngenerative model should have. We will also lay out the core probabilistic concepts\\nthat are important to know, in order to fully appreciate how different approaches\\ntackle the challenge of generative modeling.\\nThis will lead us naturally to the penultimate section, which lays out the six broad\\nfamilies of generative models that dominate the field today. The final section explains\\nhow to get started with the codebase that accompanies this book.\\n3'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 31}, page_content='What Is Generative Modeling?\\nGenerative modeling can be broadly defined as follows:\\nGenerative modeling is a branch of machine learning that involves training a model to\\nproduce new data that is similar to a given dataset.\\nWhat does this mean in practice? Suppose we have a dataset containing photos of\\nhorses. We can train a generative model on this dataset to capture the rules that gov‐\\nern the complex relationships between pixels in images of horses. Then we can sam‐\\nple from this model to create novel, realistic images of horses that did not exist in the\\noriginal dataset. This process is illustrated in Figure 1-1.\\nFigure 1-1. A generative model trained to generate realistic photos of horses\\nIn order to build a generative model, we require a dataset consisting of many exam‐\\nples of the entity we are trying to generate. This is known as the training data, and\\none such data point is called an observation.\\nEach observation consists of many features. For an image generation problem, the\\nfeatures are usually the individual pixel values; for a text generation problem, the fea‐\\ntures could be individual words or groups of letters. It is our goal to build a model\\nthat can generate new sets of features that look as if they have been created using the\\nsame rules as the original data. Conceptually, for image generation this is an incredi‐\\nbly difficult task, considering the vast number of ways that individual pixel values can\\nbe assigned and the relatively tiny number of such arrangements that constitute an\\nimage of the entity we are trying to generate.\\nA generative model must also be probabilistic rather than deterministic, because we\\nwant to be able to sample many different variations of the output, rather than get the\\nsame output every time. If our model is merely a fixed calculation, such as taking the\\naverage value of each pixel in the training dataset, it is not generative. A generative\\n4 \\n| \\nChapter 1: Generative Modeling'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 32}, page_content='model must include a random component that influences the individual samples gen‐\\nerated by the model.\\nIn other words, we can imagine that there is some unknown probabilistic distribution\\nthat explains why some images are likely to be found in the training dataset and other\\nimages are not. It is our job to build a model that mimics this distribution as closely\\nas possible and then sample from it to generate new, distinct observations that look as\\nif they could have been included in the original training set.\\nGenerative Versus Discriminative Modeling\\nIn order to truly understand what generative modeling aims to achieve and why this\\nis important, it is useful to compare it to its counterpart, discriminative modeling. If\\nyou have studied machine learning, most problems you will have faced will have most\\nlikely been discriminative in nature. To understand the difference, let’s look at an\\nexample.\\nSuppose we have a dataset of paintings, some painted by Van Gogh and some by\\nother artists. With enough data, we could train a discriminative model to predict if a\\ngiven painting was painted by Van Gogh. Our model would learn that certain colors,\\nshapes, and textures are more likely to indicate that a painting is by the Dutch master,\\nand for paintings with these features, the model would upweight its prediction\\naccordingly. Figure 1-2 shows the discriminative modeling process—note how it dif‐\\nfers from the generative modeling process shown in Figure 1-1.\\nFigure 1-2. A discriminative model trained to predict if a given image is painted by\\nVan Gogh\\nWhen performing discriminative modeling, each observation in the training data has\\na label. For a binary classification problem such as our artist discriminator, Van Gogh\\npaintings would be labeled 1 and non–Van Gogh paintings labeled 0. Our model then\\nWhat Is Generative Modeling? \\n| \\n5'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 33}, page_content='learns how to discriminate between these two groups and outputs the probability that\\na new observation has label 1—i.e., that it was painted by Van Gogh.\\nIn contrast, generative modeling doesn’t require the dataset to be labeled because it\\nconcerns itself with generating entirely new images, rather than trying to predict a\\nlabel of a given image.\\nLet’s define these types of modeling formally, using mathematical notation:\\nDiscriminative modeling estimates p y �.\\nThat is, discriminative modeling aims to model the probability of a label y given some\\nobservation �.\\nGenerative modeling estimates p �.\\nThat is, generative modeling aims to model the probability of observing an observa‐\\ntion �. Sampling from this distribution allows us to generate new observations.\\nConditional Generative Models\\nNote that we can also build a generative model to model the condi‐\\ntional probability p �y —the probability of seeing an observation\\n� with a specific label y.\\nFor example, if our dataset contains different types of fruit, we\\ncould tell our generative model to specifically generate an image of\\nan apple.\\nAn important point to note is that even if we were able to build a perfect discrimina‐\\ntive model to identify Van Gogh paintings, it would still have no idea how to create a\\npainting that looks like a Van Gogh. It can only output probabilities against existing\\nimages, as this is what it has been trained to do. We would instead need to train a\\ngenerative model and sample from this model to generate images that have a high\\nchance of belonging to the original training dataset.\\nThe Rise of Generative Modeling\\nUntil recently, discriminative modeling has been the driving force behind most pro‐\\ngress in machine learning. This is because for any discriminative problem, the corre‐\\nsponding generative modeling problem is typically much more difficult to tackle. For\\nexample, it is much easier to train a model to predict if a painting is by Van Gogh\\nthan it is to train a model to generate a Van Gogh–style painting from scratch.\\n6 \\n| \\nChapter 1: Generative Modeling'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 34}, page_content='Similarly, it is much easier to train a model to predict if a page of text was written by\\nCharles Dickens than it is to build a model to generate a set of paragraphs in the style\\nof Dickens. Until recently, most generative challenges were simply out of reach and\\nmany doubted that they could ever be solved. Creativity was considered a purely\\nhuman capability that couldn’t be rivaled by AI.\\nHowever, as machine learning technologies have matured, this assumption has grad‐\\nually weakened. In the last 10 years many of the most interesting advancements in the\\nfield have come through novel applications of machine learning to generative model‐\\ning tasks. For example, Figure 1-3 shows the striking progress that has already been\\nmade in facial image generation since 2014.\\nFigure 1-3. Face generation using generative modeling has improved significantly over\\nthe last decade (adapted from Brundage et al., 2018)1\\nAs well as being easier to tackle, discriminative modeling has historically been more\\nreadily applicable to practical problems across industry than generative modeling.\\nFor example, a doctor may benefit from a model that predicts if a given retinal image\\nshows signs of glaucoma, but wouldn’t necessarily benefit from a model that can gen‐\\nerate novel pictures of the back of an eye.\\nHowever, this is also starting to change, with the proliferation of companies offering\\ngenerative services that target specific business problems. For example, it is now pos‐\\nsible to access APIs that generate original blog posts given a particular subject matter,\\nproduce a variety of images of your product in any setting you desire, or write social\\nmedia content and ad copy to match your brand and target message. There are also\\nclear positive applications of generative AI for industries such as game design and\\ncinematography, where models trained to output video and music are beginning to\\nadd value.\\nWhat Is Generative Modeling? \\n| \\n7'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 35}, page_content='Generative Modeling and AI\\nAs well as the practical uses of generative modeling (many of which are yet to be dis‐\\ncovered), there are three deeper reasons why generative modeling can be considered\\nthe key to unlocking a far more sophisticated form of artificial intelligence that goes\\nbeyond what discriminative modeling alone can achieve.\\nFirstly, purely from a theoretical point of view, we shouldn’t limit our machine train‐\\ning to simply categorizing data. For completeness, we should also be concerned with\\ntraining models that capture a more complete understanding of the data distribution,\\nbeyond any particular label. This is undoubtedly a more difficult problem to solve,\\ndue to the high dimensionality of the space of feasible outputs and the relatively small\\nnumber of creations that we would class as belonging to the dataset. However, as we\\nshall see, many of the same techniques that have driven development in discrimina‐\\ntive modeling, such as deep learning, can be utilized by generative models too.\\nSecondly, as we shall see in Chapter 12, generative modeling is now being used to\\ndrive progress in other fields of AI, such as reinforcement learning (the study of\\nteaching agents to optimize a goal in an environment through trial and error). Sup‐\\npose we want to train a robot to walk across a given terrain. A traditional approach\\nwould be to run many experiments where the agent tries out different strategies in the\\nterrain, or a computer simulation of the terrain. Over time the agent would learn\\nwhich strategies are more successful than others and therefore gradually improve. A\\nchallenge with this approach is that it is fairly inflexible because it is trained to opti‐\\nmize the policy for one particular task. An alternative approach that has recently\\ngained traction is to instead train the agent to learn a world model of the environment\\nusing a generative model, independent of any particular task. The agent can quickly\\nadapt to new tasks by testing strategies in its own world model, rather than in the real\\nenvironment, which is often computationally more efficient and does not require\\nretraining from scratch for each new task.\\nFinally, if we are to truly say that we have built a machine that has acquired a form of\\nintelligence that is comparable to a human’s, generative modeling must surely be part\\nof the solution. One of the finest examples of a generative model in the natural world\\nis the person reading this book. Take a moment to consider what an incredible gener‐\\native model you are. You can close your eyes and imagine what an elephant would\\nlook like from any possible angle. You can imagine a number of plausible different\\nendings to your favorite TV show, and you can plan your week ahead by working\\nthrough various futures in your mind’s eye and taking action accordingly. Current\\nneuroscientific theory suggests that our perception of reality is not a highly complex\\ndiscriminative model operating on our sensory input to produce predictions of what\\nwe are experiencing, but is instead a generative model that is trained from birth to\\nproduce simulations of our surroundings that accurately match the future. Some the‐\\nories even suggest that the output from this generative model is what we directly\\n8 \\n| \\nChapter 1: Generative Modeling'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 36}, page_content='perceive as reality. Clearly, a deep understanding of how we can build machines to\\nacquire this ability will be central to our continued understanding of the workings of\\nthe brain and general artificial intelligence.\\nOur First Generative Model\\nWith this in mind, let’s begin our journey into the exciting world of generative model‐\\ning. To begin with, we’ll look at a toy example of a generative model and introduce\\nsome of the ideas that will help us to work through the more complex architectures\\nthat we will encounter later in the book.\\nHello World!\\nLet’s start by playing a generative modeling game in just two dimensions. I have\\nchosen a rule that has been used to generate the set of points � in Figure 1-4. Let’s call\\nthis rule pdata. Your challenge is to choose a different point �= x1, x2  in the space\\nthat looks like it has been generated by the same rule.\\nFigure 1-4. A set of points in two dimensions, generated by an unknown rule pdata\\nWhere did you choose? You probably used your knowledge of the existing data points\\nto construct a mental model, pmodel, of whereabouts in the space the point is more\\nlikely to be found. In this respect, pmodel is an estimate of pdata. Perhaps you decided\\nthat pmodel should look like Figure 1-5—a rectangular box where points may be\\nfound, and an area outside of the box where there is no chance of finding any points.\\nOur First Generative Model \\n| \\n9'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 37}, page_content='Figure 1-5. The orange box, pmodel, is an estimate of the true data-generating distribu‐\\ntion, pdata\\nTo generate a new observation, you can simply choose a point at random within the\\nbox, or more formally, sample from the distribution pmodel. Congratulations, you have\\njust built your first generative model! You have used the training data (the black\\npoints) to construct a model (the orange region) that you can easily sample from to\\ngenerate other points that appear to belong to the training set.\\nLet’s now formalize this thinking into a framework that can help us understand what\\ngenerative modeling is trying to achieve.\\nThe Generative Modeling Framework\\nWe can capture our motivations and goals for building a generative model in the fol‐\\nlowing framework.\\nThe Generative Modeling Framework\\n• We have a dataset of observations �.\\n• We assume that the observations have been generated according to some\\nunknown distribution, pdata.\\n• We want to build a generative model pmodel that mimics pdata. If we achieve this\\ngoal, we can sample from pmodel to generate observations that appear to have\\nbeen drawn from pdata.\\n• Therefore, the desirable properties of pmodel are:\\n10 \\n| \\nChapter 1: Generative Modeling'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 38}, page_content='Accuracy\\nIf pmodel is high for a generated observation, it should look like it has been\\ndrawn from pdata. If \\\\(p_{model}\\\\) is low for a generated observation, it\\nshould not look like it has been drawn from pdata.\\nGeneration\\nIt should be possible to easily sample a new observation from pmodel.\\nRepresentation\\nIt should be possible to understand how different high-level features in the\\ndata are represented by pmodel.\\nLet’s now reveal the true data-generating distribution, pdata, and see how the frame‐\\nwork applies to this example. As we can see from Figure 1-6, the data-generating rule\\nis simply a uniform distribution over the land mass of the world, with no chance of\\nfinding a point in the sea.\\nFigure 1-6. The orange box, pmodel, is an estimate of the true data-generating distribu‐\\ntion, pdata (the gray area)\\nClearly, our model, pmodel, is an oversimplification of pdata. We can inspect points A,\\nB, and C to understand the successes and failures of our model in terms of how accu‐\\nrately it mimics pdata:\\n• Point A is an observation that is generated by our model but does not appear to\\nhave been generated by pdata as it’s in the middle of the sea.\\nOur First Generative Model \\n| \\n11'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 39}, page_content='• Point B could never have been generated by pmodel as it sits outside the orange\\nbox. Therefore, our model has some gaps in its ability to produce observations\\nacross the entire range of potential possibilities.\\n• Point C is an observation that could be generated by pmodel and also by pdata.\\nDespite its shortcomings, the model is easy to sample from, because it is simply a uni‐\\nform distribution over the orange box. We can easily choose a point at random from\\ninside this box, in order to sample from it.\\nAlso, we can certainly say that our model is a simple representation of the underlying\\ncomplex distribution that captures some of the underlying high-level features. The\\ntrue distribution is separated into areas with lots of land mass (continents) and those\\nwith no land mass (the sea). This is a high-level feature that is also true of our model,\\nexcept we have one large continent, rather than many.\\nThis example has demonstrated the fundamental concepts behind generative model‐\\ning. The problems we will be tackling in this book will be far more complex and high-\\ndimensional, but the underlying framework through which we approach the problem\\nwill be the same.\\nRepresentation Learning\\nIt is worth delving a little deeper into what we mean by learning a representation of\\nthe high-dimensional data, as it is a topic that will recur throughout this book.\\nSuppose you wanted to describe your appearance to someone who was looking for\\nyou in a crowd of people and didn’t know what you looked like. You wouldn’t start by\\nstating the color of pixel 1 of a photo of you, then pixel 2, then pixel 3, etc. Instead,\\nyou would make the reasonable assumption that the other person has a general idea\\nof what an average human looks like, then amend this baseline with features that\\ndescribe groups of pixels, such as I have very blond hair or I wear glasses. With no\\nmore than 10 or so of these statements, the person would be able to map the descrip‐\\ntion back into pixels to generate an image of you in their head. The image wouldn’t be\\nperfect, but it would be a close enough likeness to your actual appearance for them to\\nfind you among possibly hundreds of other people, even if they’ve never seen you\\nbefore.\\n12 \\n| \\nChapter 1: Generative Modeling'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 40}, page_content='This is the core idea behind representation learning. Instead of trying to model the\\nhigh-dimensional sample space directly, we describe each observation in the training\\nset using some lower-dimensional latent space and then learn a mapping function\\nthat can take a point in the latent space and map it to a point in the original domain.\\nIn other words, each point in the latent space is a representation of some high-\\ndimensional observation.\\nWhat does this mean in practice? Let’s suppose we have a training set consisting of\\ngrayscale images of biscuit tins (Figure 1-7).\\nFigure 1-7. The biscuit tin dataset\\nTo us, it is obvious that there are two features that can uniquely represent each of\\nthese tins: the height and width of the tin. That is, we can convert each image of a tin\\nto a point in a latent space of just two dimensions, even though the training set of\\nimages is provided in high-dimensional pixel space. Notably, this means that we can\\nalso produce images of tins that do not exist in the training set, by applying a suitable\\nmapping function f  to a new point in the latent space, as shown in Figure 1-8.\\nRealizing that the original dataset can be described by the simpler latent space is not\\nso easy for a machine—it would first need to establish that height and width are the\\ntwo latent space dimensions that best describe this dataset, then learn the mapping\\nfunction f  that can take a point in this space and map it to a grayscale biscuit tin\\nimage. Machine learning (and specifically, deep learning) gives us the ability to train\\nmachines that can find these complex relationships without human guidance.\\nOur First Generative Model \\n| \\n13'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 41}, page_content='Figure 1-8. The 2D latent space of biscuit tins and the function f  that maps a point in\\nthe latent space back to the original image domain\\nOne of the benefits of training models that utilize a latent space is that we can per‐\\nform operations that affect high-level properties of the image by manipulating its rep‐\\nresentation vector within the more manageable latent space. For example, it is not\\nobvious how to adjust the shading of every single pixel to make an image of a biscuit\\ntin taller. However, in the latent space, it’s simply a case of increasing the height latent\\ndimension, then applying the mapping function to return to the image domain. We\\nshall see an explicit example of this in the next chapter, applied not to biscuit tins but\\nto faces.\\nThe concept of encoding the training dataset into a latent space so that we can sample\\nfrom it and decode the point back to the original domain is common to many genera‐\\ntive modeling techniques, as we shall see in later chapters of this book. Mathemati‐\\ncally speaking, encoder-decoder techniques try to transform the highly nonlinear\\nmanifold on which the data lies (e.g., in pixel space) into a simpler latent space that\\ncan be sampled from, so that it is likely that any point in the latent space is the repre‐\\nsentation of a well-formed image, as shown in Figure 1-9.\\n14 \\n| \\nChapter 1: Generative Modeling'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 42}, page_content='Figure 1-9. The dog manifold in high-dimensional pixel space is mapped to a simpler\\nlatent space that can be sampled from\\nCore Probability Theory\\nWe have already seen that generative modeling is closely connected to statistical mod‐\\neling of probability distributions. Therefore, it now makes sense to introduce some\\ncore probabilistic and statistical concepts that will be used throughout this book to\\nexplain the theoretical background of each model.\\nIf you have never studied probability or statistics, don’t worry. To build many of the\\ndeep learning models that we shall see later in this book, it is not essential to have a\\ndeep understanding of statistical theory. However, to gain a full appreciation of the\\ntask that we are trying to tackle, it’s worth trying to build up a solid understanding of\\nbasic probabilistic theory. This way, you will have the foundations in place to under‐\\nstand the different families of generative models that will be introduced later in this\\nchapter.\\nCore Probability Theory \\n| \\n15'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 43}, page_content='As a first step, we shall define five key terms, linking each one back to our earlier\\nexample of a generative model that models the world map in two dimensions:\\nSample space\\nThe sample space is the complete set of all values an observation � can take.\\nIn our previous example, the sample space consists of all\\npoints of latitude and longitude �= x1, x2  on the world map.\\nFor example, � = (40.7306, –73.9352) is a point in the sample\\nspace (New York City) that belongs to the true data-generating\\ndistribution. � = (11.3493, 142.1996) is a point in the sample\\nspace that does not belong to the true data-generating distri‐\\nbution (it’s in the sea).\\nProbability density function\\nA probability density function (or simply density function) is a function p � that\\nmaps a point � in the sample space to a number between 0 and 1. The integral of\\nthe density function over all points in the sample space must equal 1, so that it is\\na well-defined probability distribution.\\nIn the world map example, the density function of our genera‐\\ntive model is 0 outside of the orange box and constant inside\\nof the box, so that the integral of the density function over the\\nentire sample space equals 1.\\nWhile there is only one true density function pdata � that is assumed to have\\ngenerated the observable dataset, there are infinitely many density functions\\npmodel � that we can use to estimate pdata �.\\nParametric modeling\\nParametric modeling is a technique that we can use to structure our approach to\\nfinding a suitable pmodel �. A parametric model is a family of density functions\\npθ � that can be described using a finite number of parameters, θ.\\nIf we assume a uniform distribution as our model family, then\\nthe set all possible boxes we could draw on Figure 1-5 is an\\nexample of a parametric model. In this case, there are four\\nparameters: the coordinates of the bottom-left θ1, θ2  and\\ntop-right θ3, θ4  corners of the box.\\nThus, each density function pθ � in this parametric model\\n(i.e., each box) can be uniquely represented by four numbers,\\nθ = θ1, θ2, θ3, θ4  .\\n16 \\n| \\nChapter 1: Generative Modeling'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 44}, page_content='Likelihood\\nThe likelihood ℒθ � of a parameter set θ is a function that measures the plausi‐\\nbility of θ, given some observed point �. It is defined as follows:\\nℒθ �= pθ �\\nThat is, the likelihood of θ given some observed point � is defined to be the value\\nof the density function parameterized by θ, at the point �. If we have a whole\\ndataset � of independent observations, then we can write:\\nℒθ �= ∏\\n�∈�\\npθ �\\nIn the world map example, an orange box that only covered\\nthe left half of the map would have a likelihood of 0—it\\ncouldn’t possibly have generated the dataset, as we have\\nobserved points in the right half of the map. The orange box in\\nFigure 1-5 has a positive likelihood, as the density function is\\npositive for all data points under this model.\\nSince the product of a large number of terms between 0 and 1 can be quite com‐\\nputationally difficult to work with, we often use the log-likelihood ℓ instead:\\nℓθ �= ∑\\n�∈�\\nlog pθ �\\nThere are statistical reasons why the likelihood is defined in this way, but we can\\nalso see that this definition intuitively makes sense. The likelihood of a set of\\nparameters θ is defined to be the probability of seeing the data if the true data-\\ngenerating distribution was the model parameterized by θ.\\nNote that the likelihood is a function of the parameters, not the\\ndata. It should not be interpreted as the probability that a given\\nparameter set is correct—in other words, it is not a probability\\ndistribution over the parameter space (i.e., it doesn’t sum/inte‐\\ngrate to 1, with respect to the parameters).\\nIt makes intuitive sense that the focus of parametric modeling should be to find\\nthe optimal value θ of the parameter set that maximizes the likelihood of observ‐\\ning the dataset �.\\nCore Probability Theory \\n| \\n17'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 45}, page_content='Maximum likelihood estimation\\nMaximum likelihood estimation is the technique that allows us to estimate θ—the\\nset of parameters θ of a density function pθ � that is most likely to explain some\\nobserved data �. More formally:\\nθ = arg max\\n�\\nℓθ �\\nθ is also called the maximum likelihood estimate (MLE).\\nIn the world map example, the MLE is the smallest rectangle\\nthat still contains all of the points in the training set.\\nNeural networks typically minimize a loss function, so we can equivalently talk\\nabout finding the set of parameters that minimize the negative log-likelihood:\\nθ = arg min\\nθ\\n−ℓθ �\\n= arg min\\nθ\\n−log pθ �\\nGenerative modeling can be thought of as a form of maximum likelihood estimation,\\nwhere the parameters θ are the weights of the neural networks contained in the\\nmodel. We are trying to find the values of these parameters that maximize the likeli‐\\nhood of observing the given data (or equivalently, minimize the negative log-\\nlikelihood).\\nHowever, for high-dimensional problems, it is generally not possible to directly cal‐\\nculate pθ �—it is intractable. As we shall see in the next section, different families of\\ngenerative models take different approaches to tackling this problem.\\nGenerative Model Taxonomy\\nWhile all types of generative models ultimately aim to solve the same task, they all\\ntake slightly different approaches to modeling the density function pθ �. Broadly\\nspeaking, there are three possible approaches:\\n1. Explicitly model the density function, but constrain the model in some way, so\\nthat the density function is tractable (i.e., it can be calculated).\\n2. Explicitly model a tractable approximation of the density function.\\n18 \\n| \\nChapter 1: Generative Modeling'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 46}, page_content='3. Implicitly model the density function, through a stochastic process that directly\\ngenerates data.\\nThese are shown in Figure 1-10 as a taxonomy, alongside the six families of genera‐\\ntive models that we will explore in Part II of this book. Note that these families are\\nnot mutually exclusive—there are many examples of models that are hybrids between\\ntwo different kinds of approaches. You should think of the families as different gen‐\\neral approaches to generative modeling, rather than explicit model architectures.\\nFigure 1-10. A taxonomy of generative modeling approaches\\nThe first split that we can make is between models where the probability density\\nfunction p � is modeled explicitly and those where it is modeled implicitly.\\nImplicit density models do not aim to estimate the probability density at all, but\\ninstead focus solely on producing a stochastic process that directly generates data.\\nThe best-known example of an implicit generative model is a generative adversarial\\nnetwork. We can further split explicit density models into those that directly optimize\\nthe density function (tractable models) and those that only optimize an approxima‐\\ntion of it.\\nTractable models place constraints on the model architecture, so that the density func‐\\ntion has a form that makes it easy to calculate. For example, autoregressive models\\nimpose an ordering on the input features, so that the output can be generated sequen‐\\ntially—e.g., word by word, or pixel by pixel. Normalizing flow models apply a series of\\ntractable, invertible functions to a simple distribution, in order to generate more\\ncomplex distributions.\\nGenerative Model Taxonomy \\n| \\n19'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 47}, page_content='Approximate density models include variational autoencoders, which introduce a latent\\nvariable and optimize an approximation of the joint density function. Energy-based\\nmodels also utilize approximate methods, but do so via Markov chain sampling,\\nrather than variational methods. Diffusion models approximate the density function\\nby training a model to gradually denoise a given image that has been previously\\ncorrupted.\\nA common thread that runs through all of the generative model family types is deep\\nlearning. Almost all sophisticated generative models have a deep neural network at\\ntheir core, because they can be trained from scratch to learn the complex relation‐\\nships that govern the structure of the data, rather than having to be hardcoded with\\ninformation a priori. We’ll explore deep learning in Chapter 2, with practical exam‐\\nples of how to get started building your own deep neural networks.\\nThe Generative Deep Learning Codebase\\nThe final section of this chapter will get you set up to start building generative deep\\nlearning models by introducing the codebase that accompanies this book.\\nMany of the examples in this book are adapted from the excellent\\nopen source implementations that are available through the Keras\\nwebsite. I highly recommend you check out this resource, as new\\nmodels and examples are constantly being added.\\nCloning the Repository\\nTo get started, you’ll first need to clone the Git repository. Git is an open source ver‐\\nsion control system and will allow you to copy the code locally so that you can run\\nthe notebooks on your own machine, or in a cloud-based environment. You may\\nalready have this installed, but if not, follow the instructions relevant to your operat‐\\ning system.\\nTo clone the repository for this book, navigate to the folder where you would like to\\nstore the files and type the following into your terminal:\\ngit clone https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition.git\\nYou should now be able to see the files in a folder on your machine.\\n20 \\n| \\nChapter 1: Generative Modeling'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 48}, page_content='Using Docker\\nThe codebase for this book is intended to be used with Docker, a free containerization\\ntechnology that makes getting started with a new codebase extremely easy, regardless\\nof your architecture or operating system. If you have never used Docker, don’t\\nworry—there is a description of how to get started in the README file in the book\\nrepository.\\nRunning on a GPU\\nIf you don’t have access to your own GPU, that’s also no problem! All of the examples\\nin this book will train on a CPU, though this will take longer than if you use a GPU-\\nenabled machine. There is also a section in the README about setting up a Google\\nCloud environment that gives you access to a GPU on a pay-as-you-go basis.\\nSummary\\nThis chapter introduced the field of generative modeling, an important branch of\\nmachine learning that complements the more widely studied discriminative model‐\\ning. We discussed how generative modeling is currently one of the most active and\\nexciting areas of AI research, with many recent advances in both theory and\\napplications.\\nWe started with a simple toy example and saw how generative modeling ultimately\\nfocuses on modeling the underlying distribution of the data. This presents many\\ncomplex and interesting challenges, which we summarized into a framework for\\nunderstanding the desirable properties of any generative model.\\nWe then walked through the key probabilistic concepts that will help to fully under‐\\nstand the theoretical foundations of each approach to generative modeling and laid\\nout the six different families of generative models that we will explore in Part II of\\nthis book. We also saw how to get started with the Generative Deep Learning code‐\\nbase, by cloning the repository.\\nIn Chapter 2, we will begin our exploration of deep learning and see how to use Keras\\nto build models that can perform discriminative modeling tasks. This will give us the\\nnecessary foundation to tackle generative deep learning problems in later chapters.\\nReferences\\n1. Miles Brundage et al., “The Malicious Use of Artificial Intelligence: Forecasting,\\nPrevention, and Mitigation,” February 20, 2018, https://www.eff.org/files/2018/02/20/\\nmalicious_ai_report_final.pdf.\\nSummary \\n| \\n21'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 49}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 50}, page_content='CHAPTER 2\\nDeep Learning\\nChapter Goals\\nIn this chapter you will:\\n• Learn about the different types of unstructured data that can be modeled using\\ndeep learning.\\n• Define a deep neural network and understand how it can be used to model com‐\\nplex datasets.\\n• Build a multilayer perceptron to predict the content of an image.\\n• Improve the performance of the model by using convolutional layers, dropout,\\nand batch normalization layers.\\nLet’s start with a basic definition of deep learning:\\nDeep learning is a class of machine learning algorithms that uses multiple stacked layers\\nof processing units to learn high-level representations from unstructured data.\\nTo understand deep learning fully, we need to delve into this definition a bit further.\\nFirst, we’ll take a look at the different types of unstructured data that deep learning\\ncan be used to model, then we’ll dive into the mechanics of building multiple stacked\\nlayers of processing units to solve classification tasks. This will provide the founda‐\\ntion for future chapters where we focus on deep learning for generative tasks.\\n23'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 51}, page_content='Data for Deep Learning\\nMany types of machine learning algorithms require structured, tabular data as input,\\narranged into columns of features that describe each observation. For example, a per‐\\nson’s age, income, and number of website visits in the last month are all features that\\ncould help to predict if the person will subscribe to a particular online service in\\nthe coming month. We could use a structured table of these features to train a logistic\\nregression, random forest, or XGBoost model to predict the binary response\\nvariable—did the person subscribe (1) or not (0)? Here, each individual feature con‐\\ntains a nugget of information about the observation, and the model would learn how\\nthese features interact to influence the response.\\nUnstructured data refers to any data that is not naturally arranged into columns of\\nfeatures, such as images, audio, and text. There is of course spatial structure to an\\nimage, temporal structure to a recording or passage of text, and both spatial and tem‐\\nporal structure to video data, but since the data does not arrive in columns of fea‐\\ntures, it is considered unstructured, as shown in Figure 2-1.\\nFigure 2-1. The difference between structured and unstructured data\\nWhen our data is unstructured, individual pixels, frequencies, or characters are\\nalmost entirely uninformative. For example, knowing that pixel 234 of an image is a\\nmuddy shade of brown doesn’t really help identify if the image is of a house or a dog,\\nand knowing that character 24 of a sentence is an e doesn’t help predict if the text is\\nabout football or politics.\\nPixels or characters are really just the dimples of the canvas into which higher-level\\ninformative features, such as an image of a chimney or the word striker, are embed‐\\nded. If the chimney in the image were placed on the other side of the house, the\\nimage would still contain a chimney, but this information would now be carried by\\ncompletely different pixels. If the word striker appeared slightly earlier or later in the\\ntext, the text would still be about football, but different character positions would\\nprovide this information. The granularity of the data combined with the high degree\\n24 \\n| \\nChapter 2: Deep Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 52}, page_content='of spatial dependence destroys the concept of the pixel or character as an informative\\nfeature in its own right.\\nFor this reason, if we train logistic regression, random forest, or XGBoost models on\\nraw pixel values, the trained model will often perform poorly for all but the simplest\\nof classification tasks. These models rely on the input features to be informative and\\nnot spatially dependent. A deep learning model, on the other hand, can learn how to\\nbuild high-level informative features by itself, directly from the unstructured data.\\nDeep learning can be applied to structured data, but its real power, especially with\\nregard to generative modeling, comes from its ability to work with unstructured data.\\nMost often, we want to generate unstructured data such as new images or original\\nstrings of text, which is why deep learning has had such a profound impact on the\\nfield of generative modeling.\\nDeep Neural Networks\\nThe majority of deep learning systems are artificial neural networks (ANNs, or just\\nneural networks for short) with multiple stacked hidden layers. For this reason, deep\\nlearning has now almost become synonymous with deep neural networks. However,\\nany system that employs many layers to learn high-level representations of the input\\ndata is also a form of deep learning (e.g., deep belief networks).\\nLet’s start by breaking down exactly what we mean by a neural network and then see\\nhow they can be used to learn high-level features from unstructured data.\\nWhat Is a Neural Network?\\nA neural network consists of a series of stacked layers. Each layer contains units that\\nare connected to the previous layer’s units through a set of weights. As we shall see,\\nthere are many different types of layers, but one of the most common is the fully con‐\\nnected (or dense) layer that connects all units in the layer directly to every unit in the\\nprevious layer.\\nNeural networks where all adjacent layers are fully connected are called multilayer\\nperceptrons (MLPs). This is the first type of neural network that we will study. An\\nexample of an MLP is shown in Figure 2-2.\\nDeep Neural Networks \\n| \\n25'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 53}, page_content='Figure 2-2. An example of a multilayer perceptron that predicts if a face is smiling\\nThe input (e.g., an image) is transformed by each layer in turn, in what is known as a\\nforward pass through the network, until it reaches the output layer. Specifically, each\\nunit applies a nonlinear transformation to a weighted sum of its inputs and passes the\\noutput through to the subsequent layer. The final output layer is the culmination of\\nthis process, where the single unit outputs a probability that the original input\\nbelongs to a particular category (e.g., smiling).\\nThe magic of deep neural networks lies in finding the set of weights for each layer\\nthat results in the most accurate predictions. The process of finding these weights is\\nwhat we mean by training the network.\\nDuring the training process, batches of images are passed through the network and\\nthe predicted outputs are compared to the ground truth. For example, the network\\nmight output a probability of 80% for an image of someone who really is smiling and\\na probability of 23% for an image of someone who really isn’t smiling. A perfect pre‐\\ndiction would output 100% and 0% for these examples, so there is a small amount of\\nerror. The error in the prediction is then propagated backward through the network,\\nadjusting each set of weights a small amount in the direction that improves the pre‐\\ndiction most significantly. This process is appropriately called backpropagation. Grad‐\\nually, each unit becomes skilled at identifying a particular feature that ultimately\\nhelps the network to make better predictions.\\nLearning High-Level Features\\nThe critical property that makes neural networks so powerful is their ability to learn\\nfeatures from the input data, without human guidance. In other words, we do not\\nneed to do any feature engineering, which is why neural networks are so useful! We\\n26 \\n| \\nChapter 2: Deep Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 54}, page_content='can let the model decide how it wants to arrange its weights, guided only by its desire\\nto minimize the error in its predictions.\\nFor example, let’s walk through the network shown in Figure 2-2, assuming it has\\nalready been trained to accurately predict if a given input face is smiling:\\n1. Unit A receives the value for an individual channel of an input pixel.\\n2. Unit B combines its input values so that it fires strongest when a particular low-\\nlevel feature such as an edge is present.\\n3. Unit C combines the low-level features so that it fires strongest when a higher-\\nlevel feature such as teeth are seen in the image.\\n4. Unit D combines the high-level features so that it fires strongest when the person\\nin the original image is smiling.\\nUnits in each subsequent layer are able to represent increasingly sophisticated aspects\\nof the original input, by combining lower-level features from the previous layer.\\nAmazingly, this arises naturally out of the training process—we do not need to tell\\neach unit what to look for, or whether it should look for high-level features or low-\\nlevel features.\\nThe layers between the input and output layers are called hidden layers. While our\\nexample only has two hidden layers, deep neural networks can have many more.\\nStacking large numbers of layers allows the neural network to learn progressively\\nhigher-level features by gradually building up information from the lower-level fea‐\\ntures in previous layers. For example, ResNet,1 designed for image recognition, con‐\\ntains 152 layers.\\nNext, we’ll dive straight into the practical side of deep learning and get set up with\\nTensorFlow and Keras so that you can start building your own deep neural networks.\\nTensorFlow and Keras\\nTensorFlow is an open source Python library for machine learning, developed by\\nGoogle. TensorFlow is one of the most utilized frameworks for building machine\\nlearning solutions, with particular emphasis on the manipulation of tensors (hence\\nthe name). It provides the low-level functionality required to train neural networks,\\nsuch as computing the gradient of arbitrary differentiable expressions and efficiently\\nexecuting tensor operations.\\nKeras is a high-level API for building neural networks, built on top of TensorFlow\\n(Figure 2-3). It is extremely flexible and very user-friendly, making it an ideal choice\\nfor getting started with deep learning. Moreover, Keras provides numerous useful\\nbuilding blocks that can be plugged together to create highly complex deep learning\\narchitectures through its functional API.\\nDeep Neural Networks \\n| \\n27'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 55}, page_content='Figure 2-3. TensorFlow and Keras are excellent tools for building deep learning solutions\\nIf you are just getting started with deep learning, I can highly recommend using\\nTensorFlow and Keras. This setup will allow you to build any network that you can\\nthink of in a production environment, while also giving you an easy-to-learn API that\\nenables rapid development of new ideas and concepts. Let’s start by seeing how easy it\\nis to build a multilayer perceptron using Keras.\\nMultilayer Perceptron (MLP)\\nIn this section, we will train an MLP to classify a given image using supervised learn‐\\ning. Supervised learning is a type of machine learning algorithm in which the com‐\\nputer is trained on a labeled dataset. In other words, the dataset used for training\\nincludes input data with corresponding output labels. The goal of the algorithm is to\\nlearn a mapping between the input data and the output labels, so that it can make\\npredictions on new, unseen data.\\nThe MLP is a discriminative (rather than generative) model, but supervised learning\\nwill still play a role in many types of generative models that we will explore in later\\nchapters of this book, so it is a good place to start our journey.\\nRunning the Code for This Example\\nThe code for this example can be found in the Jupyter notebook\\nlocated at notebooks/02_deeplearning/01_mlp/mlp.ipynb in the book\\nrepository.\\nPreparing the Data\\nFor this example we will be using the CIFAR-10 dataset, a collection of 60,000 32 ×\\n32–pixel color images that comes bundled with Keras out of the box. Each image is\\nclassified into exactly one of 10 classes, as shown in Figure 2-4.\\n28 \\n| \\nChapter 2: Deep Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 56}, page_content='Figure 2-4. Example images from the CIFAR-10 dataset (source: Krizhevsky, 2009)2\\nBy default, the image data consists of integers between 0 and 255 for each pixel chan‐\\nnel. We first need to preprocess the images by scaling these values to lie between\\n0 and 1, as neural networks work best when the absolute value of each input is less\\nthan 1.\\nWe also need to change the integer labeling of the images to one-hot encoded vectors,\\nbecause the neural network output will be a probability that the image belongs to\\neach class. If the class integer label of an image is i, then its one-hot encoding is a\\nvector of length 10 (the number of classes) that has 0s in all but the ith element,\\nwhich is 1. These steps are shown in Example 2-1.\\nExample 2-1. Preprocessing the CIFAR-10 dataset\\nimport numpy as np\\nfrom tensorflow.keras import datasets, utils\\n(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data() \\nMultilayer Perceptron (MLP) \\n| \\n29'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 57}, page_content=\"NUM_CLASSES = 10\\nx_train = x_train.astype('float32') / 255.0 \\nx_test = x_test.astype('float32') / 255.0\\ny_train = utils.to_categorical(y_train, NUM_CLASSES) \\ny_test = utils.to_categorical(y_test, NUM_CLASSES)\\nLoad the CIFAR-10 dataset. x_train and x_test are numpy arrays of shape\\n[50000, 32, 32, 3] and [10000, 32, 32, 3], respectively. y_train and\\ny_test are numpy arrays of shape [50000, 1] and [10000, 1], respectively, con‐\\ntaining the integer labels in the range 0 to 9 for the class of each image.\\nScale each image so that the pixel channel values lie between 0 and 1.\\nOne-hot encode the labels—the new shapes of y_train and y_test are [50000,\\n10] and [10000, 10], respectively.\\nWe can see that the training image data (x_train) is stored in a tensor of shape\\n[50000, 32, 32, 3]. There are no columns or rows in this dataset; instead, this is a\\ntensor with four dimensions. A tensor is just a multidimensional array—it is the nat‐\\nural extension of a matrix to more than two dimensions. The first dimension of this\\ntensor references the index of the image in the dataset, the second and third relate to\\nthe size of the image, and the last is the channel (i.e., red, green, or blue, since these\\nare RGB images).\\nFor example, Example 2-2 shows how we can find the channel value of a specific pixel\\nin an image.\\nExample 2-2. The green channel (1) value of the pixel in the (12,13) position of image 54\\nx_train[54, 12, 13, 1]\\n# 0.36862746\\nBuilding the Model\\nIn Keras you can either define the structure of a neural network as a Sequential\\nmodel or using the functional API.\\nA Sequential model is useful for quickly defining a linear stack of layers (i.e., where\\none layer follows on directly from the previous layer without any branching). We can\\ndefine our MLP model using the Sequential class as shown in Example 2-3.\\n30 \\n| \\nChapter 2: Deep Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 58}, page_content=\"Example 2-3. Building our MLP using a Sequential model\\nfrom tensorflow.keras import layers, models\\nmodel = models.Sequential([\\n    layers.Flatten(input_shape=(32, 32, 3)),\\n    layers.Dense(200, activation = 'relu'),\\n    layers.Dense(150, activation = 'relu'),\\n    layers.Dense(10, activation = 'softmax'),\\n])\\nMany of the models in this book require that the output from a layer is passed to mul‐\\ntiple subsequent layers, or conversely, that a layer receives input from multiple pre‐\\nceding layers. For these models, the Sequential class is not suitable and we would\\nneed to use the functional API instead, which is a lot more flexible.\\nI recommend that even if you are just starting out building linear\\nmodels with Keras, you still use the functional API rather than\\nSequential models, since it will serve you better in the long run as\\nyour neural networks become more architecturally complex. The\\nfunctional API will give you complete freedom over the design of\\nyour deep neural network.\\nExample 2-4 shows the same MLP coded using the functional API. When using the\\nfunctional API, we use the Model class to define the overall input and output layers of\\nthe model.\\nExample 2-4. Building our MLP using the functional API\\nfrom tensorflow.keras import layers, models\\ninput_layer = layers.Input(shape=(32, 32, 3))\\nx = layers.Flatten()(input_layer)\\nx = layers.Dense(units=200, activation = 'relu')(x)\\nx = layers.Dense(units=150, activation = 'relu')(x)\\noutput_layer = layers.Dense(units=10, activation = 'softmax')(x)\\nmodel = models.Model(input_layer, output_layer)\\nBoth methods give identical models—a diagram of the architecture is shown in\\nFigure 2-5.\\nMultilayer Perceptron (MLP) \\n| \\n31\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 59}, page_content='Figure 2-5. A diagram of the MLP architecture\\nLet’s now look in more detail at the different layers and activation functions used\\nwithin the MLP.\\nLayers\\nTo build our MLP, we used three different types of layers: Input, Flatten, and Dense.\\nThe Input layer is an entry point into the network. We tell the network the shape of\\neach data element to expect as a tuple. Notice that we do not specify the batch size;\\nthis isn’t necessary as we can pass any number of images into the Input layer simulta‐\\nneously. We do not need to explicitly state the batch size in the Input layer definition.\\nNext we flatten this input into a vector, using a Flatten layer. This results in a vector\\nof length 3,072 (= 32 × 32 × 3). The reason we do this is because the subsequent\\nDense layer requires that its input is flat, rather than a multidimensional array. As we\\nshall see later, other layer types require multidimensional arrays as input, so you need\\nto be aware of the required input and output shape of each layer type to understand\\nwhen it is necessary to use Flatten.\\nThe Dense layer is one of the most fundamental building blocks of a neural network.\\nIt contains a given number of units that are densely connected to the previous layer—\\nthat is, every unit in the layer is connected to every unit in the previous layer, through\\na single connection that carries a weight (which can be positive or negative). The out‐\\nput from a given unit is the weighted sum of the inputs it receives from the previous\\nlayer, which is then passed through a nonlinear activation function before being sent\\nto the following layer. The activation function is critical to ensure the neural network\\nis able to learn complex functions and doesn’t just output a linear combination of its\\ninputs.\\n32 \\n| \\nChapter 2: Deep Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 60}, page_content='Activation functions\\nThere are many kinds of activation function, but three of the most important are\\nReLU, sigmoid, and softmax.\\nThe ReLU (rectified linear unit) activation function is defined to be 0 if the input is\\nnegative and is otherwise equal to the input. The LeakyReLU activation function is\\nvery similar to ReLU, with one key difference: whereas the ReLU activation function\\nreturns 0 for input values less than 0, the LeakyReLU function returns a small nega‐\\ntive number proportional to the input. ReLU units can sometimes die if they always\\noutput 0, because of a large bias toward negative values pre-activation. In this case,\\nthe gradient is 0 and therefore no error is propagated back through this unit.\\nLeakyReLU activations fix this issue by always ensuring the gradient is nonzero.\\nReLU-based functions are among the most reliable activations to use between the lay‐\\ners of a deep network to encourage stable training.\\nThe sigmoid activation is useful if you wish the output from the layer to be scaled\\nbetween 0 and 1—for example, for binary classification problems with one output\\nunit or multilabel classification problems, where each observation can belong to more\\nthan one class. Figure 2-6 shows ReLU, LeakyReLU, and sigmoid activation functions\\nside by side for comparison.\\nFigure 2-6. The ReLU, LeakyReLU, and sigmoid activation functions\\nThe softmax activation function is useful if you want the total sum of the output from\\nthe layer to equal 1; for example, for multiclass classification problems where each\\nobservation only belongs to exactly one class. It is defined as:\\nyi =\\ne\\nxi\\n∑j = 1\\nJ\\ne\\nxj\\nHere, J is the total number of units in the layer. In our neural network, we use a soft‐\\nmax activation in the final layer to ensure that the output is a set of 10 probabilities\\nMultilayer Perceptron (MLP) \\n| \\n33'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 61}, page_content=\"that sum to 1, which can be interpreted as the likelihood that the image belongs to\\neach class.\\nIn Keras, activation functions can be defined within a layer (Example 2-5) or as a sep‐\\narate layer (Example 2-6).\\nExample 2-5. A ReLU activation function defined as part of a Dense layer\\nx = layers.Dense(units=200, activation = 'relu')(x)\\nExample 2-6. A ReLU activation function defined as its own layer\\nx = layers.Dense(units=200)(x)\\nx = layers.Activation('relu')(x)\\nIn our example, we pass the input through two Dense layers, the first with 200 units\\nand the second with 150, both with ReLU activation functions.\\nInspecting the model\\nWe can use the model.summary() method to inspect the shape of the network at each\\nlayer, as shown in Table 2-1.\\nTable 2-1. Output from the model.summary() method\\nLayer (type)\\nOutput shape\\nParam #\\nInputLayer\\n(None, 32, 32, 3)\\n0\\nFlatten\\n(None, 3072)\\n0\\nDense\\n(None, 200)\\n614,600\\nDense\\n(None, 150)\\n30,150\\nDense\\n(None, 10)\\n1,510\\nTotal params\\n646,260\\nTrainable params\\n646,260\\nNon-trainable params\\n0\\nNotice how the shape of our Input layer matches the shape of x_train and the shape\\nof our Dense output layer matches the shape of y_train. Keras uses None as a marker\\nfor the first dimension to show that it doesn’t yet know the number of observations\\nthat will be passed into the network. In fact, it doesn’t need to; we could just as easily\\npass 1 observation through the network at a time as 1,000. That’s because tensor oper‐\\nations are conducted across all observations simultaneously using linear algebra—this\\nis the part handled by TensorFlow. It is also the reason why you get a performance\\nincrease when training deep neural networks on GPUs instead of CPUs: GPUs are\\n34 \\n| \\nChapter 2: Deep Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 62}, page_content=\"optimized for large tensor operations since these calculations are also necessary for\\ncomplex graphics manipulation.\\nThe summary method also gives the number of parameters (weights) that will be\\ntrained at each layer. If ever you find that your model is training too slowly, check the\\nsummary to see if there are any layers that contain a huge number of weights. If so,\\nyou should consider whether the number of units in the layer could be reduced to\\nspeed up training.\\nMake sure you understand how the number of parameters is calcu‐\\nlated in each layer! It’s important to remember that by default, each\\nunit within a given layer is also connected to one additional bias\\nunit that always outputs 1. This ensures that the output from the\\nunit can still be nonzero even when all inputs from the previous\\nlayer are 0.\\nTherefore, the number of parameters in the 200-unit Dense layer is\\n200 * (3,072 + 1) = 614,600.\\nCompiling the Model\\nIn this step, we compile the model with an optimizer and a loss function, as shown in\\nExample 2-7.\\nExample 2-7. Defining the optimizer and the loss function\\nfrom tensorflow.keras import optimizers\\nopt = optimizers.Adam(learning_rate=0.0005)\\nmodel.compile(loss='categorical_crossentropy', optimizer=opt,\\n              metrics=['accuracy'])\\nLet’s now look in more detail at what we mean by loss functions and optimizers.\\nLoss functions\\nThe loss function is used by the neural network to compare its predicted output to the\\nground truth. It returns a single number for each observation; the greater this num‐\\nber, the worse the network has performed for this observation.\\nKeras provides many built-in loss functions to choose from, or you can create your\\nown. Three of the most commonly used are mean squared error, categorical cross-\\nentropy, and binary cross-entropy. It is important to understand when it is appropri‐\\nate to use each.\\nIf your neural network is designed to solve a regression problem (i.e., the output is\\ncontinuous), then you might use the mean squared error loss. This is the mean of the\\nMultilayer Perceptron (MLP) \\n| \\n35\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 63}, page_content='squared difference between the ground truth yi and predicted value pi of each output\\nunit, where the mean is taken over all n output units:\\nMSE = 1\\nn ∑\\ni = 1\\nn\\nyi −pi\\n2\\nIf you are working on a classification problem where each observation only belongs\\nto one class, then categorical cross-entropy is the correct loss function. This is defined\\nas follows:\\n−∑\\ni = 1\\nn\\nyi log pi\\nFinally, if you are working on a binary classification problem with one output unit, or\\na multilabel problem where each observation can belong to multiple classes simulta‐\\nneously, you should use binary cross-entropy:\\n−1\\nn ∑\\ni = 1\\nn\\nyi log pi + 1 −yi log 1 −pi\\nOptimizers\\nThe optimizer is the algorithm that will be used to update the weights in the neural\\nnetwork based on the gradient of the loss function. One of the most commonly used\\nand stable optimizers is Adam (Adaptive Moment Estimation).3 In most cases, you\\nshouldn’t need to tweak the default parameters of the Adam optimizer, except the\\nlearning rate. The greater the learning rate, the larger the change in weights at each\\ntraining step. While training is initially faster with a large learning rate, the downside\\nis that it may result in less stable training and may not find the global minimum of\\nthe loss function. This is a parameter that you may want to tune or adjust during\\ntraining.\\nAnother common optimizer that you may come across is RMSProp (Root Mean\\nSquared Propagation). Again, you shouldn’t need to adjust the parameters of this\\noptimizer too much, but it is worth reading the Keras documentation to understand\\nthe role of each parameter.\\nWe pass both the loss function and the optimizer into the compile method of the\\nmodel, as well as a metrics parameter where we can specify any additional metrics\\nthat we would like to report on during training, such as accuracy.\\n36 \\n| \\nChapter 2: Deep Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 64}, page_content='Training the Model\\nThus far, we haven’t shown the model any data. We have just set up the architecture\\nand compiled the model with a loss function and optimizer.\\nTo train the model against the data, we simply call the fit method, as shown in\\nExample 2-8.\\nExample 2-8. Calling the fit method to train the model\\nmodel.fit(x_train \\n          , y_train \\n          , batch_size = 32 \\n          , epochs = 10 \\n          , shuffle = True \\n          )\\nThe raw image data.\\nThe one-hot encoded class labels.\\nThe batch_size determines how many observations will be passed to the net‐\\nwork at each training step.\\nThe epochs determine how many times the network will be shown the full train‐\\ning data.\\nIf shuffle = True, the batches will be drawn randomly without replacement\\nfrom the training data at each training step.\\nThis will start training a deep neural network to predict the category of an image\\nfrom the CIFAR-10 dataset. The training process works as follows.\\nFirst, the weights of the network are initialized to small random values. Then the net‐\\nwork performs a series of training steps. At each training step, one batch of images is\\npassed through the network and the errors are backpropagated to update the weights.\\nThe batch_size determines how many images are in each training step batch. The\\nlarger the batch size, the more stable the gradient calculation, but the slower each\\ntraining step.\\nIt would be far too time-consuming and computationally intensive\\nto use the entire dataset to calculate the gradient at each training\\nstep, so generally a batch size between 32 and 256 is used. It is also\\nnow recommended practice to increase the batch size as training\\nprogresses.4\\nMultilayer Perceptron (MLP) \\n| \\n37'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 65}, page_content='This continues until all observations in the dataset have been seen once. This com‐\\npletes the first epoch. The data is then passed through the network again in batches as\\npart of the second epoch. This process repeats until the specified number of epochs\\nhave elapsed.\\nDuring training, Keras outputs the progress of the procedure, as shown in Figure 2-7.\\nWe can see that the training dataset has been split into 1,563 batches (each containing\\n32 images) and it has been shown to the network 10 times (i.e., over 10 epochs), at a\\nrate of approximately 2 milliseconds per batch. The categorical cross-entropy loss has\\nfallen from 1.8377 to 1.3696, resulting in an accuracy increase from 33.69% after the\\nfirst epoch to 51.67% after the tenth epoch.\\nFigure 2-7. The output from the fit method\\nEvaluating the Model\\nWe know the model achieves an accuracy of 51.9% on the training set, but how does\\nit perform on data it has never seen?\\nTo answer this question we can use the evaluate method provided by Keras, as\\nshown in Example 2-9.\\nExample 2-9. Evaluating the model performance on the test set\\nmodel.evaluate(x_test, y_test)\\n38 \\n| \\nChapter 2: Deep Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 66}, page_content=\"Figure 2-8 shows the output from this method.\\nFigure 2-8. The output from the evaluate method\\nThe output is a list of the metrics we are monitoring: categorical cross-entropy and\\naccuracy. We can see that model accuracy is still 49.0% even on images that it has\\nnever seen before. Note that if the model were guessing randomly, it would achieve\\napproximately 10% accuracy (because there are 10 classes), so 49.0% is a good result,\\ngiven that we have used a very basic neural network.\\nWe can view some of the predictions on the test set using the predict method, as\\nshown in Example 2-10.\\nExample 2-10. Viewing predictions on the test set using the predict method\\nCLASSES = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog'\\n                   , 'frog', 'horse', 'ship', 'truck'])\\npreds = model.predict(x_test) \\npreds_single = CLASSES[np.argmax(preds, axis = -1)] \\nactual_single = CLASSES[np.argmax(y_test, axis = -1)]\\npreds is an array of shape [10000, 10]—i.e., a vector of 10 class probabilities for\\neach observation.\\nWe convert this array of probabilities back into a single prediction using numpy’s\\nargmax function. Here, axis = –1 tells the function to collapse the array over the\\nlast dimension (the classes dimension), so that the shape of preds_single is then\\n[10000, 1].\\nWe can view some of the images alongside their labels and predictions with the code\\nin Example 2-11. As expected, around half are correct.\\nExample 2-11. Displaying predictions of the MLP against the actual labels\\nimport matplotlib.pyplot as plt\\nn_to_show = 10\\nindices = np.random.choice(range(len(x_test)), n_to_show)\\nfig = plt.figure(figsize=(15, 3))\\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\\nMultilayer Perceptron (MLP) \\n| \\n39\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 67}, page_content=\"for i, idx in enumerate(indices):\\n    img = x_test[idx]\\n    ax = fig.add_subplot(1, n_to_show, i+1)\\n    ax.axis('off')\\n    ax.text(0.5, -0.35, 'pred = ' + str(preds_single[idx]), fontsize=10\\n       , ha='center', transform=ax.transAxes)\\n    ax.text(0.5, -0.7, 'act = ' + str(actual_single[idx]), fontsize=10\\n        , ha='center', transform=ax.transAxes)\\n    ax.imshow(img)\\nFigure 2-9 shows a randomly chosen selection of predictions made by the model,\\nalongside the true labels.\\nFigure 2-9. Some predictions made by the model, alongside the actual labels\\nCongratulations! You’ve just built a multilayer perceptron using Keras and used it to\\nmake predictions on new data. Even though this is a supervised learning problem,\\nwhen we come to building generative models in future chapters many of the core\\nideas from this chapter (such as loss functions, activation functions, and understand‐\\ning layer shapes) will still be extremely important. Next we’ll look at ways of improv‐\\ning this model, by introducing a few new layer types.\\nConvolutional Neural Network (CNN)\\nOne of the reasons our network isn’t yet performing as well as it might is because\\nthere isn’t anything in the network that takes into account the spatial structure of the\\ninput images. In fact, our first step is to flatten the image into a single vector, so that\\nwe can pass it to the first Dense layer!\\nTo achieve this we need to use a convolutional layer.\\n40 \\n| \\nChapter 2: Deep Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 68}, page_content='Convolutional Layers\\nFirst, we need to understand what is meant by a convolution in the context of deep\\nlearning.\\nFigure 2-10 shows two different 3 × 3 × 1 portions of a grayscale image being convo‐\\nluted with a 3 × 3 × 1 filter (or kernel). The convolution is performed by multiplying\\nthe filter pixelwise with the portion of the image, and summing the results. The out‐\\nput is more positive when the portion of the image closely matches the filter and\\nmore negative when the portion of the image is the inverse of the filter. The top\\nexample resonates strongly with the filter, so it produces a large positive value. The\\nbottom example does not resonate much with the filter, so it produces a value near\\nzero.\\nFigure 2-10. A 3 × 3 convolutional filter applied to two portions of a grayscale image\\nIf we move the filter across the entire image from left to right and top to bottom,\\nrecording the convolutional output as we go, we obtain a new array that picks out a\\nparticular feature of the input, depending on the values in the filter. For example,\\nFigure 2-11 shows two different filters that highlight horizontal and vertical edges.\\nRunning the Code for This Example\\nYou can see this convolutional process worked through manually\\nin the Jupyter notebook located at notebooks/02_deeplearning/\\n02_cnn/convolutions.ipynb in the book repository.\\nConvolutional Neural Network (CNN) \\n| \\n41'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 69}, page_content='Figure 2-11. Two convolutional filters applied to a grayscale image\\nA convolutional layer is simply a collection of filters, where the values stored in the\\nfilters are the weights that are learned by the neural network through training. Ini‐\\ntially these are random, but gradually the filters adapt their weights to start picking\\nout interesting features such as edges or particular color combinations.\\nIn Keras, the Conv2D layer applies convolutions to an input tensor with two spatial\\ndimensions (such as an image). For example, the code shown in Example 2-12 builds\\na convolutional layer with two filters, to match the example in Figure 2-11.\\nExample 2-12. A Conv2D layer applied to grayscale input images\\nfrom tensorflow.keras import layers\\ninput_layer = layers.Input(shape=(64,64,1))\\nconv_layer_1 = layers.Conv2D(\\n    filters = 2\\n    , kernel_size = (3,3)\\n    , strides = 1\\n    , padding = \"same\"\\n    )(input_layer)\\nNext, let’s look at two of the arguments to the Conv2D layer in more detail—strides\\nand padding.\\n42 \\n| \\nChapter 2: Deep Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 70}, page_content='Stride\\nThe strides parameter is the step size used by the layer to move the filters across the\\ninput. Increasing the stride therefore reduces the size of the output tensor. For exam‐\\nple, when strides = 2, the height and width of the output tensor will be half the size\\nof the input tensor. This is useful for reducing the spatial size of the tensor as it passes\\nthrough the network, while increasing the number of channels.\\nPadding\\nThe padding = \"same\" input parameter pads the input data with zeros so that the\\noutput size from the layer is exactly the same as the input size when strides = 1.\\nFigure 2-12 shows a 3 × 3 kernel being passed over a 5 × 5 input image, with padding\\n= \"same\" and strides = 1. The output size from this convolutional layer would also\\nbe 5 × 5, as the padding allows the kernel to extend over the edge of the image, so that\\nit fits five times in both directions. Without padding, the kernel could only fit three\\ntimes along each direction, giving an output size of 3 × 3.\\nFigure 2-12. A 3 × 3 × 1 kernel (gray) being passed over a 5 × 5 × 1 input image (blue),\\nwith padding = \"same\" and strides = 1, to generate the 5 × 5 × 1 output (green)\\n(source: Dumoulin and Visin, 2018)5\\nSetting padding = \"same\" is a good way to ensure that you are able to easily keep\\ntrack of the size of the tensor as it passes through many convolutional layers. The\\nshape of the output from a convolutional layer with padding = \"same\" is:\\ninput height\\nstride\\n, input width\\nstride\\n, f ilters\\nStacking convolutional layers\\nThe output of a Conv2D layer is another four-dimensional tensor, now of shape\\n(batch_size, height, width, filters), so we can stack Conv2D layers on top of\\neach other to grow the depth of our neural network and make it more powerful. To\\ndemonstrate this, let’s imagine we are applying Conv2D layers to the CIFAR-10 dataset\\nand wish to predict the label of a given image. Note that this time, instead of one\\ninput channel (grayscale) we have three (red, green, and blue).\\nConvolutional Neural Network (CNN) \\n| \\n43'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 71}, page_content=\"Example 2-13 shows how to build a simple convolutional neural network that we\\ncould train to succeed at this task.\\nExample 2-13. Code to build a convolutional neural network model using Keras\\nfrom tensorflow.keras import layers, models\\ninput_layer = layers.Input(shape=(32,32,3))\\nconv_layer_1 = layers.Conv2D(\\n    filters = 10\\n    , kernel_size = (4,4)\\n    , strides = 2\\n    , padding = 'same'\\n    )(input_layer)\\nconv_layer_2 = layers.Conv2D(\\n    filters = 20\\n    , kernel_size = (3,3)\\n    , strides = 2\\n    , padding = 'same'\\n    )(conv_layer_1)\\nflatten_layer = layers.Flatten()(conv_layer_2)\\noutput_layer = layers.Dense(units=10, activation = 'softmax')(flatten_layer)\\nmodel = models.Model(input_layer, output_layer)\\nThis code corresponds to the diagram shown in Figure 2-13.\\nFigure 2-13. A diagram of a convolutional neural network\\nNote that now that we are working with color images, each filter in the first convolu‐\\ntional layer has a depth of 3 rather than 1 (i.e., each filter has shape 4 × 4 × 3, rather\\nthan 4 × 4 × 1). This is to match the three channels (red, green, blue) of the input\\n44 \\n| \\nChapter 2: Deep Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 72}, page_content='image. The same idea applies to the filters in the second convolutional layer that have\\na depth of 10, to match the 10 channels output by the first convolutional layer.\\nIn general, the depth of the filters in a layer is always equal to the\\nnumber of channels output by the preceding layer.\\nInspecting the model\\nIt’s really informative to look at how the shape of the tensor changes as data flows\\nthrough from one convolutional layer to the next. We can use the model.summary()\\nmethod to inspect the shape of the tensor as it passes through the network\\n(Table 2-2).\\nTable 2-2. CNN model summary\\nLayer (type)\\nOutput shape\\nParam #\\nInputLayer\\n(None, 32, 32, 3)\\n0\\nConv2D\\n(None, 16, 16, 10) 490\\nConv2D\\n(None, 8, 8, 20)\\n1,820\\nFlatten\\n(None, 1280)\\n0\\nDense\\n(None, 10)\\n12,810\\nTotal params\\n15,120\\nTrainable params\\n15,120\\nNon-trainable params\\n0\\nLet’s walk through our network layer by layer, noting the shape of the tensor as we go:\\n1. The input shape is (None, 32, 32, 3)—Keras uses None to represent the fact\\nthat we can pass any number of images through the network simultaneously.\\nSince the network is just performing tensor algebra, we don’t need to pass images\\nthrough the network individually, but instead can pass them through together as\\na batch.\\n2. The shape of each of the 10 filters in the first convolutional layer is 4 × 4 × 3. This\\nis because we have chosen each filter to have a height and width of 4 (ker\\nnel_size = (4,4)) and there are three channels in the preceding layer (red,\\ngreen, and blue). Therefore, the number of parameters (or weights) in the layer is\\n(4 × 4 × 3 + 1) × 10 = 490, where the + 1 is due to the inclusion of a bias term\\nattached to each of the filters. The output from each filter will be the pixelwise\\nmultiplication of the filter weights and the 4 × 4 × 3 section of the image it is\\nConvolutional Neural Network (CNN) \\n| \\n45'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 73}, page_content='covering. As strides = 2 and padding = \"same\", the width and height of the\\noutput are both halved to 16, and since there are 10 filters the output of the first\\nlayer is a batch of tensors each having shape [16, 16, 10].\\n3. In the second convolutional layer, we choose the filters to be 3 × 3 and they now\\nhave depth 10, to match the number of channels in the previous layer. Since there\\nare 20 filters in this layer, this gives a total number of parameters (weights) of (3\\n× 3 × 10 + 1) × 20 = 1,820. Again, we use strides = 2 and padding = \"same\",\\nso the width and height both halve. This gives us an overall output shape of\\n(None, 8, 8, 20).\\n4. We now flatten the tensor using the Keras Flatten layer. This results in a set of 8\\n× 8 × 20 = 1,280 units. Note that there are no parameters to learn in a Flatten\\nlayer as the operation is just a restructuring of the tensor.\\n5. We finally connect these units to a 10-unit Dense layer with softmax activation,\\nwhich represents the probability of each category in a 10-category classification\\ntask. This creates an extra 1,280 × 10 = 12,810 parameters (weights) to learn.\\nThis example demonstrates how we can chain convolutional layers together to create\\na convolutional neural network. Before we see how this compares in accuracy to our\\ndensely connected neural network, we’ll examine two more techniques that can also\\nimprove performance: batch normalization and dropout.\\nBatch Normalization\\nOne common problem when training a deep neural network is ensuring that the\\nweights of the network remain within a reasonable range of values—if they start to\\nbecome too large, this is a sign that your network is suffering from what is known as\\nthe exploding gradient problem. As errors are propagated backward through the\\nnetwork, the calculation of the gradient in the earlier layers can sometimes grow\\nexponentially large, causing wild fluctuations in the weight values.\\nIf your loss function starts to return NaN, chances are that your\\nweights have grown large enough to cause an overflow error.\\nThis doesn’t necessarily happen immediately as you start training the network. Some‐\\ntimes it can be happily training for hours when suddenly the loss function returns\\nNaN and your network has exploded. This can be incredibly annoying. To prevent it\\nfrom happening, you need to understand the root cause of the exploding gradient\\nproblem.\\n46 \\n| \\nChapter 2: Deep Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 74}, page_content='Covariate shift\\nOne of the reasons for scaling input data to a neural network is to ensure a stable start\\nto training over the first few iterations. Since the weights of the network are initially\\nrandomized, unscaled input could potentially create huge activation values that\\nimmediately lead to exploding gradients. For example, instead of passing pixel values\\nfrom 0–255 into the input layer, we usually scale these values to between –1 and 1.\\nBecause the input is scaled, it’s natural to expect the activations from all future layers\\nto be relatively well scaled as well. Initially this may be true, but as the network trains\\nand the weights move further away from their random initial values, this assumption\\ncan start to break down. This phenomenon is known as covariate shift.\\nCovariate Shift Analogy\\nImagine you’re carrying a tall pile of books, and you get hit by a\\ngust of wind. You move the books in a direction opposite to the\\nwind to compensate, but as you do so, some of the books shift, so\\nthat the tower is slightly more unstable than before. Initially, this is\\nOK, but with every gust the pile becomes more and more unstable,\\nuntil eventually the books have shifted so much that the pile collap‐\\nses. This is covariate shift.\\nRelating this to neural networks, each layer is like a book in the\\npile. To remain stable, when the network updates the weights, each\\nlayer implicitly assumes that the distribution of its input from the\\nlayer beneath is approximately consistent across iterations. How‐\\never, since there is nothing to stop any of the activation distribu‐\\ntions shifting significantly in a certain direction, this can\\nsometimes lead to runaway weight values and an overall collapse of\\nthe network.\\nTraining using batch normalization\\nBatch normalization is a technique that drastically reduces this problem. The solution\\nis surprisingly simple. During training, a batch normalization layer calculates the\\nmean and standard deviation of each of its input channels across the batch and nor‐\\nmalizes by subtracting the mean and dividing by the standard deviation. There are\\nthen two learned parameters for each channel, the scale (gamma) and shift (beta).\\nThe output is simply the normalized input, scaled by gamma and shifted by beta.\\nFigure 2-14 shows the whole process.\\nConvolutional Neural Network (CNN) \\n| \\n47'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 75}, page_content='Figure 2-14. The batch normalization process (source: Ioffe and Szegedy, 2015)6\\nWe can place batch normalization layers after dense or convolutional layers to nor‐\\nmalize the output.\\nReferring to our previous example, it’s a bit like connecting the lay‐\\ners of books with small sets of adjustable springs that ensure there\\naren’t any overall huge shifts in their positions over time.\\nPrediction using batch normalization\\nYou might be wondering how this layer works at prediction time. When it comes to\\nprediction, we may only want to predict a single observation, so there is no batch over\\nwhich to calculate the mean and standard deviation. To get around this problem, dur‐\\ning training a batch normalization layer also calculates the moving average of the\\nmean and standard deviation of each channel and stores this value as part of the layer\\nto use at test time.\\nHow many parameters are contained within a batch normalization layer? For every\\nchannel in the preceding layer, two weights need to be learned: the scale (gamma) and\\nshift (beta). These are the trainable parameters. The moving average and standard\\ndeviation also need to be calculated for each channel, but since they are derived from\\nthe data passing through the layer rather than trained through backpropagation, they\\nare called nontrainable parameters. In total, this gives four parameters for each chan‐\\nnel in the preceding layer, where two are trainable and two are nontrainable.\\n48 \\n| \\nChapter 2: Deep Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 76}, page_content='In Keras, the BatchNormalization layer implements the batch normalization\\nfunctionality, as shown in Example 2-14.\\nExample 2-14. A BatchNormalization layer in Keras\\nfrom tensorflow.keras import layers\\nlayers.BatchNormalization(momentum = 0.9)\\nThe momentum parameter is the weight given to the previous value when calculating\\nthe moving average and moving standard deviation.\\nDropout\\nWhen studying for an exam, it is common practice for students to use past papers\\nand sample questions to improve their knowledge of the subject material. Some stu‐\\ndents try to memorize the answers to these questions, but then come unstuck in the\\nexam because they haven’t truly understood the subject matter. The best students use\\nthe practice material to further their general understanding, so that they are still able\\nto answer correctly when faced with new questions that they haven’t seen before.\\nThe same principle holds for machine learning. Any successful machine learning\\nalgorithm must ensure that it generalizes to unseen data, rather than simply remem‐\\nbering the training dataset. If an algorithm performs well on the training dataset, but\\nnot the test dataset, we say that it is suffering from overfitting. To counteract this\\nproblem, we use regularization techniques, which ensure that the model is penalized\\nif it starts to overfit.\\nThere are many ways to regularize a machine learning algorithm, but for deep learn‐\\ning, one of the most common is by using dropout layers. This idea was introduced by\\nHinton et al. in 20127 and presented in a 2014 paper by Srivastava et al.8\\nDropout layers are very simple. During training, each dropout layer chooses a ran‐\\ndom set of units from the preceding layer and sets their output to 0, as shown in\\nFigure 2-15.\\nIncredibly, this simple addition drastically reduces overfitting by ensuring that the\\nnetwork doesn’t become overdependent on certain units or groups of units that, in\\neffect, just remember observations from the training set. If we use dropout layers, the\\nnetwork cannot rely too much on any one unit and therefore knowledge is more\\nevenly spread across the whole network.\\nConvolutional Neural Network (CNN) \\n| \\n49'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 77}, page_content='Figure 2-15. A dropout layer\\nThis makes the model much better at generalizing to unseen data, because the net‐\\nwork has been trained to produce accurate predictions even under unfamiliar condi‐\\ntions, such as those caused by dropping random units. There are no weights to learn\\nwithin a dropout layer, as the units to drop are decided stochastically. At prediction\\ntime, the dropout layer doesn’t drop any units, so that the full network is used to\\nmake predictions.\\nDropout Analogy\\nReturning to our analogy, it’s a bit like a math student practicing\\npast papers with a random selection of key formulae missing from\\ntheir formula book. This way, they learn how to answer questions\\nthrough an understanding of the core principles, rather than\\nalways looking up the formulae in the same places in the book.\\nWhen it comes to test time, they will find it much easier to answer\\nquestions that they have never seen before, due to their ability to\\ngeneralize beyond the training material.\\nThe Dropout layer in Keras implements this functionality, with the rate parameter\\nspecifying the proportion of units to drop from the preceding layer, as shown in\\nExample 2-15.\\nExample 2-15. A Dropout layer in Keras\\nfrom tensorflow.keras import layers\\nlayers.Dropout(rate = 0.25)\\n50 \\n| \\nChapter 2: Deep Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 78}, page_content=\"Dropout layers are used most commonly after dense layers since these are the most\\nprone to overfitting due to the higher number of weights, though you can also use\\nthem after convolutional layers.\\nBatch normalization also has been shown to reduce overfitting, and\\ntherefore many modern deep learning architectures don’t use drop‐\\nout at all, relying solely on batch normalization for regularization.\\nAs with most deep learning principles, there is no golden rule that\\napplies in every situation—the only way to know for sure what’s\\nbest is to test different architectures and see which performs best\\non a holdout set of data.\\nBuilding the CNN\\nYou’ve now seen three new Keras layer types: Conv2D, BatchNormalization, and\\nDropout. Let’s put these pieces together into a CNN model and see how it performs\\non the CIFAR-10 dataset.\\nRunning the Code for This Example\\nYou can run the following example in the Jupyter notebook in\\nthe book repository called notebooks/02_deeplearning/02_cnn/\\ncnn.ipynb.\\nThe model architecture we shall test is shown in Example 2-16.\\nExample 2-16. Code to build a CNN model using Keras\\nfrom tensorflow.keras import layers, models\\ninput_layer = layers.Input((32,32,3))\\nx = layers.Conv2D(filters = 32, kernel_size = 3\\n \\n, strides = 1, padding = 'same')(input_layer)\\nx = layers.BatchNormalization()(x)\\nx = layers.LeakyReLU()(x)\\nx = layers.Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = 'same')(x)\\nx = layers.BatchNormalization()(x)\\nx = layers.LeakyReLU()(x)\\nx = layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(x)\\nx = layers.BatchNormalization()(x)\\nx = layers.LeakyReLU()(x)\\nx = layers.Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = 'same')(x)\\nx = layers.BatchNormalization()(x)\\nConvolutional Neural Network (CNN) \\n| \\n51\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 79}, page_content=\"x = layers.LeakyReLU()(x)\\nx = layers.Flatten()(x)\\nx = layers.Dense(128)(x)\\nx = layers.BatchNormalization()(x)\\nx = layers.LeakyReLU()(x)\\nx = layers.Dropout(rate = 0.5)(x)\\noutput_layer = layers.Dense(10, activation = 'softmax')(x)\\nmodel = models.Model(input_layer, output_layer)\\nWe use four stacked Conv2D layers, each followed by a BatchNormalization and a\\nLeakyReLU layer. After flattening the resulting tensor, we pass the data through a\\nDense layer of size 128, again followed by a BatchNormalization and a LeakyReLU\\nlayer. This is immediately followed by a Dropout layer for regularization, and the net‐\\nwork is concluded with an output Dense layer of size 10.\\nThe order in which to use the batch normalization and activation\\nlayers is a matter of preference. Usually batch normalization layers\\nare placed before the activation, but some successful architectures\\nuse these layers the other way around. If you do choose to use\\nbatch normalization before activation, you can remember the order\\nusing the acronym BAD (batch normalization, activation, then\\ndropout)!\\nThe model summary is shown in Table 2-3.\\nTable 2-3. Model summary of the CNN for CIFAR-10\\nLayer (type)\\nOutput shape\\nParam #\\nInputLayer\\n(None, 32, 32, 3)\\n0\\nConv2D\\n(None, 32, 32, 32) 896\\nBatchNormalization (None, 32, 32, 32) 128\\nLeakyReLU\\n(None, 32, 32, 32) 0\\nConv2D\\n(None, 16, 16, 32) 9,248\\nBatchNormalization (None, 16, 16, 32) 128\\nLeakyReLU\\n(None, 16, 16, 32) 0\\nConv2D\\n(None, 16, 16, 64) 18,496\\nBatchNormalization (None, 16, 16, 64) 256\\nLeakyReLU\\n(None, 16, 16, 64) 0\\nConv2D\\n(None, 8, 8, 64)\\n36,928\\nBatchNormalization (None, 8, 8, 64)\\n256\\n52 \\n| \\nChapter 2: Deep Learning\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 80}, page_content='Layer (type)\\nOutput shape\\nParam #\\nLeakyReLU\\n(None, 8, 8, 64)\\n0\\nFlatten\\n(None, 4096)\\n0\\nDense\\n(None, 128)\\n524,416\\nBatchNormalization (None, 128)\\n512\\nLeakyReLU\\n(None, 128)\\n0\\nDropout\\n(None, 128)\\n0\\nDense\\n(None, 10)\\n1290\\nTotal params\\n592,554\\nTrainable params\\n591,914\\nNon-trainable params\\n640\\nBefore moving on, make sure you are able to calculate the output\\nshape and number of parameters for each layer by hand. It’s a good\\nexercise to prove to yourself that you have fully understood how\\neach layer is constructed and how it is connected to the preceding\\nlayer! Don’t forget to include the bias weights that are included as\\npart of the Conv2D and Dense layers.\\nTraining and Evaluating the CNN\\nWe compile and train the model in exactly the same way as before and call the\\nevaluate method to determine its accuracy on the holdout set (Figure 2-16).\\nFigure 2-16. CNN performance\\nAs you can see, this model is now achieving 71.5% accuracy, up from 49.0% previ‐\\nously. Much better! Figure 2-17 shows some predictions from our new convolutional\\nmodel.\\nThis improvement has been achieved simply by changing the architecture of the\\nmodel to include convolutional, batch normalization, and dropout layers. Notice that\\nthe number of parameters is actually fewer in our new model than the previous\\nmodel, even though the number of layers is far greater. This demonstrates the impor‐\\ntance of being experimental with your model design and being comfortable with how\\nthe different layer types can be used to your advantage. When building generative\\nConvolutional Neural Network (CNN) \\n| \\n53'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 81}, page_content='models, it becomes even more important to understand the inner workings of your\\nmodel since it is the middle layers of your network that capture the high-level fea‐\\ntures that you are most interested in.\\nFigure 2-17. CNN predictions\\nSummary\\nThis chapter introduced the core deep learning concepts that you will need to start\\nbuilding deep generative models. We started by building a multilayer perceptron\\n(MLP) using Keras and trained the model to predict the category of a given image\\nfrom the CIFAR-10 dataset. Then, we improved upon this architecture by introduc‐\\ning convolutional, batch normalization, and dropout layers to create a convolutional\\nneural network (CNN).\\nA really important point to take away from this chapter is that deep neural networks\\nare completely flexible by design, and there really are no fixed rules when it comes to\\nmodel architecture. There are guidelines and best practices, but you should feel free\\nto experiment with layers and the order in which they appear. Don’t feel constrained\\nto only use the architectures that you have read about in this book or elsewhere! Like\\na child with a set of building blocks, the design of your neural network is only limited\\nby your own imagination.\\nIn the next chapter, we shall see how we can use these building blocks to design a net‐\\nwork that can generate images.\\nReferences\\n1. Kaiming He et al., “Deep Residual Learning for Image Recognition,” December 10,\\n2015, https://arxiv.org/abs/1512.03385.\\n54 \\n| \\nChapter 2: Deep Learning'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 82}, page_content='2. Alex Krizhevsky, “Learning Multiple Layers of Features from Tiny Images,” April 8,\\n2009, https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.\\n3. Diederik Kingma and Jimmy Ba, “Adam: A Method for Stochastic Optimization,”\\nDecember 22, 2014, https://arxiv.org/abs/1412.6980v8.\\n4. Samuel L. Smith et al., “Don’t Decay the Learning Rate, Increase the Batch Size,”\\nNovember 1, 2017, https://arxiv.org/abs/1711.00489.\\n5. Vincent Dumoulin and Francesco Visin, “A Guide to Convolution Arithmetic for\\nDeep Learning,” January 12, 2018, https://arxiv.org/abs/1603.07285.\\n6. Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Net‐\\nwork Training by Reducing Internal Covariate Shift,” February 11, 2015, https://\\narxiv.org/abs/1502.03167.\\n7. Hinton et al., “Networks by Preventing Co-Adaptation of Feature Detectors,” July 3,\\n2012, https://arxiv.org/abs/1207.0580.\\n8. Nitish Srivastava et al., “Dropout: A Simple Way to Prevent Neural Networks from\\nOverfitting,” Journal of Machine Learning Research 15 (2014): 1929–1958, http://\\njmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf.\\nSummary \\n| \\n55'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 83}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 84}, page_content='PART II\\nMethods\\nIn Part II we will dive into the six families of generative models, including the theory\\nbehind how they work and practical examples of how to build each type of model.\\nIn Chapter 3 we shall take a look at our first generative deep learning model, the var‐\\niational autoencoder. This technique will allow us to not only generate realistic faces,\\nbut also alter existing images—for example, by adding a smile or changing the color\\nof someone’s hair.\\nChapter 4 explores one of the most successful generative modeling techniques of\\nrecent years, the generative adversarial network. We shall see the ways that GAN train‐\\ning has been fine-tuned and adapted to continually push the boundaries of what gen‐\\nerative modeling is able to achieve.\\nIn Chapter 5 we will delve into several examples of autoregressive models, including\\nLSTMs and PixelCNN. This family of models treats the generation process as a\\nsequence prediction problem—it underpins today’s state-of-the-art text generation\\nmodels and can also be used for image generation.\\nIn Chapter 6 we will cover the family of normalizing flow models, including RealNVP.\\nThis model is based on a change of variables formula, which allows the transforma‐\\ntion of a simple distribution, such as a Gaussian distribution, into a more complex\\ndistribution in way that preserves tractability.\\nChapter 7 introduces the family of energy-based models. These models train a scalar\\nenergy function to score the validity of a given input. We will explore a technique for\\ntraining energy-based models called contrastive divergence and a technique for sam‐\\npling new observations called Langevin dynamics.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 85}, page_content='Finally, in Chapter 8 we shall explore the family of diffusion models. This technique is\\nbased on the idea of iteratively adding noise to an image and then training a model to\\nremove the noise, giving us the ability to transform pure noise into realistic samples.\\nBy the end of Part II you will have built practical examples of generative models from\\neach of the six generative modeling families and be able to explain how each works\\nfrom a theoretical perspective.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 86}, page_content='CHAPTER 3\\nVariational Autoencoders\\nChapter Goals\\nIn this chapter you will:\\n• Learn how the architectural design of autoencoders makes them perfectly suited\\nto generative modeling.\\n• Build and train an autoencoder from scratch using Keras.\\n• Use autoencoders to generate new images, but understand the limitations of this\\napproach.\\n• Learn about the architecture of the variational autoencoder and how it solves\\nmany of the problems associated with standard autoencoders.\\n• Build a variational autoencoder from scratch using Keras.\\n• Use variational autoencoders to generate new images.\\n• Use variational autoencoders to manipulate generated images using latent space\\narithmetic.\\nIn 2013, Diederik P. Kingma and Max Welling published a paper that laid the founda‐\\ntions for a type of neural network known as a variational autoencoder (VAE).1 This is\\nnow one of the most fundamental and well-known deep learning architectures for\\ngenerative modeling and an excellent place to start our journey into generative deep\\nlearning.\\nIn this chapter, we shall start by building a standard autoencoder and then see how\\nwe can extend this framework to develop a variational autoencoder. Along the way,\\nwe will pick apart both types of models, to understand how they work at a granular\\nlevel. By the end of the chapter you should have a complete understanding of how to\\n59'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 87}, page_content='build and manipulate autoencoder-based models and, in particular, how to build a\\nvariational autoencoder from scratch to generate images based on your own dataset.\\nIntroduction\\nLet’s start with a simple story that will help to explain the fundamental problem that\\nan autoencoder is trying to solve.\\nBrian, the Stitch, and the Wardrobe\\nImagine that on the floor in front of you is a pile of all the clothing you own—trou‐\\nsers, tops, shoes, and coats, all of different styles. Your stylist, Brian, is becoming\\nincreasingly frustrated with how long it takes him to find the items you require, so he\\ndevises a clever plan.\\nHe tells you to organize your clothes into a wardrobe that is infinitely high and wide\\n(Figure 3-1). When you want to request a particular item, you simply need to tell\\nBrian its location and he will sew the item from scratch using his trusty sewing\\nmachine. It soon becomes obvious that you will need to place similar items near to\\neach other, so that Brian can accurately re-create each item given only its location.\\nFigure 3-1. A man standing in front of an infinite 2D wardrobe (created with\\nMidjourney)\\nAfter several weeks of practice, you and Brian have adjusted to each other’s under‐\\nstandings of the wardrobe layout. It is now possible for you to tell Brian the location\\nof any item of clothing that you desire, and he can accurately sew it from scratch!\\nThis gives you an idea—what would happen if you gave Brian a wardrobe location\\nthat was empty? To your amazement, you find that Brian is able to generate entirely\\n60 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 88}, page_content='new items of clothing that haven’t existed before! The process isn’t perfect, but you\\nnow have limitless options for generating new clothing, just by picking an empty\\nlocation in the infinite wardrobe and letting Brian work his magic with the sewing\\nmachine.\\nLet’s now explore how this story relates to building autoencoders.\\nAutoencoders\\nA diagram of the process described by the story is shown in Figure 3-2. You play the\\npart of the encoder, moving each item of clothing to a location in the wardrobe. This\\nprocess is called encoding. Brian plays the part of the decoder, taking a location in the\\nwardrobe and attempting to re-create the item. This process is called decoding.\\nFigure 3-2. Items of clothing in the infinite wardrobe—each black dot represents an item\\nof clothing\\nEach location in the wardrobe is represented by two numbers (i.e., a 2D vector). For\\nexample, the trousers in Figure 3-2 are encoded to the point [6.3, –0.9]. This vector is\\nalso known as an embedding because the encoder attempts to embed as much infor‐\\nmation into it as possible, so that the decoder can produce an accurate\\nreconstruction.\\nAutoencoders \\n| \\n61'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 89}, page_content='An autoencoder is simply a neural network that is trained to perform the task of\\nencoding and decoding an item, such that the output from this process is as close to\\nthe original item as possible. Crucially, it can be used as a generative model, because\\nwe can decode any point in the 2D space that we want (in particular, those that are\\nnot embeddings of original items) to produce a novel item of clothing.\\nLet’s now see how we can build an autoencoder using Keras and apply it to a real\\ndataset!\\nRunning the Code for This Example\\nThe code for this example can be found in the Jupyter notebook\\nlocated at notebooks/03_vae/01_autoencoder/autoencoder.ipynb in\\nthe book repository.\\nThe Fashion-MNIST Dataset\\nFor this example, we’ll be using the Fashion-MNIST dataset—a collection of grayscale\\nimages of clothing items, each of size 28 × 28 pixels. Some example images from the\\ndataset are shown in Figure 3-3.\\nFigure 3-3. Examples of images from the Fashion-MNIST dataset\\nThe dataset comes prepackaged with TensorFlow, so it can be downloaded as shown\\nin Example 3-1.\\nExample 3-1. Loading the Fashion-MNIST dataset\\nfrom tensorflow.keras import datasets\\n(x_train,y_train), (x_test,y_test) = datasets.fashion_mnist.load_data()\\nThese are 28 × 28 grayscale images (pixel values between 0 and 255) out of the box,\\nwhich we need to preprocess to ensure that the pixel values are scaled between 0 and\\n1. We will also pad each image to 32 × 32 for easier manipulation of the tensor shape\\nas it passes through the network, as shown in Example 3-2.\\n62 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 90}, page_content='Example 3-2. Preprocessing the data\\ndef preprocess(imgs):\\n    imgs = imgs.astype(\"float32\") / 255.0\\n    imgs = np.pad(imgs, ((0, 0), (2, 2), (2, 2)), constant_values=0.0)\\n    imgs = np.expand_dims(imgs, -1)\\n    return imgs\\nx_train = preprocess(x_train)\\nx_test = preprocess(x_test)\\nNext, we need to understand the overall structure of an autoencoder, so that we can\\ncode it up using TensorFlow and Keras.\\nThe Autoencoder Architecture\\nAn autoencoder is a neural network made up of two parts:\\n• An encoder network that compresses high-dimensional input data such as an\\nimage into a lower-dimensional embedding vector\\n• A decoder network that decompresses a given embedding vector back to the orig‐\\ninal domain (e.g., back to an image)\\nA diagram of the network architecture is shown in Figure 3-4. An input image is\\nencoded to a latent embedding vector z, which is then decoded back to the original\\npixel space.\\nFigure 3-4. Autoencoder architecture diagram\\nThe autoencoder is trained to reconstruct an image, after it has passed through the\\nencoder and back out through the decoder. This may seem strange at first—why\\nwould you want to reconstruct a set of images that you already have available to you?\\nHowever, as we shall see, it is the embedding space (also called the latent space) that is\\nthe interesting part of the autoencoder, as sampling from this space will allow us to\\ngenerate new images.\\nLet’s first define what we mean by an embedding. The embedding (z) is a compres‐\\nsion of the original image into a lower-dimensional latent space. The idea is that by\\nchoosing any point in the latent space, we can generate novel images by passing this\\npoint through the decoder, since the decoder has learned how to convert points in the\\nlatent space into viable images.\\nAutoencoders \\n| \\n63'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 91}, page_content='In our example, we will embed images into a two-dimensional latent space. This will\\nhelp us to visualize the latent space, since we can easily plot points in 2D. In practice,\\nthe latent space of an autoencoder will usually have more than two dimensions in\\norder to have more freedom to capture greater nuance in the images.\\nAutoencoders as Denoising Models\\nAutoencoders can be used to clean noisy images, since the encoder\\nlearns that it is not useful to capture the position of the random\\nnoise inside the latent space in order to reconstruct the original.\\nFor tasks such as this, a 2D latent space is probably too small to\\nencode sufficient relevant information from the input. However, as\\nwe shall see, increasing the dimensionality of the latent space\\nquickly leads to problems if we want to use the autoencoder as a\\ngenerative model.\\nLet’s now see how to build the encoder and decoder.\\nThe Encoder\\nIn an autoencoder, the encoder’s job is to take the input image and map it to an\\nembedding vector in the latent space. The architecture of the encoder we will be\\nbuilding is shown in Table 3-1.\\nTable 3-1. Model summary of the encoder\\nLayer (type)\\nOutput shape\\nParam #\\nInputLayer\\n(None, 32, 32, 1)\\n0\\nConv2D\\n(None, 16, 16, 32) 320\\nConv2D\\n(None, 8, 8, 64)\\n18,496\\nConv2D\\n(None, 4, 4, 128)\\n73,856\\nFlatten\\n(None, 2048)\\n0\\nDense\\n(None, 2)\\n4,098\\nTotal params\\n96,770\\nTrainable params\\n96,770\\nNon-trainable params\\n0\\nTo achieve this, we first create an Input layer for the image and pass this through\\nthree Conv2D layers in sequence, each capturing increasingly high-level features. We\\nuse a stride of 2 to halve the size of the output of each layer, while increasing the\\nnumber of channels. The last convolutional layer is flattened and connected to a\\nDense layer of size 2, which represents our two-dimensional latent space.\\n64 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 92}, page_content='Example 3-3 shows how to build this in Keras.\\nExample 3-3. The encoder\\nencoder_input = layers.Input(\\n    shape=(32, 32, 1), name = \"encoder_input\"\\n) \\nx = layers.Conv2D(32, (3, 3), strides = 2, activation = \\'relu\\', padding=\"same\")(\\n    encoder_input\\n) \\nx = layers.Conv2D(64, (3, 3), strides = 2, activation = \\'relu\\', padding=\"same\")(x)\\nx = layers.Conv2D(128, (3, 3), strides = 2, activation = \\'relu\\', padding=\"same\")(x)\\nshape_before_flattening = K.int_shape(x)[1:]\\nx = layers.Flatten()(x) \\nencoder_output = layers.Dense(2, name=\"encoder_output\")(x) \\nencoder = models.Model(encoder_input, encoder_output) \\nDefine the Input layer of the encoder (the image).\\nStack Conv2D layers sequentially on top of each other.\\nFlatten the last convolutional layer to a vector.\\nConnect this vector to the 2D embeddings with a Dense layer.\\nThe Keras Model that defines the encoder—a model that takes an input image\\nand encodes it into a 2D embedding.\\nI strongly encourage you to experiment with the number of convo‐\\nlutional layers and filters to understand how the architecture\\naffects the overall number of model parameters, model perfor‐\\nmance, and model runtime.\\nThe Decoder\\nThe decoder is a mirror image of the encoder—instead of convolutional layers, we\\nuse convolutional transpose layers, as shown in Table 3-2.\\nTable 3-2. Model summary of the decoder\\nLayer (type)\\nOutput shape\\nParam #\\nInputLayer\\n(None, 2)\\n0\\nDense\\n(None, 2048)\\n6,144\\nAutoencoders \\n| \\n65'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 93}, page_content='Layer (type)\\nOutput shape\\nParam #\\nReshape\\n(None, 4, 4, 128)\\n0\\nConv2DTranspose (None, 8, 8, 128)\\n147,584\\nConv2DTranspose (None, 16, 16, 64) 73,792\\nConv2DTranspose (None, 32, 32, 32) 18,464\\nConv2D\\n(None, 32, 32, 1)\\n289\\nTotal params\\n246,273\\nTrainable params\\n246,273\\nNon-trainable params\\n0\\nConvolutional Transpose Layers\\nStandard convolutional layers allow us to halve the size of an input tensor in both\\ndimensions (height and width), by setting strides = 2.\\nThe convolutional transpose layer uses the same principle as a standard convolutional\\nlayer (passing a filter across the image), but is different in that setting strides = 2\\ndoubles the size of the input tensor in both dimensions.\\nIn a convolutional transpose layer, the strides parameter determines the internal\\nzero padding between pixels in the image, as shown in Figure 3-5. Here, a 3 × 3 × 1\\nfilter (gray) is being passed across a 3 × 3 × 1 image (blue) with strides = 2, to pro‐\\nduce a 6 × 6 × 1 output tensor (green).\\nFigure 3-5. A convolutional transpose layer example (source: Dumoulin and Visin,\\n2018)2\\nIn Keras, the Conv2DTranspose layer allows us to perform convolutional transpose\\noperations on tensors. By stacking these layers, we can gradually expand the size of\\neach layer, using strides of 2, until we get back to the original image dimension of\\n32 × 32.\\n66 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 94}, page_content='Example 3-4 shows how we build the decoder in Keras.\\nExample 3-4. The decoder\\ndecoder_input = layers.Input(shape=(2,), name=\"decoder_input\") \\nx = layers.Dense(np.prod(shape_before_flattening))(decoder_input) \\nx = layers.Reshape(shape_before_flattening)(x) \\nx = layers.Conv2DTranspose(\\n    128, (3, 3), strides=2, activation = \\'relu\\', padding=\"same\"\\n)(x) \\nx = layers.Conv2DTranspose(\\n    64, (3, 3), strides=2, activation = \\'relu\\', padding=\"same\"\\n)(x)\\nx = layers.Conv2DTranspose(\\n    32, (3, 3), strides=2, activation = \\'relu\\', padding=\"same\"\\n)(x)\\ndecoder_output = layers.Conv2D(\\n    1,\\n    (3, 3),\\n    strides = 1,\\n    activation=\"sigmoid\",\\n    padding=\"same\",\\n    name=\"decoder_output\"\\n)(x)\\ndecoder = models.Model(decoder_input, decoder_output) \\nDefine the Input layer of the decoder (the embedding).\\nConnect the input to a Dense layer.\\nReshape this vector into a tensor that can be fed as input into the first\\nConv2DTranspose layer.\\nStack Conv2DTranspose layers on top of each other.\\nThe Keras Model that defines the decoder—a model that takes an embedding in\\nthe latent space and decodes it into the original image domain.\\nJoining the Encoder to the Decoder\\nTo train the encoder and decoder simultaneously, we need to define a model that will\\nrepresent the flow of an image through the encoder and back out through the\\ndecoder. Luckily, Keras makes it extremely easy to do this, as you can see in\\nExample 3-5. Notice the way in which we specify that the output from the autoen‐\\ncoder is simply the output from the encoder after it has been passed through the\\ndecoder.\\nAutoencoders \\n| \\n67'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 95}, page_content='Example 3-5. The full autoencoder\\nautoencoder = Model(encoder_input, decoder(encoder_output)) \\nThe Keras Model that defines the full autoencoder—a model that takes an image\\nand passes it through the encoder and back out through the decoder to generate\\na reconstruction of the original image.\\nNow that we’ve defined our model, we just need to compile it with a loss function and\\noptimizer, as shown in Example 3-6. The loss function is usually chosen to be either\\nthe root mean squared error (RMSE) or binary cross-entropy between the individual\\npixels of the original image and the reconstruction.\\nExample 3-6. Compiling the autoencoder\\n# Compile the autoencoder\\nautoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\\nChoosing the Loss Function\\nOptimizing for RMSE means that your generated output will be symmetrically dis‐\\ntributed around the average pixel values (because an overestimation is penalized\\nequivalently to an underestimation).\\nOn the other hand, binary cross-entropy loss is asymmetrical—it penalizes errors\\ntoward the extremes more heavily than errors toward the center. For example, if the\\ntrue pixel value is high (say 0.7), then generating a pixel with value 0.8 is penalized\\nmore heavily than generating a pixel with value 0.6. If the true pixel value is low (say\\n0.3), then generating a pixel with value 0.2 is penalized more heavily than generating\\na pixel with value 0.4.\\nThis has the effect of binary cross-entropy loss producing slightly blurrier images\\nthan RMSE loss (as it tends to push predictions toward 0.5), but sometimes this is\\ndesirable as RMSE can lead to obviously pixelized edges.\\nThere is no right or wrong choice—you should choose whichever works best for your\\nuse case after experimentation.\\nWe can now train the autoencoder by passing in the input images as both the input\\nand output, as shown in Example 3-7.\\n68 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 96}, page_content='Example 3-7. Training the autoencoder\\nautoencoder.fit(\\n    x_train,\\n    x_train,\\n    epochs=5,\\n    batch_size=100,\\n    shuffle=True,\\n    validation_data=(x_test, x_test),\\n)\\nNow that our autoencoder is trained, the first thing we need to check is that it is able\\nto accurately reconstruct the input images.\\nReconstructing Images\\nWe can test the ability to reconstruct images by passing images from the test set\\nthrough the autoencoder and comparing the output to the original images. The code\\nfor this is shown in Example 3-8.\\nExample 3-8. Reconstructing images using the autoencoder\\nexample_images = x_test[:5000]\\npredictions = autoencoder.predict(example_images)\\nIn Figure 3-6 you can see some examples of original images (top row), the 2D vectors\\nafter encoding, and the reconstructed items after decoding (bottom row).\\nFigure 3-6. Examples of encoding and decoding items of clothing\\nNotice how the reconstruction isn’t perfect—there are still some details of the original\\nimages that aren’t captured by the decoding process, such as logos. This is because by\\nreducing each image to just two numbers, we naturally lose some information.\\nLet’s now investigate how the encoder is representing images in the latent space.\\nAutoencoders \\n| \\n69'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 97}, page_content='Visualizing the Latent Space\\nWe can visualize how images are embedded into the latent space by passing the test\\nset through the encoder and plotting the resulting embeddings, as shown in\\nExample 3-9.\\nExample 3-9. Embedding images using the encoder\\nembeddings = encoder.predict(example_images)\\nplt.figure(figsize=(8, 8))\\nplt.scatter(embeddings[:, 0], embeddings[:, 1], c=\"black\", alpha=0.5, s=3)\\nplt.show()\\nThe resulting plot is the scatter plot shown in Figure 3-2—each black point represents\\nan image that has been embedded into the latent space.\\nIn order to better understand how this latent space is structured, we can make use of\\nthe labels that come with the Fashion-MNIST dataset, describing the type of item in\\neach image. There are 10 groups altogether, shown in Table 3-3.\\nTable 3-3. The Fashion-MNIST labels\\nID Clothing label\\n0\\nT-shirt/top\\n1\\nTrouser\\n2\\nPullover\\n3\\nDress\\n4\\nCoat\\n5\\nSandal\\n6\\nShirt\\n7\\nSneaker\\n8\\nBag\\n9\\nAnkle boot\\nWe can color each point based on the label of the corresponding image to produce\\nthe plot in Figure 3-7. Now the structure becomes very clear! Even though the cloth‐\\ning labels were never shown to the model during training, the autoencoder has natu‐\\nrally grouped items that look alike into the same parts of the latent space. For\\nexample, the dark blue cloud of points in the bottom-right corner of the latent space\\nare all different images of trousers and the red cloud of points toward the center are\\nall ankle boots.\\n70 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 98}, page_content='Figure 3-7. Plot of the latent space, colored by clothing label\\nGenerating New Images\\nWe can generate novel images by sampling some points in the latent space and using\\nthe decoder to convert these back into pixel space, as shown in Example 3-10.\\nExample 3-10. Generating novel images using the decoder\\nmins, maxs = np.min(embeddings, axis=0), np.max(embeddings, axis=0)\\nsample = np.random.uniform(mins, maxs, size=(18, 2))\\nreconstructions = decoder.predict(sample)\\nSome examples of generated images are shown in Figure 3-8, alongside their embed‐\\ndings in the latent space.\\nAutoencoders \\n| \\n71'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 99}, page_content='Figure 3-8. Generated items of clothing\\nEach blue dot maps to one of the images shown on the right of the diagram, with the\\nembedding vector shown underneath. Notice how some of the generated items are\\nmore realistic than others. Why is this?\\nTo answer this, let’s first make a few observations about the overall distribution of\\npoints in the latent space, referring back to Figure 3-7:\\n• Some clothing items are represented over a very small area and others over a\\nmuch larger area.\\n• The distribution is not symmetrical about the point (0, 0), or bounded. For\\nexample, there are far more points with positive y-axis values than negative, and\\nsome points even extend to a y-axis value > 8.\\n• There are large gaps between colors containing few points.\\nThese observations actually make sampling from the latent space quite challenging. If\\nwe overlay the latent space with images of decoded points on a grid, as shown in\\nFigure 3-9, we can begin to understand why the decoder may not always generate\\nimages to a satisfactory standard.\\n72 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 100}, page_content='Figure 3-9. A grid of decoded embeddings, overlaid with the embeddings from the origi‐\\nnal images in the dataset, colored by item type\\nFirstly, we can see that if we pick points uniformly in a bounded space that we define,\\nwe’re more likely to sample something that decodes to look like a bag (ID 8) than an\\nankle boot (ID 9) because the part of the latent space carved out for bags (orange) is\\nlarger than the ankle boot area (red).\\nSecondly, it is not obvious how we should go about choosing a random point in the\\nlatent space, since the distribution of these points is undefined. Technically, we would\\nbe justified in choosing any point in the 2D plane! It’s not even guaranteed that points\\nwill be centered around (0, 0). This makes sampling from our latent space\\nproblematic.\\nLastly, we can see holes in the latent space where none of the original images are\\nencoded. For example, there are large white spaces at the edges of the domain—the\\nautoencoder has no reason to ensure that points here are decoded to recognizable\\nclothing items as very few images in the training set are encoded here.\\nEven points that are central may not be decoded into well-formed images. This is\\nbecause the autoencoder is not forced to ensure that the space is continuous. For\\nexample, even though the point (–1, –1) might be decoded to give a satisfactory\\nAutoencoders \\n| \\n73'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 101}, page_content='image of a sandal, there is no mechanism in place to ensure that the point (–1.1, –1.1)\\nalso produces a satisfactory image of a sandal.\\nIn two dimensions this issue is subtle; the autoencoder only has a small number of\\ndimensions to work with, so naturally it has to squash clothing groups together,\\nresulting in the space between clothing groups being relatively small. However, as we\\nstart to use more dimensions in the latent space to generate more complex images\\nsuch as faces, this problem becomes even more apparent. If we give the autoencoder\\nfree rein over how it uses the latent space to encode images, there will be huge gaps\\nbetween groups of similar points with no incentive for the spaces in between to gen‐\\nerate well-formed images.\\nIn order to solve these three problems, we need to convert our autoencoder into a\\nvariational autoencoder.\\nVariational Autoencoders\\nTo explain, let’s revisit the infinite wardrobe and make a few changes…\\nRevisiting the Infinite Wardrobe\\nSuppose now, instead of placing every item of clothing at a single point in the ward‐\\nrobe, you decide to allocate a general area where the item is more likely to be found.\\nYou reason that this more relaxed approach to item location will help to solve the cur‐\\nrent issue around local discontinuities in the wardrobe.\\nAlso, in order to ensure you do not become too careless with the new placement sys‐\\ntem, you agree with Brian that you will try to place the center of each item’s area as\\nclose to the middle of the wardrobe as possible and that deviation of the item from\\nthe center should be as close to one meter as possible (not smaller and not larger).\\nThe further you stray from this rule, the more you have to pay Brian as your stylist.\\nAfter several months of operating with these two simple changes, you step back and\\nadmire the new wardrobe layout, alongside some examples of new clothing items that\\nBrian has generated. Much better! There is plenty of diversity in the generated items,\\nand this time there are no examples of poor-quality garments. It seems the two\\nchanges have made all the difference!\\nLet’s now try to understand what we need to do to our autoencoder model to convert\\nit into a variational autoencoder and thus make it a more sophisticated generative\\nmodel.\\nThe two parts that we need to change are the encoder and the loss function.\\n74 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 102}, page_content='The Encoder\\nIn an autoencoder, each image is mapped directly to one point in the latent space. In\\na variational autoencoder, each image is instead mapped to a multivariate normal dis‐\\ntribution around a point in the latent space, as shown in Figure 3-10.\\nFigure 3-10. The difference between the encoders in an autoencoder and a variational\\nautoencoder\\nThe Multivariate Normal Distribution\\nA normal distribution (or Gaussian distribution) �μ, σ  is a probability distribution\\ncharacterized by a distinctive bell curve shape, defined by two variables: the mean (μ)\\nand the variance (σ2). The standard deviation (\\\\(\\\\sigma\\\\)) is the square root of the\\nvariance.\\nThe probability density function of the normal distribution in one dimension is:\\nf x ∣μ, σ2 =\\n1\\n2πσ2e\\n−x −μ 2\\n2σ2\\nFigure 3-11 shows several normal distributions in one dimension, for different values\\nof the mean and variance. The red curve is the standard normal (or unit normal)\\n�0, 1 —the normal distribution with mean equal to 0 and variance equal to 1.\\nWe can sample a point z from a normal distribution with mean μ and standard devia‐\\ntion σ using the following equation:\\nz = μ + σ�\\nVariational Autoencoders \\n| \\n75'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 103}, page_content='where � is sampled from a standard normal distribution.\\nFigure 3-11. The normal distribution in one dimension (source: Wikipedia)\\nThe concept of a normal distribution extends to more than one dimension—the\\nprobability density function for a multivariate normal distribution (or multivariate\\nGaussian distribution) �μ, Σ  in k dimensions with mean vector μ and symmetric\\ncovariance matrix Σ is as follows:\\nf x1, ..., xk =\\nexp\\n−1\\n2 �−μ TΣ−1 �−μ\\n2π k Σ\\nIn this book, we will typically be using isotropic multivariate normal distributions,\\nwhere the covariance matrix is diagonal. This means that the distribution is inde‐\\npendent in each dimension (i.e., we can sample a vector where each element is nor‐\\nmally distributed with independent mean and variance). This is the case for the\\nmultivariate normal distribution that we will use in our variational autoencoder.\\nA multivariate standard normal distribution �0, � is a multivariate distribution with\\na zero-valued mean vector and identity covariance matrix.\\nNormal Versus Gaussian\\nIn this book, the terms normal and Gaussian are used inter‐\\nchangeably and the isotropic and multivariate nature of the\\ndistribution is usually implied. For example, “we sample from\\na Gaussian distribution” can be interpreted to mean “we sam‐\\nple from an isotropic, multivariate Gaussian distribution.”\\n76 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 104}, page_content='The encoder only needs to map each input to a mean vector and a variance vector\\nand does not need to worry about covariance between dimensions. Variational\\nautoencoders assume that there is no correlation between dimensions in the latent\\nspace.\\nVariance values are always positive, so we actually choose to map to the logarithm of\\nthe variance, as this can take any real number in the range (−∞, ∞). This way we can\\nuse a neural network as the encoder to perform the mapping from the input image to\\nthe mean and log variance vectors.\\nTo summarize, the encoder will take each input image and encode it to two vectors\\nthat together define a multivariate normal distribution in the latent space:\\nz_mean\\nThe mean point of the distribution\\nz_log_var\\nThe logarithm of the variance of each dimension\\nWe can sample a point z from the distribution defined by these values using the fol‐\\nlowing equation:\\nz = z_mean + z_sigma * epsilon\\nwhere:\\nz_sigma = exp(z_log_var * 0.5)\\nepsilon ~ N(0,I)\\nThe derivation of the relationship between z_sigma (σ) and\\nz_log_var (log σ2 ) is as follows:\\nσ = exp\\nlog σ\\n= exp 2 log σ /2 = exp\\nlog σ2 /2\\nThe decoder of a variational autoencoder is identical to the decoder of a plain autoen‐\\ncoder, giving the overall architecture shown in Figure 3-12.\\nFigure 3-12. VAE architecture diagram\\nVariational Autoencoders \\n| \\n77'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 105}, page_content='Why does this small change to the encoder help?\\nPreviously, we saw that there was no requirement for the latent space to be continu‐\\nous—even if the point (–2, 2) decodes to a well-formed image of a sandal, there’s no\\nrequirement for (–2.1, 2.1) to look similar. Now, since we are sampling a random\\npoint from an area around z_mean, the decoder must ensure that all points in the\\nsame neighborhood produce very similar images when decoded, so that the recon‐\\nstruction loss remains small. This is a very nice property that ensures that even when\\nwe choose a point in the latent space that has never been seen by the decoder, it is\\nlikely to decode to an image that is well formed.\\nBuilding the VAE encoder\\nLet’s now see how we build this new version of the encoder in Keras.\\nRunning the Code for This Example\\nThe code for this example can be found in the Jupyter notebook\\nlocated at notebooks/03_vae/02_vae_fashion/vae_fashion.ipynb in\\nthe book repository.\\nThe code has been adapted from the excellent VAE tutorial created\\nby Francois Chollet, available on the Keras website.\\nFirst, we need to create a new type of Sampling layer that will allow us to sample from\\nthe distribution defined by z_mean and z_log_var, as shown in Example 3-11.\\nExample 3-11. The Sampling layer\\nclass Sampling(layers.Layer): \\n    def call(self, inputs):\\n        z_mean, z_log_var = inputs\\n        batch = tf.shape(z_mean)[0]\\n        dim = tf.shape(z_mean)[1]\\n        epsilon = K.random_normal(shape=(batch, dim))\\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon \\nWe create a new layer by subclassing the Keras base Layer class (see the “Sub‐\\nclassing the Layer Class” sidebar).\\nWe use the reparameterization trick (see “The Reparameterization Trick” side‐\\nbar) to build a sample from the normal distribution parameterized by z_mean\\nand z_log_var.\\n78 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 106}, page_content='Subclassing the Layer Class\\nYou can create new layers in Keras by subclassing the abstract Layer class and defin‐\\ning the call method, which describes how a tensor is transformed by the layer.\\nFor example, in the variational autoencoder, we can create a Sampling layer that can\\nhandle the sampling of z from a normal distribution with parameters defined by\\nz_mean and z_log_var.\\nThis is useful when you want to apply a transformation to a tensor that isn’t already\\nincluded as one of the out-of-the-box Keras layer types.\\nThe Reparameterization Trick\\nRather than sample directly from a normal distribution with parameters z_mean and\\nz_log_var, we can sample epsilon from a standard normal and then manually adjust\\nthe sample to have the correct mean and variance.\\nThis is known as the reparameterization trick, and it’s important as it means gradients\\ncan backpropagate freely through the layer. By keeping all of the randomness of the\\nlayer contained within the variable epsilon, the partial derivative of the layer output\\nwith respect to its input can be shown to be deterministic (i.e., independent of the\\nrandom epsilon), which is essential for backpropagation through the layer to be\\npossible.\\nThe complete code for the encoder, including the new Sampling layer, is shown in\\nExample 3-12.\\nExample 3-12. The encoder\\nencoder_input = layers.Input(\\n    shape=(32, 32, 1), name=\"encoder_input\"\\n)\\nx = layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(\\n    encoder_input\\n)\\nx = layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\\nx = layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\\nshape_before_flattening = K.int_shape(x)[1:]\\nx = layers.Flatten()(x)\\nz_mean = layers.Dense(2, name=\"z_mean\")(x) \\nz_log_var = layers.Dense(2, name=\"z_log_var\")(x)\\nz = Sampling()([z_mean, z_log_var]) \\nVariational Autoencoders \\n| \\n79'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 107}, page_content='encoder = models.Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\") \\nInstead of connecting the Flatten layer directly to the 2D latent space, we con‐\\nnect it to layers z_mean and z_log_var.\\nThe Sampling layer samples a point z in the latent space from the normal distri‐\\nbution defined by the parameters z_mean and z_log_var.\\nThe Keras Model that defines the encoder—a model that takes an input image\\nand outputs z_mean, z_log_var, and a sampled point z from the normal distribu‐\\ntion defined by these parameters.\\nA summary of the encoder is shown in Table 3-4.\\nTable 3-4. Model summary of the VAE encoder\\nLayer (type)\\nOutput shape\\nParam #\\nConnected to\\nInputLayer (input)\\n(None, 32, 32, 1)\\n0\\n[]\\nConv2D (conv2d_1)\\n(None, 16, 16, 32) 320\\n[input]\\nConv2D (conv2d_2)\\n(None, 8, 8, 64)\\n18,496\\n[conv2d_1]\\nConv2D (conv2d_3)\\n(None, 4, 4, 128)\\n73,856\\n[conv2d_2]\\nFlatten (flatten)\\n(None, 2048)\\n0\\n[conv2d_3]\\nDense (z_mean)\\n(None, 2)\\n4,098\\n[flatten]\\nDense (z_log_var)\\n(None, 2)\\n4,098\\n[flatten]\\nSampling (z)\\n(None, 2)\\n0\\n[z_mean, z_log_var]\\nTotal params\\n100,868\\nTrainable params\\n100,868\\nNon-trainable params\\n0\\nThe only other part of the original autoencoder that we need to change is the loss\\nfunction.\\nThe Loss Function\\nPreviously, our loss function only consisted of the reconstruction loss between images\\nand their attempted copies after being passed through the encoder and decoder. The\\nreconstruction loss also appears in a variational autoencoder, but we now require one\\nextra component: the Kullback–Leibler (KL) divergence term.\\n80 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 108}, page_content='KL divergence is a way of measuring how much one probability distribution differs\\nfrom another. In a VAE, we want to measure how much our normal distribution with\\nparameters z_mean and z_log_var differs from a standard normal distribution. In\\nthis special case, it can be shown that the KL divergence has the following closed\\nform:\\nkl_loss = -0.5 * sum(1 + z_log_var - z_mean ^ 2 - exp(z_log_var))\\nor in mathematical notation:\\nDKL N μ, σ ∥N 0, 1\\n= −1\\n2∑1 + log σ2 −μ2 −σ2\\nThe sum is taken over all the dimensions in the latent space. kl_loss is minimized to\\n0 when z_mean = 0 and z_log_var = 0 for all dimensions. As these two terms start\\nto differ from 0, kl_loss increases.\\nIn summary, the KL divergence term penalizes the network for encoding observa‐\\ntions to z_mean and z_log_var variables that differ significantly from the parameters\\nof a standard normal distribution, namely z_mean = 0 and z_log_var = 0.\\nWhy does this addition to the loss function help?\\nFirstly, we now have a well-defined distribution that we can use for choosing points\\nin the latent space—the standard normal distribution. Secondly, since this term tries\\nto force all encoded distributions toward the standard normal distribution, there is\\nless chance that large gaps will form between point clusters. Instead, the encoder will\\ntry to use the space around the origin symmetrically and efficiently.\\nIn the original VAE paper, the loss function for a VAE was simply the addition of the\\nreconstruction loss and the KL divergence loss term. A variant on this (the β-VAE)\\nincludes a factor that weights the KL divergence to ensure that it is well balanced with\\nthe reconstruction loss. If we weight the reconstruction loss too heavily, the KL loss\\nwill not have the desired regulatory effect and we will see the same problems that we\\nexperienced with the plain autoencoder. If the KL divergence term is weighted too\\nheavily, the KL divergence loss will dominate and the reconstructed images will be\\npoor. This weighting term is one of the parameters to tune when you’re training your\\nVAE.\\nVariational Autoencoders \\n| \\n81'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 109}, page_content='Training the Variational Autoencoder\\nExample 3-13 shows how we build the overall VAE model as a subclass of the abstract\\nKeras Model class. This allows us to include the calculation of the KL divergence term\\nof the loss function in a custom train_step method.\\nExample 3-13. Training the VAE\\nclass VAE(models.Model):\\n    def __init__(self, encoder, decoder, **kwargs):\\n        super(VAE, self).__init__(**kwargs)\\n        self.encoder = encoder\\n        self.decoder = decoder\\n        self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\\n        self.reconstruction_loss_tracker = metrics.Mean(\\n            name=\"reconstruction_loss\"\\n        )\\n        self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\\n    @property\\n    def metrics(self):\\n        return [\\n            self.total_loss_tracker,\\n            self.reconstruction_loss_tracker,\\n            self.kl_loss_tracker,\\n        ]\\n    def call(self, inputs): \\n        z_mean, z_log_var, z = encoder(inputs)\\n        reconstruction = decoder(z)\\n        return z_mean, z_log_var, reconstruction\\n    def train_step(self, data): \\n        with tf.GradientTape() as tape:\\n            z_mean, z_log_var, reconstruction = self(data)\\n            reconstruction_loss = tf.reduce_mean(\\n                500\\n                * losses.binary_crossentropy(\\n                    data, reconstruction, axis=(1, 2, 3)\\n                )\\n            ) \\n            kl_loss = tf.reduce_mean(\\n                tf.reduce_sum(\\n                    -0.5\\n                    * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)),\\n                    axis = 1,\\n                )\\n            )\\n            total_loss = reconstruction_loss + kl_loss \\n        grads = tape.gradient(total_loss, self.trainable_weights)\\n82 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 110}, page_content='self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\\n        self.total_loss_tracker.update_state(total_loss)\\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\\n        self.kl_loss_tracker.update_state(kl_loss)\\n        return {m.name: m.result() for m in self.metrics}\\nvae = VAE(encoder, decoder)\\nvae.compile(optimizer=\"adam\")\\nvae.fit(\\n    train,\\n    epochs=5,\\n    batch_size=100\\n)\\nThis function describes what we would like returned what we call the VAE on a\\nparticular input image.\\nThis function describes one training step of the VAE, including the calculation of\\nthe loss function.\\nA beta value of 500 is used in the reconstruction loss.\\nThe total loss is the sum of the reconstruction loss and the KL divergence loss.\\nGradient Tape\\nTensorFlow’s Gradient Tape is a mechanism that allows the compu‐\\ntation of gradients of operations executed during a forward pass of\\na model. To use it, you need to wrap the code that performs the\\noperations you want to differentiate in a tf.GradientTape() con‐\\ntext. Once you have recorded the operations, you can compute the\\ngradient of the loss function with respect to some variables by call‐\\ning tape.gradient(). The gradients can then be used to update the\\nvariables with the optimizer.\\nThis mechanism is useful for calculating the gradient of custom\\nloss functions (as we have done here) and also for creating custom\\ntraining loops, as we shall see in Chapter 4.\\nVariational Autoencoders \\n| \\n83'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 111}, page_content='Analysis of the Variational Autoencoder\\nNow that we have trained our VAE, we can use the encoder to encode the images in\\nthe test set and plot the z_mean values in the latent space. We can also sample from a\\nstandard normal distribution to generate points in the latent space and use the\\ndecoder to decode these points back into pixel space to see how the VAE performs.\\nFigure 3-13 shows the structure of the new latent space, alongside some sampled\\npoints and their decoded images. We can immediately see several changes in how the\\nlatent space is organized.\\nFigure 3-13. The new latent space: the black dots show the z_mean value of each encoded\\nimage, while blue dots show some sampled points in the latent space (with their decoded\\nimages on the right)\\nFirstly, the KL divergence loss term ensures that the z_mean and z_log_var values of\\nthe encoded images never stray too far from a standard normal distribution. Sec‐\\nondly, there are not so many poorly formed images as the latent space is now much\\nmore continuous, due to fact that the encoder is now stochastic, rather than\\ndeterministic.\\nFinally, by coloring points in the latent space by clothing type (Figure 3-14), we can\\nsee that there is no preferential treatment of any one type. The righthand plot shows\\nthe space transformed into p-values—we can see that each color is approximately\\nequally represented. Again, it’s important to remember that the labels were not used\\nat all during training; the VAE has learned the various forms of clothing by itself in\\norder to help minimize reconstruction loss.\\n84 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 112}, page_content='Figure 3-14. The latent space of the VAE colored by clothing type\\nExploring the Latent Space\\nSo far, all of our work on autoencoders and variational autoencoders has been limited\\nto a latent space with two dimensions. This has helped us to visualize the inner work‐\\nings of a VAE on the page and understand why the small tweaks that we made to the\\narchitecture of the autoencoder helped transform it into a more powerful class of net‐\\nwork that can be used for generative modeling.\\nLet’s now turn our attention to a more complex dataset and see the amazing things\\nthat variational autoencoders can achieve when we increase the dimensionality of the\\nlatent space.\\nRunning the Code for This Example\\nThe code for this example can be found in the Jupyter notebook\\nlocated at notebooks/03_vae/03_faces/vae_faces.ipynb in the book\\nrepository.\\nThe CelebA Dataset\\nWe shall be using the CelebFaces Attributes (CelebA) dataset to train our next varia‐\\ntional autoencoder. This is a collection of over 200,000 color images of celebrity faces,\\neach annotated with various labels (e.g., wearing hat, smiling, etc.). A few examples\\nare shown in Figure 3-15.\\nExploring the Latent Space \\n| \\n85'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 113}, page_content='Figure 3-15. Some examples from the CelebA dataset (source: Liu et al., 2015)3\\nOf course, we don’t need the labels to train the VAE, but these will be useful later\\nwhen we start exploring how these features are captured in the multidimensional\\nlatent space. Once our VAE is trained, we can sample from the latent space to gener‐\\nate new examples of celebrity faces.\\nThe CelebA dataset is also available through Kaggle, so you can download the dataset\\nby running the Kaggle dataset downloader script in the book repository, as shown in\\nExample 3-14. This will save the images and accompanying metadata locally to\\nthe /data folder.\\nExample 3-14. Downloading the CelebA dataset\\nbash scripts/download_kaggle_data.sh jessicali9530 celeba-dataset\\nWe use the Keras function image_dataset_from_directory to create a TensorFlow\\nDataset pointed at the directory where the images are stored, as shown in\\nExample 3-15. This allows us to read batches of images into memory only when\\nrequired (e.g., during training), so that we can work with large datasets and not worry\\nabout having to fit the entire dataset into memory. It also resizes the images to 64 ×\\n64, interpolating between pixel values.\\n86 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 114}, page_content='Example 3-15. Preprocessing the CelebA dataset\\ntrain_data = utils.image_dataset_from_directory(\\n    \"/app/data/celeba-dataset/img_align_celeba/img_align_celeba\",\\n    labels=None,\\n    color_mode=\"rgb\",\\n    image_size=(64, 64),\\n    batch_size=128,\\n    shuffle=True,\\n    seed=42,\\n    interpolation=\"bilinear\",\\n)\\nThe original data is scaled in the range [0, 255] to denote the pixel intensity, which we\\nrescale to the range [0, 1] as shown in Example 3-16.\\nExample 3-16. Preprocessing the CelebA dataset\\ndef preprocess(img):\\n    img = tf.cast(img, \"float32\") / 255.0\\n    return img\\ntrain = train_data.map(lambda x: preprocess(x))\\nTraining the Variational Autoencoder\\nThe network architecture for the faces model is similar to the Fashion-MNIST exam‐\\nple, with a few slight differences:\\n• Our data now has three input channels (RGB) instead of one (grayscale). This\\nmeans we need to change the number of channels in the final convolutional\\ntranspose layer of the decoder to 3.\\n• We shall be using a latent space with 200 dimensions instead of 2. Since faces are\\nmuch more complex than the Fashion-MNIST images, we increase the dimen‐\\nsionality of the latent space so that the network can encode a satisfactory amount\\nof detail from the images.\\n• There are batch normalization layers after each convolutional layer to stabilize\\ntraining. Even though each batch takes a longer time to run, the number of\\nbatches required to reach the same loss is greatly reduced.\\n• We increase the β factor for the KL divergence to 2,000. This is a parameter that\\nrequires tuning; for this dataset and architecture this value was found to generate\\ngood results.\\nExploring the Latent Space \\n| \\n87'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 115}, page_content='The full architectures of the encoder and decoder are shown in Tables 3-5 and 3-6,\\nrespectively.\\nTable 3-5. Model summary of the VAE faces encoder\\nLayer (type)\\nOutput shape\\nParam #\\nConnected to\\nInputLayer (input)\\n(None, 32, 32, 3)\\n0\\n[]\\nConv2D (conv2d_1)\\n(None, 16, 16, 128) 3,584\\n[input]\\nBatchNormalization (bn_1)\\n(None, 16, 16, 128) 512\\n[conv2d_1]\\nLeakyReLU (lr_1)\\n(None, 16, 16, 128) 0\\n[bn_1]\\nConv2D (conv2d_2)\\n(None, 8, 8, 128)\\n147,584\\n[lr_1]\\nBatchNormalization (bn_2)\\n(None, 8, 8, 128)\\n512\\n[conv2d_2]\\nLeakyReLU (lr_2)\\n(None, 8, 8, 128)\\n0\\n[bn_2]\\nConv2D (conv2d_3)\\n(None, 4, 4, 128)\\n147,584\\n[lr_2]\\nBatchNormalization (bn_3)\\n(None, 4, 4, 128)\\n512\\n[conv2d_3]\\nLeakyReLU (lr_3)\\n(None, 4, 4, 128)\\n0\\n[bn_3]\\nConv2D (conv2d_4)\\n(None, 2, 2, 128)\\n147,584\\n[lr_3]\\nBatchNormalization (bn_4)\\n(None, 2, 2, 128)\\n512\\n[conv2d_4]\\nLeakyReLU (lr_4)\\n(None, 2, 2, 128)\\n0\\n[bn_4]\\nFlatten (flatten)\\n(None, 512)\\n0\\n[lr_4]\\nDense (z_mean)\\n(None, 200)\\n102,600\\n[flatten]\\nDense (z_log_var)\\n(None, 200)\\n102,600\\n[flatten]\\nSampling (z)\\n(None, 200)\\n0\\n[z_mean, z_log_var]\\nTotal params\\n653,584\\nTrainable params\\n652,560\\nNon-trainable params\\n1,024\\nTable 3-6. Model summary of the VAE faces decoder\\nLayer (type)\\nOutput shape\\nParam #\\nInputLayer\\n(None, 200)\\n0\\nDense\\n(None, 512)\\n102,912\\nBatchNormalization (None, 512)\\n2,048\\nLeakyReLU\\n(None, 512)\\n0\\nReshape\\n(None, 2, 2, 128)\\n0\\nConv2DTranspose\\n(None, 4, 4, 128)\\n147,584\\nBatchNormalization (None, 4, 4, 128)\\n512\\nLeakyReLU\\n(None, 4, 4, 128)\\n0\\nConv2DTranspose\\n(None, 8, 8, 128)\\n147,584\\n88 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 116}, page_content='Layer (type)\\nOutput shape\\nParam #\\nBatchNormalization (None, 8, 8, 128)\\n512\\nLeakyReLU\\n(None, 8, 8, 128)\\n0\\nConv2DTranspose\\n(None, 16, 16, 128) 147,584\\nBatchNormalization (None, 16, 16, 128) 512\\nLeakyReLU\\n(None, 16, 16, 128) 0\\nConv2DTranspose\\n(None, 32, 32, 128) 147,584\\nBatchNormalization (None, 32, 32, 128) 512\\nLeakyReLU\\n(None, 32, 32, 128) 0\\nConv2DTranspose\\n(None, 32, 32, 3)\\n3,459\\nTotal params\\n700,803\\nTrainable params\\n698,755\\nNon-trainable params\\n2,048\\nAfter around five epochs of training, our VAE should be able to produce novel\\nimages of celebrity faces!\\nAnalysis of the Variational Autoencoder\\nFirst, let’s take a look at a sample of reconstructed faces. The top row in Figure 3-16\\nshows the original images and the bottom row shows the reconstructions once they\\nhave passed through the encoder and decoder.\\nFigure 3-16. Reconstructed faces, after passing through the encoder and decoder\\nWe can see that the VAE has successfully captured the key features of each face—the\\nangle of the head, the hairstyle, the expression, etc. Some of the fine detail is missing,\\nbut it is important to remember that the aim of building variational autoencoders\\nisn’t to achieve perfect reconstruction loss. Our end goal is to sample from the latent\\nspace in order to generate new faces.\\nFor this to be possible we must check that the distribution of points in the latent\\nspace approximately resembles a multivariate standard normal distribution. If we see\\nany dimensions that are significantly different from a standard normal distribution,\\nExploring the Latent Space \\n| \\n89'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 117}, page_content='we should probably reduce the reconstruction loss factor, since the KL divergence\\nterm isn’t having enough effect.\\nThe first 50 dimensions in our latent space are shown in Figure 3-17. There aren’t any\\ndistributions that stand out as being significantly different from the standard normal,\\nso we can move on to generating some faces!\\nFigure 3-17. Distributions of points for the first 50 dimensions in the latent space\\nGenerating New Faces\\nTo generate new faces, we can use the code in Example 3-17.\\nExample 3-17. Generating new faces from the latent space\\ngrid_width, grid_height = (10,3)\\nz_sample = np.random.normal(size=(grid_width * grid_height, 200)) \\nreconstructions = decoder.predict(z_sample) \\nfig = plt.figure(figsize=(18, 5))\\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\\nfor i in range(grid_width * grid_height):\\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\\n    ax.axis(\"off\")\\n    ax.imshow(reconstructions[i, :, :]) \\nSample 30 points from a standard multivariate normal distribution with 200\\ndimensions.\\nDecode the sampled points.\\nPlot the images!\\n90 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 118}, page_content='The output is shown in Figure 3-18.\\nFigure 3-18. New generated faces\\nAmazingly, the VAE is able to take the set of points that we sampled from a standard\\nnormal distribution and convert each into a convincing image of a person’s face. This\\nis our first glimpse of the true power of generative models!\\nNext, let’s see if we can start to use the latent space to perform some interesting oper‐\\nations on generated images.\\nLatent Space Arithmetic\\nOne benefit of mapping images into a lower-dimensional latent space is that we can\\nperform arithmetic on vectors in this latent space that has a visual analogue when\\ndecoded back into the original image domain.\\nFor example, suppose we want to take an image of somebody who looks sad and give\\nthem a smile. To do this we first need to find a vector in the latent space that points in\\nthe direction of increasing smile. Adding this vector to the encoding of the original\\nimage in the latent space will give us a new point which, when decoded, should give\\nus a more smiley version of the original image.\\nSo how can we find the smile vector? Each image in the CelebA dataset is labeled with\\nattributes, one of which is Smiling. If we take the average position of encoded images\\nin the latent space with the attribute Smiling and subtract the average position of\\nencoded images that do not have the attribute Smiling, we will obtain the vector that\\npoints in the direction of Smiling, which is exactly what we need.\\nConceptually, we are performing the following vector arithmetic in the latent space,\\nwhere alpha is a factor that determines how much of the feature vector is added or\\nsubtracted:\\nz_new = z + alpha * feature_vector\\nExploring the Latent Space \\n| \\n91'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 119}, page_content='Let’s see this in action. Figure 3-19 shows several images that have been encoded into\\nthe latent space. We then add or subtract multiples of a certain vector (e.g., Smiling,\\nBlack_Hair, Eyeglasses, Young, Male, Blond_Hair) to obtain different versions of the\\nimage, with only the relevant feature changed.\\nFigure 3-19. Adding and subtracting features to and from faces\\nIt is remarkable that even though we are moving the point a significantly large dis‐\\ntance in the latent space, the core image remains approximately the same, except for\\nthe one feature that we want to manipulate. This demonstrates the power of varia‐\\ntional autoencoders for capturing and adjusting high-level features in images.\\nMorphing Between Faces\\nWe can use a similar idea to morph between two faces. Imagine two points in the\\nlatent space, A and B, that represent two images. If you started at point A and walked\\ntoward point B in a straight line, decoding each point on the line as you went, you\\nwould see a gradual transition from the starting face to the end face.\\nMathematically, we are traversing a straight line, which can be described by the fol‐\\nlowing equation:\\nz_new = z_A * (1- alpha) + z_B * alpha\\nHere, alpha is a number between 0 and 1 that determines how far along the line we\\nare, away from point A.\\n92 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 120}, page_content='Figure 3-20 shows this process in action. We take two images, encode them into the\\nlatent space, and then decode points along the straight line between them at regular\\nintervals.\\nFigure 3-20. Morphing between two faces\\nIt is worth noting the smoothness of the transition—even where there are multiple\\nfeatures to change simultaneously (e.g., removal of glasses, hair color, gender), the\\nVAE manages to achieve this fluidly, showing that the latent space of the VAE is truly\\na continuous space that can be traversed and explored to generate a multitude of dif‐\\nferent human faces.\\nSummary\\nIn this chapter we have seen how variational autoencoders are a powerful tool in the\\ngenerative modeling toolbox. We started by exploring how plain autoencoders can be\\nused to map high-dimensional images into a low-dimensional latent space, so that\\nhigh-level features can be extracted from the individually uninformative pixels. How‐\\never, we quickly found that there were some drawbacks to using plain autoencoders\\nas a generative model—sampling from the learned latent space was problematic, for\\nexample.\\nVariational autoencoders solve these problems by introducing randomness into the\\nmodel and constraining how points in the latent space are distributed. We saw that\\nwith a few minor adjustments, we can transform our autoencoder into a variational\\nautoencoder, thus giving it the power to be a true generative model.\\nSummary \\n| \\n93'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 121}, page_content='Finally, we applied our new technique to the problem of face generation and saw how\\nwe can simply decode points from a standard normal distribution to generate new\\nfaces. Moreover, by performing vector arithmetic within the latent space, we can\\nachieve some amazing effects, such as face morphing and feature manipulation.\\nIn the next chapter, we shall explore a different kind of model that remains a popular\\nchoice for generative image modeling: the generative adversarial network.\\nReferences\\n1. Diederik P. Kingma and Max Welling, “Auto-Encoding Variational Bayes,” Decem‐\\nber 20, 2013, https://arxiv.org/abs/1312.6114.\\n2. Vincent Dumoulin and Francesco Visin, “A Guide to Convolution Arithmetic for\\nDeep Learning,” January 12, 2018, https://arxiv.org/abs/1603.07285.\\n3. Ziwei Liu et al., “Large-Scale CelebFaces Attributes (CelebA) Dataset,” 2015, http://\\nmmlab.ie.cuhk.edu.hk/projects/CelebA.html.\\n94 \\n| \\nChapter 3: Variational Autoencoders'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 122}, page_content='CHAPTER 4\\nGenerative Adversarial Networks\\nChapter Goals\\nIn this chapter you will:\\n• Learn about the architectural design of a generative adversarial network (GAN).\\n• Build and train a deep convolutional GAN (DCGAN) from scratch using Keras.\\n• Use the DCGAN to generate new images.\\n• Understand some of the common problems faced when training a DCGAN.\\n• Learn how the Wasserstein GAN (WGAN) architecture addresses these\\nproblems.\\n• Understand additional enhancements that can be made to the WGAN, such as\\nincorporating a gradient penalty (GP) term into the loss function.\\n• Build a WGAN-GP from scratch using Keras.\\n• Use the WGAN-GP to generate faces.\\n• Learn how a conditional GAN (CGAN) gives you the ability to condition gener‐\\nated output on a given label.\\n• Build and train a CGAN in Keras and use it to manipulate a generated image.\\nIn 2014, Ian Goodfellow et al. presented a paper entitled “Generative Adversarial\\nNets”1 at the Neural Information Processing Systems conference (NeurIPS) in Mon‐\\ntreal. The introduction of generative adversarial networks (or GANs, as they are more\\ncommonly known) is now regarded as a key turning point in the history of generative\\nmodeling, as the core ideas presented in this paper have spawned some of the most\\nsuccessful and impressive generative models ever created.\\n95'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 123}, page_content='This chapter will first lay out the theoretical underpinning of GANs, then we will see\\nhow to build our own GAN using Keras.\\nIntroduction\\nLet’s start with a short story to illustrate some of the fundamental concepts used in\\nthe GAN training process.\\nBrickki Bricks and the Forgers\\nIt’s your first day at your new job as head of quality control for Brickki, a company\\nthat specializes in producing high-quality building blocks of all shapes and sizes\\n(Figure 4-1).\\nFigure 4-1. The production line of a company making bricks of many different shapes\\nand sizes (created with Midjourney)\\nYou are immediately alerted to a problem with some of the items coming off the pro‐\\nduction line. A competitor has started to make counterfeit copies of Brickki bricks\\nand has found a way to mix them into the bags received by your customers. You\\ndecide to become an expert at telling the difference between the counterfeit bricks\\nand the real thing, so that you can intercept the forged bricks on the production line\\nbefore they are given to customers. Over time, by listening to customer feedback, you\\ngradually become more adept at spotting the fakes.\\nThe forgers are not happy about this—they react to your improved detection abilities\\nby making some changes to their forgery process so that now, the difference between\\nthe real bricks and the fakes is even harder for you to spot.\\n96 \\n| \\nChapter 4: Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 124}, page_content='Not one to give up, you retrain yourself to identify the more sophisticated fakes and\\ntry to keep one step ahead of the forgers. This process continues, with the forgers iter‐\\natively updating their brick creation technologies while you try to become increas‐\\ningly more accomplished at intercepting their fakes.\\nWith every week that passes, it becomes more and more difficult to tell the difference\\nbetween the real Brickki bricks and those created by the forgers. It seems that this\\nsimple game of cat and mouse is enough to drive significant improvement in both the\\nquality of the forgery and the quality of the detection.\\nThe story of Brickki bricks and the forgers describes the training process of a genera‐\\ntive adversarial network.\\nA GAN is a battle between two adversaries, the generator and the discriminator. The\\ngenerator tries to convert random noise into observations that look as if they have\\nbeen sampled from the original dataset, and the discriminator tries to predict whether\\nan observation comes from the original dataset or is one of the generator’s forgeries.\\nExamples of the inputs and outputs to the two networks are shown in Figure 4-2.\\nFigure 4-2. Inputs and outputs of the two networks in a GAN\\nAt the start of the process, the generator outputs noisy images and the discriminator\\npredicts randomly. The key to GANs lies in how we alternate the training of the two\\nnetworks, so that as the generator becomes more adept at fooling the discriminator,\\nthe discriminator must adapt in order to maintain its ability to correctly identify\\nwhich observations are fake. This drives the generator to find new ways to fool the\\ndiscriminator, and so the cycle continues.\\nDeep Convolutional GAN (DCGAN)\\nTo see this in action, let’s start building our first GAN in Keras, to generate pictures of\\nbricks.\\nWe will be closely following one of the first major papers on GANs, “Unsupervised\\nRepresentation Learning with Deep Convolutional Generative Adversarial\\nDeep Convolutional GAN (DCGAN) \\n| \\n97'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 125}, page_content='Networks.”2 In this 2015 paper, the authors show how to build a deep convolutional\\nGAN to generate realistic images from a variety of datasets. They also introduce sev‐\\neral changes that significantly improve the quality of the generated images.\\nRunning the Code for This Example\\nThe code for this example can be found in the Jupyter notebook\\nlocated at notebooks/04_gan/01_dcgan/dcgan.ipynb in the book\\nrepository.\\nThe Bricks Dataset\\nFirst, you’ll need to download the training data. We’ll be using the Images of LEGO\\nBricks dataset that is available through Kaggle. This is a computer-rendered collec‐\\ntion of 40,000 photographic images of 50 different toy bricks, taken from multiple\\nangles. Some example images of Brickki products are shown in Figure 4-3.\\nFigure 4-3. Examples of images from the Bricks dataset\\nYou can download the dataset by running the Kaggle dataset downloader script in the\\nbook repository, as shown in Example 4-1. This will save the images and accompany‐\\ning metadata locally to the /data folder.\\nExample 4-1. Downloading the Bricks dataset\\nbash scripts/download_kaggle_data.sh joosthazelzet lego-brick-images\\nWe use the Keras function image_dataset_from_directory to create a TensorFlow\\nDataset pointed at the directory where the images are stored, as shown in\\nExample 4-2. This allows us to read batches of images into memory only when\\nrequired (e.g., during training), so that we can work with large datasets and not worry\\nabout having to fit the entire dataset into memory. It also resizes the images to 64 ×\\n64, interpolating between pixel values.\\nExample 4-2. Creating a TensorFlow Dataset from image files in a directory\\ntrain_data = utils.image_dataset_from_directory(\\n    \"/app/data/lego-brick-images/dataset/\",\\n    labels=None,\\n    color_mode=\"grayscale\",\\n    image_size=(64, 64),\\n    batch_size=128,\\n98 \\n| \\nChapter 4: Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 126}, page_content='shuffle=True,\\n    seed=42,\\n    interpolation=\"bilinear\",\\n)\\nThe original data is scaled in the range [0, 255] to denote the pixel intensity. When\\ntraining GANs we rescale the data to the range [–1, 1] so that we can use the tanh\\nactivation function on the final layer of the generator, which tends to provide stron‐\\nger gradients than the sigmoid function (Example 4-3).\\nExample 4-3. Preprocessing the Bricks dataset\\ndef preprocess(img):\\n    img = (tf.cast(img, \"float32\") - 127.5) / 127.5\\n    return img\\ntrain = train_data.map(lambda x: preprocess(x))\\nLet’s now take a look at how we build the discriminator.\\nThe Discriminator\\nThe goal of the discriminator is to predict if an image is real or fake. This is a super‐\\nvised image classification problem, so we can use a similar architecture to those we\\nworked with in Chapter 2: stacked convolutional layers, with a single output node.\\nThe full architecture of the discriminator we will be building is shown in Table 4-1.\\nTable 4-1. Model summary of the discriminator\\nLayer (type)\\nOutput shape\\nParam #\\nInputLayer\\n(None, 64, 64, 1)\\n0\\nConv2D\\n(None, 32, 32, 64)\\n1,024\\nLeakyReLU\\n(None, 32, 32, 64)\\n0\\nDropout\\n(None, 32, 32, 64)\\n0\\nConv2D\\n(None, 16, 16, 128) 131,072\\nBatchNormalization (None, 16, 16, 128) 512\\nLeakyReLU\\n(None, 16, 16, 128) 0\\nDropout\\n(None, 16, 16, 128) 0\\nConv2D\\n(None, 8, 8, 256)\\n524,288\\nBatchNormalization (None, 8, 8, 256)\\n1,024\\nLeakyReLU\\n(None, 8, 8, 256)\\n0\\nDropout\\n(None, 8, 8, 256)\\n0\\nConv2D\\n(None, 4, 4, 512)\\n2,097,152\\nBatchNormalization (None, 4, 4, 512)\\n2,048\\nDeep Convolutional GAN (DCGAN) \\n| \\n99'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 127}, page_content='Layer (type)\\nOutput shape\\nParam #\\nLeakyReLU\\n(None, 4, 4, 512)\\n0\\nDropout\\n(None, 4, 4, 512)\\n0\\nConv2D\\n(None, 1, 1, 1)\\n8,192\\nFlatten\\n(None, 1)\\n0\\nTotal params\\n2,765,312\\nTrainable params\\n2,763,520\\nNon-trainable params\\n1,792\\nThe Keras code to build the discriminator is provided in Example 4-4.\\nExample 4-4. The discriminator\\ndiscriminator_input = layers.Input(shape=(64, 64, 1)) \\nx = layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\", use_bias = False)(\\n    discriminator_input\\n) \\nx = layers.LeakyReLU(0.2)(x)\\nx = layers.Dropout(0.3)(x)\\nx = layers.Conv2D(\\n    128, kernel_size=4, strides=2, padding=\"same\", use_bias = False\\n)(x)\\nx = layers.BatchNormalization(momentum = 0.9)(x)\\nx = layers.LeakyReLU(0.2)(x)\\nx = layers.Dropout(0.3)(x)\\nx = layers.Conv2D(\\n    256, kernel_size=4, strides=2, padding=\"same\", use_bias = False\\n)(x)\\nx = layers.BatchNormalization(momentum = 0.9)(x)\\nx = layers.LeakyReLU(0.2)(x)\\nx = layers.Dropout(0.3)(x)\\nx = layers.Conv2D(\\n    512, kernel_size=4, strides=2, padding=\"same\", use_bias = False\\n)(x)\\nx = layers.BatchNormalization(momentum = 0.9)(x)\\nx = layers.LeakyReLU(0.2)(x)\\nx = layers.Dropout(0.3)(x)\\nx = layers.Conv2D(\\n    1,\\n    kernel_size=4,\\n    strides=1,\\n    padding=\"valid\",\\n    use_bias = False,\\n    activation = \\'sigmoid\\'\\n)(x)\\ndiscriminator_output = layers.Flatten()(x) \\n100 \\n| \\nChapter 4: Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 128}, page_content='discriminator = models.Model(discriminator_input, discriminator_output) \\nDefine the Input layer of the discriminator (the image).\\nStack Conv2D layers on top of each other, with BatchNormalization, LeakyReLU\\nactivation, and Dropout layers sandwiched in between.\\nFlatten the last convolutional layer—by this point, the shape of the tensor is 1 × 1\\n× 1, so there is no need for a final Dense layer.\\nThe Keras model that defines the discriminator—a model that takes an input\\nimage and outputs a single number between 0 and 1.\\nNotice how we use a stride of 2 in some of the Conv2D layers to reduce the spatial\\nshape of the tensor as it passes through the network (64 in the original image, then\\n32, 16, 8, 4, and finally 1), while increasing the number of channels (1 in the grayscale\\ninput image, then 64, 128, 256, and finally 512), before collapsing to a single\\nprediction.\\nWe use a sigmoid activation on the final Conv2D layer to output a number between 0\\nand 1.\\nThe Generator\\nNow let’s build the generator. The input to the generator will be a vector drawn from\\na multivariate standard normal distribution. The output is an image of the same size\\nas an image in the original training data.\\nThis description may remind you of the decoder in a variational autoencoder. In fact,\\nthe generator of a GAN fulfills exactly the same purpose as the decoder of a VAE:\\nconverting a vector in the latent space to an image. The concept of mapping from a\\nlatent space back to the original domain is very common in generative modeling, as it\\ngives us the ability to manipulate vectors in the latent space to change high-level fea‐\\ntures of images in the original domain.\\nThe architecture of the generator we will be building is shown in Table 4-2.\\nTable 4-2. Model summary of the generator\\nLayer (type)\\nOutput shape\\nParam #\\nInputLayer\\n(None, 100)\\n0\\nReshape\\n(None, 1, 1, 100)\\n0\\nConv2DTranspose\\n(None, 4, 4, 512)\\n819,200\\nBatchNormalization (None, 4, 4, 512)\\n2,048\\nDeep Convolutional GAN (DCGAN) \\n| \\n101'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 129}, page_content='Layer (type)\\nOutput shape\\nParam #\\nReLU\\n(None, 4, 4, 512)\\n0\\nConv2DTranspose\\n(None, 8, 8, 256)\\n2,097,152\\nBatchNormalization (None, 8, 8, 256)\\n1,024\\nReLU\\n(None, 8, 8, 256)\\n0\\nConv2DTranspose\\n(None, 16, 16, 128) 524,288\\nBatchNormalization (None, 16, 16, 128) 512\\nReLU\\n(None, 16, 16, 128) 0\\nConv2DTranspose\\n(None, 32, 32, 64)\\n131,072\\nBatchNormalization (None, 32, 32, 64)\\n256\\nReLU\\n(None, 32, 32, 64)\\n0\\nConv2DTranspose\\n(None, 64, 64, 1)\\n1,024\\nTotal params\\n3,576,576\\nTrainable params\\n3,574,656\\nNon-trainable params\\n1,920\\nThe code for building the generator is given in Example 4-5.\\nExample 4-5. The generator\\ngenerator_input = layers.Input(shape=(100,)) \\nx = layers.Reshape((1, 1, 100))(generator_input) \\nx = layers.Conv2DTranspose(\\n    512, kernel_size=4, strides=1, padding=\"valid\", use_bias = False\\n)(x) \\nx = layers.BatchNormalization(momentum=0.9)(x)\\nx = layers.LeakyReLU(0.2)(x)\\nx = layers.Conv2DTranspose(\\n    256, kernel_size=4, strides=2, padding=\"same\", use_bias = False\\n)(x)\\nx = layers.BatchNormalization(momentum=0.9)(x)\\nx = layers.LeakyReLU(0.2)(x)\\nx = layers.Conv2DTranspose(\\n    128, kernel_size=4, strides=2, padding=\"same\", use_bias = False\\n)(x)\\nx = layers.BatchNormalization(momentum=0.9)(x)\\nx = layers.LeakyReLU(0.2)(x)\\nx = layers.Conv2DTranspose(\\n    64, kernel_size=4, strides=2, padding=\"same\", use_bias = False\\n)(x)\\nx = layers.BatchNormalization(momentum=0.9)(x)\\nx = layers.LeakyReLU(0.2)(x)\\ngenerator_output = layers.Conv2DTranspose(\\n    1,\\n102 \\n| \\nChapter 4: Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 130}, page_content='kernel_size=4,\\n    strides=2,\\n    padding=\"same\",\\n    use_bias = False,\\n    activation = \\'tanh\\'\\n)(x) \\ngenerator = models.Model(generator_input, generator_output) \\nDefine the Input layer of the generator—a vector of length 100.\\nWe use a Reshape layer to give a 1 × 1 × 100 tensor, so that we can start applying\\nconvolutional transpose operations.\\nWe pass this through four Conv2DTranspose layers, with BatchNormalization\\nand LeakyReLU layers sandwiched in between.\\nThe final Conv2DTranspose layer uses a tanh activation function to transform the\\noutput to the range [–1, 1], to match the original image domain.\\nThe Keras model that defines the generator—a model that accepts a vector of\\nlength 100 and outputs a tensor of shape [64, 64, 1].\\nNotice how we use a stride of 2 in some of the Conv2DTranspose layers to increase the\\nspatial shape of the tensor as it passes through the network (1 in the original vector,\\nthen 4, 8, 16, 32, and finally 64), while decreasing the number of channels (512 then\\n256, 128, 64, and finally 1 to match the grayscale output).\\nUpsampling Versus Conv2DTranspose\\nAn alternative to using Conv2DTranspose layers is to instead use an UpSampling2D\\nlayer followed by a normal Conv2D layer with stride 1, as shown in Example 4-6.\\nExample 4-6. Upsampling example\\nx = layers.UpSampling2D(size = 2)(x)\\nx = layers.Conv2D(256, kernel_size=4, strides=1, padding=\"same\")(x)\\nThe UpSampling2D layer simply repeats each row and column of its input in order to\\ndouble the size. The Conv2D layer with stride 1 then performs the convolution opera‐\\ntion. It is a similar idea to convolutional transpose, but instead of filling the gaps\\nbetween pixels with zeros, upsampling just repeats the existing pixel values.\\nIt has been shown that the Conv2DTranspose method can lead to artifacts, or small\\ncheckerboard patterns in the output image (see Figure 4-4) that spoil the quality of\\nthe output. However, they are still used in many of the most impressive GANs in the\\nDeep Convolutional GAN (DCGAN) \\n| \\n103'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 131}, page_content='literature and have proven to be a powerful tool in the deep learning practitioner’s\\ntoolbox.\\nFigure 4-4. Artifacts when using convolutional transpose layers (source: Odena et al.,\\n2016)3\\nBoth of these methods—UpSampling2D + Conv2D and Conv2DTranspose—are accepta‐\\nble ways to transform back to the original image domain. It really is a case of testing\\nboth methods in your own problem setting and seeing which produces better results.\\nTraining the DCGAN\\nAs we have seen, the architectures of the generator and discriminator in a DCGAN\\nare very simple and not so different from the VAE models that we looked at in Chap‐\\nter 3. The key to understanding GANs lies in understanding the training process for\\nthe generator and discriminator.\\nWe can train the discriminator by creating a training set where some of the images\\nare real observations from the training set and some are fake outputs from the gener‐\\nator. We then treat this as a supervised learning problem, where the labels are 1 for\\nthe real images and 0 for the fake images, with binary cross-entropy as the loss\\nfunction.\\nHow should we train the generator? We need to find a way of scoring each generated\\nimage so that it can optimize toward high-scoring images. Luckily, we have a discrim‐\\ninator that does exactly that! We can generate a batch of images and pass these\\nthrough the discriminator to get a score for each image. The loss function for the\\ngenerator is then simply the binary cross-entropy between these probabilities and a\\n104 \\n| \\nChapter 4: Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 132}, page_content='vector of ones, because we want to train the generator to produce images that the dis‐\\ncriminator thinks are real.\\nCrucially, we must alternate the training of these two networks, making sure that we\\nonly update the weights of one network at a time. For example, during the generator\\ntraining process, only the generator’s weights are updated. If we allowed the discrimi‐\\nnator’s weights to change as well, the discriminator would just adjust so that it is more\\nlikely to predict the generated images to be real, which is not the desired outcome.\\nWe want generated images to be predicted close to 1 (real) because the generator is\\nstrong, not because the discriminator is weak.\\nA diagram of the training process for the discriminator and generator is shown in\\nFigure 4-5.\\nFigure 4-5. Training the DCGAN—gray boxes indicate that the weights are frozen dur‐\\ning training\\nKeras provides us with the ability to create a custom train_step function to imple‐\\nment this logic. Example 4-7 shows the full DCGAN model class.\\nDeep Convolutional GAN (DCGAN) \\n| \\n105'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 133}, page_content='Example 4-7. Compiling the DCGAN\\nclass DCGAN(models.Model):\\n    def __init__(self, discriminator, generator, latent_dim):\\n        super(DCGAN, self).__init__()\\n        self.discriminator = discriminator\\n        self.generator = generator\\n        self.latent_dim = latent_dim\\n    def compile(self, d_optimizer, g_optimizer):\\n        super(DCGAN, self).compile()\\n        self.loss_fn = losses.BinaryCrossentropy() \\n        self.d_optimizer = d_optimizer\\n        self.g_optimizer = g_optimizer\\n        self.d_loss_metric = metrics.Mean(name=\"d_loss\")\\n        self.g_loss_metric = metrics.Mean(name=\"g_loss\")\\n    @property\\n    def metrics(self):\\n        return [self.d_loss_metric, self.g_loss_metric]\\n    def train_step(self, real_images):\\n        batch_size = tf.shape(real_images)[0]\\n        random_latent_vectors = tf.random.normal(\\n            shape=(batch_size, self.latent_dim)\\n        ) \\n        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\\n            generated_images = self.generator(\\n                random_latent_vectors, training = True\\n            ) \\n            real_predictions = self.discriminator(real_images, training = True) \\n            fake_predictions = self.discriminator(\\n                generated_images, training = True\\n            ) \\n            real_labels = tf.ones_like(real_predictions)\\n            real_noisy_labels = real_labels + 0.1 * tf.random.uniform(\\n                tf.shape(real_predictions)\\n            )\\n            fake_labels = tf.zeros_like(fake_predictions)\\n            fake_noisy_labels = fake_labels - 0.1 * tf.random.uniform(\\n                tf.shape(fake_predictions)\\n            )\\n            d_real_loss = self.loss_fn(real_noisy_labels, real_predictions)\\n            d_fake_loss = self.loss_fn(fake_noisy_labels, fake_predictions)\\n            d_loss = (d_real_loss + d_fake_loss) / 2.0 \\n            g_loss = self.loss_fn(real_labels, fake_predictions) \\n        gradients_of_discriminator = disc_tape.gradient(\\n106 \\n| \\nChapter 4: Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 134}, page_content='d_loss, self.discriminator.trainable_variables\\n        )\\n        gradients_of_generator = gen_tape.gradient(\\n            g_loss, self.generator.trainable_variables\\n        )\\n        self.d_optimizer.apply_gradients(\\n            zip(gradients_of_discriminator, discriminator.trainable_variables)\\n        ) \\n        self.g_optimizer.apply_gradients(\\n            zip(gradients_of_generator, generator.trainable_variables)\\n        )\\n        self.d_loss_metric.update_state(d_loss)\\n        self.g_loss_metric.update_state(g_loss)\\n        return {m.name: m.result() for m in self.metrics}\\ndcgan = DCGAN(\\n    discriminator=discriminator, generator=generator, latent_dim=100\\n)\\ndcgan.compile(\\n    d_optimizer=optimizers.Adam(\\n        learning_rate=0.0002, beta_1 = 0.5, beta_2 = 0.999\\n    ),\\n    g_optimizer=optimizers.Adam(\\n        learning_rate=0.0002, beta_1 = 0.5, beta_2 = 0.999\\n    ),\\n)\\ndcgan.fit(train, epochs=300)\\nThe loss function for the generator and discriminator is BinaryCrossentropy.\\nTo train the network, first sample a batch of vectors from a multivariate standard\\nnormal distribution.\\nNext, pass these through the generator to produce a batch of generated images.\\nNow ask the discriminator to predict the realness of the batch of real images…\\n…and the batch of generated images.\\nThe discriminator loss is the average binary cross-entropy across both the real\\nimages (with label 1) and the fake images (with label 0).\\nThe generator loss is the binary cross-entropy between the discriminator predic‐\\ntions for the generated images and a label of 1.\\nDeep Convolutional GAN (DCGAN) \\n| \\n107'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 135}, page_content='Update the weights of the discriminator and generator separately.\\nThe discriminator and generator are constantly fighting for dominance, which can\\nmake the DCGAN training process unstable. Ideally, the training process will find an\\nequilibrium that allows the generator to learn meaningful information from the dis‐\\ncriminator and the quality of the images will start to improve. After enough epochs,\\nthe discriminator tends to end up dominating, as shown in Figure 4-6, but this may\\nnot be a problem as the generator may have already learned to produce sufficiently\\nhigh-quality images by this point.\\nFigure 4-6. Loss and accuracy of the discriminator and generator during training\\nAdding Noise to the Labels\\nA useful trick when training GANs is to add a small amount of ran‐\\ndom noise to the training labels. This helps to improve the stability\\nof the training process and sharpen the generated images. This\\nlabel smoothing acts as way to tame the discriminator, so that it is\\npresented with a more challenging task and doesn’t overpower the\\ngenerator.\\n108 \\n| \\nChapter 4: Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 136}, page_content='Analysis of the DCGAN\\nBy observing images produced by the generator at specific epochs during training\\n(Figure 4-7), it is clear that the generator is becoming increasingly adept at producing\\nimages that could have been drawn from the training set.\\nFigure 4-7. Output from the generator at specific epochs during training\\nIt is somewhat miraculous that a neural network is able to convert random noise into\\nsomething meaningful. It is worth remembering that we haven’t provided the model\\nwith any additional features beyond the raw pixels, so it has to work out high-level\\nconcepts such as how to draw shadows, cuboids, and circles entirely by itself.\\nAnother requirement of a successful generative model is that it doesn’t only repro‐\\nduce images from the training set. To test this, we can find the image from the\\ntraining set that is closest to a particular generated example. A good measure for dis‐\\ntance is the L1 distance, defined as:\\ndef compare_images(img1, img2):\\n    return np.mean(np.abs(img1 - img2))\\nFigure 4-8 shows the closest observations in the training set for a selection of gener‐\\nated images. We can see that while there is some degree of similarity between the gen‐\\nerated images and the training set, they are not identical. This shows that the\\ngenerator has understood these high-level features and can generate examples that\\nare distinct from those it has already seen.\\nDeep Convolutional GAN (DCGAN) \\n| \\n109'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 137}, page_content='Figure 4-8. Closest matches of generated images from the training set\\nGAN Training: Tips and Tricks\\nWhile GANs are a major breakthrough for generative modeling, they are also notori‐\\nously difficult to train. We will explore some of the most common problems and\\nchallenges encountered when training GANs in this section, alongside potential solu‐\\ntions. In the next section, we will look at some more fundamental adjustments to the\\nGAN framework that we can make to remedy many of these problems.\\nDiscriminator overpowers the generator\\nIf the discriminator becomes too strong, the signal from the loss function becomes\\ntoo weak to drive any meaningful improvements in the generator. In the worst-case\\nscenario, the discriminator perfectly learns to separate real images from fake images\\nand the gradients vanish completely, leading to no training whatsoever, as can be seen\\nin Figure 4-9.\\n110 \\n| \\nChapter 4: Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 138}, page_content='Figure 4-9. Example output when the discriminator overpowers the generator\\nIf you find your discriminator loss function collapsing, you need to find ways to\\nweaken the discriminator. Try the following suggestions:\\n• Increase the rate parameter of the Dropout layers in the discriminator to\\ndampen the amount of information that flows through the network.\\n• Reduce the learning rate of the discriminator.\\n• Reduce the number of convolutional filters in the discriminator.\\n• Add noise to the labels when training the discriminator.\\n• Flip the labels of some images at random when training the discriminator.\\nGenerator overpowers the discriminator\\nIf the discriminator is not powerful enough, the generator will find ways to easily\\ntrick the discriminator with a small sample of nearly identical images. This is known\\nas mode collapse.\\nFor example, suppose we were to train the generator over several batches without\\nupdating the discriminator in between. The generator would be inclined to find a sin‐\\ngle observation (also known as a mode) that always fools the discriminator and would\\nstart to map every point in the latent input space to this image. Moreover, the gradi‐\\nents of the loss function would collapse to near 0, so it wouldn’t be able to recover\\nfrom this state.\\nEven if we then tried to retrain the discriminator to stop it being fooled by this one\\npoint, the generator would simply find another mode that fools the discriminator,\\nsince it has already become numb to its input and therefore has no incentive to diver‐\\nsify its output.\\nDeep Convolutional GAN (DCGAN) \\n| \\n111'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 139}, page_content='The effect of mode collapse can be seen in Figure 4-10.\\nFigure 4-10. Example of mode collapse when the generator overpowers the discriminator\\nIf you find that your generator is suffering from mode collapse, you can try strength‐\\nening the discriminator using the opposite suggestions to those listed in the previous\\nsection. Also, you can try reducing the learning rate of both networks and increasing\\nthe batch size.\\nUninformative loss\\nSince the deep learning model is compiled to minimize the loss function, it would be\\nnatural to think that the smaller the loss function of the generator, the better the qual‐\\nity of the images produced. However, since the generator is only graded against the\\ncurrent discriminator and the discriminator is constantly improving, we cannot com‐\\npare the loss function evaluated at different points in the training process. Indeed, in\\nFigure 4-6, the loss function of the generator actually increases over time, even\\nthough the quality of the images is clearly improving. This lack of correlation\\nbetween the generator loss and image quality sometimes makes GAN training diffi‐\\ncult to monitor.\\nHyperparameters\\nAs we have seen, even with simple GANs, there are a large number of hyperparame‐\\nters to tune. As well as the overall architecture of both the discriminator and the gen‐\\nerator, there are the parameters that govern batch normalization, dropout, learning\\nrate, activation layers, convolutional filters, kernel size, striding, batch size, and latent\\nspace size to consider. GANs are highly sensitive to very slight changes in all of these\\nparameters, and finding a set of parameters that works is often a case of educated trial\\nand error, rather than following an established set of guidelines.\\n112 \\n| \\nChapter 4: Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 140}, page_content='This is why it is important to understand the inner workings of the GAN and know\\nhow to interpret the loss function—so that you can identify sensible adjustments to\\nthe hyperparameters that might improve the stability of the model.\\nTackling GAN challenges\\nIn recent years, several key advancements have drastically improved the overall sta‐\\nbility of GAN models and diminished the likelihood of some of the problems listed\\nearlier, such as mode collapse.\\nIn the remainder of this chapter we shall examine the Wasserstein GAN with Gradi‐\\nent Penalty (WGAN-GP), which makes several key adjustments to the GAN frame‐\\nwork we have explored thus far to improve the stability and quality of the image\\ngeneration process.\\nWasserstein GAN with Gradient Penalty (WGAN-GP)\\nIn this section we will build a WGAN-GP to generate faces from the CelebA dataset\\nthat we utilized in Chapter 3.\\nRunning the Code for This Example\\nThe code for this example can be found in the Jupyter notebook\\nlocated at notebooks/04_gan/02_wgan_gp/wgan_gp.ipynb in the\\nbook repository.\\nThe code has been adapted from the excellent WGAN-GP tutorial\\ncreated by Aakash Kumar Nain, available on the Keras website.\\nThe Wasserstein GAN (WGAN), introduced in a 2017 paper by Arjovsky et al.,4 was\\none of the first big steps toward stabilizing GAN training. With a few changes, the\\nauthors were able to show how to train GANs that have the following two properties\\n(quoted from the paper):\\n• A meaningful loss metric that correlates with the generator’s convergence and\\nsample quality\\n• Improved stability of the optimization process\\nSpecifically, the paper introduces the Wasserstein loss function for both the discrimi‐\\nnator and the generator. Using this loss function instead of binary cross-entropy\\nresults in a more stable convergence of the GAN.\\nIn this section we’ll define the Wasserstein loss function and then see what other\\nchanges we need to make to the model architecture and training process to incorpo‐\\nrate our new loss function.\\nWasserstein GAN with Gradient Penalty (WGAN-GP) \\n| \\n113'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 141}, page_content='You can find the full model class in the Jupyter notebook located at chapter05/wgan-\\ngp/faces/train.ipynb in the book repository.\\nWasserstein Loss\\nLet’s first remind ourselves of the definition of binary cross-entropy loss—the func‐\\ntion that we are currently using to train the discriminator and generator of the GAN\\n(Equation 4-1).\\nEquation 4-1. Binary cross-entropy loss\\n−1\\nn ∑\\ni = 1\\nn\\nyi log pi + 1 −yi log 1 −pi\\nTo train the GAN discriminator D, we calculate the loss when comparing predictions\\nfor real images pi = D xi  to the response yi = 1 and predictions for generated images\\npi = D G zi  to the response yi = 0. Therefore, for the GAN discriminator, minimiz‐\\ning the loss function can be written as shown in Equation 4-2.\\nEquation 4-2. GAN discriminator loss minimization\\nmin\\nD\\n−�x ∼pX log D x\\n+ �z ∼pZ log 1 −D G z\\nTo train the GAN generator G, we calculate the loss when comparing predictions for\\ngenerated images pi = D G zi  to the response yi = 1. Therefore, for the GAN gener‐\\nator, minimizing the loss function can be written as shown in Equation 4-3.\\nEquation 4-3. GAN generator loss minimization\\nmin\\nG\\n−�z ∼pZ log D G z\\nNow let’s compare this to the Wasserstein loss function.\\nFirst, the Wasserstein loss requires that we use yi = 1 and yi = –1 as labels, rather than\\n1 and 0. We also remove the sigmoid activation from the final layer of the discrimina‐\\ntor, so that predictions pi are no longer constrained to fall in the range [0, 1]\\nbut instead can now be any number in the range (−∞, ∞). For this reason, the\\ndiscriminator in a WGAN is usually referred to as a critic that outputs a score rather\\nthan a probability.\\nThe Wasserstein loss function is defined as follows:\\n114 \\n| \\nChapter 4: Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 142}, page_content='−1\\nn ∑\\ni = 1\\nn\\nyipi\\nTo train the WGAN critic D, we calculate the loss when comparing predictions for\\nreal images pi = D xi  to the response yi = 1 and predictions for generated images\\npi = D G zi  to the response yi = –1. Therefore, for the WGAN critic, minimizing\\nthe loss function can be written as follows:\\nmin\\nD\\n−�x ∼pX D x\\n−�z ∼pZ D G z\\nIn other words, the WGAN critic tries to maximize the difference between its predic‐\\ntions for real images and generated images.\\nTo train the WGAN generator, we calculate the loss when comparing predictions for\\ngenerated images pi = D G zi  to the response yi = 1. Therefore, for the WGAN gen‐\\nerator, minimizing the loss function can be written as follows:\\nmin\\nG\\n−�z ∼pZ D G z\\nIn other words, the WGAN generator tries to produce images that are scored as\\nhighly as possible by the critic (i.e., the critic is fooled into thinking they are real).\\nThe Lipschitz Constraint\\nIt may surprise you that we are now allowing the critic to output any number in the\\nrange (−∞, ∞), rather than applying a sigmoid function to restrict the output to the\\nusual [0, 1] range. The Wasserstein loss can therefore be very large, which is unset‐\\ntling—usually, large numbers in neural networks are to be avoided!\\nIn fact, the authors of the WGAN paper show that for the Wasserstein loss function\\nto work, we also need to place an additional constraint on the critic. Specifically, it is\\nrequired that the critic is a 1-Lipschitz continuous function. Let’s pick this apart to\\nunderstand what it means in more detail.\\nThe critic is a function D that converts an image into a prediction. We say that this\\nfunction is 1-Lipschitz if it satisfies the following inequality for any two input images,\\nx1 and x2:\\nD x1 −D x2\\nx1 −x2\\n≤1\\nWasserstein GAN with Gradient Penalty (WGAN-GP) \\n| \\n115'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 143}, page_content='Here, x1 −x2  is the average pixelwise absolute difference between two images and\\nD x1 −D x2  is the absolute difference between the critic predictions. Essentially,\\nwe require a limit on the rate at which the predictions of the critic can change\\nbetween two images (i.e., the absolute value of the gradient must be at most 1 every‐\\nwhere). We can see this applied to a Lipschitz continuous 1D function in Figure 4-11\\n—at no point does the line enter the cone, wherever you place the cone on the line. In\\nother words, there is a limit on the rate at which the line can rise or fall at any point.\\nFigure 4-11. A Lipschitz continuous function (source: Wikipedia)\\nFor those who want to delve deeper into the mathematical rationale\\nbehind why the Wasserstein loss only works when this constraint is\\nenforced, Jonathan Hui offers an excellent explanation.\\nEnforcing the Lipschitz Constraint\\nIn the original WGAN paper, the authors show how it is possible to enforce the Lip‐\\nschitz constraint by clipping the weights of the critic to lie within a small range,\\n[–0.01, 0.01], after each training batch.\\nOne of the criticisms of this approach is that the capacity of the critic to learn is\\ngreatly diminished, since we are clipping its weights. In fact, even in the original\\nWGAN paper the authors write, “Weight clipping is a clearly terrible way to enforce a\\nLipschitz constraint.” A strong critic is pivotal to the success of a WGAN, since\\nwithout accurate gradients, the generator cannot learn how to adapt its weights to\\nproduce better samples.\\nTherefore, other researchers have looked for alternative ways to enforce the Lipschitz\\nconstraint and improve the capacity of the WGAN to learn complex features. One\\nsuch method is the Wasserstein GAN with Gradient Penalty.\\nIn the paper introducing this variant,5 the authors show how the Lipschitz constraint\\ncan be enforced directly by including a gradient penalty term in the loss function for\\n116 \\n| \\nChapter 4: Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 144}, page_content='the critic that penalizes the model if the gradient norm deviates from 1. This results\\nin a far more stable training process.\\nIn the next section, we’ll see how to build this extra term into the loss function for our\\ncritic.\\nThe Gradient Penalty Loss\\nFigure 4-12 is a diagram of the training process for the critic of a WGAN-GP. If we\\ncompare this to the original discriminator training process from Figure 4-5, we can\\nsee that the key addition is the gradient penalty loss included as part of the overall\\nloss function, alongside the Wasserstein loss from the real and fake images.\\nFigure 4-12. The WGAN-GP critic training process\\nThe gradient penalty loss measures the squared difference between the norm of the\\ngradient of the predictions with respect to the input images and 1. The model will\\nnaturally be inclined to find weights that ensure the gradient penalty term is mini‐\\nmized, thereby encouraging the model to conform to the Lipschitz constraint.\\nWasserstein GAN with Gradient Penalty (WGAN-GP) \\n| \\n117'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 145}, page_content='It is intractable to calculate this gradient everywhere during the training process, so\\ninstead the WGAN-GP evaluates the gradient at only a handful of points. To ensure a\\nbalanced mix, we use a set of interpolated images that lie at randomly chosen points\\nalong lines connecting the batch of real images to the batch of fake images pairwise,\\nas shown in Figure 4-13.\\nFigure 4-13. Interpolating between images\\nIn Example 4-8, we show how the gradient penalty is calculated in code.\\nExample 4-8. The gradient penalty loss function\\ndef gradient_penalty(self, batch_size, real_images, fake_images):\\n    alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0) \\n    diff = fake_images - real_images\\n    interpolated = real_images + alpha * diff \\n    with tf.GradientTape() as gp_tape:\\n        gp_tape.watch(interpolated)\\n        pred = self.critic(interpolated, training=True) \\n    grads = gp_tape.gradient(pred, [interpolated])[0] \\n    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3])) \\n    gp = tf.reduce_mean((norm - 1.0) ** 2) \\n    return gp\\nEach image in the batch gets a random number, between 0 and 1, stored as the\\nvector alpha.\\nA set of interpolated images is calculated.\\nThe critic is asked to score each of these interpolated images.\\nThe gradient of the predictions is calculated with respect to the input images.\\n118 \\n| \\nChapter 4: Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 146}, page_content='The L2 norm of this vector is calculated.\\nThe function returns the average squared distance between the L2 norm and 1.\\nTraining the WGAN-GP\\nA key benefit of using the Wasserstein loss function is that we no longer need to\\nworry about balancing the training of the critic and the generator—in fact, when\\nusing the Wasserstein loss, the critic must be trained to convergence before updating\\nthe generator, to ensure that the gradients for the generator update are accurate. This\\nis in contrast to a standard GAN, where it is important not to let the discriminator get\\ntoo strong.\\nTherefore, with Wasserstein GANs, we can simply train the critic several times\\nbetween generator updates, to ensure it is close to convergence. A typical ratio used is\\nthree to five critic updates per generator update.\\nWe have now introduced both of the key concepts behind the WGAN-GP—the Was‐\\nserstein loss and the gradient penalty term that is included in the critic loss function.\\nThe training step of the WGAN model that incorporates all of these ideas is shown in\\nExample 4-9.\\nExample 4-9. Training the WGAN-GP\\ndef train_step(self, real_images):\\n    batch_size = tf.shape(real_images)[0]\\n    for i in range(3): \\n        random_latent_vectors = tf.random.normal(\\n            shape=(batch_size, self.latent_dim)\\n        )\\n        with tf.GradientTape() as tape:\\n            fake_images = self.generator(\\n                random_latent_vectors, training = True\\n            )\\n            fake_predictions = self.critic(fake_images, training = True)\\n            real_predictions = self.critic(real_images, training = True)\\n            c_wass_loss = tf.reduce_mean(fake_predictions) - tf.reduce_mean(\\n                real_predictions\\n            ) \\n            c_gp = self.gradient_penalty(\\n                batch_size, real_images, fake_images\\n            ) \\n            c_loss = c_wass_loss + c_gp * self.gp_weight \\n        c_gradient = tape.gradient(c_loss, self.critic.trainable_variables)\\nWasserstein GAN with Gradient Penalty (WGAN-GP) \\n| \\n119'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 147}, page_content='self.c_optimizer.apply_gradients(\\n            zip(c_gradient, self.critic.trainable_variables)\\n        ) \\n    random_latent_vectors = tf.random.normal(\\n        shape=(batch_size, self.latent_dim)\\n    )\\n    with tf.GradientTape() as tape:\\n        fake_images = self.generator(random_latent_vectors, training=True)\\n        fake_predictions = self.critic(fake_images, training=True)\\n        g_loss = -tf.reduce_mean(fake_predictions) \\n    gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\\n    self.g_optimizer.apply_gradients(\\n        zip(gen_gradient, self.generator.trainable_variables)\\n    ) \\n    self.c_loss_metric.update_state(c_loss)\\n    self.c_wass_loss_metric.update_state(c_wass_loss)\\n    self.c_gp_metric.update_state(c_gp)\\n    self.g_loss_metric.update_state(g_loss)\\n    return {m.name: m.result() for m in self.metrics}\\nPerform three critic updates.\\nCalculate the Wasserstein loss for the critic—the difference between the average\\nprediction for the fake images and the real images.\\nCalculate the gradient penalty term (see Example 4-8).\\nThe critic loss function is a weighted sum of the Wasserstein loss and the gradi‐\\nent penalty.\\nUpdate the weights of the critic.\\nCalculate the Wasserstein loss for the generator.\\nUpdate the weights of the generator.\\nBatch Normalization in a WGAN-GP\\nOne last consideration we should note before training a WGAN-\\nGP is that batch normalization shouldn’t be used in the critic. This\\nis because batch normalization creates correlation between images\\nin the same batch, which makes the gradient penalty loss less effec‐\\ntive. Experiments have shown that WGAN-GPs can still produce\\nexcellent results even without batch normalization in the critic.\\n120 \\n| \\nChapter 4: Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 148}, page_content='We have now covered all of the key differences between a standard GAN and a\\nWGAN-GP. To recap:\\n• A WGAN-GP uses the Wasserstein loss.\\n• The WGAN-GP is trained using labels of 1 for real and –1 for fake.\\n• There is no sigmoid activation in the final layer of the critic.\\n• Include a gradient penalty term in the loss function for the critic.\\n• Train the critic multiple times for each update of the generator.\\n• There are no batch normalization layers in the critic.\\nAnalysis of the WGAN-GP\\nLet’s take a look at some example outputs from the generator, after 25 epochs of train‐\\ning (Figure 4-14).\\nFigure 4-14. WGAN-GP face examples\\nThe model has learned the significant high-level attributes of a face, and there is no\\nsign of mode collapse.\\nWe can also see how the loss functions of the model evolve over time (Figure 4-15)—\\nthe loss functions of both the critic and generator are highly stable and convergent.\\nIf we compare the WGAN-GP output to the VAE output from the previous chapter,\\nwe can see that the GAN images are generally sharper—especially the definition\\nbetween the hair and the background. This is true in general; VAEs tend to produce\\nsofter images that blur color boundaries, whereas GANs are known to produce\\nsharper, more well-defined images.\\nWasserstein GAN with Gradient Penalty (WGAN-GP) \\n| \\n121'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 149}, page_content='Figure 4-15. WGAN-GP loss curves: the critic loss (epoch_c_loss) is broken down into\\nthe Wasserstein loss (epoch_c_wass) and the gradient penalty loss (epoch_c_gp)\\nIt is also true that GANs are generally more difficult to train than VAEs and take\\nlonger to reach a satisfactory quality. However, many state-of-the-art generative mod‐\\nels today are GAN-based, as the rewards for training large-scale GANs on GPUs over\\na longer period of time are significant.\\nConditional GAN (CGAN)\\nSo far in this chapter, we have built GANs that are able to generate realistic images\\nfrom a given training set. However, we haven’t been able to control the type of image\\nwe would like to generate—for example, a male or female face, or a large or small\\nbrick. We can sample a random point from the latent space, but we do not have the\\nability to easily understand what kind of image will be produced given the choice of\\nlatent variable.\\nIn the final part of this chapter we shall turn our attention to building a GAN where\\nwe are able to control the output—a so called conditional GAN. This idea, first intro‐\\nduced in “Conditional Generative Adversarial Nets” by Mirza and Osindero in 2014,6\\nis a relatively simple extension to the GAN architecture.\\n122 \\n| \\nChapter 4: Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 150}, page_content='Running the Code for This Example\\nThe code for this example can be found in the Jupyter notebook\\nlocated at notebooks/04_gan/03_cgan/cgan.ipynb in the book\\nrepository.\\nThe code has been adapted from the excellent CGAN tutorial cre‐\\nated by Sayak Paul, available on the Keras website.\\nCGAN Architecture\\nIn this example, we will condition our CGAN on the blond hair attribute of the faces\\ndataset. That is, we will be able to explicitly specify whether we want to generate an\\nimage with blond hair or not. This label is provided as part of the CelebA dataset.\\nThe high-level CGAN architecture is shown in Figure 4-16.\\nFigure 4-16. Inputs and outputs of the generator and critic in a CGAN\\nThe key difference between a standard GAN and a CGAN is that in a CGAN we pass\\nin extra information to the generator and critic relating to the label. In the generator,\\nthis is simply appended to the latent space sample as a one-hot encoded vector. In the\\ncritic, we add the label information as extra channels to the RGB image. We do this\\nby repeating the one-hot encoded vector to fill the same shape as the input images.\\nCGANs work because the critic now has access to extra information regarding the\\ncontent of the image, so the generator must ensure that its output agrees with the\\nprovided label, in order to keep fooling the critic. If the generator produced perfect\\nConditional GAN (CGAN) \\n| \\n123'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 151}, page_content='images that disagreed with the image label the critic would be able to tell that they\\nwere fake simply because the images and labels did not match.\\nIn our example, our one-hot encoded label will have length 2,\\nbecause there are two classes (Blonde and Not Blond). However,\\nyou can have as many labels as you like—for example, you could\\ntrain a CGAN on the Fashion-MNIST dataset to output one of the\\n10 different fashion items, by incorporating a one-hot encoded\\nlabel vector of length 10 into the input of the generator and 10\\nadditional one-hot encoded label channels into the input of the\\ncritic.\\nThe only change we need to make to the architecture is to concatenate the label infor‐\\nmation to the existing inputs of the generator and the critic, as shown in\\nExample 4-10.\\nExample 4-10. Input layers in the CGAN\\ncritic_input = layers.Input(shape=(64, 64, 3)) \\nlabel_input = layers.Input(shape=(64, 64, 2))\\nx = layers.Concatenate(axis = -1)([critic_input, label_input])\\n...\\ngenerator_input = layers.Input(shape=(32,)) \\nlabel_input = layers.Input(shape=(2,))\\nx = layers.Concatenate(axis = -1)([generator_input, label_input])\\nx = layers.Reshape((1,1, 34))(x)\\n...\\nThe image channels and label channels are passed in separately to the critic and\\nconcatenated.\\nThe latent vector and the label classes are passed in separately to the generator\\nand concatenated before being reshaped.\\nTraining the CGAN\\nWe must also make some changes to the train_step of the CGAN to match the new\\ninput formats of the generator and critic, as shown in Example 4-11.\\nExample 4-11. The train_step of the CGAN\\ndef train_step(self, data):\\n    real_images, one_hot_labels = data \\n    image_one_hot_labels = one_hot_labels[:, None, None, :] \\n    image_one_hot_labels = tf.repeat(\\n124 \\n| \\nChapter 4: Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 152}, page_content='image_one_hot_labels, repeats=64, axis = 1\\n    )\\n    image_one_hot_labels = tf.repeat(\\n        image_one_hot_labels, repeats=64, axis = 2\\n    )\\n    batch_size = tf.shape(real_images)[0]\\n    for i in range(self.critic_steps):\\n        random_latent_vectors = tf.random.normal(\\n            shape=(batch_size, self.latent_dim)\\n        )\\n        with tf.GradientTape() as tape:\\n            fake_images = self.generator(\\n                [random_latent_vectors, one_hot_labels], training = True\\n            ) \\n            fake_predictions = self.critic(\\n                [fake_images, image_one_hot_labels], training = True\\n            ) \\n            real_predictions = self.critic(\\n                [real_images, image_one_hot_labels], training = True\\n            )\\n            c_wass_loss = tf.reduce_mean(fake_predictions) - tf.reduce_mean(\\n                real_predictions\\n            )\\n            c_gp = self.gradient_penalty(\\n                batch_size, real_images, fake_images, image_one_hot_labels\\n            ) \\n            c_loss = c_wass_loss + c_gp * self.gp_weight\\n        c_gradient = tape.gradient(c_loss, self.critic.trainable_variables)\\n        self.c_optimizer.apply_gradients(\\n            zip(c_gradient, self.critic.trainable_variables)\\n        )\\n    random_latent_vectors = tf.random.normal(\\n        shape=(batch_size, self.latent_dim)\\n    )\\n    with tf.GradientTape() as tape:\\n        fake_images = self.generator(\\n            [random_latent_vectors, one_hot_labels], training=True\\n        ) \\n        fake_predictions = self.critic(\\n            [fake_images, image_one_hot_labels], training=True\\n        )\\n        g_loss = -tf.reduce_mean(fake_predictions)\\n    gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\\nConditional GAN (CGAN) \\n| \\n125'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 153}, page_content='self.g_optimizer.apply_gradients(\\n        zip(gen_gradient, self.generator.trainable_variables)\\n    )\\nThe images and labels are unpacked from the input data.\\nThe one-hot encoded vectors are expanded to one-hot encoded images that have\\nthe same spatial size as the input images (64 × 64).\\nThe generator is now fed with a list of two inputs—the random latent vectors and\\nthe one-hot encoded label vectors.\\nThe critic is now fed with a list of two inputs—the fake/real images and the one-\\nhot encoded label channels.\\nThe gradient penalty function also requires the one-hot encoded label channels\\nto be passed through as it uses the critic.\\nThe changes made to the critic training step also apply to the generator training\\nstep.\\nAnalysis of the CGAN\\nWe can control the CGAN output by passing a particular one-hot encoded label into\\nthe input of the generator. For example, to generate a face with nonblond hair, we\\npass in the vector [1, 0]. To generate a face with blond hair, we pass in the vector\\n[0, 1].\\nThe output from the CGAN can be seen in Figure 4-17. Here, we keep the random\\nlatent vectors the same across the examples and change only the conditional label\\nvector. It is clear that the CGAN has learned to use the label vector to control only the\\nhair color attribute of the images. It is impressive that the rest of the image barely\\nchanges—this is proof that GANs are able to organize points in the latent space in\\nsuch a way that individual features can be decoupled from each other.\\n126 \\n| \\nChapter 4: Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 154}, page_content='Figure 4-17. Output from the CGAN when the Blond and Not Blond vectors are\\nappended to the latent sample\\nIf labels are available for your dataset, it is generally a good idea to\\ninclude them as input to your GAN even if you do not necessarily\\nneed to condition the generated output on the label, as they tend to\\nimprove the quality of images generated. You can think of the\\nlabels as just a highly informative extension to the pixel input.\\nSummary\\nIn this chapter we explored three different generative adversarial network (GAN)\\nmodels: the deep convolutional GAN (DCGAN), the more sophisticated Wasserstein\\nGAN with Gradient Penalty (WGAN-GP), and the conditional GAN (CGAN).\\nAll GANs are characterized by a generator versus discriminator (or critic) architec‐\\nture, with the discriminator trying to “spot the difference” between real and fake\\nimages and the generator aiming to fool the discriminator. By balancing how these\\ntwo adversaries are trained, the GAN generator can gradually learn how to produce\\nsimilar observations to those in the training set.\\nWe first saw how to train a DCGAN to generate images of toy bricks. It was able to\\nlearn how to realistically represent 3D objects as images, including accurate represen‐\\ntations of shadow, shape, and texture. We also explored the different ways in which\\nGAN training can fail, including mode collapse and vanishing gradients.\\nSummary \\n| \\n127'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 155}, page_content='We then explored how the Wasserstein loss function remedied many of these prob‐\\nlems and made GAN training more predictable and reliable. The WGAN-GP places\\nthe 1-Lipschitz requirement at the heart of the training process by including a term in\\nthe loss function to pull the gradient norm toward 1.\\nWe applied the WGAN-GP to the problem of face generation and saw how by simply\\nchoosing points from a standard normal distribution, we can generate new faces. This\\nsampling process is very similar to a VAE, though the faces produced by a GAN are\\nquite different—often sharper, with greater distinction between different parts of the\\nimage.\\nFinally, we built a CGAN that allowed us to control the type of image that is gener‐\\nated. This works by passing in the label as input to the critic and generator, thereby\\ngiving the network the additional information it needs in order to condition the gen‐\\nerated output on a given label.\\nOverall, we have seen how the GAN framework is extremely flexible and able to be\\nadapted to many interesting problem domains. In particular, GANs have driven sig‐\\nnificant progress in the field of image generation with many interesting extensions to\\nthe underlying framework, as we shall see in Chapter 10.\\nIn the next chapter, we will explore a different family of generative model that is ideal\\nfor modeling sequential data—autoregressive models.\\nReferences\\n1. Ian J. Goodfellow et al., “Generative Adversarial Nets,” June 10, 2014, https://\\narxiv.org/abs/1406.2661\\n2. Alec Radford et al., “Unsupervised Representation Learning with Deep Convolu‐\\ntional Generative Adversarial Networks,” January 7, 2016, https://arxiv.org/abs/\\n1511.06434.\\n3. Augustus Odena et al., “Deconvolution and Checkerboard Artifacts,” October 17,\\n2016, https://distill.pub/2016/deconv-checkerboard.\\n4. Martin Arjovsky et al., “Wasserstein GAN,” January 26, 2017, https://arxiv.org/abs/\\n1701.07875.\\n5. Ishaan Gulrajani et al., “Improved Training of Wasserstein GANs,” March 31, 2017,\\nhttps://arxiv.org/abs/1704.00028.\\n6. Mehdi Mirza and Simon Osindero, “Conditional Generative Adversarial Nets,”\\nNovember 6, 2014, https://arxiv.org/abs/1411.1784.\\n128 \\n| \\nChapter 4: Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 156}, page_content='CHAPTER 5\\nAutoregressive Models\\nChapter Goals\\nIn this chapter you will:\\n• Learn why autoregressive models are well suited to generating sequential data\\nsuch as text.\\n• Learn how to process and tokenize text data.\\n• Learn about the architectural design of recurrent neural networks (RNNs).\\n• Build and train a long short-term memory network (LSTM) from scratch using\\nKeras.\\n• Use the LSTM to generate new text.\\n• Learn about other variations of RNNs, including gated recurrent units (GRUs)\\nand bidirectional cells.\\n• Understand how image data can be treated as a sequence of pixels.\\n• Learn about the architectural design of a PixelCNN.\\n• Build a PixelCNN from scratch using Keras.\\n• Use the PixelCNN to generate images.\\nSo far, we have explored two different families of generative models that have both\\ninvolved latent variables—variational autoencoders (VAEs) and generative adversarial\\nnetworks (GANs). In both cases, a new variable is introduced with a distribution that\\nis easy to sample from and the model learns how to decode this variable back into the\\noriginal domain.\\n129'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 157}, page_content='We will now turn our attention to autoregressive models—a family of models that sim‐\\nplify the generative modeling problem by treating it as a sequential process. Autore‐\\ngressive models condition predictions on previous values in the sequence, rather than\\non a latent random variable. Therefore, they attempt to explicitly model the data-\\ngenerating distribution rather than an approximation of it (as in the case of VAEs).\\nIn this chapter we shall explore two different autoregressive models: long short-term\\nmemory networks and PixelCNN. We will apply the LSTM to text data and the Pix‐\\nelCNN to image data. We will cover another highly successful autoregressive model,\\nthe Transformer, in detail in Chapter 9.\\nIntroduction\\nTo understand how an LSTM works, we will first pay a visit to a strange prison, where\\nthe inmates have formed a literary society…\\nThe Literary Society for Troublesome Miscreants\\nEdward Sopp hated his job as a prison warden. He spent his days watching over the\\nprisoners and had no time to follow his true passion of writing short stories. He was\\nrunning low on inspiration and needed to find a way to generate new content.\\nOne day, he came up with a brilliant idea that would allow him to produce new works\\nof fiction in his style, while also keeping the inmates occupied—he would get the\\ninmates to collectively write the stories for him! He branded the new society the Liter‐\\nary Society for Troublesome Miscreants, or LSTM (Figure 5-1).\\nFigure 5-1. A large cell of prisoners reading books (created with Midjourney)\\n130 \\n| \\nChapter 5: Autoregressive Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 158}, page_content='The prison is particularly strange because it only consists of one large cell, containing\\n256 prisoners. Each prisoner has an opinion on how Edward’s current story should\\ncontinue. Every day, Edward posts the latest word from his novel into the cell, and it\\nis the job of the inmates to individually update their opinions on the current state of\\nthe story, based on the new word and the opinions of the inmates from the previous\\nday.\\nEach prisoner uses a specific thought process to update their own opinion, which\\ninvolves balancing information from the new incoming word and other prisoners’\\nopinions with their own prior beliefs. First, they decide how much of yesterday’s\\nopinion they wish to forget, taking into account the information from the new word\\nand the opinions of other prisoners in the cell. They also use this information to form\\nnew thoughts and decide to what extent they want to mix these into the old beliefs\\nthat they have chosen to carry forward from the previous day. This then forms the\\nprisoner’s new opinion for the day.\\nHowever, the prisoners are secretive and don’t always tell their fellow inmates all of\\ntheir opinions. They each also use the latest chosen word and the opinions of the\\nother inmates to decide how much of their opinion they wish to disclose.\\nWhen Edward wants the cell to generate the next word in the sequence, the prisoners\\neach tell their disclosable opinions to the guard at the door, who combines this infor‐\\nmation to ultimately decide on the next word to be appended to the end of the novel.\\nThis new word is then fed back into the cell, and the process continues until the full\\nstory is completed.\\nTo train the inmates and the guard, Edward feeds short sequences of words that he\\nhas written previously into the cell and monitors whether the inmates’ chosen next\\nword is correct. He updates them on their accuracy, and gradually they begin to learn\\nhow to write stories in his own unique style.\\nAfter many iterations of this process, Edward finds that the system has become quite\\naccomplished at generating realistic-looking text. Satisfied with the results, he pub‐\\nlishes a collection of the generated tales in his new book, entitled E. Sopp’s Fables.\\nThe story of Mr. Sopp and his crowdsourced fables is an analogy for one of the most\\nnotorious autoregressive techniques for sequential data such as text: the long short-\\nterm memory network.\\nLong Short-Term Memory Network (LSTM)\\nAn LSTM is a particular type of recurrent neural network (RNN). RNNs contain a\\nrecurrent layer (or cell) that is able to handle sequential data by making its own out‐\\nput at a particular timestep form part of the input to the next timestep.\\nLong Short-Term Memory Network (LSTM) \\n| \\n131'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 159}, page_content=\"When RNNs were first introduced, recurrent layers were very simple and consisted\\nsolely of a tanh operator that ensured that the information passed between timesteps\\nwas scaled between –1 and 1. However, this approach was shown to suffer from the\\nvanishing gradient problem and didn’t scale well to long sequences of data.\\nLSTM cells were first introduced in 1997 in a paper by Sepp Hochreiter and Jürgen\\nSchmidhuber.1 In the paper, the authors describe how LSTMs do not suffer from the\\nsame vanishing gradient problem experienced by vanilla RNNs and can be trained on\\nsequences that are hundreds of timesteps long. Since then, the LSTM architecture has\\nbeen adapted and improved, and variations such as gated recurrent units (discussed\\nlater in this chapter) are now widely utilized and available as layers in Keras.\\nLSTMs have been applied to a wide range of problems involving sequential data,\\nincluding time series forecasting, sentiment analysis, and audio classification. In this\\nchapter we will be using LSTMs to tackle the challenge of text generation.\\nRunning the Code for This Example\\nThe code for this example can be found in the Jupyter notebook\\nlocated at notebooks/05_autoregressive/01_lstm/lstm.ipynb in the\\nbook repository.\\nThe Recipes Dataset\\nWe’ll be using the Epicurious Recipes dataset that is available through Kaggle. This is\\na set of over 20,000 recipes, with accompanying metadata such as nutritional infor‐\\nmation and ingredient lists.\\nYou can download the dataset by running the Kaggle dataset downloader script in the\\nbook repository, as shown in Example 5-1. This will save the recipes and accompany‐\\ning metadata locally to the /data folder.\\nExample 5-1. Downloading the Epicurious Recipe dataset\\nbash scripts/download_kaggle_data.sh hugodarwood epirecipes\\nExample 5-2 shows how the data can be loaded and filtered so that only recipes with a\\ntitle and a description remain. An example of a recipe text string is given in\\nExample 5-3.\\nExample 5-2. Loading the data\\nwith open('/app/data/epirecipes/full_format_recipes.json') as json_data:\\n    recipe_data = json.load(json_data)\\nfiltered_data = [\\n132 \\n| \\nChapter 5: Autoregressive Models\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 160}, page_content=\"'Recipe for ' + x['title']+ ' | ' + ' '.join(x['directions'])\\n    for x in recipe_data\\n    if 'title' in x\\n    and x['title'] is not None\\n    and 'directions' in x\\n    and x['directions'] is not None\\n]\\nExample 5-3. A text string from the Recipes dataset\\nRecipe for Ham Persillade with Mustard Potato Salad and Mashed Peas  | Chop enough\\nparsley leaves to measure 1 tablespoon; reserve. Chop remaining leaves and stems\\nand simmer with broth and garlic in a small saucepan, covered, 5 minutes.\\nMeanwhile, sprinkle gelatin over water in a medium bowl and let soften 1 minute.\\nStrain broth through a fine-mesh sieve into bowl with gelatin and stir to dissolve.\\nSeason with salt and pepper. Set bowl in an ice bath and cool to room temperature,\\nstirring. Toss ham with reserved parsley and divide among jars. Pour gelatin on top\\nand chill until set, at least 1 hour. Whisk together mayonnaise, mustard, vinegar,\\n1/4 teaspoon salt, and 1/4 teaspoon pepper in a large bowl. Stir in celery,\\ncornichons, and potatoes. Pulse peas with marjoram, oil, 1/2 teaspoon pepper, and\\n1/4 teaspoon salt in a food processor to a coarse mash. Layer peas, then potato\\nsalad, over ham.\\nBefore taking a look at how to build an LSTM network in Keras, we must first take a\\nquick detour to understand the structure of text data and how it is different from the\\nimage data that we have seen so far in this book.\\nWorking with Text Data\\nThere are several key differences between text and image data that mean that many of\\nthe methods that work well for image data are not so readily applicable to text data.\\nIn particular:\\n• Text data is composed of discrete chunks (either characters or words), whereas\\npixels in an image are points in a continuous color spectrum. We can easily make\\na green pixel more blue, but it is not obvious how we should go about making the\\nword cat more like the word dog, for example. This means we can easily apply\\nbackpropagation to image data, as we can calculate the gradient of our loss func‐\\ntion with respect to individual pixels to establish the direction in which pixel col‐\\nors should be changed to minimize the loss. With discrete text data, we can’t\\nobviously apply backpropagation in the same way, so we need to find a way\\naround this problem.\\n• Text data has a time dimension but no spatial dimension, whereas image data has\\ntwo spatial dimensions but no time dimension. The order of words is highly\\nimportant in text data and words wouldn’t make sense in reverse, whereas images\\ncan usually be flipped without affecting the content. Furthermore, there are often\\nLong Short-Term Memory Network (LSTM) \\n| \\n133\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 161}, page_content='long-term sequential dependencies between words that need to be captured by\\nthe model: for example, the answer to a question or carrying forward the context\\nof a pronoun. With image data, all pixels can be processed simultaneously.\\n• Text data is highly sensitive to small changes in the individual units (words or\\ncharacters). Image data is generally less sensitive to changes in individual pixel\\nunits—a picture of a house would still be recognizable as a house even if some\\npixels were altered—but with text data, changing even a few words can drastically\\nalter the meaning of the passage, or make it nonsensical. This makes it very diffi‐\\ncult to train a model to generate coherent text, as every word is vital to the overall\\nmeaning of the passage.\\n• Text data has a rules-based grammatical structure, whereas image data doesn’t\\nfollow set rules about how the pixel values should be assigned. For example, it\\nwouldn’t make grammatical sense in any context to write “The cat sat on the hav‐\\ning.” There are also semantic rules that are extremely difficult to model; it\\nwouldn’t make sense to say “I am in the beach,” even though grammatically, there\\nis nothing wrong with this statement.\\nAdvances in Text-Based Generative Deep Learning\\nUntil recently, most of the most sophisticated generative deep\\nlearning models have focused on image data, because many of the\\nchallenges presented in the preceding list were beyond the reach of\\neven the most advanced techniques. However, in the last five years\\nastonishing progress has been made in the field of text-based gen‐\\nerative deep learning, thanks to the introduction of the Trans‐\\nformer model architecture, which we will explore in Chapter 9.\\nWith these points in mind, let’s now take a look at the steps we need to take in order\\nto get the text data into the right shape to train an LSTM network.\\nTokenization\\nThe first step is to clean up and tokenize the text. Tokenization is the process of split‐\\nting the text up into individual units, such as words or characters.\\nHow you tokenize your text will depend on what you are trying to achieve with your\\ntext generation model. There are pros and cons to using both word and character\\ntokens, and your choice will affect how you need to clean the text prior to modeling\\nand the output from your model.\\n134 \\n| \\nChapter 5: Autoregressive Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 162}, page_content='If you use word tokens:\\n• All text can be converted to lowercase, to ensure capitalized words at the start of\\nsentences are tokenized the same way as the same words appearing in the middle\\nof a sentence. In some cases, however, this may not be desirable; for example,\\nsome proper nouns, such as names or places, may benefit from remaining capi‐\\ntalized so that they are tokenized independently.\\n• The text vocabulary (the set of distinct words in the training set) may be very\\nlarge, with some words appearing very sparsely or perhaps only once. It may be\\nwise to replace sparse words with a token for unknown word, rather than includ‐\\ning them as separate tokens, to reduce the number of weights the neural network\\nneeds to learn.\\n• Words can be stemmed, meaning that they are reduced to their simplest form, so\\nthat different tenses of a verb remained tokenized together. For example, browse,\\nbrowsing, browses, and browsed would all be stemmed to brows.\\n• You will need to either tokenize the punctuation, or remove it altogether.\\n• Using word tokenization means that the model will never be able to predict\\nwords outside of the training vocabulary.\\nIf you use character tokens:\\n• The model may generate sequences of characters that form new words outside of\\nthe training vocabulary—this may be desirable in some contexts, but not in\\nothers.\\n• Capital letters can either be converted to their lowercase counterparts, or remain\\nas separate tokens.\\n• The vocabulary is usually much smaller when using character tokenization. This\\nis beneficial for model training speed as there are fewer weights to learn in the\\nfinal output layer.\\nFor this example, we’ll use lowercase word tokenization, without word stemming.\\nWe’ll also tokenize punctuation marks, as we would like the model to predict when it\\nshould end sentences or use commas, for example.\\nThe code in Example 5-4 cleans and tokenizes the text.\\nExample 5-4. Tokenization\\ndef pad_punctuation(s):\\n    s = re.sub(f\"([{string.punctuation}])\", r\\' \\\\1 \\', s)\\n    s = re.sub(\\' +\\', \\' \\', s)\\n    return s\\nLong Short-Term Memory Network (LSTM) \\n| \\n135'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 163}, page_content='text_data = [pad_punctuation(x) for x in filtered_data] \\ntext_ds = tf.data.Dataset.from_tensor_slices(text_data).batch(32).shuffle(1000) \\nvectorize_layer = layers.TextVectorization( \\n    standardize = \\'lower\\',\\n    max_tokens = 10000,\\n    output_mode = \"int\",\\n    output_sequence_length = 200 + 1,\\n)\\nvectorize_layer.adapt(text_ds) \\nvocab = vectorize_layer.get_vocabulary() \\nPad the punctuation marks, to treat them as separate words.\\nConvert to a TensorFlow Dataset.\\nCreate a Keras TextVectorization layer to convert text to lowercase, give the\\nmost prevalent 10,000 words a corresponding integer token, and trim or pad the\\nsequence to 201 tokens long.\\nApply the TextVectorization layer to the training data.\\nThe vocab variable stores a list of the word tokens.\\nAn example of a recipe after tokenization is shown in Example 5-5. The sequence\\nlength that we use to train the model is a parameter of the training process. In this\\nexample we choose to use a sequence length of 200, so we pad or clip the recipe to\\none more than this length, to allow us to create the target variable (more on this in\\nthe next section). To achieve this desired length, the end of the vector is padded with\\nzeros.\\nStop Tokens\\nThe 0 token is known as a the stop token, signifying that the text\\nstring has come to an end.\\nExample 5-5. The recipe from Example 5-3 tokenized\\n[  26   16  557    1    8  298  335  189    4 1054  494   27  332  228\\n  235  262    5  594   11  133   22  311    2  332   45  262    4  671\\n    4   70    8  171    4   81    6    9   65   80    3  121    3   59\\n   12    2  299    3   88  650   20   39    6    9   29   21    4   67\\n  529   11  164    2  320  171  102    9  374   13  643  306   25   21\\n    8  650    4   42    5  931    2   63    8   24    4   33    2  114\\n   21    6  178  181 1245    4   60    5  140  112    3   48    2  117\\n136 \\n| \\nChapter 5: Autoregressive Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 164}, page_content='557    8  285  235    4  200  292  980    2  107  650   28   72    4\\n  108   10  114    3   57  204   11  172    2   73  110  482    3  298\\n    3  190    3   11   23   32  142   24    3    4   11   23   32  142\\n   33    6    9   30   21    2   42    6  353    3 3224    3    4  150\\n    2  437  494    8 1281    3   37    3   11   23   15  142   33    3\\n    4   11   23   32  142   24    6    9  291  188    5    9  412  572\\n    2  230  494    3   46  335  189    3   20  557    2    0    0    0\\n    0    0    0    0    0]\\nIn Example 5-6, we can see a subset of the list of tokens mapped to their respective\\nindices. The layer reserves the 0 token for padding (i.e., it is the stop token) and the 1\\ntoken for unknown words that fall outside the top 10,000 words (e.g., persillade). The\\nother words are assigned tokens in order of frequency. The number of words to\\ninclude in the vocabulary is also a parameter of the training process. The more words\\nincluded, the fewer unknown tokens you will see in the text; however, your model will\\nneed to be larger to accommodate the larger vocabulary size.\\nExample 5-6. The vocabulary of the TextVectorization layer\\n0:\\n1: [UNK]\\n2: .\\n3: ,\\n4: and\\n5: to\\n6: in\\n7: the\\n8: with\\n9: a\\nCreating the Training Set\\nOur LSTM will be trained to predict the next word in a sequence, given a sequence of\\nwords preceding this point. For example, we could feed the model the tokens for gril‐\\nled chicken with boiled and would expect the model to output a suitable next word\\n(e.g., potatoes, rather than bananas).\\nWe can therefore simply shift the entire sequence by one token in order to create our\\ntarget variable.\\nThe dataset generation step can be achieved with the code in Example 5-7.\\nExample 5-7. Creating the training dataset\\ndef prepare_inputs(text):\\n    text = tf.expand_dims(text, -1)\\n    tokenized_sentences = vectorize_layer(text)\\n    x = tokenized_sentences[:, :-1]\\nLong Short-Term Memory Network (LSTM) \\n| \\n137'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 165}, page_content='y = tokenized_sentences[:, 1:]\\n    return x, y\\ntrain_ds = text_ds.map(prepare_inputs) \\nCreate the training set consisting of recipe tokens (the input) and the same vector\\nshifted by one token (the target).\\nThe LSTM Architecture\\nThe architecture of the overall LSTM model is shown in Table 5-1. The input to the\\nmodel is a sequence of integer tokens and the output is the probability of each word\\nin the 10,000-word vocabulary appearing next in the sequence. To understand how\\nthis works in detail, we need to introduce two new layer types, Embedding and LSTM.\\nTable 5-1. Model summary of the LSTM\\nLayer (type)\\nOutput shape\\nParam #\\nInputLayer\\n(None, None)\\n0\\nEmbedding\\n(None, None, 100)\\n1,000,000\\nLSTM\\n(None, None, 128)\\n117,248\\nDense\\n(None, None, 10000) 1,290,000\\nTotal params\\n2,407,248\\nTrainable params\\n2,407,248\\nNon-trainable params\\n0\\nThe Input Layer of the LSTM\\nNotice that the Input layer does not need us to specify the\\nsequence length in advance. Both the batch size and the sequence\\nlength are flexible (hence the (None, None) shape). This is because\\nall downstream layers are agnostic to the length of the sequence\\nbeing passed through.\\nThe Embedding Layer\\nAn embedding layer is essentially a lookup table that converts each integer token into\\na vector of length embedding_size, as shown in Figure 5-2. The lookup vectors are\\nlearned by the model as weights. Therefore, the number of weights learned by this\\nlayer is equal to the size of the vocabulary multiplied by the dimension of the embed‐\\nding vector (i.e., 10,000 × 100 = 1,000,000).\\n138 \\n| \\nChapter 5: Autoregressive Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 166}, page_content='Figure 5-2. An embedding layer is a lookup table for each integer token\\nWe embed each integer token into a continuous vector because it enables the model\\nto learn a representation for each word that is able to be updated through backpropa‐\\ngation. We could also just one-hot encode each input token, but using an embedding\\nlayer is preferred because it makes the embedding itself trainable, thus giving the\\nmodel more flexibility in deciding how to embed each token to improve its\\nperformance.\\nTherefore, the Input layer passes a tensor of integer sequences of shape\\n[batch_size, seq_length] to the Embedding layer, which outputs a tensor of shape\\n[batch_size, seq_length, embedding_size]. This is then passed on to the LSTM\\nlayer (Figure 5-3).\\nFigure 5-3. A single sequence as it flows through an embedding layer\\nLong Short-Term Memory Network (LSTM) \\n| \\n139'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 167}, page_content='The LSTM Layer\\nTo understand the LSTM layer, we must first look at how a general recurrent layer\\nworks.\\nA recurrent layer has the special property of being able to process sequential input\\ndata x1, ⋯, xn. It consists of a cell that updates its hidden state, ht, as each element of\\nthe sequence xt is passed through it, one timestep at a time.\\nThe hidden state is a vector with length equal to the number of units in the cell—it\\ncan be thought of as the cell’s current understanding of the sequence. At timestep t,\\nthe cell uses the previous value of the hidden state, ht −1, together with the data from\\nthe current timestep xt to produce an updated hidden state vector, ht. This recurrent\\nprocess continues until the end of the sequence. Once the sequence is finished, the\\nlayer outputs the final hidden state of the cell, hn, which is then passed on to the next\\nlayer of the network. This process is shown in Figure 5-4.\\nFigure 5-4. A simple diagram of a recurrent layer\\nTo explain this in more detail, let’s unroll the process so that we can see exactly how a\\nsingle sequence is fed through the layer (Figure 5-5).\\nCell Weights\\nIt’s important to remember that all of the cells in this diagram share\\nthe same weights (as they are really the same cell). There is no dif‐\\nference between this diagram and Figure 5-4; it’s just a different\\nway of drawing the mechanics of a recurrent layer.\\n140 \\n| \\nChapter 5: Autoregressive Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 168}, page_content='Figure 5-5. How a single sequence flows through a recurrent layer\\nHere, we represent the recurrent process by drawing a copy of the cell at each time‐\\nstep and show how the hidden state is constantly being updated as it flows through\\nthe cells. We can clearly see how the previous hidden state is blended with the current\\nsequential data point (i.e., the current embedded word vector) to produce the next\\nhidden state. The output from the layer is the final hidden state of the cell, after each\\nword in the input sequence has been processed.\\nLong Short-Term Memory Network (LSTM) \\n| \\n141'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 169}, page_content='The fact that the output from the cell is called a hidden state is an\\nunfortunate naming convention—it’s not really hidden, and you\\nshouldn’t think of it as such. Indeed, the last hidden state is the\\noverall output from the layer, and we will be making use of the fact\\nthat we can access the hidden state at each individual timestep later\\nin this chapter.\\nThe LSTM Cell\\nNow that we have seen how a generic recurrent layer works, let’s take a look inside an\\nindividual LSTM cell.\\nThe job of the LSTM cell is to output a new hidden state, ht, given its previous hidden\\nstate, ht −1, and the current word embedding, xt. To recap, the length of ht is equal to\\nthe number of units in the LSTM. This is a parameter that is set when you define the\\nlayer and has nothing to do with the length of the sequence.\\nMake sure you do not confuse the term cell with unit. There is one\\ncell in an LSTM layer that is defined by the number of units it con‐\\ntains, in the same way that the prisoner cell from our earlier story\\ncontained many prisoners. We often draw a recurrent layer as a\\nchain of cells unrolled, as it helps to visualize how the hidden state\\nis updated at each timestep.\\nAn LSTM cell maintains a cell state, Ct, which can be thought of as the cell’s internal\\nbeliefs about the current status of the sequence. This is distinct from the hidden state,\\nht, which is ultimately output by the cell after the final timestep. The cell state is the\\nsame length as the hidden state (the number of units in the cell).\\nLet’s look more closely at a single cell and how the hidden state is updated\\n(Figure 5-6).\\nThe hidden state is updated in six steps:\\n1. The hidden state of the previous timestep, ht −1, and the current word embed‐\\nding, xt, are concatenated and passed through the forget gate. This gate is simply a\\ndense layer with weights matrix W f, bias bf, and a sigmoid activation function.\\nThe resulting vector, f t, has length equal to the number of units in the cell and\\ncontains values between 0 and 1 that determine how much of the previous cell\\nstate, Ct −1, should be retained.\\n142 \\n| \\nChapter 5: Autoregressive Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 170}, page_content='Figure 5-6. An LSTM cell\\n2. The concatenated vector is also passed through an input gate that, like the forget\\ngate, is a dense layer with weights matrix Wi, bias bi, and a sigmoid activation\\nfunction. The output from this gate, it, has length equal to the number of units in\\nthe cell and contains values between 0 and 1 that determine how much new\\ninformation will be added to the previous cell state, Ct −1.\\n3. The concatenated vector is passed through a dense layer with weights matrix WC,\\nbias bC, and a tanh activation function to generate a vector Ct that contains the\\nnew information that the cell wants to consider keeping. It also has length equal\\nto the number of units in the cell and contains values between –1 and 1.\\n4. f t and Ct −1 are multiplied element-wise and added to the element-wise multipli‐\\ncation of it and Ct. This represents forgetting parts of the previous cell state and\\nthen adding new relevant information to produce the updated cell state, Ct.\\n5. The concatenated vector is passed through an output gate: a dense layer with\\nweights matrix Wo, bias bo, and a sigmoid activation. The resulting vector, ot, has\\nlength equal to the number of units in the cell and stores values between 0 and 1\\nthat determine how much of the updated cell state, Ct, to output from the cell.\\nLong Short-Term Memory Network (LSTM) \\n| \\n143'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 171}, page_content='6. ot is multiplied element-wise with the updated cell state, Ct, after a tanh activa‐\\ntion has been applied to produce the new hidden state, ht.\\nThe Keras LSTM Layer\\nAll of this complexity is wrapped up within the LSTM layer type in\\nKeras, so you don’t have to worry about implementing it yourself!\\nTraining the LSTM\\nThe code to build, compile, and train the LSTM is given in Example 5-8.\\nExample 5-8. Building, compiling, and training the LSTM\\ninputs = layers.Input(shape=(None,), dtype=\"int32\") \\nx = layers.Embedding(10000, 100)(inputs) \\nx = layers.LSTM(128, return_sequences=True)(x) \\noutputs = layers.Dense(10000, activation = \\'softmax\\')(x) \\nlstm = models.Model(inputs, outputs) \\nloss_fn = losses.SparseCategoricalCrossentropy()\\nlstm.compile(\"adam\", loss_fn) \\nlstm.fit(train_ds, epochs=25) \\nThe Input layer does not need us to specify the sequence length in advance (it\\ncan be flexible), so we use None as a placeholder.\\nThe Embedding layer requires two parameters, the size of the vocabulary (10,000\\ntokens) and the dimensionality of the embedding vector (100).\\nThe LSTM layers require us to specify the dimensionality of the hidden vector\\n(128). We also choose to return the full sequence of hidden states, rather than just\\nthe hidden state at the final timestep.\\nThe Dense layer transforms the hidden states at each timestep into a vector of\\nprobabilities for the next token.\\nThe overall Model predicts the next token, given an input sequence of tokens. It\\ndoes this for each token in the sequence.\\nThe model is compiled with SparseCategoricalCrossentropy loss—this is the\\nsame as categorical cross-entropy, but is used when the labels are integers rather\\nthan one-hot encoded vectors.\\n144 \\n| \\nChapter 5: Autoregressive Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 172}, page_content='The model is fit to the training dataset.\\nIn Figure 5-7 you can see the first few epochs of the LSTM training process—notice\\nhow the example output becomes more comprehensible as the loss metric falls.\\nFigure 5-8 shows the cross-entropy loss metric falling throughout the training\\nprocess.\\nFigure 5-7. The first few epochs of the LSTM training process\\nFigure 5-8. The cross-entropy loss metric of the LSTM training process by epoch\\nLong Short-Term Memory Network (LSTM) \\n| \\n145'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 173}, page_content='Analysis of the LSTM\\nNow that we have compiled and trained the LSTM, we can start to use it to generate\\nlong strings of text by applying the following process:\\n1. Feed the network with an existing sequence of words and ask it to predict the fol‐\\nlowing word.\\n2. Append this word to the existing sequence and repeat.\\nThe network will output a set of probabilities for each word that we can sample from.\\nTherefore, we can make the text generation stochastic, rather than deterministic.\\nMoreover, we can introduce a temperature parameter to the sampling process to indi‐\\ncate how deterministic we would like the process to be.\\nThe Temperature Parameter\\nA temperature close to 0 makes the sampling more deterministic\\n(i.e., the word with the highest probability is very likely to be\\nchosen), whereas a temperature of 1 means each word is chosen\\nwith the probability output by the model.\\nThis is achieved with the code in Example 5-9, which creates a callback function that\\ncan be used to generate text at the end of each training epoch.\\nExample 5-9. The TextGenerator callback function\\nclass TextGenerator(callbacks.Callback):\\n    def __init__(self, index_to_word, top_k=10):\\n        self.index_to_word = index_to_word\\n        self.word_to_index = {\\n            word: index for index, word in enumerate(index_to_word)\\n        } \\n    def sample_from(self, probs, temperature): \\n        probs = probs ** (1 / temperature)\\n        probs = probs / np.sum(probs)\\n        return np.random.choice(len(probs), p=probs), probs\\n    def generate(self, start_prompt, max_tokens, temperature):\\n        start_tokens = [\\n            self.word_to_index.get(x, 1) for x in start_prompt.split()\\n        ] \\n        sample_token = None\\n        info = []\\n        while len(start_tokens) < max_tokens and sample_token != 0: \\n            x = np.array([start_tokens])\\n            y = self.model.predict(x) \\n146 \\n| \\nChapter 5: Autoregressive Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 174}, page_content='sample_token, probs = self.sample_from(y[0][-1], temperature) \\n            info.append({\\'prompt\\': start_prompt , \\'word_probs\\': probs})\\n            start_tokens.append(sample_token) \\n            start_prompt = start_prompt + \\' \\' + self.index_to_word[sample_token]\\n        print(f\"\\\\ngenerated text:\\\\n{start_prompt}\\\\n\")\\n        return info\\n    def on_epoch_end(self, epoch, logs=None):\\n        self.generate(\"recipe for\", max_tokens = 100, temperature = 1.0)\\nCreate an inverse vocabulary mapping (from word to token).\\nThis function updates the probabilities with a temperature scaling factor.\\nThe start prompt is a string of words that you would like to give the model to\\nstart the generation process (for example, recipe for). The words are first con‐\\nverted to a list of tokens.\\nThe sequence is generated until it is max_tokens long or a stop token (0) is\\nproduced.\\nThe model outputs the probabilities of each word being next in the sequence.\\nThe probabilities are passed through the sampler to output the next word, para‐\\nmeterized by temperature.\\nWe append the new word to the prompt text, ready for the next iteration of the\\ngenerative process.\\nLet’s take a look at this in action, at two different temperature values (Figure 5-9).\\nFigure 5-9. Generated outputs at temperature = 1.0 and temperature = 0.2\\nLong Short-Term Memory Network (LSTM) \\n| \\n147'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 175}, page_content='There are a few things to note about these two passages. First, both are stylistically\\nsimilar to a recipe from the original training set. They both open with a recipe title\\nand contain generally grammatically correct constructions. The difference is that the\\ngenerated text with a temperature of 1.0 is more adventurous and therefore less accu‐\\nrate than the example with a temperature of 0.2. Generating multiple samples with a\\ntemperature of 1.0 will therefore lead to more variety, as the model is sampling from a\\nprobability distribution with greater variance.\\nTo demonstrate this, Figure 5-10 shows the top five tokens with the highest probabili‐\\nties for a range of prompts, for both temperature values.\\nFigure 5-10. Distribution of word probabilities following various sequences, for tempera‐\\nture values of 1.0 and 0.2\\nThe model is able to generate a suitable distribution for the next most likely word\\nacross a range of contexts. For example, even though the model was never told about\\nparts of speech such as nouns, verbs, or numbers, it is generally able to separate\\nwords into these classes and use them in a way that is grammatically correct.\\n148 \\n| \\nChapter 5: Autoregressive Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 176}, page_content='Moreover, the model is able to select an appropriate verb to begin the recipe instruc‐\\ntions, depending on the preceding title. For roasted vegetables, it selects preheat,\\nprepare, heat, put, or combine as the most likely possibilities, whereas for ice cream\\nit selects in, combine, stir, whisk, and mix. This shows that the model has some con‐\\ntextual understanding of the differences between recipes depending on their\\ningredients.\\nNotice also how the probabilities for the temperature = 0.2 examples are much\\nmore heavily weighted toward the first choice token. This is the reason why there is\\ngenerally less variety in generations when the temperature is lower.\\nWhile our basic LSTM model is doing a great job at generating realistic text, it is clear\\nthat it still struggles to grasp some of the semantic meaning of the words that it is\\ngenerating. It introduces ingredients that are not likely to work well together (for\\nexample, sour Japanese potatoes, pecan crumbs, and sorbet)! In some cases, this may\\nbe desirable—say, if we want our LSTM to generate interesting and unique patterns of\\nwords—but in other cases, we will need our model to have a deeper understanding of\\nthe ways in which words can be grouped together and a longer memory of ideas\\nintroduced earlier in the text.\\nIn the next section, we’ll explore some of the ways that we can improve our basic\\nLSTM network. In Chapter 9, we’ll take a look at a new kind of autoregressive model,\\nthe Transformer, which takes language modeling to the next level.\\nRecurrent Neural Network (RNN) Extensions\\nThe model in the preceding section is a simple example of how an LSTM can be\\ntrained to learn how to generate text in a given style. In this section we will explore\\nseveral extensions to this idea.\\nStacked Recurrent Networks\\nThe network we just looked at contained a single LSTM layer, but we can also train\\nnetworks with stacked LSTM layers, so that deeper features can be learned from the\\ntext.\\nTo achieve this, we simply introduce another LSTM layer after the first. The second\\nLSTM layer can then use the hidden states from the first layer as its input data. This is\\nshown in Figure 5-11, and the overall model architecture is shown in Table 5-2.\\nRecurrent Neural Network (RNN) Extensions \\n| \\n149'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 177}, page_content='Figure 5-11. Diagram of a multilayer RNN: gt denotes hidden states of the first layer and ht denotes hidden states of the second layer\\n150 \\n| \\nChapter 5: Autoregressive Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 178}, page_content=\"Table 5-2. Model summary of the stacked LSTM\\nLayer (type) Output shape\\nParam #\\nInputLayer\\n(None, None)\\n0\\nEmbedding\\n(None, None, 100)\\n1,000,000\\nLSTM\\n(None, None, 128)\\n117,248\\nLSTM\\n(None, None, 128)\\n131,584\\nDense\\n(None, None, 10000) 1,290,000\\nTotal params\\n2,538,832\\nTrainable params\\n2,538,832\\nNon-trainable params\\n0\\nThe code to build the stacked LSTM is given in Example 5-10.\\nExample 5-10. Building a stacked LSTM\\ntext_in = layers.Input(shape = (None,))\\nembedding = layers.Embedding(total_words, embedding_size)(text_in)\\nx = layers.LSTM(n_units, return_sequences = True)(x)\\nx = layers.LSTM(n_units, return_sequences = True)(x)\\nprobabilites = layers.Dense(total_words, activation = 'softmax')(x)\\nmodel = models.Model(text_in, probabilites)\\nGated Recurrent Units\\nAnother type of commonly used RNN layer is the gated recurrent unit (GRU).2 The\\nkey differences from the LSTM unit are as follows:\\n1. The forget and input gates are replaced by reset and update gates.\\n2. There is no cell state or output gate, only a hidden state that is output from the\\ncell.\\nThe hidden state is updated in four steps, as illustrated in Figure 5-12.\\nRecurrent Neural Network (RNN) Extensions \\n| \\n151\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 179}, page_content='Figure 5-12. A single GRU cell\\nThe process is as follows:\\n1. The hidden state of the previous timestep, ht −1, and the current word embed‐\\nding, xt, are concatenated and used to create the reset gate. This gate is a dense\\nlayer, with weights matrix Wr and a sigmoid activation function. The resulting\\nvector, rt, has length equal to the number of units in the cell and stores values\\nbetween 0 and 1 that determine how much of the previous hidden state, ht −1,\\nshould be carried forward into the calculation for the new beliefs of the cell.\\n2. The reset gate is applied to the hidden state, ht −1, and concatenated with the cur‐\\nrent word embedding, xt. This vector is then fed to a dense layer with weights\\nmatrix W and a tanh activation function to generate a vector, ht, that stores the\\nnew beliefs of the cell. It has length equal to the number of units in the cell and\\nstores values between –1 and 1.\\n3. The concatenation of the hidden state of the previous timestep, ht −1, and the\\ncurrent word embedding, xt, are also used to create the update gate. This gate is a\\ndense layer with weights matrix Wz and a sigmoid activation. The resulting vec‐\\ntor, zt, has length equal to the number of units in the cell and stores values\\n152 \\n| \\nChapter 5: Autoregressive Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 180}, page_content='between 0 and 1, which are used to determine how much of the new beliefs, ht, to\\nblend into the current hidden state, ht −1.\\n4. The new beliefs of the cell, ht, and the current hidden state, ht −1, are blended in a\\nproportion determined by the update gate, zt, to produce the updated hidden\\nstate, ht, that is output from the cell.\\nBidirectional Cells\\nFor prediction problems where the entire text is available to the model at inference\\ntime, there is no reason to process the sequence only in the forward direction—it\\ncould just as well be processed backward. A Bidirectional layer takes advantage of\\nthis by storing two sets of hidden states: one that is produced as a result of the\\nsequence being processed in the usual forward direction and another that is pro‐\\nduced when the sequence is processed backward. This way, the layer can learn from\\ninformation both preceding and succeeding the given timestep.\\nIn Keras, this is implemented as a wrapper around a recurrent layer, as shown in\\nExample 5-11.\\nExample 5-11. Building a bidirectional GRU layer\\nlayer = layers.Bidirectional(layers.GRU(100))\\nHidden State\\nThe hidden states in the resulting layer are vectors of length equal\\nto double the number of units in the wrapped cell (a concatenation\\nof the forward and backward hidden states). Thus, in this example\\nthe hidden states of the layer are vectors of length 200.\\nSo far, we have only applied autoregressive models (LSTMs) to text data. In the next\\nsection, we will see how autoregressive models can also be used to generate images.\\nPixelCNN\\nIn 2016, van den Oord et al.3 introduced a model that generates images pixel by pixel\\nby predicting the likelihood of the next pixel based on the pixels before it. The model\\nis called PixelCNN, and it can be trained to generate images autoregressively.\\nThere are two new concepts that we need to introduce to understand the PixelCNN—\\nmasked convolutional layers and residual blocks.\\nPixelCNN \\n| \\n153'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 181}, page_content='Running the Code for This Example\\nThe code for this example can be found in the Jupyter notebook\\nlocated at notebooks/05_autoregressive/02_pixelcnn/pixelcnn.ipynb\\nin the book repository.\\nThe code has been adapted from the excellent PixelCNN tutorial\\ncreated by ADMoreau, available on the Keras website.\\nMasked Convolutional Layers\\nAs we saw in Chapter 2, a convolutional layer can be used to extract features from an\\nimage by applying a series of filters. The output of the layer at a particular pixel is a\\nweighted sum of the filter weights multiplied by the preceding layer values over a\\nsmall square centered on the pixel. This method can detect edges and textures and, at\\ndeeper layers, shapes and higher-level features.\\nWhilst convolutional layers are extremely useful for feature detection, they cannot\\ndirectly be used in an autoregressive sense, because there is no ordering placed on the\\npixels. They rely on the fact that all pixels are treated equally—no pixel is treated as\\nthe start or end of the image. This is in contrast to the text data that we have already\\nseen in this chapter, where there is a clear ordering to the tokens so recurrent models\\nsuch as LSTMs can be readily applied.\\nFor us to be able to apply convolutional layers to image generation in an autoregres‐\\nsive sense, we must first place an ordering on the pixels and ensure that the filters are\\nonly able to see pixels that precede the pixel in question. We can then generate images\\none pixel at a time, by applying convolutional filters to the current image to predict\\nthe value of the next pixel from all preceding pixels.\\nWe first need to choose an ordering for the pixels—a sensible suggestion is to order\\nthe pixels from top left to bottom right, moving first along the rows and then down\\nthe columns.\\nWe then mask the convolutional filters so that the output of the layer at each pixel is\\nonly influenced by pixel values that precede the pixel in question. This is achieved by\\nmultiplying a mask of ones and zeros with the filter weights matrix, so that the values\\nof any pixels that are after the target pixel are zeroed.\\nThere are actually two different kinds of masks in a PixelCNN, as shown in\\nFigure 5-13:\\n• Type A, where the value of the central pixel is masked\\n• Type B, where the value of the central pixel is not masked\\n154 \\n| \\nChapter 5: Autoregressive Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 182}, page_content='Figure 5-13. Left: a convolutional filter mask; right: a mask applied to a set of pixels to\\npredict the distribution of the central pixel value (source: van den Oord et al., 2016)\\nThe initial masked convolutional layer (i.e., the one that is applied directly to the\\ninput image) cannot use the central pixel, because this is precisely the pixel we want\\nthe network to guess! However, subsequent layers can use the central pixel because\\nthis will have been calculated only as a result of information from preceding pixels in\\nthe original input image.\\nWe can see in Example 5-12 how a MaskedConvLayer can be built using Keras.\\nExample 5-12. A MaskedConvLayer in Keras\\nclass MaskedConvLayer(layers.Layer):\\n    def __init__(self, mask_type, **kwargs):\\n        super(MaskedConvLayer, self).__init__()\\n        self.mask_type = mask_type\\n        self.conv = layers.Conv2D(**kwargs) \\n    def build(self, input_shape):\\n        self.conv.build(input_shape)\\n        kernel_shape = self.conv.kernel.get_shape()\\n        self.mask = np.zeros(shape=kernel_shape) \\n        self.mask[: kernel_shape[0] // 2, ...] = 1.0 \\n        self.mask[kernel_shape[0] // 2, : kernel_shape[1] // 2, ...] = 1.0 \\n        if self.mask_type == \"B\":\\n            self.mask[kernel_shape[0] // 2, kernel_shape[1] // 2, ...] = 1.0 \\n    def call(self, inputs):\\n        self.conv.kernel.assign(self.conv.kernel * self.mask) \\n        return self.conv(inputs)\\nThe MaskedConvLayer is based on the normal Conv2D layer.\\nPixelCNN \\n| \\n155'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 183}, page_content='The mask is initialized with all zeros.\\nThe pixels in the preceding rows are unmasked with ones.\\nThe pixels in the preceding columns that are in the same row are unmasked with\\nones.\\nIf the mask type is B, the central pixel is unmasked with a one.\\nThe mask is multiplied with the filter weights.\\nNote that this simplified example assumes a grayscale image (i.e., with one channel).\\nIf we have color images, we’ll have three color channels that we can also place an\\nordering on so that, for example, the red channel precedes the blue channel, which\\nprecedes the green channel.\\nResidual Blocks\\nNow that we have seen how to mask the convolutional layer, we can start to build our\\nPixelCNN. The core building block that we will use is the residual block.\\nA residual block is a set of layers where the output is added to the input before being\\npassed on to the rest of the network. In other words, the input has a fast-track route\\nto the output, without having to go through the intermediate layers—this is called a\\nskip connection. The rationale behind including a skip connection is that if the opti‐\\nmal transformation is just to keep the input the same, this can be achieved by simply\\nzeroing the weights of the intermediate layers. Without the skip connection, the net‐\\nwork would have to find an identity mapping through the intermediate layers, which\\nis much harder.\\nA diagram of the residual block in our PixelCNN is shown in Figure 5-14.\\n156 \\n| \\nChapter 5: Autoregressive Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 184}, page_content='Figure 5-14. A PixelCNN residual block (the numbers of filters are next to the arrows\\nand the filter sizes are next to the layers)\\nWe can build a ResidualBlock using the code shown in Example 5-13.\\nExample 5-13. A ResidualBlock\\nclass ResidualBlock(layers.Layer):\\n    def __init__(self, filters, **kwargs):\\n        super(ResidualBlock, self).__init__(**kwargs)\\n        self.conv1 = layers.Conv2D(\\n            filters=filters // 2, kernel_size=1, activation=\"relu\"\\n        ) \\n        self.pixel_conv = MaskedConv2D(\\n            mask_type=\"B\",\\n            filters=filters // 2,\\n            kernel_size=3,\\n            activation=\"relu\",\\n            padding=\"same\",\\n        ) \\n        self.conv2 = layers.Conv2D(\\n            filters=filters, kernel_size=1, activation=\"relu\"\\n        ) \\n    def call(self, inputs):\\n        x = self.conv1(inputs)\\n        x = self.pixel_conv(x)\\n        x = self.conv2(x)\\n        return layers.add([inputs, x]) \\nThe initial Conv2D layer halves the number of channels.\\nPixelCNN \\n| \\n157'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 185}, page_content='The Type B MaskedConv2D layer with kernel size of 3 only uses information from\\nfive pixels—three pixels in the row above the focus pixel, one to the left, and the\\nfocus pixel itself.\\nThe final Conv2D layer doubles the number of channels to again match the input\\nshape.\\nThe output from the convolutional layers is added to the input—this is the skip\\nconnection.\\nTraining the PixelCNN\\nIn Example 5-14 we put together the whole PixelCNN network, approximately fol‐\\nlowing the structure laid out in the original paper. In the original paper, the output\\nlayer is a 256-filter Conv2D layer, with softmax activation. In other words, the network\\ntries to re-create its input by predicting the correct pixel values, a bit like an autoen‐\\ncoder. The difference is that the PixelCNN is constrained so that no information from\\nearlier pixels can flow through to influence the prediction for each pixel, due to the\\nway that network is designed, using MaskedConv2D layers.\\nA challenge with this approach is that the network has no way to understand that a\\npixel value of, say, 200 is very close to a pixel value of 201. It must learn every pixel\\noutput value independently, which means training can be very slow, even for the sim‐\\nplest datasets. Therefore, in our implementation, we instead simplify the input so that\\neach pixel can take only one of four values. This way, we can use a 4-filter Conv2D\\noutput layer instead of 256.\\nExample 5-14. The PixelCNN architecture\\ninputs = layers.Input(shape=(16, 16, 1)) \\nx = MaskedConv2D(mask_type=\"A\"\\n                   , filters=128\\n                   , kernel_size=7\\n                   , activation=\"relu\"\\n                   , padding=\"same\")(inputs)\\nfor _ in range(5):\\n    x = ResidualBlock(filters=128)(x) \\nfor _ in range(2):\\n    x = MaskedConv2D(\\n        mask_type=\"B\",\\n        filters=128,\\n        kernel_size=1,\\n        strides=1,\\n        activation=\"relu\",\\n158 \\n| \\nChapter 5: Autoregressive Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 186}, page_content='padding=\"valid\",\\n    )(x) \\nout = layers.Conv2D(\\n    filters=4, kernel_size=1, strides=1, activation=\"softmax\", padding=\"valid\"\\n)(x) \\npixel_cnn = models.Model(inputs, out) \\nadam = optimizers.Adam(learning_rate=0.0005)\\npixel_cnn.compile(optimizer=adam, loss=\"sparse_categorical_crossentropy\")\\npixel_cnn.fit(\\n    input_data\\n    , output_data\\n    , batch_size=128\\n    , epochs=150\\n) \\nThe model Input is a grayscale image of size 16 × 16 × 1, with inputs scaled\\nbetween 0 and 1.\\nThe first Type A MaskedConv2D layer with a kernel size of 7 uses information\\nfrom 24 pixels—21 pixels in the three rows above the focus pixel and 3 to the left\\n(the focus pixel itself is not used).\\nFive ResidualBlock layer groups are stacked sequentially.\\nTwo Type B MaskedConv2D layers with a kernel size of 1 act as Dense layers across\\nthe number of channels for each pixel.\\nThe final Conv2D layer reduces the number of channels to four—the number of\\npixel levels for this example.\\nThe Model is built to accept an image and output an image of the same\\ndimensions.\\nFit the model—input_data is scaled in the range [0, 1] (floats); output_data is\\nscaled in the range [0, 3] (integers).\\nAnalysis of the PixelCNN\\nWe can train our PixelCNN on images from the Fashion-MNIST dataset that we\\nencountered in Chapter 3. To generate new images, we need to ask the model to pre‐\\ndict the next pixel given all preceding pixels, one pixel at a time. This is a very slow\\nprocess compared to a model such as a variational autoencoder! For a 32 × 32\\nPixelCNN \\n| \\n159'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 187}, page_content='grayscale image, we need to make 1,024 predictions sequentially using the model,\\ncompared to the single prediction that we need to make for a VAE. This is one of the\\nmajor downsides to autoregressive models such as a PixelCNN—they are slow to\\nsample from, because of the sequential nature of the sampling process.\\nFor this reason, we use an image size of 16 × 16, rather than 32 × 32, to speed up the\\ngeneration of new images. The generation callback class is shown in Example 5-15.\\nExample 5-15. Generating new images using the PixelCNN\\nclass ImageGenerator(callbacks.Callback):\\n    def __init__(self, num_img):\\n        self.num_img = num_img\\n    def sample_from(self, probs, temperature):\\n        probs = probs ** (1 / temperature)\\n        probs = probs / np.sum(probs)\\n        return np.random.choice(len(probs), p=probs)\\n    def generate(self, temperature):\\n        generated_images = np.zeros(\\n            shape=(self.num_img,) + (pixel_cnn.input_shape)[1:]\\n        ) \\n        batch, rows, cols, channels = generated_images.shape\\n        for row in range(rows):\\n            for col in range(cols):\\n                for channel in range(channels):\\n                    probs = self.model.predict(generated_images)[\\n                        :, row, col, :\\n                    ] \\n                    generated_images[:, row, col, channel] = [\\n                        self.sample_from(x, temperature) for x in probs\\n                    ] \\n                    generated_images[:, row, col, channel] /= 4 \\n        return generated_images\\n    def on_epoch_end(self, epoch, logs=None):\\n        generated_images = self.generate(temperature = 1.0)\\n        display(\\n            generated_images,\\n            save_to = \"./output/generated_img_%03d.png\" % (epoch)\\n        s)\\nimg_generator_callback = ImageGenerator(num_img=10)\\nStart with a batch of empty images (all zeros).\\n160 \\n| \\nChapter 5: Autoregressive Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 188}, page_content='Loop over the rows, columns, and channels of the current image, predicting the\\ndistribution of the next pixel value.\\nSample a pixel level from the predicted distribution (for our example, a level in\\nthe range [0, 3]).\\nConvert the pixel level to the range [0, 1] and overwrite the pixel value in the cur‐\\nrent image, ready for the next iteration of the loop.\\nIn Figure 5-15, we can see several images from the original training set, alongside\\nimages that have been generated by the PixelCNN.\\nFigure 5-15. Example images from the training set and generated images created by the\\nPixelCNN model\\nThe model does a great job of re-creating the overall shape and style of the original\\nimages! It is quite amazing that we can treat images as a series of tokens (pixel values)\\nand apply autoregressive models such as a PixelCNN to produce realistic samples.\\nAs mentioned previously, one of the downsides to autoregressive models is that they\\nare slow to sample from, which is why a simple example of their application is presen‐\\nted in this book. However, as we shall see in Chapter 10, more complex forms of\\nautoregressive model can be applied to images to produce state-of-the-art outputs. In\\nPixelCNN \\n| \\n161'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 189}, page_content='such cases, the slow generation speed is a necessary price to pay in return for\\nexceptional-quality outputs.\\nSince the original paper was published, several improvements have been made to the\\narchitecture and training process of the PixelCNN. The following section introduces\\none of those changes—using mixture distributions—and demonstrates how to train a\\nPixelCNN model with this improvement using a built-in TensorFlow function.\\nMixture Distributions\\nFor our previous example, we reduced the output of the PixelCNN to just 4 pixel lev‐\\nels to ensure the network didn’t have to learn a distribution over 256 independent\\npixel values, which would slow the training process. However, this is far from ideal—\\nfor color images, we wouldn’t want our canvas to be restricted to only a handful of\\npossible colors.\\nTo get around this problem, we can make the output of the network a mixture distri‐\\nbution, instead of a softmax over 256 discrete pixel values, following the ideas presen‐\\nted by Salimans et al.4 A mixture distribution is quite simply a mixture of two or\\nmore other probability distributions. For example, we could have a mixture distribu‐\\ntion of five logistic distributions, each with different parameters. The mixture distri‐\\nbution also requires a discrete categorical distribution that denotes the probability of\\nchoosing each of the distributions included in the mix. An example is shown in\\nFigure 5-16.\\nFigure 5-16. A mixture distribution of three normal distributions with different parame‐\\nters—the categorical distribution over the three normal distributions is [0.5, 0.3,\\n0.2]\\nTo sample from a mixture distribution, we first sample from the categorical distribu‐\\ntion to choose a particular subdistribution and then sample from this in the usual\\nway. This way, we can create complex distributions with relatively few parameters.\\n162 \\n| \\nChapter 5: Autoregressive Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 190}, page_content='For example, the mixture distribution in Figure 5-16 only requires eight parameters\\n—two for the categorical distribution and a mean and variance for each of the three\\nnormal distributions. This is compared to the 255 parameters that would define a cat‐\\negorical distribution over the entire pixel range.\\nConveniently, the TensorFlow Probability library provides a function that allows us to\\ncreate a PixelCNN with mixture distribution output in a single line. Example 5-16\\nillustrates how to build a PixelCNN using this function.\\nRunning the Code for This Example\\nThe code for this example can be found in the Jupyter notebook in\\nnotebooks/05_autoregressive/03_pixelcnn_md/pixelcnn_md.ipynb in\\nthe book repository.\\nExample 5-16. Building a PixelCNN using the TensorFlow function\\nimport tensorflow_probability as tfp\\ndist = tfp.distributions.PixelCNN(\\n    image_shape=(32, 32, 1),\\n    num_resnet=1,\\n    num_hierarchies=2,\\n    num_filters=32,\\n    num_logistic_mix=5,\\n    dropout_p=.3,\\n) \\nimage_input = layers.Input(shape=(32, 32, 1)) \\nlog_prob = dist.log_prob(image_input)\\nmodel = models.Model(inputs=image_input, outputs=log_prob) \\nmodel.add_loss(-tf.reduce_mean(log_prob)) \\nDefine the PixelCNN as a distribution—i.e., the output layer is a mixture distri‐\\nbution made up of five logistic distributions.\\nThe input is a grayscale image of size 32 × 32 × 1.\\nThe Model takes a grayscale image as input and outputs the log-likelihood of the\\nimage under the mixture distribution calculated by the PixelCNN.\\nThe loss function is the mean negative log-likelihood over the batch of input\\nimages.\\nPixelCNN \\n| \\n163'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 191}, page_content='The model is trained in the same way as before, but this time accepting integer pixel\\nvalues as input, in the range [0, 255]. Outputs can be generated from the distribution\\nusing the sample function, as shown in Example 5-17.\\nExample 5-17. Sampling from the PixelCNN mixture distribution\\ndist.sample(10).numpy()\\nExample generated images are shown in Figure 5-17. The difference from our previ‐\\nous examples is that now the full range of pixel values is being utilized.\\nFigure 5-17. Outputs from the PixelCNN using a mixture distribution output\\nSummary\\nIn this chapter we have seen how autoregressive models such as recurrent neural net‐\\nworks can be applied to generate text sequences that mimic a particular style of writ‐\\ning, and also how a PixelCNN can generate images in a sequential fashion, one pixel\\nat a time.\\nWe explored two different types of recurrent layers—long short-term memory\\n(LSTM) and gated recurrent unit (GRU)—and saw how these cells can be stacked or\\nmade bidirectional to form more complex network architectures. We built an LSTM\\nto generate realistic recipes using Keras and saw how to manipulate the temperature\\nof the sampling process to increase or decrease the randomness of the output.\\nWe also saw how images can be generated in an autoregressive manner, using a Pix‐\\nelCNN. We built a PixelCNN from scratch using Keras, coding the masked convolu‐\\ntional layers and residual blocks to allow information to flow through the network so\\nthat only preceding pixels could be used to generate the current pixel. Finally, we dis‐\\ncussed how the TensorFlow Probability library provides a standalone PixelCNN func‐\\ntion that implements a mixture distribution as the output layer, allowing us to further\\nimprove the learning process.\\n164 \\n| \\nChapter 5: Autoregressive Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 192}, page_content='In the next chapter we will explore another generative modeling family that explicitly\\nmodels the data-generating distribution—normalizing flow models.\\nReferences\\n1. Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory,” Neural\\nComputation 9 (1997): 1735–1780, https://www.bioinf.jku.at/publications/older/\\n2604.pdf.\\n2. Kyunghyun Cho et al., “Learning Phrase Representations Using RNN Encoder-\\nDecoder for Statistical Machine Translation,” June 3, 2014, https://arxiv.org/abs/\\n1406.1078.\\n3. Aaron van den Oord et al., “Pixel Recurrent Neural Networks,” August 19, 2016,\\nhttps://arxiv.org/abs/1601.06759.\\n4. Tim Salimans et al., “PixelCNN++: Improving the PixelCNN with Discretized\\nLogistic Mixture Likelihood and Other Modifications,” January 19, 2017, http://\\narxiv.org/abs/1701.05517.\\nSummary \\n| \\n165'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 193}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 194}, page_content='CHAPTER 6\\nNormalizing Flow Models\\nChapter Goals\\nIn this chapter you will:\\n• Learn how normalizing flow models utilize the change of variables equation.\\n• See how the Jacobian determinant plays a vital role in our ability to compute an\\nexplicit density function.\\n• Understand how we can restrict the form of the Jacobian using coupling layers.\\n• See how the neural network is designed to be invertible.\\n• Build a RealNVP model—a particular example of a normalizing flow to generate\\npoints in 2D.\\n• Use the RealNVP model to generate new points that appear to have been drawn\\nfrom the data distribution.\\n• Learn about two key extensions of the RealNVP model, GLOW and FFJORD.\\nSo far, we have discussed three families of generative models: variational autoencod‐\\ners, generative adversarial networks, and autoregressive models. Each presents a dif‐\\nferent way to address the challenge of modeling the distribution p x , either by\\nintroducing a latent variable that can be easily sampled (and transformed using the\\ndecoder in VAEs or generator in GANs), or by tractably modeling the distribution as\\na function of the values of preceding elements (autoregressive models).\\nIn this chapter, we will cover a new family of generative models—normalizing flow\\nmodels. As we shall see, normalizing flows share similarities with both autoregressive\\nmodels and variational autoencoders. Like autoregressive models, normalizing flows\\nare able to explicitly and tractably model the data-generating distribution p x . Like\\n167'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 195}, page_content='VAEs, normalizing flows attempt to map the data into a simpler distribution, such as\\na Gaussian distribution. The key difference is that normalizing flows place a con‐\\nstraint on the form of the mapping function, so that it is invertible and can therefore\\nbe used to generate new data points.\\nWe will dig into this definition in detail in the first section of this chapter before\\nimplementing a normalizing flow model called RealNVP using Keras. We will also see\\nhow normalizing flows can be extended to create more powerful models, such as\\nGLOW and FFJORD.\\nIntroduction\\nWe will begin with a short story to illustrate the key concepts behind normalizing\\nflows.\\nJacob and the F.L.O.W. Machine\\nUpon visiting a small village, you notice a mysterious-looking shop with a sign above\\nthe door that says JACOB’S. Intrigued, you cautiously enter and ask the old man\\nstanding behind the counter what he sells (Figure 6-1).\\nFigure 6-1. Inside a steampunk shop, with a large metallic bell (created with\\nMidjourney)\\nHe replies that he offers a service for digitizing paintings, with a difference. After a\\nbrief moment rummaging around the back of the shop, he brings out a silver box,\\nembossed with the letters F.L.O.W. He tells you that this stands for Finding Likenesses\\nOf Watercolors, which approximately describes what the machine does. You decide to\\ngive the machine a try.\\n168 \\n| \\nChapter 6: Normalizing Flow Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 196}, page_content='You come back the next day and hand the shopkeeper a set of your favorite paintings,\\nand he passes them through the machine. The F.L.O.W. machine begins to hum and\\nwhistle and after a while outputs a set of numbers that appear randomly generated.\\nThe shopkeeper hands you the list and begins to walk to the till to calculate how\\nmuch you owe him for the digitization process and the F.L.O.W. box. Distinctly unim‐\\npressed, you ask the shopkeeper what you should do with this long list of numbers,\\nand how you can get your favorite paintings back.\\nThe shopkeeper rolls his eyes, as if the answer should be obvious. He walks back to\\nthe machine and passes in the long list of numbers, this time from the opposite side.\\nYou hear the machine whir again and wait, puzzled, until finally your original paint‐\\nings drop out from where they entered.\\nRelieved to finally have your paintings back, you decide that it might be best to just\\nstore them in the attic instead. However, before you have a chance to leave, the shop‐\\nkeeper ushers you across to a different corner of the shop, where a giant bell hangs\\nfrom the rafters. He hits the bell curve with a huge stick, sending vibrations around\\nthe store.\\nInstantly, the F.L.O.W. machine under your arm begins to hiss and whirr in reverse, as\\nif a new set of numbers had just been passed in. After a few moments, more beautiful\\nwatercolor paintings begin to fall out of the F.L.O.W. machine, but they are not the\\nsame as the ones you originally digitized. They resemble the style and form of your\\noriginal set of paintings, but each one is completely unique!\\nYou ask the shopkeeper how this incredible device works. He explains that the magic\\nlies in the fact that he has developed a special process that ensures the transformation\\nis extremely fast and simple to calculate while still being sophisticated enough to con‐\\nvert the vibrations produced by the bell into the complex patterns and shapes present\\nin the paintings.\\nRealizing the potential of this contraption, you hurriedly pay for the device and exit\\nthe store, happy that you now have a way to generate new paintings in your favorite\\nstyle, simply by visiting the shop, chiming the bell, and waiting for your F.L.O.W.\\nmachine to work its magic!\\nThe story of Jacob and the F.L.O.W. machine is a depiction of a normalizing flow\\nmodel. Let’s now explore the theory of normalizing flows in more detail, before we\\nimplement a practical example using Keras.\\nNormalizing Flows\\nThe motivation of normalizing flow models is similar to that of variational autoen‐\\ncoders, which we explored in Chapter 3. To recap, in a variational autoencoder, we\\nlearn an encoder mapping function between a complex distribution and a much sim‐\\npler distribution that we can sample from. We then also learn a decoder mapping\\nNormalizing Flows \\n| \\n169'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 197}, page_content='function from the simpler distribution to the complex distribution, so that we can\\ngenerate a new data point by sampling a point z from the simpler distribution and\\napplying the learned transformation. Probabilistically speaking, the decoder models\\np x z  but the encoder is only an approximation q z x  of the true p z x —the\\nencoder and decoder are two completely distinct neural networks.\\nIn a normalizing flow model, the decoding function is designed to be the exact\\ninverse of the encoding function and quick to calculate, giving normalizing flows the\\nproperty of tractability. However, neural networks are not by default invertible func‐\\ntions. This raises the question of how we can create an invertible process that con‐\\nverts between a complex distribution (such as the data generation distribution of a set\\nof watercolor paintings) and a much simpler distribution (such as a bell-shaped\\nGaussian distribution) while still making use of the flexibility and power of deep\\nlearning.\\nTo answer this question, we first need to understand a technique known as change of\\nvariables. For this section, we will work with a simple example in just two dimen‐\\nsions, so that you can see exactly how normalizing flows work in fine detail. More\\ncomplex examples are just extensions of the basic techniques presented here.\\nChange of Variables\\nSuppose we have a probability distribution pX x  defined over a rectangle X in two\\ndimensions (x = x1, x2 ), as shown in Figure 6-2.\\nFigure 6-2. A probability distribution pX x  defined over two dimensions, shown in 2D\\n(left) and 3D (right)\\nThis function integrates to 1 over the domain of the distribution (i.e., x1in the range\\n[1, 4] and x2 in the range [0, 2]), so it represents a well-defined probability distribu‐\\ntion. We can write this as follows:\\n170 \\n| \\nChapter 6: Normalizing Flow Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 198}, page_content='∫0\\n2∫1\\n4\\npX x dx1dx2 = 1\\nLet’s say that we want to shift and scale this distribution so that it is instead defined\\nover a unit square Z. We can achieve this by defining a new variable z = z1, z2  and a\\nfunction f  that maps each point in X to exactly one point in Z as follows:\\nz = f x\\nz1 =\\nx1 −1\\n3\\nz2 =\\nx2\\n2\\nNote that this function is invertible. That is, there is a function g that maps every z\\nback to its corresponding x. This is essential for a change of variables, as otherwise we\\ncannot consistently map backward and forward between the two spaces. We can find\\ng simply by rearranging the equations that define f , as shown in Figure 6-3.\\nFigure 6-3. Changing variables between X and Z\\nWe now need to see how the change of variables from X to Z affects the probability\\ndistribution pX x . We can do this by plugging the equations that define g into pX x\\nto transform it into a function pZ z  that is defined in terms of z:\\npZ z\\n=\\n3z1 + 1 −1 2z2\\n9\\n=\\n2z1z2\\n3\\nNormalizing Flows \\n| \\n171'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 199}, page_content='However, if we now integrate pZ z  over the unit square, we can see that we have a\\nproblem!\\n∫0\\n1∫0\\n1 2z1z2\\n3\\ndz1dz2 = 1\\n6\\nThe transformed function pZ z  is now no longer a valid probability distribution,\\nbecause it only integrates to 1/6. If we want to transform our complex probability dis‐\\ntribution over the data into a simpler distribution that we can sample from, we must\\nensure that it integrates to 1.\\nThe missing factor of 6 is due to the fact that the domain of our transformed proba‐\\nbility distribution is six times smaller than the original domain—the original rectan‐\\ngle X had area 6, and this has been compressed into a unit square Z that only has area\\n1. Therefore, we need to multiply the new probability distribution by a normalization\\nfactor that is equal to the relative change in area (or volume in higher dimensions).\\nLuckily, there is a way to calculate this volume change for a given transformation—it\\nis the absolute value of the Jacobian determinant of the transformation. Let’s unpack\\nthat!\\nThe Jacobian Determinant\\nThe Jacobian of a function z = f x  is the matrix of its first-order partial derivatives,\\nas shown here:\\nJ = ∂z\\n∂x =\\n∂z1\\n∂x1\\n⋯\\n∂z1\\n∂xn\\n⋱\\n⋮\\n∂zm\\n∂x1\\n⋯\\n∂zm\\n∂xn\\nThe best way to explain this is with our example. If we take the partial derivative of z1\\nwith respect to x1, we obtain 1\\n3. If we take the partial derivative of z1 with respect to\\nx2, we obtain 0. Similarly, if we take the partial derivative of z2 with respect to x1, we\\nobtain 0. Lastly, if we take the partial derivative of z2 with respect to x2, we obtain 1\\n2.\\nTherefore, the Jacobian matrix for our function f x  is as follows:\\n172 \\n| \\nChapter 6: Normalizing Flow Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 200}, page_content='J =\\n1\\n3 0\\n0 1\\n2\\nThe determinant is only defined for square matrices and is equal to the signed volume\\nof the parallelepiped created by applying the transformation represented by the\\nmatrix to the unit (hyper)cube. In two dimensions, this is therefore just the signed\\narea of the parallelogram created by applying the transformation represented by the\\nmatrix to the unit square.\\nThere is a general formula for calculating the determinant of a matrix with n dimen‐\\nsions, which runs in �n3  time. For our example, we only need the formula for two\\ndimensions, which is simply as follows:\\ndet a b\\nc d = ad −bc\\nTherefore, for our example, the determinant of the Jacobian is 1\\n3 × 1\\n2 −0 × 0 = 1\\n6. This\\nis the scaling factor of 1/6 that we need to ensure that the probability distribution\\nafter transformation still integrates to 1!\\nBy definition, the determinant is signed—that is, it can be negative.\\nTherefore we need to take the absolute value of the Jacobian deter‐\\nminant in order to obtain the relative change of volume.\\nThe Change of Variables Equation\\nWe can now write down a single equation that describes the process for changing\\nvariables between X and Z. This is known as the change of variables equation (Equa‐\\ntion 6-1).\\nEquation 6-1. The change of variables equation\\npX x = pZ z det ∂z\\n∂x\\nHow does this help us build a generative model? The key is understanding that if\\npZ z  is a simple distribution from which we can easily sample (e.g., a Gaussian), then\\nin theory, all we need to do is find an appropriate invertible function f x  that can\\nmap from the data X into Z and the corresponding inverse function g z  that can be\\nNormalizing Flows \\n| \\n173'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 201}, page_content='used to map a sampled z back to a point x in the original domain. We can use the\\npreceding equation involving the Jacobian determinant to find an exact, tractable for‐\\nmula for the data distribution p x .\\nHowever, there are two major issues when applying this in practice that we first need\\nto address!\\nFirstly, calculating the determinant of a high-dimensional matrix is computationally\\nextremely expensive—specifically, it is �n3 . This is completely impractical to imple‐\\nment in practice, as even small 32 × 32–pixel grayscale images have 1,024 dimensions.\\nSecondly, it is not immediately obvious how we should go about calculating the inver‐\\ntible function f x . We could use a neural network to find some function f x  but we\\ncannot necessarily invert this network—neural networks only work in one direction!\\nTo solve these two problems, we need to use a special neural network architecture\\nthat ensures that the change of variables function f  is invertible and has a determi‐\\nnant that is easy to calculate.\\nWe shall see how to do this in the following section using a technique called real-\\nvalued non-volume preserving (RealNVP) transformations.\\nRealNVP\\nRealNVP was first introduced by Dinh et al. in 2017.1 In this paper the authors show\\nhow to construct a neural network that can transform a complex data distribution\\ninto a simple Gaussian, while also possessing the desired properties of being inverti‐\\nble and having a Jacobian that can be easily calculated.\\nRunning the Code for This Example\\nThe code for this example can be found in the Jupyter notebook\\nlocated at notebooks/06_normflow/01_realnvp/realnvp.ipynb in the\\nbook repository.\\nThe code has been adapted from the excellent RealNVP tutorial\\ncreated by Mandolini Giorgio Maria et al. available on the Keras\\nwebsite.\\nThe Two Moons Dataset\\nThe dataset we will use for this example is created by the make_moons function from\\nthe Python library sklearn. This creates a noisy dataset of points in 2D that resemble\\ntwo crescents, as shown in Figure 6-4.\\n174 \\n| \\nChapter 6: Normalizing Flow Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 202}, page_content='Figure 6-4. The two moons dataset in two dimensions\\nThe code for creating this dataset is given in Example 6-1.\\nExample 6-1. Creating a moons dataset\\ndata = datasets.make_moons(3000, noise=0.05)[0].astype(\"float32\") \\nnorm = layers.Normalization()\\nnorm.adapt(data)\\nnormalized_data = norm(data) \\nMake a noisy, unnormalized moons dataset of 3,000 points.\\nNormalize the dataset to have mean 0 and standard deviation 1.\\nWe will build a RealNVP model that can generate points in 2D that follow a similar\\ndistribution to the two moons dataset. Whilst this is a very simple example, it will\\nhelp us understand how a normalizing flow model works in practice, in fine detail.\\nFirst, however, we need to introduce a new type of layer, called a coupling layer.\\nCoupling Layers\\nA coupling layer produces a scale and translation factor for each element of its input.\\nIn other words, it produces two tensors that are exactly the same size as the input,\\none for the scale factor and one for the translation factor, as shown in Figure 6-5.\\nFigure 6-5. A coupling layer outputs two tensors that are the same shape as the input: a\\nscaling factor (s) and a translation factor (t)\\nRealNVP \\n| \\n175'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 203}, page_content='To build a custom Coupling layer for our simple example, we can stack Dense layers\\nto create the scale output and a different set of Dense layers to create the translation\\noutput, as shown in Example 6-2.\\nFor images, Coupling layer blocks use Conv2D layers instead of\\nDense layers.\\nExample 6-2. A Coupling layer in Keras\\ndef Coupling():\\n    input_layer = layers.Input(shape=2) \\n    s_layer_1 = layers.Dense(\\n        256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)\\n    )(input_layer) \\n    s_layer_2 = layers.Dense(\\n        256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)\\n    )(s_layer_1)\\n    s_layer_3 = layers.Dense(\\n        256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)\\n    )(s_layer_2)\\n    s_layer_4 = layers.Dense(\\n        256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)\\n    )(s_layer_3)\\n    s_layer_5 = layers.Dense(\\n        2, activation=\"tanh\", kernel_regularizer=regularizers.l2(0.01)\\n    )(s_layer_4) \\n    t_layer_1 = layers.Dense(\\n        256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)\\n    )(input_layer) \\n    t_layer_2 = layers.Dense(\\n        256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)\\n    )(t_layer_1)\\n    t_layer_3 = layers.Dense(\\n        256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)\\n    )(t_layer_2)\\n    t_layer_4 = layers.Dense(\\n        256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)\\n    )(t_layer_3)\\n    t_layer_5 = layers.Dense(\\n        2, activation=\"linear\", kernel_regularizer=regularizers.l2(0.01)\\n    )(t_layer_4) \\n    return models.Model(inputs=input_layer, outputs=[s_layer_5, t_layer_5]) \\nThe input to the Coupling layer block in our example has two dimensions.\\n176 \\n| \\nChapter 6: Normalizing Flow Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 204}, page_content='The scaling stream is a stack of Dense layers of size 256.\\nThe final scaling layer is of size 2 and has tanh activation.\\nThe translation stream is a stack of Dense layers of size 256.\\nThe final translation layer is of size 2 and has linear activation.\\nThe Coupling layer is constructed as a Keras Model with two outputs (the scaling\\nand translation factors).\\nNotice how the number of channels is temporarily increased to allow for a more\\ncomplex representation to be learned, before being collapsed back down to the same\\nnumber of channels as the input. In the original paper, the authors also use regulariz‐\\ners on each layer to penalize large weights.\\nPassing data through a coupling layer\\nThe architecture of a coupling layer is not particularly interesting—what makes it\\nunique is the way the input data is masked and transformed as it is fed through the\\nlayer, as shown in Figure 6-6.\\nFigure 6-6. The process of transforming the input x through a coupling layer\\nNotice how only the first d dimensions of the data are fed through to the first cou‐\\npling layer—the remaining D −d dimensions are completely masked (i.e., set to\\nzero). In our simple example with D = 2, choosing d = 1 means that instead of the\\ncoupling layer seeing two values, x1, x2 , the layer sees x1, 0 .\\nRealNVP \\n| \\n177'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 205}, page_content='The outputs from the layer are the scale and translation factors. These are again\\nmasked, but this time with the inverse mask to previously, so that only the second\\nhalves are let through—i.e., in our example, we obtain 0, s2  and 0, t2 . These are\\nthen applied element-wise to the second half of the input x2 and the first half of the\\ninput x1 is simply passed straight through, without being updated at all. In summary,\\nfor a vector with dimension D where d < D, the update equations are as follows:\\nz1:d = x1:d\\nzd + 1:D = xd + 1:D ⊙exp s x1:d\\n+ t x1:d\\nYou may be wondering why we go to the trouble of building a layer that masks so\\nmuch information. The answer is clear if we investigate the structure of the Jacobian\\nmatrix of this function:\\n∂z\\n∂x =\\n�\\n0\\n∂zd + 1:D\\n∂x1:d\\ndiag exp s x1:d\\nThe top-left d × d submatrix is simply the identity matrix, because z1:d = x1:d. These\\nelements are passed straight through without being updated. The top-right submatrix\\nis therefore 0, because z1:d is not dependent on xd + 1:D.\\nThe bottom-left submatrix is complex, and we do not seek to simplify this. The\\nbottom-right submatrix is simply a diagonal matrix, filled with the elements of\\nexp s x1:d , because zd + 1:D is linearly dependent on xd + 1:D and the gradient is\\ndependent only on the scaling factor (not on the translation factor). Figure 6-7 shows\\na diagram of this matrix form, where only the nonzero elements are filled in with\\ncolor.\\nNotice how there are no nonzero elements above the diagonal—for this reason, this\\nmatrix form is called lower triangular. Now we see the benefit of structuring the\\nmatrix in this way—the determinant of a lower-triangular matrix is just equal to the\\nproduct of the diagonal elements. In other words, the determinant is not dependent\\non any of the complex derivatives in the bottom-left submatrix!\\n178 \\n| \\nChapter 6: Normalizing Flow Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 206}, page_content='Figure 6-7. The Jacobian matrix of the transformation—a lower triangular matrix, with\\ndeterminant equal to the product of the elements along the diagonal\\nTherefore, we can write the determinant of this matrix as follows:\\ndet J = exp ∑\\nj s x1:d j\\nThis is easily computable, which was one of the two original goals of building a nor‐\\nmalizing flow model.\\nThe other goal was that the function must be easily invertible. We can see that this is\\ntrue as we can write down the invertible function just by rearranging the forward\\nequations, as follows:\\nx1:d = z1:d\\nxd + 1:D = zd + 1:D −t x1:d\\n⊙exp −s x1:d\\nRealNVP \\n| \\n179'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 207}, page_content='The equivalent diagram is shown in Figure 6-8.\\nFigure 6-8. The inverse function x = g(z)\\nWe now have almost everything we need to build our RealNVP model. However,\\nthere is one issue that still remains—how should we update the first d elements of the\\ninput? Currently they are left completely unchanged by the model!\\nStacking coupling layers\\nTo resolve this problem, we can use a really simple trick. If we stack coupling layers\\non top of each other but alternate the masking pattern, the layers that are left\\nunchanged by one layer will be updated in the next. This architecture has the added\\nbenefit of being able to learn more complex representations of the data, as it is a\\ndeeper neural network.\\nThe Jacobian of this composition of coupling layers will still be simple to compute,\\nbecause linear algebra tells us that the determinant of a matrix product is the product\\nof the determinants. Similarly, the inverse of the composition of two functions is just\\nthe composition of the inverses, as shown in the following equations:\\ndet A · B = det A det B\\nf b ∘f a\\n−1 = f a\\n−1 ∘f b\\n−1\\nTherefore, if we stack coupling layers, flipping the masking each time, we can build a\\nneural network that is able to transform the whole input tensor, while retaining the\\nessential properties of having a simple Jacobian determinant and being invertible.\\nFigure 6-9 shows the overall structure.\\n180 \\n| \\nChapter 6: Normalizing Flow Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 208}, page_content='Figure 6-9. Stacking coupling layers, alternating the masking with each layer\\nTraining the RealNVP Model\\nNow that we have built the RealNVP model, we can train it to learn the complex dis‐\\ntribution of the two moons dataset. Remember, we want to minimize the negative\\nlog-likelihood of the data under the model −log pX x . Using Equation 6-1, we can\\nwrite this as follows:\\n−log pX x = −log pZ z −log det ∂z\\n∂x\\nWe choose the target output distribution pZ z  of the forward process f  to be a stan‐\\ndard Gaussian, because we can easily sample from this distribution. We can then\\ntransform a point sampled from the Gaussian back into the original image domain by\\napplying the inverse process g, as shown in Figure 6-10.\\nFigure 6-10. Transforming between the complex distribution pX x  and a simple Gaus‐\\nsian pZ z  in 1D (middle row) and 2D (bottom row)\\nRealNVP \\n| \\n181'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 209}, page_content='Example 6-3 shows how to build a RealNVP network, as a custom Keras Model.\\nExample 6-3. Building the RealNVP model in Keras\\nclass RealNVP(models.Model):\\n    def __init__(self, input_dim, coupling_layers, coupling_dim, regularization):\\n        super(RealNVP, self).__init__()\\n        self.coupling_layers = coupling_layers\\n        self.distribution = tfp.distributions.MultivariateNormalDiag(\\n            loc=[0.0, 0.0], scale_diag=[1.0, 1.0]\\n        ) \\n        self.masks = np.array(\\n            [[0, 1], [1, 0]] * (coupling_layers // 2), dtype=\"float32\"\\n        ) \\n        self.loss_tracker = metrics.Mean(name=\"loss\")\\n        self.layers_list = [\\n            Coupling(input_dim, coupling_dim, regularization)\\n            for i in range(coupling_layers)\\n        ] \\n    @property\\n    def metrics(self):\\n        return [self.loss_tracker]\\n    def call(self, x, training=True):\\n        log_det_inv = 0\\n        direction = 1\\n        if training:\\n            direction = -1\\n        for i in range(self.coupling_layers)[::direction]: \\n            x_masked = x * self.masks[i]\\n            reversed_mask = 1 - self.masks[i]\\n            s, t = self.layers_list[i](x_masked)\\n            s *= reversed_mask\\n            t *= reversed_mask\\n            gate = (direction - 1) / 2\\n            x = (\\n                reversed_mask\\n                * (x * tf.exp(direction * s) + direction * t * tf.exp(gate * s))\\n                + x_masked\\n            ) \\n            log_det_inv += gate * tf.reduce_sum(s, axis = 1) \\n        return x, log_det_inv\\n    def log_loss(self, x):\\n        y, logdet = self(x)\\n        log_likelihood = self.distribution.log_prob(y) + logdet \\n        return -tf.reduce_mean(log_likelihood)\\n    def train_step(self, data):\\n        with tf.GradientTape() as tape:\\n182 \\n| \\nChapter 6: Normalizing Flow Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 210}, page_content='loss = self.log_loss(data)\\n        g = tape.gradient(loss, self.trainable_variables)\\n        self.optimizer.apply_gradients(zip(g, self.trainable_variables))\\n        self.loss_tracker.update_state(loss)\\n        return {\"loss\": self.loss_tracker.result()}\\n    def test_step(self, data):\\n        loss = self.log_loss(data)\\n        self.loss_tracker.update_state(loss)\\n        return {\"loss\": self.loss_tracker.result()}\\nmodel = RealNVP(\\n    input_dim = 2\\n    , coupling_layers= 6\\n    , coupling_dim = 256\\n    , regularization = 0.01\\n)\\nmodel.compile(optimizer=optimizers.Adam(learning_rate=0.0001))\\nmodel.fit(\\n    normalized_data\\n    , batch_size=256\\n    , epochs=300\\n)\\nThe target distribution is a standard 2D Gaussian.\\nHere, we create the alternating mask pattern.\\nA list of Coupling layers that define the RealNVP network.\\nIn the main call function of the network, we loop over the Coupling layers. If\\ntraining=True, then we move forward through the layers (i.e., from data to\\nlatent space). If training=False, then we move backward through the layers (i.e.,\\nfrom latent space to data).\\nThis line describes both the forward and backward equations dependent on the\\ndirection (try plugging in direction = -1 and direction = 1 to prove this to\\nyourself!).\\nThe log determinant of the Jacobian, which we need to calculate the loss func‐\\ntion, is simply the sum of the scaling factors.\\nThe loss function is the negative sum of the log probability of the transformed\\ndata, under our target Gaussian distribution and the log determinant of the\\nJacobian.\\nRealNVP \\n| \\n183'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 211}, page_content='Analysis of the RealNVP Model\\nOnce the model is trained, we can use it to transform the training set into the latent\\nspace (using the forward direction, f ) and, more importantly, to transform a sampled\\npoint in the latent space into a point that looks like it could have been sampled from\\nthe original data distribution (using the backward direction, g).\\nFigure 6-11 shows the output from the network before any learning has taken place—\\nthe forward and backward directions just pass information straight through with\\nhardly any transformation.\\nFigure 6-11. The RealNVP model inputs (left) and outputs (right) before training, for the\\nforward process (top) and the reverse process (bottom)\\nAfter training (Figure 6-12), the forward process is able to convert the points from\\nthe training set into a distribution that resembles a Gaussian. Likewise, the backward\\nprocess can take points sampled from a Gaussian distribution and map them back to\\na distribution that resembles the original data.\\n184 \\n| \\nChapter 6: Normalizing Flow Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 212}, page_content='Figure 6-12. The RealNVP model inputs (left) and outputs (right) after training, for the\\nforward process (top) and the reverse process (bottom)\\nThe loss curve for the training process is shown in Figure 6-13.\\nFigure 6-13. The loss curve for the RealNVP training process\\nThis completes our discussion of RealNVP, a specific case of a normalizing flow gen‐\\nerative model. In the next section, we’ll cover some modern normalizing flow models\\nthat extend the ideas introduced in the RealNVP paper.\\nRealNVP \\n| \\n185'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 213}, page_content='Other Normalizing Flow Models\\nTwo other successful and important normalizing flow models are GLOW and\\nFFJORD. The following sections describe the key advancements they made.\\nGLOW\\nPresented at NeurIPS 2018, GLOW was one of the first models to demonstrate the\\nability of normalizing flows to generate high-quality samples and produce a meaning‐\\nful latent space that can be traversed to manipulate samples. The key step was to\\nreplace the reverse masking setup with invertible 1 × 1 convolutional layers. For\\nexample, with RealNVP applied to images, the ordering of the channels is flipped\\nafter each step, to ensure that the network gets the chance to transform all of the\\ninput. In GLOW a 1 × 1 convolution is applied instead, which effectively acts as a\\ngeneral method to produce any permutation of the channels that the model desires.\\nThe authors show that even with this addition, the distribution as a whole remains\\ntractable, with determinants and inverses that are easy to compute at scale.\\nFigure 6-14. Random samples from the GLOW model (source: Kingma and Dhariwal,\\n2018)2\\n186 \\n| \\nChapter 6: Normalizing Flow Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 214}, page_content='FFJORD\\nRealNVP and GLOW are discrete time normalizing flows—that is, they transform the\\ninput through a discrete set of coupling layers. FFJORD (Free-Form Continuous\\nDynamics for Scalable Reversible Generative Models), presented at ICLR 2019, shows\\nhow it is possible to model the transformation as a continuous time process (i.e., by\\ntaking the limit as the number of steps in the flow tends to infinity and the step size\\ntends to zero). In this case, the dynamics are modeled using an ordinary differential\\nequation (ODE) whose parameters are produced by a neural network (f θ). A black-\\nbox solver is used to solve the ODE at time t1—i.e., to find z1 given some initial point\\nz0 sampled from a Gaussian at t0, as described by the following equations:\\nz0 ∼p z0\\n∂z t\\n∂t\\n= f θ x t , t\\nx = z1\\nA diagram of the transformation process is shown in Figure 6-15.\\nFigure 6-15. FFJORD models the transformation between the data distribution and a\\nstandard Gaussian via an ordinary differential equation, parameterized by a neural\\nnetwork (source: Will Grathwohl et al., 2018)3\\nOther Normalizing Flow Models \\n| \\n187'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 215}, page_content='Summary\\nIn this chapter we explored normalizing flow models such as RealNVP, GLOW, and\\nFFJORD.\\nA normalizing flow model is an invertible function defined by a neural network that\\nallows us to directly model the data density via a change of variables. In the general\\ncase, the change of variables equation requires us to calculate a highly complex Jaco‐\\nbian determinant, which is impractical for all but the simplest of examples.\\nTo sidestep this issue, the RealNVP model restricts the form of the neural network,\\nsuch that it adheres to the two essential criteria: it is invertible and has a Jacobian\\ndeterminant that is easy to compute.\\nIt does this through stacking coupling layers, which produce scale and translation fac‐\\ntors at each step. Importantly, the coupling layer masks the data as it flows through\\nthe network, in a way that ensures that the Jacobian is lower triangular and therefore\\nhas a simple-to-compute determinant. Full visibility of the input data is achieved\\nthrough flipping the masks at each layer.\\nBy design, the scale and translation operations can be easily inverted, so that once the\\nmodel is trained it is possible to run data through the network in reverse. This means\\nthat we can target the forward transformation process toward a standard Gaussian,\\nwhich we can easily sample from. We can then run the sampled points backward\\nthrough the network to generate new observations.\\nThe RealNVP paper also shows how it is possible to apply this technique to images,\\nby using convolutions inside the coupling layers, rather than densely connected lay‐\\ners. The GLOW paper extended this idea to remove the necessity for any hardcoded\\npermutation of the masks. The FFJORD model introduced the concept of continuous\\ntime normalizing flows, by modeling the transformation process as an ODE defined\\nby a neural network.\\nOverall, we have seen how normalizing flows are a powerful generative modeling\\nfamily that can produce high-quality samples, while maintaining the ability to tracta‐\\nbly describe the data density function.\\nReferences\\n1. Laurent Dinh et al., “Density Estimation Using Real NVP,” May 27, 2016, https://\\narxiv.org/abs/1605.08803v3.\\n2. Diedrick P. Kingma and Prafulla Dhariwal, “Glow: Generative Flow with Invertible\\n1x1 Convolutions,” July 10, 2018, https://arxiv.org/abs/1807.03039.\\n3. Will Grathwohl et al., “FFJORD: Free-Form Continuous Dynamics for Scalable\\nReversible Generative Models,” October 22, 2018, https://arxiv.org/abs/1810.01367.\\n188 \\n| \\nChapter 6: Normalizing Flow Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 216}, page_content='CHAPTER 7\\nEnergy-Based Models\\nChapter Goals\\nIn this chapter you will:\\n• Understand how to formulate a deep energy-based model (EBM).\\n• See how to sample from an EBM using Langevin dynamics.\\n• Train your own EBM using contrastive divergence.\\n• Analyze the EBM, including viewing snapshots of the Langevin dynamics sam‐\\npling process.\\n• Learn about other types of EBM, such as restricted Boltzmann machines.\\nEnergy-based models are a broad class of generative model that borrow a key idea\\nfrom modeling physical systems—namely, that the probability of an event can be\\nexpressed using a Boltzmann distribution, a specific function that normalizes a real-\\nvalued energy function between 0 and 1. This distribution was originally formulated\\nin 1868 by Ludwig Boltzmann, who used it to describe gases in thermal equilibrium.\\nIn this chapter, we will see how we can use this idea to train a generative model that\\ncan be used to produce images of handwritten digits. We will explore several new\\nconcepts, including contrastive divergence for training the EBM and Langevin\\ndynamics for sampling.\\nIntroduction\\nWe will begin with a short story to illustrate the key concepts behind energy-based\\nmodels.\\n189'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 217}, page_content='The Long-au-Vin Running Club\\nDiane Mixx was head coach of the long-distance running team in the fictional French\\ntown of Long-au-Vin. She was well known for her exceptional abilities as a trainer\\nand had acquired a reputation for being able to turn even the most mediocre of ath‐\\nletes into world-class runners (Figure 7-1).\\nFigure 7-1. A running coach training some elite athletes (created with Midjourney)\\nHer methods were based around assessing the energy levels of each athlete. Over\\nyears of working with athletes of all abilities, she had developed an incredibly accurate\\nsense of just how much energy a particular athlete had left after a race, just by looking\\nat them. The lower an athlete’s energy level, the better—elite athletes always gave\\neverything they had during the race!\\nTo keep her skills sharp, she regularly trained herself by measuring the contrast\\nbetween her energy sensing abilities on known elite athletes and the best athletes\\nfrom her club. She ensured that the divergence between her predictions for these two\\ngroups was as large as possible, so that people would take her seriously if she said that\\nshe had found a true elite athlete within her club.\\nThe real magic was her ability to convert a mediocre runner into a top-class runner.\\nThe process was simple—she measured the current energy level of the athlete and\\nworked out the optimal set of adjustments the athlete needed to make to improve\\ntheir performance next time. Then, after making these adjustments, she measured the\\nathlete’s energy level again, looking for it to be slightly lower than before, explaining\\nthe improved performance on the track. This process of assessing the optimal adjust‐\\nments and taking a small step in the right direction would continue until eventually\\nthe athlete was indistinguishable from a world-class runner.\\n190 \\n| \\nChapter 7: Energy-Based Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 218}, page_content='After many years Diane retired from coaching and published a book on her methods\\nfor generating elite athletes—a system she branded the “Long-au-Vin, Diane Mixx”\\ntechnique.\\nThe story of Diane Mixx and the Long-au-Vin running club captures the key ideas\\nbehind energy-based modeling. Let’s now explore the theory in more detail, before\\nwe implement a practical example using Keras.\\nEnergy-Based Models\\nEnergy-based models attempt to model the true data-generating distribution using a\\nBoltzmann distribution (Equation 7-1) where E x  is know as the energy function (or\\nscore) of an observation x.\\nEquation 7-1. Boltzmann distribution\\np �=\\ne−E �\\n∫�∈�e−E �\\nIn practice, this amounts to training a neural network E x  to output low scores for\\nlikely observations (so p� is close to 1) and high scores for unlikely observations (so\\np� is close to 0).\\nThere are two challenges with modeling the data in this way. Firstly, it is not clear\\nhow we should use our model for sampling new observations—we can use it to gen‐\\nerate a score given an observation, but how do we generate an observation that has a\\nlow score (i.e., a plausible observation)?\\nSecondly, the normalizing denominator of Equation 7-1 contains an integral that is\\nintractable for all but the simplest of problems. If we cannot calculate this integral,\\nthen we cannot use maximum likelihood estimation to train the model, as this\\nrequires that p� is a valid probability distribution.\\nThe key idea behind an energy-based model is that we can use approximation tech‐\\nniques to ensure we never need to calculate the intractable denominator. This is in\\ncontrast to, say, a normalizing flow, where we go to great lengths to ensure that the\\ntransformations that we apply to our standard Gaussian distribution do not change\\nthe fact that the output is still a valid probability distribution.\\nWe sidestep the tricky intractable denominator problem by using a technique called\\ncontrastive divergence (for training) and a technique called Langevin dynamics\\n(for sampling), following the ideas from Du and Mordatch’s 2019 paper “Implicit\\nEnergy-Based Models \\n| \\n191'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 219}, page_content='Generation and Modeling with Energy-Based Models.”1 We shall explore these tech‐\\nniques in detail while building our own EBM later in the chapter.\\nFirst, let’s get set up with a dataset and design a simple neural network that will repre‐\\nsent our real-valued energy function E x .\\nRunning the Code for This Example\\nThe code for this example can be found in the Jupyter notebook\\nlocated at notebooks/07_ebm/01_ebm/ebm.ipynb in the book\\nrepository.\\nThe code is adapted from the excellent tutorial on deep energy-\\nbased generative models by Phillip Lippe.\\nThe MNIST Dataset\\nWe’ll be using the standard MNIST dataset, consisting of grayscale images of hand‐\\nwritten digits. Some example images from the dataset are shown in Figure 7-2.\\nFigure 7-2. Examples of images from the MNIST dataset\\nThe dataset comes prepackaged with TensorFlow, so it can be downloaded as shown\\nin Example 7-1.\\nExample 7-1. Loading the MNIST dataset\\nfrom tensorflow.keras import datasets\\n(x_train, _), (x_test, _) = datasets.mnist.load_data()\\nAs usual, we’ll scale the pixel values to the range [-1, 1] and add some padding to\\nmake the images 32 × 32 pixels in size. We also convert it to a TensorFlow Dataset, as\\nshown in Example 7-2.\\nExample 7-2. Preprocessing the MNIST dataset\\ndef preprocess(imgs):\\n    imgs = (imgs.astype(\"float32\") - 127.5) / 127.5\\n    imgs = np.pad(imgs , ((0,0), (2,2), (2,2)), constant_values= -1.0)\\n192 \\n| \\nChapter 7: Energy-Based Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 220}, page_content='imgs = np.expand_dims(imgs, -1)\\n    return imgs\\nx_train = preprocess(x_train)\\nx_test = preprocess(x_test)\\nx_train = tf.data.Dataset.from_tensor_slices(x_train).batch(128)\\nx_test = tf.data.Dataset.from_tensor_slices(x_test).batch(128)\\nNow that we have our dataset, we can build the neural network that will represent our\\nenergy function E x .\\nThe Energy Function\\nThe energy function Eθ x  is a neural network with parameters θ that can transform\\nan input image x into a scalar value. Throughout this network, we make use of an\\nactivation function called swish, as described in the following sidebar.\\nSwish Activation\\nSwish is an alternative to ReLU that was introduced by Google in 20172 and is defined\\nas follows:\\nswish x = x · sigmoid x =\\nx\\ne−x + 1\\nSwish is visually similar to ReLU, with the key difference being that it is smooth,\\nwhich helps to alleviate the vanishing gradient problem. This is particularly impor‐\\ntant for energy-based models. A plot of the swish function is shown in Figure 7-3.\\nFigure 7-3. The swish activation function\\nEnergy-Based Models \\n| \\n193'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 221}, page_content='The network is a set of stacked Conv2D layers that gradually reduce the size of the\\nimage while increasing the number of channels. The final layer is a single fully con‐\\nnected unit with linear activation, so the network can output values in the range (−∞,\\n∞). The code to build it is given in Example 7-3.\\nExample 7-3. Building the energy function E x  neural network\\nebm_input = layers.Input(shape=(32, 32, 1))\\nx = layers.Conv2D(\\n    16, kernel_size=5, strides=2, padding=\"same\", activation = activations.swish\\n)(ebm_input) \\nx = layers.Conv2D(\\n    32, kernel_size=3, strides=2, padding=\"same\", activation = activations.swish\\n)(x)\\nx = layers.Conv2D(\\n    64, kernel_size=3, strides=2, padding=\"same\", activation = activations.swish\\n)(x)\\nx = layers.Conv2D(\\n    64, kernel_size=3, strides=2, padding=\"same\", activation = activations.swish\\n)(x)\\nx = layers.Flatten()(x)\\nx = layers.Dense(64, activation = activations.swish)(x)\\nebm_output = layers.Dense(1)(x) \\nmodel = models.Model(ebm_input, ebm_output) \\nThe energy function is a set of stacked Conv2D layers, with swish activation.\\nThe final layer is a single fully connected unit, with a linear activation function.\\nA Keras Model that converts the input image into a scalar energy value.\\nSampling Using Langevin Dynamics\\nThe energy function only outputs a score for a given input—how can we use this\\nfunction to generate new samples that have a low energy score?\\nWe will use a technique called Langevin dynamics, which makes use of the fact that we\\ncan compute the gradient of the energy function with respect to its input. If we start\\nfrom a random point in the sample space and take small steps in the opposite direc‐\\ntion of the calculated gradient, we will gradually reduce the energy function. If our\\nneural network is trained correctly, then the random noise should transform into an\\nimage that resembles an observation from the training set before our eyes!\\n194 \\n| \\nChapter 7: Energy-Based Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 222}, page_content='Stochastic Gradient Langevin Dynamics\\nImportantly, we must also add a small amount of random noise to\\nthe input as we travel across the sample space; otherwise, there is a\\nrisk of falling into local minima. The technique is therefore known\\nas stochastic gradient Langevin dynamics.3\\nWe can visualize this gradient descent as shown in Figure 7-4, for a two-dimensional\\nspace with the energy function value on the third dimension. The path is a noisy\\ndescent downhill, following the negative gradient of the energy function E x  with\\nrespect to the input x. In the MNIST image dataset, we have 1,024 pixels so are navi‐\\ngating a 1,024-dimensional space, but the same principles apply!\\nFigure 7-4. Gradient descent using Langevin dynamics\\nIt is worth noting the difference between this kind of gradient descent and the kind of\\ngradient descent we normally use to train a neural network.\\nWhen training a neural network, we calculate the gradient of the loss function with\\nrespect to the parameters of the network (i.e., the weights) using backpropagation.\\nThen we update the parameters a small amount in the direction of the negative gradi‐\\nent, so that over many iterations, we gradually minimize the loss.\\nWith Langevin dynamics, we keep the neural network weights fixed and calculate the\\ngradient of the output with respect to the input. Then we update the input a small\\namount in the direction of the negative gradient, so that over many iterations, we\\ngradually minimize the output (the energy score).\\nBoth processes utilize the same idea (gradient descent), but are applied to different\\nfunctions and with respect to different entities.\\nEnergy-Based Models \\n| \\n195'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 223}, page_content='Formally, Langevin dynamics can be described by the following equation:\\nxk = xk −1 −η∇xEθ xk −1 + ω\\nwhere ω ∼�0, σ  and x0 ∼�(–1,1). η is the step size hyperparameter that must be\\ntuned—too large and the steps jump over minima, too small and the algorithm will\\nbe too slow to converge.\\nx0 ∼�(–1,1) is the uniform distribution on the range [–1, 1].\\nWe can code up our Langevin sampling function as illustrated in Example 7-4.\\nExample 7-4. The Langevin sampling function\\ndef generate_samples(model, inp_imgs, steps, step_size, noise):\\n    imgs_per_step = []\\n    for _ in range(steps): \\n        inp_imgs += tf.random.normal(inp_imgs.shape, mean = 0, stddev = noise) \\n        inp_imgs = tf.clip_by_value(inp_imgs, -1.0, 1.0)\\n        with tf.GradientTape() as tape:\\n            tape.watch(inp_imgs)\\n            out_score = -model(inp_imgs) \\n        grads = tape.gradient(out_score, inp_imgs) \\n        grads = tf.clip_by_value(grads, -0.03, 0.03)\\n        inp_imgs += -step_size * grads \\n        inp_imgs = tf.clip_by_value(inp_imgs, -1.0, 1.0)\\n        return inp_imgs\\nLoop over given number of steps.\\nAdd a small amount of noise to the image.\\nPass the image through the model to obtain the energy score.\\nCalculate the gradient of the output with respect to the input.\\nAdd a small amount of the gradient to the input image.\\n196 \\n| \\nChapter 7: Energy-Based Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 224}, page_content='Training with Contrastive Divergence\\nNow that we know how to sample a novel low-energy point from the sample space,\\nlet’s turn our attention to training the model.\\nWe cannot apply maximum likelihood estimation, because the energy function does\\nnot output a probability; it outputs a score that does not integrate to 1 across the sam‐\\nple space. Instead, we will apply a technique first proposed in 2002 by Geoffrey Hin‐\\nton, called contrastive divergence, for training unnormalized scoring models.4\\nThe value that we want to minimize (as always) is the negative log-likelihood of the\\ndata:\\nℒ= −�x ∼data log pθ �\\nWhen pθ � has the form of a Boltzmann distribution, with energy function Eθ �, it\\ncan be shown that the gradient of this value can be written as follows (Oliver Wood‐\\nford’s “Notes on Contrastive Divergence” for the full derivation):5\\n∇θℒ= �x ∼data ∇θEθ �\\n−�x ∼model ∇θEθ �\\nThis intuitively makes a lot of sense—we want to train the model to output large neg‐\\native energy scores for real observations and large positive energy scores for gener‐\\nated fake observations so that the contrast between these two extremes is as large as\\npossible.\\nIn other words, we can calculate the difference between the energy scores of real and\\nfake samples and use this as our loss function.\\nTo calculate the energy scores of fake samples, we would need to be able to sample\\nexactly from the distribution pθ �, which isn’t possible due to the intractable denom‐\\ninator. Instead, we can use our Langevin sampling procedure to generate a set of\\nobservations with low energy scores. The process would need to run for infinitely\\nmany steps to produce a perfect sample (which is obviously impractical), so instead\\nwe run for some small number of steps, on the assumption that this is good enough\\nto produce a meaningful loss function.\\nWe also maintain a buffer of samples from previous iterations, so that we can use this\\nas the starting point for the next batch, rather than pure random noise. The code to\\nproduce the sampling buffer is shown in Example 7-5.\\nEnergy-Based Models \\n| \\n197'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 225}, page_content='Example 7-5. The Buffer\\nclass Buffer:\\n    def __init__(self, model):\\n        super().__init__()\\n        self.model = model\\n        self.examples = [\\n            tf.random.uniform(shape = (1, 32, 32, 1)) * 2 - 1\\n            for _ in range(128)\\n        ] \\n    def sample_new_exmps(self, steps, step_size, noise):\\n        n_new = np.random.binomial(128, 0.05) \\n        rand_imgs = (\\n            tf.random.uniform((n_new, 32, 32, 1)) * 2 - 1\\n        )\\n        old_imgs = tf.concat(\\n            random.choices(self.examples, k=128-n_new), axis=0\\n        ) \\n        inp_imgs = tf.concat([rand_imgs, old_imgs], axis=0)\\n        inp_imgs = generate_samples(\\n            self.model, inp_imgs, steps=steps, step_size=step_size, noise = noise\\n        ) \\n        self.examples = tf.split(inp_imgs, 128, axis = 0) + self.examples \\n        self.examples = self.examples[:8192]\\n        return inp_imgs\\nThe sampling buffer is initialized with a batch of random noise.\\nOn average, 5% of observations are generated from scratch (i.e., random noise)\\neach time.\\nThe rest are pulled at random from the existing buffer.\\nThe observations are concatenated and run through the Langevin sampler.\\nThe resulting sample is added to the buffer, which is trimmed to a max length of\\n8,192 observations.\\nFigure 7-5 shows one training step of contrastive divergence. The scores of real\\nobservations are pushed down by the algorithm and the scores of fake observations\\nare pulled up, without caring about normalizing these scores after each step.\\n198 \\n| \\nChapter 7: Energy-Based Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 226}, page_content='Figure 7-5. One step of contrastive divergence\\nWe can code up the training step of the contrastive divergence algorithm within a\\ncustom Keras model as shown in Example 7-6.\\nExample 7-6. EBM trained using contrastive divergence\\nclass EBM(models.Model):\\n    def __init__(self):\\n        super(EBM, self).__init__()\\n        self.model = model\\n        self.buffer = Buffer(self.model)\\n        self.alpha = 0.1\\n        self.loss_metric = metrics.Mean(name=\"loss\")\\n        self.reg_loss_metric = metrics.Mean(name=\"reg\")\\n        self.cdiv_loss_metric = metrics.Mean(name=\"cdiv\")\\n        self.real_out_metric = metrics.Mean(name=\"real\")\\n        self.fake_out_metric = metrics.Mean(name=\"fake\")\\n    @property\\n    def metrics(self):\\n        return [\\n            self.loss_metric,\\n            self.reg_loss_metric,\\n            self.cdiv_loss_metric,\\n            self.real_out_metric,\\n            self.fake_out_metric\\n        ]\\n    def train_step(self, real_imgs):\\n        real_imgs += tf.random.normal(\\n            shape=tf.shape(real_imgs), mean = 0, stddev = 0.005\\n        ) \\n        real_imgs = tf.clip_by_value(real_imgs, -1.0, 1.0)\\n        fake_imgs = self.buffer.sample_new_exmps(\\n            steps=60, step_size=10, noise = 0.005\\n        ) \\n        inp_imgs = tf.concat([real_imgs, fake_imgs], axis=0)\\n        with tf.GradientTape() as training_tape:\\nEnergy-Based Models \\n| \\n199'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 227}, page_content='real_out, fake_out = tf.split(self.model(inp_imgs), 2, axis=0) \\n            cdiv_loss = tf.reduce_mean(fake_out, axis = 0) - tf.reduce_mean(\\n                real_out, axis = 0\\n            ) \\n            reg_loss = self.alpha * tf.reduce_mean(\\n                real_out ** 2 + fake_out ** 2, axis = 0\\n            ) \\n            loss = reg_loss + cdiv_loss\\n        grads = training_tape.gradient(loss, self.model.trainable_variables) \\n        self.optimizer.apply_gradients(\\n            zip(grads, self.model.trainable_variables)\\n        )\\n        self.loss_metric.update_state(loss)\\n        self.reg_loss_metric.update_state(reg_loss)\\n        self.cdiv_loss_metric.update_state(cdiv_loss)\\n        self.real_out_metric.update_state(tf.reduce_mean(real_out, axis = 0))\\n        self.fake_out_metric.update_state(tf.reduce_mean(fake_out, axis = 0))\\n        return {m.name: m.result() for m in self.metrics}\\n    def test_step(self, real_imgs): \\n        batch_size = real_imgs.shape[0]\\n        fake_imgs = tf.random.uniform((batch_size, 32, 32, 1)) * 2 - 1\\n        inp_imgs = tf.concat([real_imgs, fake_imgs], axis=0)\\n        real_out, fake_out = tf.split(self.model(inp_imgs), 2, axis=0)\\n        cdiv = tf.reduce_mean(fake_out, axis = 0) - tf.reduce_mean(\\n            real_out, axis = 0\\n        )\\n        self.cdiv_loss_metric.update_state(cdiv)\\n        self.real_out_metric.update_state(tf.reduce_mean(real_out, axis = 0))\\n        self.fake_out_metric.update_state(tf.reduce_mean(fake_out, axis = 0))\\n        return {m.name: m.result() for m in self.metrics[2:]}\\nebm = EBM()\\nebm.compile(optimizer=optimizers.Adam(learning_rate=0.0001), run_eagerly=True)\\nebm.fit(x_train, epochs=60, validation_data = x_test,)\\nA small amount of random noise is added to the real images, to avoid the model\\noverfitting to the training set.\\nA set of fake images are sampled from the buffer.\\nThe real and fake images are run through the model to produce real and fake\\nscores.\\nThe contrastive divergence loss is simply the difference between the scores of real\\nand fake observations.\\nA regularization loss is added to avoid the scores becoming too large.\\n200 \\n| \\nChapter 7: Energy-Based Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 228}, page_content='Gradients of the loss function with respect to the weights of the network are cal‐\\nculated for backpropagation.\\nThe test_step is used during validation and calculates the contrastive diver‐\\ngence between the scores of a set of random noise and data from the training set.\\nIt can be used as a measure for how well the model is training (see the following\\nsection).\\nAnalysis of the Energy-Based Model\\nThe loss curves and supporting metrics from the training process are shown in\\nFigure 7-6.\\nFigure 7-6. Loss curves and metrics for the training process of the EBM\\nFirstly, notice that the loss calculated during the training step is approximately con‐\\nstant and small across epochs. While the model is constantly improving, so is the\\nquality of generated images in the buffer that it is required to compare against real\\nimages from the training set, so we shouldn’t expect the training loss to fall\\nsignificantly.\\nTherefore, to judge model performance, we also set up a validation process that\\ndoesn’t sample from the buffer, but instead scores a sample of random noise and\\ncompares this against the scores of examples from the training set. If the model is\\nimproving, we should see that the contrastive divergence falls over the epochs (i.e., it\\nis getting better at distinguishing random noise from real images), as can be seen in\\nFigure 7-6.\\nEnergy-Based Models \\n| \\n201'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 229}, page_content='Generating new samples from the EBM is simply a case of running the Langevin sam‐\\npler for a large number of steps, from a standing start (random noise), as shown in\\nExample 7-7. The observation is forced downhill, following the gradients of the scor‐\\ning function with respect to the input, so that out of the noise, a plausible observation\\nappears.\\nExample 7-7. Generating new observations using the EBM\\nstart_imgs = np.random.uniform(size = (10, 32, 32, 1)) * 2 - 1\\ngen_img = generate_samples(\\n    ebm.model,\\n    start_imgs,\\n    steps=1000,\\n    step_size=10,\\n    noise = 0.005,\\n    return_img_per_step=True,\\n)\\nSome examples of observations produced by the sampler after 50 epochs of training\\nare shown in Figure 7-7.\\nFigure 7-7. Examples produced by the Langevin sampler using the EBM model to direct\\nthe gradient descent\\nWe can even show a replay of how a single observation is generated by taking snap‐\\nshots of the current observations during the Langevin sampling process—this is\\nshown in Figure 7-8.\\nFigure 7-8. Snapshots of an observation at different steps of the Langevin sampling\\nprocess\\nOther Energy-Based Models\\nIn the previous example we made use of a deep EBM trained using contrastive diver‐\\ngence with a Langevin dynamics sampler. However, early EBM models did not make\\nuse of Langevin sampling, but instead relied on other techniques and architectures.\\n202 \\n| \\nChapter 7: Energy-Based Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 230}, page_content='One of the earliest examples of an EBM was the Boltzmann machine.6 This is a fully\\nconnected, undirected neural network, where binary units are either visible (v) or hid‐\\nden (h). The energy of a given configuration of the network is defined as follows:\\nEθ v, h = −1\\n2 vTLv + hTJh + vTWh\\nwhere W, L, J are the weights matrices that are learned by the model. Training is\\nachieved by contrastive divergence, but using Gibbs sampling to alternate between\\nthe visible and hidden layers until an equilibrium is found. In practice this is very\\nslow and not scalable to large numbers of hidden units.\\nSee Jessica Stringham’s blog post “Gibbs Sampling in Python” for an\\nexcellent simple example of Gibbs sampling.\\nAn extension to this model, the restricted Boltzmann machine (RBM), removes the\\nconnections between units of the same type, therefore creating a two-layer bipartite\\ngraph. This allows RBMs to be stacked into deep belief networks to model more com‐\\nplex distributions. However, modeling high-dimensional data with RBMs remains\\nimpractical, due to the fact that Gibbs sampling with long mixing times is still\\nrequired.\\nIt was only in the late 2000s that EBMs were shown to have potential for modeling\\nmore high-dimensional datasets and a framework for building deep EBMs was estab‐\\nlished.7 Langevin dynamics became the preferred sampling method for EBMs, which\\nlater evolved into a training technique known as score matching. This further devel‐\\noped into a model class known as Denoising Diffusion Probabilistic Models, which\\npower state-of-the-art generative models such as DALL.E 2 and ImageGen. We will\\nexplore diffusion models in more detail in Chapter 8.\\nSummary\\nEnergy-based models are a class of generative model that make use of an energy scor‐\\ning function—a neural network that is trained to output low scores for real observa‐\\ntions and high scores for generated observations. Calculating the probability\\ndistribution given by this score function would require normalizing by an intractable\\ndenominator. EBMs avoid this problem by utilizing two tricks: contrastive divergence\\nfor training the network and Langevin dynamics for sampling new observations.\\nThe energy function is trained by minimizing the difference between the generated\\nsample scores and the scores of the training data, a technique known as contrastive\\nSummary \\n| \\n203'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 231}, page_content='divergence. This can be shown to be equivalent to minimizing the negative log-\\nlikelihood, as required by maximum likelihood estimation, but does not require us to\\ncalculate the intractable normalizing denominator. In practice, we approximate the\\nsampling process for the fake samples to ensure the algorithm remains efficient.\\nSampling of deep EBMs is achieved through Langevin dynamics, a technique that\\nuses the gradient of the score with respect to the input image to gradually transform\\nrandom noise into a plausible observation by updating the input in small steps, fol‐\\nlowing the gradient downhill. This improves upon earlier methods such as Gibbs\\nsampling, which is utilized by restricted Boltzmann machines.\\nReferences\\n1. Yilun Du and Igor Mordatch, “Implicit Generation and Modeling with Energy-\\nBased Models,” March 20, 2019, https://arxiv.org/abs/1903.08689.\\n2. Prajit Ramachandran et al., “Searching for Activation Functions,” October 16, 2017,\\nhttps://arxiv.org/abs/1710.05941v2.\\n3. Max Welling and Yee Whye Teh, “Bayesian Learning via Stochastic Gradient Lan‐\\ngevin \\nDynamics,” \\n2011, \\nhttps://www.stats.ox.ac.uk/~teh/research/compstats/\\nWelTeh2011a.pdf\\n4. Geoffrey E. Hinton, “Training Products of Experts by Minimizing Contrastive\\nDivergence,” 2002, https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf.\\n5. \\nOliver \\nWoodford, \\n“Notes \\non \\nContrastive \\nDivergence,” \\n2006, \\nhttps://\\nwww.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf.\\n6. David H. Ackley et al., “A Learning Algorithm for Boltzmann Machines,” 1985,\\nCognitive Science 9(1), 147-165.\\n7. Yann Lecun et al., “A Tutorial on Energy-Based Learning,” 2006, https://\\nwww.researchgate.net/publication/200744586_A_tutorial_on_energy-based_learning.\\n204 \\n| \\nChapter 7: Energy-Based Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 232}, page_content='CHAPTER 8\\nDiffusion Models\\nChapter Goals\\nIn this chapter you will:\\n• Learn the underlying principles and components that define a diffusion model.\\n• See how the forward process is used to add noise to the training set of images.\\n• Understand the reparameterization trick and why it is important.\\n• Explore different forms of forward diffusion scheduling.\\n• Understand the reverse diffusion process and how it relates to the forward nois‐\\ning process.\\n• Explore the architecture of the U-Net, which is used to parameterize the reverse\\ndiffusion process.\\n• Build your own denoising diffusion model (DDM) using Keras to generate\\nimages of flowers.\\n• Sample new images of flowers from your model.\\n• Explore the effect of the number of diffusion steps on image quality and interpo‐\\nlate between two images in the latent space.\\nAlongside GANs, diffusion models are one of the most influential and impactful gen‐\\nerative modeling techniques for image generation to have been introduced over the\\nlast decade. Across many benchmarks, diffusion models now outperform previously\\nstate-of-the-art GANs and are quickly becoming the go-to choice for generative mod‐\\neling practitioners, particularly for visual domains (e.g., OpenAI’s DALL.E 2 and\\nGoogle’s ImageGen for text-to-image generation). Recently, there has been an\\n205'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 233}, page_content='explosion of diffusion models being applied across wide range of tasks, reminiscent of\\nthe GAN proliferation that took place between 2017–2020.\\nMany of the core ideas that underpin diffusion models share similarities with earlier\\ntypes of generative models that we have already explored in this book (e.g., denoising\\nautoencoders, energy-based models). Indeed, the name diffusion takes inspiration\\nfrom the well-studied property of thermodynamic diffusion: an important link was\\nmade between this purely physical field and deep learning in 2015.1\\nImportant progress was also being made in the field of score-based generative mod‐\\nels,2,3 a branch of energy-based modeling that directly estimates the gradient of the\\nlog distribution (also known as the score function) in order to train the model, as an\\nalternative to using contrastive divergence. In particular, Yang Song and Stefano\\nErmon used multiple scales of noise perturbations applied to the raw data to ensure\\nthe model—a noise conditional score network (NCSN)—performs well on regions of\\nlow data density.\\nThe breakthrough diffusion model paper came in the summer of 2020.4 Standing on\\nthe shoulders of earlier works, the paper uncovers a deep connection between diffu‐\\nsion models and score-based generative models, and the authors use this fact to train\\na diffusion model that can rival GANs across several datasets, called the Denoising\\nDiffusion Probabilistic Model (DDPM).\\nThis chapter will walk through the theoretical requirements for understanding how a\\ndenoising diffusion model works. You will then learn how to build your own denois‐\\ning diffusion model using Keras.\\nIntroduction\\nTo help explain the key ideas that underpin diffusion models, let’s begin with a short\\nstory!\\nDiffuseTV\\nYou are standing in an electronics store that sells television sets. However, this store is\\nclearly very different from ones you have visited in the past. Instead of a wide variety\\nof different brands, there are hundreds of identical copies of the same TV connected\\ntogether in sequence, stretching into the back of the shop as far as you can see. What’s\\nmore, the first few TV sets appear to be showing nothing but random static noise\\n(Figure 8-1).\\nThe shopkeeper comes over to ask if you need assistance. Confused, you ask her\\nabout the odd setup. She explains that this is the new DiffuseTV model that is set to\\nrevolutionize the entertainment industry and immediately starts telling you how it\\nworks, while walking deeper into the shop, alongside the line of TVs.\\n206 \\n| \\nChapter 8: Diffusion Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 234}, page_content='Figure 8-1. A long line of connected television sets stretching out along an aisle of a shop\\n(created with Midjourney)\\nShe explains that during the manufacturing process, the DiffuseTV is exposed to\\nthousands of images of previous TV shows—but each of those images has been grad‐\\nually corrupted with random static, until it is indistinguishable from pure random\\nnoise. The TVs are then designed to undo the random noise, in small steps, essentially\\ntrying to predict what the images looked like before the noise was added. You can see\\nthat as you walk further into the shop the images on each television set are indeed\\nslightly clearer than the last.\\nYou eventually reach the end of the long line of televisions, where you can see a per‐\\nfect picture on the last set. While this is certainly clever technology, you are curious to\\nunderstand how this is useful to the viewer. The shopkeeper continues with her\\nexplanation.\\nInstead of choosing a channel to watch, the viewer chooses a random initial configu‐\\nration of static. Every configuration will lead to a different output image, and in some\\nmodels can even be guided by a text prompt that you choose to input. Unlike a nor‐\\nmal TV, with a limited range of channels to watch, the DiffuseTV gives the viewer\\nunlimited choice and freedom to generate whatever they would like to appear on the\\nscreen!\\nYou purchase a DiffuseTV right away and are relieved to hear that the long line of\\nTVs in the shop is for demonstration purposes only, so you won’t have to also buy a\\nwarehouse to store your new device!\\nThe DiffuseTV story describes the general idea behind a diffusion model. Now let’s\\ndive into the technicalities of how we build such a model using Keras.\\nIntroduction \\n| \\n207'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 235}, page_content='Denoising Diffusion Models (DDM)\\nThe core idea behind a denoising diffusion model is simple—we train a deep learning\\nmodel to denoise an image over a series of very small steps. If we start from pure ran‐\\ndom noise, in theory we should be able to keep applying the model until we obtain an\\nimage that looks as if it were drawn from the training set. What’s amazing is that this\\nsimple concept works so well in practice!\\nLet’s first get set up with a dataset and then walk through the forward (noising) and\\nbackward (denoising) diffusion processes.\\nRunning the Code for This Example\\nThe code for this example can be found in the Jupyter notebook\\nlocated at notebooks/08_diffusion/01_ddm/ddm.ipynb in the book\\nrepository.\\nThe code is adapted from the excellent tutorial on denoising diffu‐\\nsion implicit models created by András Béres available on the\\nKeras website.\\nThe Flowers Dataset\\nWe’ll be using the Oxford 102 Flower dataset that is available through Kaggle. This is\\na set of over 8,000 color images of a variety of flowers.\\nYou can download the dataset by running the Kaggle dataset downloader script in the\\nbook repository, as shown in Example 8-1. This will save the flower images to\\nthe /data folder.\\nExample 8-1. Downloading the Oxford 102 Flower dataset\\nbash scripts/download_kaggle_data.sh nunenuh pytorch-challange-flower-dataset\\nAs usual, we’ll load the images in using the Keras image_dataset_from_directory\\nfunction, resize the images to 64 × 64 pixels, and scale the pixel values to the range [0,\\n1]. We’ll also repeat the dataset five times to increase the epoch length and batch the\\ndata into groups of 64 images, as shown in Example 8-2.\\nExample 8-2. Loading the Oxford 102 Flower dataset\\ntrain_data = utils.image_dataset_from_directory(\\n    \"/app/data/pytorch-challange-flower-dataset/dataset\",\\n    labels=None,\\n    image_size=(64, 64),\\n    batch_size=None,\\n    shuffle=True,\\n208 \\n| \\nChapter 8: Diffusion Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 236}, page_content='seed=42,\\n    interpolation=\"bilinear\",\\n) \\ndef preprocess(img):\\n    img = tf.cast(img, \"float32\") / 255.0\\n    return img\\ntrain = train_data.map(lambda x: preprocess(x)) \\ntrain = train.repeat(5) \\ntrain = train.batch(64, drop_remainder=True) \\nLoad dataset (when required during training) using the Keras image_data\\nset_from_directory function.\\nScale the pixel values to the range [0, 1].\\nRepeat the dataset five times.\\nBatch the dataset into groups of 64 images.\\nExample images from the dataset are shown in Figure 8-2.\\nFigure 8-2. Example images from the Oxford 102 Flower dataset\\nNow that we have our dataset we can explore how we should add noise to the images,\\nusing a forward diffusion process.\\nThe Forward Diffusion Process\\nSuppose we have an image �0 that we want to corrupt gradually over a large number\\nof steps (say, T = 1, 000), so that eventually it is indistinguishable from standard\\nGaussian noise (i.e., �T should have zero mean and unit variance). How should we go\\nabout doing this?\\nDenoising Diffusion Models (DDM) \\n| \\n209'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 237}, page_content='We can define a function q that adds a small amount of Gaussian noise with variance\\nβt to an image �t −1 to generate a new image �t. If we keep applying this function, we\\nwill generate a sequence of progressively noisier images (�0, ..., �T), as shown in\\nFigure 8-3.\\nFigure 8-3. The forward diffusion process q\\nWe can write this update process mathematically as follows (here, �t −1 is a standard\\nGaussian with zero mean and unit variance):\\n�t =\\n1 −βt�t −1 +\\nβt�t −1\\nNote that we also scale the input image �t −1, to ensure that the variance of the output\\nimage �t remains constant over time. This way, if we normalize our original image �0\\nto have zero mean and unit variance, then �T will approximate a standard Gaussian\\ndistribution for large enough T, by induction, as follows.\\nIf we assume that �t −1 has zero mean and unit variance then 1 −βt�t −1 will have\\nvariance 1 −βt and \\nβt�t −1 will have variance βt, using the rule that\\nVar aX = a2Var X . Adding these together, we obtain a new distribution �t with\\nzero \\nmean \\nand \\nvariance \\n1 −βt + βt = 1, \\nusing \\nthe \\nrule \\nthat\\nVar X + Y = Var X + Var Y  for independent X and Y. Therefore, if �0 is normal‐\\nized to a zero mean and unit variance, then we guarantee that this is also true for all\\n�t, including the final image �T, which will approximate a standard Gaussian distri‐\\nbution. This is exactly what we need, as we want to be able to easily sample �T and\\nthen apply a reverse diffusion process through our trained neural network model!\\nIn other words, our forward noising process q can also be written as follows:\\nq �t �t −1 = ��t; 1 −βt�t −1, βt�\\nThe Reparameterization Trick\\nIt would also be useful to be able to jump straight from an image �0 to any noised\\nversion of the image �t without having to go through t applications of q. Luckily,\\nthere is a reparameterization trick that we can use to do this.\\n210 \\n| \\nChapter 8: Diffusion Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 238}, page_content='If we define αt = 1 −βt and αt = ∏i = 1\\nt\\nαi, then we can write the following:\\n�t =\\nαt�t −1 +\\n1 −αt�t −1\\n=\\nαtαt −1�t −2 +\\n1 −αtαt −1�\\n= ⋯\\n=\\nαt�0 +\\n1 −αt�\\nNote that the second line uses the fact that we can add two Gaussians to obtain a new\\nGaussian. We therefore have a way to jump from the original image �0 to any step of\\nthe forward diffusion process �t. Moreover, we can define the diffusion schedule\\nusing the αt values, instead of the original βt values, with the interpretation that αt is\\nthe variance due to the signal (the original image, �0) and 1 −αt is the variance due to\\nthe noise (�).\\nThe forward diffusion process q can therefore also be written as follows:\\nq �t �0 = ��t; αt�0, 1 −αt �\\nDiffusion Schedules\\nNotice that we are also free to choose a different βt at each timestep—they don’t all\\nhave be the same. How the βt (or αt) values change with t is called the diffusion\\nschedule.\\nIn the original paper (Ho et al., 2020), the authors chose a linear diffusion schedule for\\nβt—that is, βt increases linearly with t, from β1 = 0.0001 to βT = 0.02. This ensures\\nthat in the early stages of the noising process we take smaller noising steps than in the\\nlater stages, when the image is already very noisy.\\nWe can code up a linear diffusion schedule as shown in Example 8-3.\\nExample 8-3. The linear diffusion schedule\\ndef linear_diffusion_schedule(diffusion_times):\\n    min_rate = 0.0001\\n    max_rate = 0.02\\n    betas = min_rate + tf.convert_to_tensor(diffusion_times) * (max_rate - min_rate)\\n    alphas = 1 - betas\\n    alpha_bars = tf.math.cumprod(alphas)\\n    signal_rates = alpha_bars\\n    noise_rates = 1 - alpha_bars\\n    return noise_rates, signal_rates\\nDenoising Diffusion Models (DDM) \\n| \\n211'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 239}, page_content='T = 1000\\ndiffusion_times = [x/T for x in range(T)] \\nlinear_noise_rates, linear_signal_rates = linear_diffusion_schedule(\\n    diffusion_times\\n) \\nThe diffusion times are equally spaced steps between 0 and 1.\\nThe linear diffusion schedule is applied to the diffusion times to produce the\\nnoise and signal rates.\\nIn a later paper it was found that a cosine diffusion schedule outperformed the linear\\nschedule from the original paper.5 A cosine schedule defines the following values of\\nαt:\\nαt = cos2\\nt\\nT · π\\n2\\nThe updated equation is therefore as follows (using the trigonometric identity\\ncos2 x + sin2 x = 1):\\n�t = cos\\nt\\nT · π\\n2 �0 + sin\\nt\\nT · π\\n2 �\\nThis equation is a simplified version of the actual cosine diffusion schedule used in\\nthe paper. The authors also add an offset term and scaling to prevent the noising steps\\nfrom being too small at the beginning of the diffusion process. We can code up the\\ncosine and offset cosine diffusion schedules as shown in Example 8-4.\\nExample 8-4. The cosine and offset cosine diffusion schedules\\ndef cosine_diffusion_schedule(diffusion_times): \\n    signal_rates = tf.cos(diffusion_times * math.pi / 2)\\n    noise_rates = tf.sin(diffusion_times * math.pi / 2)\\n    return noise_rates, signal_rates\\ndef offset_cosine_diffusion_schedule(diffusion_times): \\n    min_signal_rate = 0.02\\n    max_signal_rate = 0.95\\n    start_angle = tf.acos(max_signal_rate)\\n    end_angle = tf.acos(min_signal_rate)\\n    diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\\n    signal_rates = tf.cos(diffusion_angles)\\n    noise_rates = tf.sin(diffusion_angles)\\n212 \\n| \\nChapter 8: Diffusion Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 240}, page_content='return noise_rates, signal_rates\\nThe pure cosine diffusion schedule (without offset or rescaling).\\nThe offset cosine diffusion schedule that we will be using, which adjusts the\\nschedule to ensure the noising steps are not too small at the start of the noising\\nprocess.\\nWe can compute the αt values for each t to show how much signal (αt) and noise\\n(1 −αt) is let through at each stage of the process for the linear, cosine, and offset\\ncosine diffusion schedules, as shown in Figure 8-4.\\nFigure 8-4. The signal and noise at each step of the noising process, for the linear, cosine,\\nand offset cosine diffusion schedules\\nNotice how the noise level ramps up more slowly in the cosine diffusion schedule. A\\ncosine diffusion schedule adds noise to the image more gradually than a linear diffu‐\\nsion schedule, which improves training efficiency and generation quality. This can\\nalso be seen in images that have been corrupted by the linear and cosine schedules\\n(Figure 8-5).\\nFigure 8-5. An image being corrupted by the linear (top) and cosine (bottom) diffusion\\nschedules, at equally spaced values of t from 0 to T (source: Ho et al., 2020)\\nDenoising Diffusion Models (DDM) \\n| \\n213'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 241}, page_content='The Reverse Diffusion Process\\nNow let’s look at the reverse diffusion process. To recap, we are looking to build a\\nneural network pθ �t −1 �t  that can undo the noising process—that is, approximate\\nthe reverse distribution q �t −1 �t . If we can do this, we can sample random noise\\nfrom �0, � and then apply the reverse diffusion process multiple times in order to\\ngenerate a novel image. This is visualized in Figure 8-6.\\nFigure 8-6. The reverse diffusion process pθ . �t −1 �t  tries to undo the noise produced\\nby the forward diffusion process\\nThere are many similarities between the reverse diffusion process and the decoder of\\na variational autoencoder. In both, we aim to transform random noise into meaning‐\\nful output using a neural network. The difference between diffusion models and\\nVAEs is that in a VAE the forward process (converting images to noise) is part of the\\nmodel (i.e., it is learned), whereas in a diffusion model it is unparameterized.\\nTherefore, it makes sense to apply the same loss function as in a variational autoen‐\\ncoder. The original DDPM paper derives the exact form of this loss function and\\nshows that it can be optimized by training a network �θ to predict the noise � that has\\nbeen added to a given image �0 at timestep t.\\nIn other words, we sample an image �0 and transform it by t noising steps to get the\\nimage �t =\\nαt�0 +\\n1 −αt�. We give this new image and the noising rate αt to the\\nneural network and ask it to predict �, taking a gradient step against the squared error\\nbetween the prediction �θ �t  and the true �.\\nWe’ll take a look at the structure of the neural network in the next section. It is worth\\nnoting here that the diffusion model actually maintains two copies of the network:\\none that is actively trained used gradient descent and another (the EMA network)\\nthat is an exponential moving average (EMA) of the weights of the actively trained\\nnetwork over previous training steps. The EMA network is not as susceptible to\\nshort-term fluctuations and spikes in the training process, making it more robust for\\ngeneration than the actively trained network. We therefore use the EMA network\\nwhenever we want to produce generated output from the network.\\nThe training process for the model is shown in Figure 8-7.\\n214 \\n| \\nChapter 8: Diffusion Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 242}, page_content='Figure 8-7. The training process for a denoising diffusion model (source: Ho et al., 2020)\\nIn Keras, we can code up this training step as illustrated in Example 8-5.\\nExample 8-5. The train_step function of the Keras diffusion model\\nclass DiffusionModel(models.Model):\\n    def __init__(self):\\n        super().__init__()\\n        self.normalizer = layers.Normalization()\\n        self.network = unet\\n        self.ema_network = models.clone_model(self.network)\\n        self.diffusion_schedule = cosine_diffusion_schedule\\n    ...\\n    def denoise(self, noisy_images, noise_rates, signal_rates, training):\\n        if training:\\n            network = self.network\\n        else:\\n            network = self.ema_network\\n        pred_noises = network(\\n            [noisy_images, noise_rates**2], training=training\\n        )\\n        pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\\n        return pred_noises, pred_images\\n    def train_step(self, images):\\n        images = self.normalizer(images, training=True) \\n        noises = tf.random.normal(shape=tf.shape(images)) \\n        batch_size = tf.shape(images)[0]\\n        diffusion_times = tf.random.uniform(\\n            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\\n        ) \\n        noise_rates, signal_rates = self.cosine_diffusion_schedule(\\n            diffusion_times\\n        ) \\n        noisy_images = signal_rates * images + noise_rates * noises \\nDenoising Diffusion Models (DDM) \\n| \\n215'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 243}, page_content='with tf.GradientTape() as tape:\\n            pred_noises, pred_images = self.denoise(\\n                noisy_images, noise_rates, signal_rates, training=True\\n            ) \\n            noise_loss = self.loss(noises, pred_noises)  \\n        gradients = tape.gradient(noise_loss, self.network.trainable_weights)\\n        self.optimizer.apply_gradients(\\n            zip(gradients, self.network.trainable_weights)\\n        ) \\n        self.noise_loss_tracker.update_state(noise_loss)\\n        for weight, ema_weight in zip(\\n            self.network.weights, self.ema_network.weights\\n        ):\\n            ema_weight.assign(0.999 * ema_weight + (1 - 0.999) * weight) \\n        return {m.name: m.result() for m in self.metrics}\\n    ...\\nWe first normalize the batch of images to have zero mean and unit variance.\\nNext, we sample noise to match the shape of the input images.\\nWe also sample random diffusion times…\\n…and use these to generate the noise and signal rates according to the cosine dif‐\\nfusion schedule.\\nThen we apply the signal and noise weightings to the input images to generate\\nthe noisy images.\\nNext, we denoise the noisy images by asking the network to predict the noise and\\nthen undoing the noising operation, using the provided noise_rates and\\nsignal_rates.\\nWe can then calculate the loss (mean absolute error) between the predicted noise\\nand the true noise…\\n…and take a gradient step against this loss function.\\nThe EMA network weights are updated to a weighted average of the existing\\nEMA weights and the trained network weights after the gradient step.\\n216 \\n| \\nChapter 8: Diffusion Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 244}, page_content='The U-Net Denoising Model\\nNow that we have seen the kind of neural network that we need to build (one that\\npredicts the noise added to a given image), we can look at the architecture that makes\\nthis possible.\\nThe authors of the DDPM paper used a type of architecture known as a U-Net. A dia‐\\ngram of this network is shown in Figure 8-8, explicitly showing the shape of the ten‐\\nsor as it passes through the network.\\nFigure 8-8. U-Net architecture diagram\\nIn a similar manner to a variational autoencoder, a U-Net consists of two halves: the\\ndownsampling half, where input images are compressed spatially but expanded\\nchannel-wise, and the upsampling half, where representations are expanded spatially\\nwhile the number of channels is reduced. However, unlike in a VAE, there are also\\nDenoising Diffusion Models (DDM) \\n| \\n217'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 245}, page_content='skip connections between equivalent spatially shaped layers in the upsampling and\\ndownsampling parts of the network. A VAE is sequential; data flows through the net‐\\nwork from input to output, one layer after another. A U-Net is different, because the\\nskip connections allow information to shortcut parts of the network and flow\\nthrough to later layers.\\nA U-Net is particularly useful when we want the output to have the same shape as the\\ninput. In our diffusion model example, we want to predict the noise added to an\\nimage, which has exactly the same shape as the image itself, so a U-Net is the natural\\nchoice for the network architecture.\\nFirst let’s take a look at the code that builds this U-Net in Keras, shown in\\nExample 8-6.\\nExample 8-6. A U-Net model in Keras\\nnoisy_images = layers.Input(shape=(64, 64, 3)) \\nx = layers.Conv2D(32, kernel_size=1)(noisy_images) \\nnoise_variances = layers.Input(shape=(1, 1, 1)) \\nnoise_embedding = layers.Lambda(sinusoidal_embedding)(noise_variances) \\nnoise_embedding = layers.UpSampling2D(size=64, interpolation=\"nearest\")(\\n    noise_embedding\\n) \\nx = layers.Concatenate()([x, noise_embedding]) \\nskips = [] \\nx = DownBlock(32, block_depth = 2)([x, skips]) \\nx = DownBlock(64, block_depth = 2)([x, skips])\\nx = DownBlock(96, block_depth = 2)([x, skips])\\nx = ResidualBlock(128)(x) \\nx = ResidualBlock(128)(x)\\nx = UpBlock(96, block_depth = 2)([x, skips]) \\nx = UpBlock(64, block_depth = 2)([x, skips])\\nx = UpBlock(32, block_depth = 2)([x, skips])\\nx = layers.Conv2D(3, kernel_size=1, kernel_initializer=\"zeros\")(x) \\nunet = models.Model([noisy_images, noise_variances], x, name=\"unet\") \\nThe first input to the U-Net is the image that we wish to denoise.\\nThis image is passed through a Conv2D layer to increase the number of channels.\\n218 \\n| \\nChapter 8: Diffusion Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 246}, page_content='The second input to the U-Net is the noise variance (a scalar).\\nThis is encoded using a sinusoidal embedding.\\nThis embedding is copied across spatial dimensions to match the size of the input\\nimage.\\nThe two input streams are concatenated across channels.\\nThe skips list will hold the output from the DownBlock layers that we wish to\\nconnect to UpBlock layers downstream.\\nThe tensor is passed through a series of DownBlock layers that reduce the size of\\nthe image, while increasing the number of channels.\\nThe tensor is then passed through two ResidualBlock layers that hold the image\\nsize and number of channels constant.\\nNext, the tensor is passed through a series of UpBlock layers that increase the size\\nof the image, while decreasing the number of channels. The skip connections\\nincorporate output from the earlier DownBlock layers.\\nThe final Conv2D layer reduces the number of channels to three (RGB).\\nThe U-Net is a Keras Model that takes the noisy images and noise variances as\\ninput and outputs a predicted noise map.\\nTo understand the U-Net in detail, we need to explore four more concepts: the sinus‐\\noidal embedding of the noise variance, the ResidualBlock, the DownBlock, and the\\nUpBlock.\\nSinusoidal embedding\\nSinusoidal embedding was first introduced in a paper by Vaswani et al.6 We will be\\nusing an adaptation of that original idea as utilized in Mildenhall et al.’s paper titled\\n“NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.”7\\nThe idea is that we want to be able to convert a scalar value (the noise variance) into a\\ndistinct higher-dimensional vector that is able to provide a more complex representa‐\\ntion, for use downstream in the network. The original paper used this idea to encode\\nthe discrete position of words in a sentence into vectors; the NeRF paper extends this\\nidea to continuous values.\\nDenoising Diffusion Models (DDM) \\n| \\n219'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 247}, page_content='Specifically, a scalar value x is encoded as shown in the following equation:\\nγ x =\\nsin 2πe0fx , ⋯, sin 2πe L −1 f x , cos 2πe0fx , ⋯, cos 2πe L −1 fx\\nwhere we choose L = 16 to be half the size of our desired noise embedding length and\\nf = ln 1000\\nL −1  to be the maximum scaling factor for the frequencies.\\nThis produces the embedding pattern shown in Figure 8-9.\\nFigure 8-9. The pattern of sinusoidal embeddings for noise variances from 0 to 1\\nWe can code this sinusoidal embedding function as shown in Example 8-7. This con‐\\nverts a single noise variance scalar value into a vector of length 32.\\nExample 8-7. The sinusoidal_embedding function that encodes the noise variance\\ndef sinusoidal_embedding(x):\\n    frequencies = tf.exp(\\n        tf.linspace(\\n            tf.math.log(1.0),\\n            tf.math.log(1000.0),\\n            16,\\n        )\\n    )\\n    angular_speeds = 2.0 * math.pi * frequencies\\n    embeddings = tf.concat(\\n        [tf.sin(angular_speeds * x), tf.cos(angular_speeds * x)], axis=3\\n    )\\n    return embeddings\\n220 \\n| \\nChapter 8: Diffusion Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 248}, page_content='ResidualBlock\\nBoth the DownBlock and the UpBlock contain ResidualBlock layers, so let’s start with\\nthese. We already explored residual blocks in Chapter 5, when we built a PixelCNN,\\nbut we will recap here for completeness.\\nA residual block is a group of layers that contains a skip connection that adds the\\ninput to the output. Residual blocks help us to build deeper networks that can learn\\nmore complex patterns without suffering as greatly from vanishing gradient and deg‐\\nradation problems. The vanishing gradient problem is the assertion that as the net‐\\nwork gets deeper, the gradient propagated through deeper layers is tiny and therefore\\nlearning is very slow. The degradation problem is the fact that as neural networks\\nbecome deeper, they are not necessarily as accurate as their shallower counterparts—\\naccuracy seems to become saturated at a certain depth and then degrade rapidly.\\nDegradation\\nThe degradation problem is somewhat counterintuitive, but\\nobserved in practice as the deeper layers must at least learn the\\nidentity mapping, which is not trivial—especially considering other\\nproblems deeper networks face, such as the vanishing gradient\\nproblem.\\nThe solution, first introduced in the ResNet paper by He et al. in 2015,8 is very sim‐\\nple. By including a skip connection highway around the main weighted layers, the\\nblock has the option to bypass the complex weight updates and simply pass through\\nthe identity mapping. This allows the network to be trained to great depth without\\nsacrificing gradient size or network accuracy.\\nA diagram of a ResidualBlock is shown in Figure 8-10. Note that in some residual\\nblocks, we also include an extra Conv2D layer with kernel size 1 on the skip connec‐\\ntion, to bring the number of channels in line with the rest of the block.\\nFigure 8-10. The ResidualBlock in the U-Net\\nDenoising Diffusion Models (DDM) \\n| \\n221'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 249}, page_content='We can code a ResidualBlock in Keras as shown in Example 8-8.\\nExample 8-8. Code for the ResidualBlock in the U-Net\\ndef ResidualBlock(width):\\n    def apply(x):\\n        input_width = x.shape[3]\\n        if input_width == width: \\n            residual = x\\n        else:\\n            residual = layers.Conv2D(width, kernel_size=1)(x)\\n        x = layers.BatchNormalization(center=False, scale=False)(x) \\n        x = layers.Conv2D(\\n            width, kernel_size=3, padding=\"same\", activation=activations.swish\\n        )(x) \\n        x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\\n        x = layers.Add()([x, residual]) \\n        return x\\n    return apply\\nCheck if the number of channels in the input matches the number of channels\\nthat we would like the block to output. If not, include an extra Conv2D layer on\\nthe skip connection to bring the number of channels in line with the rest of the\\nblock.\\nApply a BatchNormalization layer.\\nApply two Conv2D layers.\\nAdd the original block input to the output to provide the final output from the\\nblock.\\nDownBlocks and UpBlocks\\nEach successive DownBlock increases the number of channels via block_depth (=2 in\\nour example) ResidualBlocks, while also applying a final AveragePooling2D layer in\\norder to halve the size of the image. Each ResidualBlock is added to a list for use\\nlater by the UpBlock layers as skip connections across the U-Net.\\nAn UpBlock first applies an UpSampling2D layer that doubles the size of the image,\\nthrough bilinear interpolation. Each successive UpBlock decreases the number of\\nchannels via block_depth (=2) ResidualBlocks, while also concatenating the outputs\\nfrom the DownBlocks through skip connections across the U-Net. A diagram of this\\nprocess is shown in Figure 8-11.\\n222 \\n| \\nChapter 8: Diffusion Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 250}, page_content='Figure 8-11. The DownBlock and corresponding UpBlock in the U-Net\\nWe can code the DownBlock and UpBlock using Keras as illustrated in Example 8-9.\\nExample 8-9. Code for the DownBlock and UpBlock in the U-Net model\\ndef DownBlock(width, block_depth):\\n    def apply(x):\\n        x, skips = x\\n        for _ in range(block_depth):\\n            x = ResidualBlock(width)(x) \\n            skips.append(x) \\n        x = layers.AveragePooling2D(pool_size=2)(x) \\n        return x\\n    return apply\\ndef UpBlock(width, block_depth):\\n    def apply(x):\\n        x, skips = x\\n        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x) \\n        for _ in range(block_depth):\\n            x = layers.Concatenate()([x, skips.pop()]) \\n            x = ResidualBlock(width)(x) \\n        return x\\n    return apply\\nDenoising Diffusion Models (DDM) \\n| \\n223'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 251}, page_content='The DownBlock increases the number of channels in the image using a Residual\\nBlock of a given width…\\n…each of which are saved to a list (skips) for use later by the UpBlocks.\\nA final AveragePooling2D layer reduces the dimensionality of the image by half.\\nThe UpBlock begins with an UpSampling2D layer that doubles the size of the\\nimage.\\nThe output from a DownBlock layer is glued to the current output using a\\nConcatenate layer.\\nA ResidualBlock is used to reduce the number of channels in the image as it\\npasses through the UpBlock.\\nTraining the Diffusion Model\\nWe now have all the components in place to train our denoising diffusion model!\\nExample 8-10 creates, compiles, and fits the diffusion model.\\nExample 8-10. Code for training the DiffusionModel\\nmodel = DiffusionModel() \\nmodel.compile(\\n    optimizer=optimizers.experimental.AdamW(learning_rate=1e-3, weight_decay=1e-4),\\n    loss=losses.mean_absolute_error,\\n) \\nmodel.normalizer.adapt(train) \\nmodel.fit(\\n    train,\\n    epochs=50,\\n) \\nInstantiate the model.\\nCompile the model, using the AdamW optimizer (similar to Adam but with\\nweight decay, which helps stabilize the training process) and mean absolute error\\nloss function.\\nCalculate the normalization statistics using the training set.\\nFit the model over 50 epochs.\\n224 \\n| \\nChapter 8: Diffusion Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 252}, page_content='The loss curve (noise mean absolute error [MAE]) is shown in Figure 8-12.\\nFigure 8-12. The noise mean absolute error loss curve, by epoch\\nSampling from the Denoising Diffusion Model\\nIn order to sample images from our trained model, we need to apply the reverse dif‐\\nfusion process—that is, we need to start with random noise and use the model to\\ngradually undo the noise, until we are left with a recognizable picture of a flower.\\nWe must bear in mind that our model is trained to predict the total amount of noise\\nthat has been added to a given noisy image from the training set, not just the noise\\nthat was added at the last timestep of the noising process. However, we do not want to\\nundo the noise all in one go—predicting an image from pure random noise in one\\nshot is clearly not going to work! We would rather mimic the forward process and\\nundo the predicted noise gradually over many small steps, to allow the model to\\nadjust to its own predictions.\\nTo achieve this, we can jump from xt to xt −1 in two steps—first by using our model’s\\nnoise prediction to calculate an estimate for the original image x0 and then by reap‐\\nplying the predicted noise to this image, but only over t −1 timesteps, to produce\\nxt −1. This idea is shown in Figure 8-13.\\nDenoising Diffusion Models (DDM) \\n| \\n225'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 253}, page_content='Figure 8-13. One step of the sampling process for our diffusion model\\nIf we repeat this process over a number of steps, we’ll eventually get back to an esti‐\\nmate for x0 that has been guided gradually over many small steps. In fact, we are free\\nto choose the number of steps we take, and crucially, it doesn’t have to be the same as\\nthe large number of steps in the training noising process (i.e., 1,000). It can be much\\nsmaller—in this example we choose 20.\\nThe following equation (Song et al., 2020) this process mathematically:\\n�t −1 =\\nαt −1\\n�t −\\n1 −αt�θ\\nt �t\\nαt\\npredicted �0\\n+\\n1 −αt −1 −σt\\n2 · �θ\\nt �t\\ndirection pointing to �t\\n+\\nσt�t\\nrandom noise\\nLet’s break this down. The first term inside the brackets on the righthand side of the\\nequation is the estimated image x0, calculated using the noise predicted by our net‐\\nwork �θ\\nt . We then scale this by the t −1 signal rate αt −1 and reapply the predicted\\nnoise, but this time scaled by the t −1 noise rate 1 −αt −1 −σt\\n2. Additional Gaussian\\nrandom noise σt�t is also added, with the factors σt determining how random we\\nwant our generation process to be.\\nThe special case σt = 0 for all t corresponds to a type of model known as a Denoising\\nDiffusion Implicit Model (DDIM), introduced by Song et al. in 2020.9 With a DDIM,\\nthe generation process is entirely deterministic—that is, the same random noise input\\nwill always give the same output. This is desirable as then we have a well-defined\\nmapping between samples from the latent space and the generated outputs in pixel\\nspace.\\nIn our example, we will implement a DDIM, thus making our generation process\\ndeterministic. The code for the DDIM sampling process (reverse diffusion) is shown\\nin Example 8-11.\\n226 \\n| \\nChapter 8: Diffusion Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 254}, page_content='Example 8-11. Sampling from the diffusion model\\nclass DiffusionModel(models.Model):\\n...\\n    def reverse_diffusion(self, initial_noise, diffusion_steps):\\n        num_images = initial_noise.shape[0]\\n        step_size = 1.0 / diffusion_steps\\n        current_images = initial_noise\\n        for step in range(diffusion_steps): \\n            diffusion_times = tf.ones((num_images, 1, 1, 1)) - step * step_size \\n            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times) \\n            pred_noises, pred_images = self.denoise(\\n                current_images, noise_rates, signal_rates, training=False\\n            ) \\n            next_diffusion_times = diffusion_times - step_size \\n            next_noise_rates, next_signal_rates = self.diffusion_schedule(\\n                next_diffusion_times\\n            ) \\n            current_images = (\\n                next_signal_rates * pred_images + next_noise_rates * pred_noises\\n            ) \\n        return pred_images \\nLook over a fixed number of steps (e.g., 20).\\nThe diffusion times are all set to 1 (i.e., at the start of the reverse diffusion\\nprocess).\\nThe noise and signal rates are calculated according to the diffusion schedule.\\nThe U-Net is used to predict the noise, allowing us to calculate the denoised\\nimage estimate.\\nThe diffusion times are reduced by one step.\\nThe new noise and signal rates are calculated.\\nThe t-1 images are calculated by reapplying the predicted noise to the predicted\\nimage, according to the t-1 diffusion schedule rates.\\nAfter 20 steps, the final �0 predicted images are returned.\\nDenoising Diffusion Models (DDM) \\n| \\n227'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 255}, page_content='Analysis of the Diffusion Model\\nWe’ll now take a look at three different ways that we can use our trained model: for\\ngeneration of new images, testing how the number of reverse diffusion steps affects\\nquality, and interpolating between two images in the latent space.\\nGenerating images\\nIn order to produce samples from our trained model, we can simply run the reverse\\ndiffusion process, ensuring that we denormalize the output at the end (i.e., take the\\npixel values back into the range [0, 1]). We can achieve this using the code in\\nExample 8-12 inside the DiffusionModel class.\\nExample 8-12. Generating images using the diffusion model\\nclass DiffusionModel(models.Model):\\n...\\n    def denormalize(self, images):\\n        images = self.normalizer.mean + images * self.normalizer.variance**0.5 \\n        return tf.clip_by_value(images, 0.0, 1.0)\\n    def generate(self, num_images, diffusion_steps):\\n        initial_noise = tf.random.normal(shape=(num_images, 64, 64, 3)) \\n        generated_images = self.reverse_diffusion(initial_noise, diffusion_steps) \\n        generated_images = self.denormalize(generated_images) \\n        return generated_images\\nGenerate some initial noise maps.\\nApply the reverse diffusion process.\\nThe images output by the network will have mean zero and unit variance, so we\\nneed to denormalize by reapplying the mean and variance calculated from the\\ntraining data.\\n228 \\n| \\nChapter 8: Diffusion Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 256}, page_content='In Figure 8-14 we can observe some samples from the diffusion model at different\\nepochs of the training process.\\nFigure 8-14. Samples from the diffusion model at different epochs of the training process\\nAdjusting the number of diffusion steps\\nWe can also test to see how adjusting the number of diffusion steps in the reverse\\nprocess affects image quality. Intuitively, the more steps taken by the process, the\\nhigher the quality of the image generation.\\nWe can see in Figure 8-15 that the quality of the generations does indeed improve\\nwith the number of diffusion steps. With one giant leap from the initial sampled\\nnoise, the model can only predict a hazy blob of color. With more steps, the model is\\nable to refine and sharpen its generations. However, the time taken to generate the\\nimages scales linearly with the number of diffusion steps, so there is a trade-off. There\\nis minimal improvement between 20 and 100 diffusion steps, so we choose 20 as a\\nreasonable compromise between quality and speed in this example.\\nDenoising Diffusion Models (DDM) \\n| \\n229'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 257}, page_content='Figure 8-15. Image quality improves with the number of diffusion steps\\nInterpolating between images\\nLastly, as we have seen previously with variational autoencoders, we can interpolate\\nbetween points in the Gaussian latent space in order to smoothly transition between\\nimages in pixel space. Here we choose to use a form of spherical interpolation that\\nensures that the variance remains constant while blending the two Gaussian noise\\nmaps together. Specifically, the initial noise map at each step is given by\\na sin\\nπ\\n2t + b cos\\nπ\\n2t , where t ranges smoothly from 0 to 1 and a and b are the two\\nrandomly sampled Gaussian noise tensors that we wish to interpolate between.\\nThe resulting images are shown in Figure 8-16.\\n230 \\n| \\nChapter 8: Diffusion Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 258}, page_content='Figure 8-16. Interpolating between images using the denoising diffusion model\\nSummary\\nIn this chapter we have explored one of the most exciting and promising areas of gen‐\\nerative modeling in recent times: diffusion models. In particular, we implemented the\\nideas from a key paper on generative diffusion models (Ho et al., 2020) that intro‐\\nduced the original Denoising Diffusion Probabilistic Model (DDPM). We then exten‐\\nded this with the ideas from the Denoising Diffusion Implicit Model (DDIM) paper\\nto make the generation process fully deterministic.\\nWe have seen how diffusion models are formed of a forward diffusion process and a\\nreverse diffusion process. The forward diffusion process adds noise to the training\\ndata through a series of small steps, while the reverse diffusion process consists of a\\nmodel that tries to predict the noise added.\\nWe make use of a reparameterization trick in order to calculate the noised images at\\nany step of the forward process without having to go through multiple noising steps.\\nWe have seen how the chosen schedule of parameters used to add noise to the data\\nplays an important part in the overall success of the model.\\nThe reverse diffusion process is parameterized by a U-Net that tries to predict the\\nnoise at each timestep, given the noised image and the noise rate at that step. A U-Net\\nconsists of DownBlocks that increase the number of channels while reducing the size\\nof the image and UpBlocks that decrease the number of channels while increasing the\\nsize. The noise rate is encoded using sinusoidal embedding.\\nSampling from the diffusion model is conducted over a series of steps. The U-Net is\\nused to predict the noise added to a given noised image, which is then used to\\nSummary \\n| \\n231'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 259}, page_content='calculate an estimate for the original image. The predicted noise is then reapplied\\nusing a smaller noise rate. This process is repeated over a series of steps (which may\\nbe significantly smaller than the number of steps used during training), starting from\\na random point sampled from a standard Gaussian noise distribution, to obtain the\\nfinal generation.\\nWe saw how increasing the number of diffusion steps in the reverse process improves\\nthe image generation quality, at the expense of speed. We also performed latent space\\narithmetic in order to interpolate between two images.\\nReferences\\n1. Jascha Sohl-Dickstein et al., “Deep Unsupervised Learning Using Nonequilibrium\\nThermodynamics,” March 12, 2015, https://arxiv.org/abs/1503.03585\\n2. Yang Song and Stefano Ermon, “Generative Modeling by Estimating Gradients of\\nthe Data Distribution,” July 12, 2019, https://arxiv.org/abs/1907.05600.\\n3. Yang Song and Stefano Ermon, “Improved Techniques for Training Score-Based\\nGenerative Models,” June 16, 2020, https://arxiv.org/abs/2006.09011.\\n4. Jonathon Ho et al., “Denoising Diffusion Probabilistic Models,” June 19, 2020,\\nhttps://arxiv.org/abs/2006.11239.\\n5. Alex Nichol and Prafulla Dhariwal, “Improved Denoising Diffusion Probabilistic\\nModels,” February 18, 2021, https://arxiv.org/abs/2102.09672.\\n6. Ashish Vaswani et al., “Attention Is All You Need,” June 12, 2017, https://\\narxiv.org/abs/1706.03762.\\n7. Ben Mildenhall et al., “NeRF: Representing Scenes as Neural Radiance Fields for\\nView Synthesis,” March 1, 2020, https://arxiv.org/abs/2003.08934.\\n8. Kaiming He et al., “Deep Residual Learning for Image Recognition,” December 10,\\n2015, https://arxiv.org/abs/1512.03385.\\n9. Jiaming Song et al., “Denoising Diffusion Implicit Models,” October 6, 2020,\\nhttps://arxiv.org/abs/2010.02502\\n232 \\n| \\nChapter 8: Diffusion Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 260}, page_content='PART III\\nApplications\\nIn Part III, we will explore some of the key applications of the generative modeling\\ntechniques that we have seen so far, across images, text, music, and games. We will\\nalso see how these domains can be traversed using state-of-the-art multimodal\\nmodels.\\nIn Chapter 9 we shall turn our attention to Transformers, a start-of-the-art architec‐\\nture that powers most modern-day text generation models. In particular, we shall\\nexplore the inner workings of GPT and build our own version using Keras, and we’ll\\nsee how it forms the foundation of tools such as ChatGPT.\\nIn Chapter 10 we will look at some of the most important GAN architectures that\\nhave influenced image generation, including ProGAN, StyleGAN, StyleGAN2,\\nSAGAN, BigGAN, VQ-GAN, and ViT VQ-GAN. We shall explore the key contribu‐\\ntions of each and look to understand how the technique has evolved over time.\\nChapter 11 looks at music generation, which presents additional challenges such as\\nmodeling musical pitch and rhythm. We’ll see that many of the techniques that work\\nfor text generation (such as Transformers) can also be applied in this domain, but\\nwe’ll also explore a deep learning architecture known as MuseGAN that applies a\\nGAN-based approach to generating music.\\nChapter 12 shows how generative models can be used within other machine learning\\ndomains, such as reinforcement learning. We will focus on the “World Models” paper,\\nwhich shows how a generative model can be used as the environment in which the\\nagent trains, allowing it to train within a hallucinated dream version of the environ‐\\nment rather than the real thing.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 261}, page_content='In Chapter 13 we will explore state-of-the-art multimodal models that cross over\\ndomains such as images and text. This includes text-to-image models such as\\nDALL.E 2, Imagen, and Stable Diffusion, as well as visual language models such as\\nFlamingo.\\nFinally, Chapter 14 summarizes the generative AI journey so far, the current genera‐\\ntive AI landscape, and where we may be heading in the future. We will explore how\\ngenerative AI may change the way we live and work, as well as considering whether it\\nhas the potential to unlock deeper forms of artificial intelligence in the years to come.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 262}, page_content='CHAPTER 9\\nTransformers\\nChapter Goals\\nIn this chapter you will:\\n• Learn about the origins of GPT, a powerful decoder Transformer model for text\\ngeneration.\\n• Learn conceptually how an attention mechanism mimics our way of attaching\\nmore importance to some words in a sentence than others.\\n• Delve into how the attention mechanism works from first principles, including\\nhow queries, keys, and values are created and manipulated.\\n• See the importance of causal masking for text generation tasks.\\n• Understand how attention heads can be grouped into a multihead attention layer.\\n• See how multihead attention layers form one part of a Transformer block that\\nalso includes layer normalization and skip connections.\\n• Create positional encodings that capture the position of each token as well as the\\nword token embedding.\\n• Build a GPT model in Keras to generate the text contained in wine reviews.\\n• Analyze the output from the GPT model, including interrogating the attention\\nscores to inspect where the model is looking.\\n• Learn about the different types of Transformers, including examples of the types\\nof tasks that can be tackled by each and descriptions of the most famous state-of-\\nthe-art implementations.\\n• Understand how encoder-decoder architectures work, like Google’s T5 model.\\n• Explore the training process behind OpenAI’s ChatGPT.\\n235'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 263}, page_content='We saw in Chapter 5 how we can build generative models on text data using recurrent\\nneural networks (RNNs), such as LSTMs and GRUs. These autoregressive models\\nprocess sequential data one token at a time, constantly updating a hidden vector that\\ncaptures the current latent representation of the input. The RNN can be designed to\\npredict the next word in a sequence by applying a dense layer and softmax activation\\nover the hidden vector. This was considered the most sophisticated way to genera‐\\ntively produce text until 2017, when one paper changed the landscape of text genera‐\\ntion forever.\\nIntroduction\\nThe Google Brain paper, confidently entitled “Attention Is All You Need,”1 is famous\\nfor popularizing the concept of attention—a mechanism that now powers most state-\\nof-the-art text generation models.\\nThe authors show how it is possible to create powerful neural networks called Trans‐\\nformers for sequential modeling that do not require complex recurrent or convolu‐\\ntional architectures but instead only rely on attention mechanisms. This approach\\novercomes a key downside to the RNN approach, which is that it is challenging to\\nparallelize, as it must process sequences one token as a time. Transformers are highly\\nparalellizable, allowing them to be trained on massive datasets.\\nIn this chapter, we are going to delve into how modern text generation models make\\nuse of the Transformer architecture to reach state-of-the-art performance on text\\ngeneration challenges. In particular, we will explore a type of autoregressive model\\nknown as the generative pre-trained transformer (GPT), which powers OpenAI’s\\nGPT-4 model, widely considered to be the current state of the art for text generation.\\nGPT\\nOpenAI introduced GPT in June 2018, in the paper “Improving Language Under‐\\nstanding by Generative Pre-Training,”2 almost exactly a year after the appearance of\\nthe original Transformer paper.\\nIn this paper, the authors show how a Transformer architecture can be trained on a\\nhuge amount of text data to predict the next word in a sequence and then subse‐\\nquently fine-tuned to specific downstream tasks.\\nThe pre-training process of GPT involves training the model on a large corpus of text\\ncalled BookCorpus (4.5 GB of text from 7,000 unpublished books of different genres).\\nDuring pre-training, the model is trained to predict the next word in a sequence\\ngiven the previous words. This process is known as language modeling and is used to\\nteach the model to understand the structure and patterns of natural language.\\n236 \\n| \\nChapter 9: Transformers'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 264}, page_content='After pre-training, the GPT model can be fine-tuned for a specific task by providing\\nit with a smaller, task-specific dataset. Fine-tuning involves adjusting the parameters\\nof the model to better fit the task at hand. For example, the model can be fine-tuned\\nfor tasks such as classification, similarity scoring, or question answering.\\nThe GPT architecture has since been improved and extended by OpenAI with the\\nrelease of subsequent models such as GPT-2, GPT-3, GPT-3.5, and GPT-4. These\\nmodels are trained on larger datasets and have larger capacities, so they can generate\\nmore complex and coherent text. The GPT models have been widely adopted by\\nresearchers and industry practitioners and have contributed to significant advance‐\\nments in natural language processing tasks.\\nIn this chapter, we will build our own variation of the original GPT model, trained on\\nless data, but still utilizing the same components and underlying principles.\\nRunning the Code for This Example\\nThe code for this example can be found in the Jupyter notebook\\nlocated at notebooks/09_transformer/01_gpt/gpt.ipynb in the book\\nrepository.\\nThe code is adapted from the excellent GPT tutorial created by\\nApoorv Nandan available on the Keras website.\\nThe Wine Reviews Dataset\\nWe’ll be using the Wine Reviews dataset that is available through Kaggle. This is a set\\nof over 130,000 reviews of wines, with accompanying metadata such as description\\nand price.\\nYou can download the dataset by running the Kaggle dataset downloader script in the\\nbook repository, as shown in Example 9-1. This will save the wine reviews and\\naccompanying metadata locally to the /data folder.\\nExample 9-1. Downloading the Wine Reviews dataset\\nbash scripts/download_kaggle_data.sh zynicide wine-reviews\\nThe data preparation steps are identical to the steps used in Chapter 5 for preparing\\ndata for input into an LSTM, so we will not repeat them in detail here. The steps, as\\nshown in Figure 9-1, are as follows:\\nGPT \\n| \\n237'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 265}, page_content='1. Load the data and create a list of text string descriptions of each wine.\\n2. Pad punctuation with spaces, so that each punctuation mark is treated as a sepa‐\\nrate word.\\n3. Pass the strings through a TextVectorization layer that tokenizes the data and\\npads/clips each string to a fixed length.\\n4. Create a training set where the inputs are the tokenized text strings and the out‐\\nputs to predict are the same strings shifted by one token.\\nFigure 9-1. Data processing for the Transformer\\nAttention\\nThe first step to understanding how GPT works is to understand how the attention\\nmechanism works. This mechanism is what makes the Transformer architecture\\nunique and distinct from recurrent approaches to language modeling. When we have\\ndeveloped a solid understanding of attention, we will then see how it is used within\\nTransformer architectures such as GPT.\\nWhen you write, the choice that you make for the next word in the sentence is influ‐\\nenced by other words that you have already written. For example, suppose you start a\\nsentence as follows:\\nThe pink elephant tried to get into the car but it was too\\nClearly, the next word should be something synonymous with big. How do we know\\nthis?\\nCertain other words in the sentence are important for helping us to make our deci‐\\nsion. For example, the fact that it is an elephant, rather than a sloth, means that we\\nprefer big rather than slow. If it were a swimming pool, rather than a car, we might\\nchoose scared as a possible alternative to big. Lastly, the action of getting into the car\\nimplies that size is the problem—if the elephant was trying to squash the car instead,\\nwe might choose fast as the final word, with it now referring to the car.\\n238 \\n| \\nChapter 9: Transformers'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 266}, page_content='Other words in the sentence are not important at all. For example, the fact that the\\nelephant is pink has no influence on our choice of final word. Equally, the minor\\nwords in the sentence (the, but, it, etc.) give the sentence grammatical form, but here\\naren’t important to determine the required adjective.\\nIn other words, we are paying attention to certain words in the sentence and largely\\nignoring others. Wouldn’t it be great if our model could do the same thing?\\nAn attention mechanism (also know as an attention head) in a Transformer is\\ndesigned to do exactly this. It is able to decide where in the input it wants to pull\\ninformation from, in order to efficiently extract useful information without being\\nclouded by irrelevant details. This makes it highly adaptable to a range of circumstan‐\\nces, as it can decide where it wants to look for information at inference time.\\nIn contrast, a recurrent layer tries to build up a generic hidden state that captures an\\noverall representation of the input at each timestep. A weakness of this approach is\\nthat many of the words that have already been incorporated into the hidden vector\\nwill not be directly relevant to the immediate task at hand (e.g., predicting the next\\nword), as we have just seen. Attention heads do not suffer from this problem, because\\nthey can pick and choose how to combine information from nearby words, depend‐\\ning on the context.\\nQueries, Keys, and Values\\nSo how does an attention head decide where it wants to look for information? Before\\nwe get into the details, let’s explore how it works at a high level, using our pink ele‐\\nphant example.\\nImagine that we want to predict what follows the word too. To help with this task,\\nother preceding words chime in with their opinions, but their contributions are\\nweighted by how confident they are in their own expertise in predicting words that\\nfollow too. For example, the word elephant might confidently contribute that it is\\nmore likely to be a word related to size or loudness, whereas the word was doesn’t\\nhave much to offer to narrow down the possibilities.\\nIn other words, we can think of an attention head as a kind of information retrieval\\nsystem, where a query (“What word follows too?”) is made into a key/value store\\n(other words in the sentence) and the resulting output is a sum of the values, weigh‐\\nted by the resonance between the query and each key.\\nWe will now walk through the process in detail (Figure 9-2), again with reference to\\nour pink elephant sentence.\\nGPT \\n| \\n239'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 267}, page_content='Figure 9-2. The mechanics of an attention head\\nThe query (Q) can be thought of as a representation of the current task at hand (e.g.,\\n“What word follows too?”). In this example, it is derived from the embedding of the\\nword too, by passing it through a weights matrix WQ to change the dimensionality of\\nthe vector from de to dk.\\nThe key vectors (K) are representations of each word in the sentence—you can think\\nof these as descriptions of the kinds of prediction tasks that each word can help with.\\nThey are derived in a similar fashion to the query, by passing each embedding\\nthrough a weights matrix WK to change the dimensionality of each vector from de to\\ndk. Notice that the keys and the query are the same length (dk).\\n240 \\n| \\nChapter 9: Transformers'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 268}, page_content='Inside the attention head, each key is compared to the query using a dot product\\nbetween each pair of vectors (QKT). This is why the keys and the query have to be the\\nsame length. The higher this number is for a particular key/query pair, the more the\\nkey resonates with the query, so it is allowed to make more of a contribution to the\\noutput of the attention head. The resulting vector is scaled by dk to keep the var‐\\niance of the vector sum stable (approximately equal to 1), and a softmax is applied to\\nensure the contributions sum to 1. This is a vector of attention weights.\\nThe value vectors (V) are also representations of the words in the sentence—you can\\nthink of these as the unweighted contributions of each word. They are derived by\\npassing each embedding through a weights matrix WV to change the dimensionality\\nof each vector from de to dv. Notice that the value vectors do not necessarily have to\\nhave the same length as the keys and query (but often do, for simplicity).\\nThe value vectors are multiplied by the attention weights to give the attention for a\\ngiven Q, K, and V, as shown in Equation 9-1.\\nEquation 9-1. Attention equation\\nAttention Q, K, V = sof tmax QKT\\ndk\\nV\\nTo obtain the final output vector from the attention head, the attention is summed to\\ngive a vector of length dv. This context vector captures a blended opinion from words\\nin the sentence on the task of predicting what word follows too.\\nMultihead Attention\\nThere’s no reason to stop at just one attention head! In Keras, we can build a Multi\\nHeadAttention layer that concatenates the output from multiple attention heads,\\nallowing each to learn a distinct attention mechanism so that the layer as a whole can\\nlearn more complex relationships.\\nThe concatenated outputs are passed through one final weights matrix WO to project\\nthe vector into the desired output dimension, which in our case is the same as the\\ninput dimension of the query (de), so that the layers can be stacked sequentially on\\ntop of each other.\\nFigure 9-3 shows how the output from a MultiHeadAttention layer is constructed. In\\nKeras we can simply write the line shown in Example 9-2 to create such a layer.\\nGPT \\n| \\n241'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 269}, page_content='Example 9-2. Creating a MultiHeadAttention layer in Keras\\nlayers.MultiHeadAttention(\\n    num_heads = 4, \\n    key_dim = 128, \\n    value_dim = 64, \\n    output_shape = 256 \\n    )\\nThis multihead attention layer has four heads.\\nThe keys (and query) are vectors of length 128.\\nThe values (and therefore also the output from each head) are vectors of length\\n64.\\nThe output vector has length 256.\\nFigure 9-3. A multihead attention layer with four heads\\nCausal Masking\\nSo far, we have assumed that the query input to our attention head is a single vector.\\nHowever, for efficiency during training, we would ideally like the attention layer to be\\nable to operate on every word in the input at once, predicting for each what the sub‐\\nsequent word will be. In other words, we want our GPT model to be able to handle a\\ngroup of query vectors in parallel (i.e., a matrix).\\nYou might think that we can just batch the vectors together into a matrix and let lin‐\\near algebra handle the rest. This is true, but we need one extra step—we need to apply\\na mask to the query/key dot product, to avoid information from future words leaking\\nthrough. This is known as causal masking and is shown in Figure 9-4.\\n242 \\n| \\nChapter 9: Transformers'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 270}, page_content='Figure 9-4. Matrix calculation of the attention scores for a batch of input queries, using a\\ncausal attention mask to hide keys that are not available to the query (because they\\ncome later in the sentence)\\nWithout this mask, our GPT model would be able to perfectly guess the next word in\\nthe sentence, because it would be using the key from the word itself as a feature! The\\ncode for creating a causal mask is shown in Example 9-3, and the resulting numpy\\narray (transposed to match the diagram) is shown in Figure 9-5.\\nExample 9-3. The causal mask function\\ndef causal_attention_mask(batch_size, n_dest, n_src, dtype):\\n    i = tf.range(n_dest)[:, None]\\n    j = tf.range(n_src)\\n    m = i >= j - n_src + n_dest\\n    mask = tf.cast(m, dtype)\\n    mask = tf.reshape(mask, [1, n_dest, n_src])\\nGPT \\n| \\n243'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 271}, page_content='mult = tf.concat(\\n        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\\n    )\\n    return tf.tile(mask, mult)\\nnp.transpose(causal_attention_mask(1, 10, 10, dtype = tf.int32)[0])\\nFigure 9-5. The causal mask as a numpy array—1 means unmasked and 0 means\\nmasked\\nCausal masking is only required in decoder Transformers such as\\nGPT, where the task is to sequentially generate tokens given previ‐\\nous tokens. Masking out future tokens during training is therefore\\nessential.\\nOther flavors of Transformer (e.g., encoder Transformers) do not\\nneed causal masking, because they are not trained to predict the\\nnext token. For example Google’s BERT predicts masked words\\nwithin a given sentence, so it can use context from both before and\\nafter the word in question.3\\nWe will explore the different types of Transformers in more detail\\nat the end of the chapter.\\nThis concludes our explanation of the multihead attention mechanism that is present\\nin all Transformers. It is remarkable that the learnable parameters of such an influen‐\\ntial layer consist of nothing more than three densely connected weights matrices for\\neach attention head (WQ, WK, WV) and one further weights matrix to reshape the\\noutput (WO). There are no convolutions or recurrent mechanisms at all in a multi‐\\nhead attention layer!\\nNext, we shall take a step back and see how the multihead attention layer forms just\\none part of a larger component known as a Transformer block.\\n244 \\n| \\nChapter 9: Transformers'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 272}, page_content='The Transformer Block\\nA Transformer block is a single component within a Transformer that applies some\\nskip connections, feed-forward (dense) layers, and normalization around the multi‐\\nhead attention layer. A diagram of a Transformer block is shown in Figure 9-6.\\nFigure 9-6. A Transformer block\\nFirstly, notice how the query is passed around the multihead attention layer to be\\nadded to the output—this is a skip connection and is common in modern deep learn‐\\ning architectures. It means we can build very deep neural networks that do not suffer\\nas much from the vanishing gradient problem, because the skip connection provides\\na gradient-free highway that allows the network to transfer information forward\\nuninterrupted.\\nSecondly, layer normalization is used in the Transformer block to provide stability to\\nthe training process. We have already seen the batch normalization layer in action\\nthroughout this book, where the output from each channel is normalized to have a\\nGPT \\n| \\n245'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 273}, page_content='mean of 0 and standard deviation of 1. The normalization statistics are calculated\\nacross the batch and spatial dimensions.\\nIn contrast, layer normalization in a Transformer block normalizes each position of\\neach sequence in the batch by calculating the normalizing statistics across the chan‐\\nnels. It is the complete opposite of batch normalization, in terms of how the normal‐\\nization statistics are calculated. A diagram showing the difference between batch\\nnormalization and layer normalization is shown in Figure 9-7.\\nFigure 9-7. Layer normalization versus batch normalization—the normalization statis‐\\ntics are calculated across the blue cells (source: Sheng et al., 2020)4\\nLayer Normalization Versus Batch Normalization\\nLayer normalization was used in the original GPT paper and is\\ncommonly used for text-based tasks to avoid creating normaliza‐\\ntion dependencies across sequences in the batch. However, recent\\nwork such as Shen et al.s challenges this assumption, showing that\\nwith some tweaks a form of batch normalization can still be used\\nwithin Transformers, outperforming more traditional layer\\nnormalization.\\nLastly, a set of feed-forward (i.e., densely connected) layers is included in the Trans‐\\nformer block, to allow the component to extract higher-level features as we go deeper\\ninto the network.\\n246 \\n| \\nChapter 9: Transformers'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 274}, page_content='A Keras implementation of a Transformer block is shown in Example 9-4.\\nExample 9-4. A TransformerBlock layer in Keras\\nclass TransformerBlock(layers.Layer):\\n    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate=0.1): \\n        super(TransformerBlock, self).__init__()\\n        self.num_heads = num_heads\\n        self.key_dim = key_dim\\n        self.embed_dim = embed_dim\\n        self.ff_dim = ff_dim\\n        self.dropout_rate = dropout_rate\\n        self.attn = layers.MultiHeadAttention(\\n            num_heads, key_dim, output_shape = embed_dim\\n        )\\n        self.dropout_1 = layers.Dropout(self.dropout_rate)\\n        self.ln_1 = layers.LayerNormalization(epsilon=1e-6)\\n        self.ffn_1 = layers.Dense(self.ff_dim, activation=\"relu\")\\n        self.ffn_2 = layers.Dense(self.embed_dim)\\n        self.dropout_2 = layers.Dropout(self.dropout_rate)\\n        self.ln_2 = layers.LayerNormalization(epsilon=1e-6)\\n    def call(self, inputs):\\n        input_shape = tf.shape(inputs)\\n        batch_size = input_shape[0]\\n        seq_len = input_shape[1]\\n        causal_mask = causal_attention_mask(\\n            batch_size, seq_len, seq_len, tf.bool\\n        ) \\n        attention_output, attention_scores = self.attn(\\n            inputs,\\n            inputs,\\n            attention_mask=causal_mask,\\n            return_attention_scores=True\\n        ) \\n        attention_output = self.dropout_1(attention_output)\\n        out1 = self.ln_1(inputs + attention_output) \\n        ffn_1 = self.ffn_1(out1) \\n        ffn_2 = self.ffn_2(ffn_1)\\n        ffn_output = self.dropout_2(ffn_2)\\n        return (self.ln_2(out1 + ffn_output), attention_scores) \\nThe sublayers that make up the TransformerBlock layer are defined within the\\ninitialization function.\\nThe causal mask is created to hide future keys from the query.\\nThe multihead attention layer is created, with the attention masks specified.\\nGPT \\n| \\n247'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 275}, page_content='The first add and normalization layer.\\nThe feed-forward layers.\\nThe second add and normalization layer.\\nPositional Encoding\\nThere is one final step to cover before we can put everything together to train our\\nGPT model. You may have noticed that in the multihead attention layer, there is\\nnothing that cares about the ordering of the keys. The dot product between each key\\nand the query is calculated in parallel, not sequentially, like in a recurrent neural net‐\\nwork. This is a strength (because of the parallelization efficiency gains) but also a\\nproblem, because we clearly need the attention layer to be able to predict different\\noutputs for the following two sentences:\\n• The dog looked at the boy and … (barked?)\\n• The boy looked at the dog and … (smiled?)\\nTo solve this problem, we use a technique called positional encoding when creating the\\ninputs to the initial Transformer block. Instead of only encoding each token using a\\ntoken embedding, we also encode the position of the token, using a position embed‐\\nding.\\nThe token embedding is created using a standard Embedding layer to convert each\\ntoken into a learned vector. We can create the positional embedding in the same way,\\nusing a standard Embedding layer to convert each integer position into a learned\\nvector.\\nWhile GPT uses an Embedding layer to embed the position, the\\noriginal Transformer paper used trigonometric functions—we’ll\\ncover this alternative in Chapter 11, when we explore music\\ngeneration.\\n248 \\n| \\nChapter 9: Transformers'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 276}, page_content='To construct the joint token–position encoding, the token embedding is added to the\\npositional embedding, as shown in Figure 9-8. This way, the meaning and position of\\neach word in the sequence are captured in a single vector.\\nFigure 9-8. The token embeddings are added to the positional embeddings to give the\\ntoken position encoding\\nGPT \\n| \\n249'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 277}, page_content='The code that defines our TokenAndPositionEmbedding layer is shown in\\nExample 9-5.\\nExample 9-5. The TokenAndPositionEmbedding layer\\nclass TokenAndPositionEmbedding(layers.Layer):\\n    def __init__(self, maxlen, vocab_size, embed_dim):\\n        super(TokenAndPositionEmbedding, self).__init__()\\n        self.maxlen = maxlen\\n        self.vocab_size =vocab_size\\n        self.embed_dim = embed_dim\\n        self.token_emb = layers.Embedding(\\n            input_dim=vocab_size, output_dim=embed_dim\\n        ) \\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim) \\n    def call(self, x):\\n        maxlen = tf.shape(x)[-1]\\n        positions = tf.range(start=0, limit=maxlen, delta=1)\\n        positions = self.pos_emb(positions)\\n        x = self.token_emb(x)\\n        return x + positions \\nThe tokens are embedded using an Embedding layer.\\nThe positions of the tokens are also embedded using an Embedding layer.\\nThe output from the layer is the sum of the token and position embeddings.\\nTraining GPT\\nNow we are ready to build and train our GPT model! To put everything together, we\\nneed to pass our input text through the token and position embedding layer, then\\nthrough our Transformer block. The final output of the network is a simple Dense\\nlayer with softmax activation over the number of words in the vocabulary.\\nFor simplicity, we will use just one Transformer block, rather than\\nthe 12 in the paper.\\n250 \\n| \\nChapter 9: Transformers'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 278}, page_content='The overall architecture is shown in Figure 9-9 and the equivalent code is provided in\\nExample 9-6.\\nFigure 9-9. The simplified GPT model architecture\\nExample 9-6. A GPT model in Keras\\nMAX_LEN = 80\\nVOCAB_SIZE = 10000\\nEMBEDDING_DIM = 256\\nN_HEADS = 2\\nKEY_DIM = 256\\nFEED_FORWARD_DIM = 256\\ninputs = layers.Input(shape=(None,), dtype=tf.int32) \\nx = TokenAndPositionEmbedding(MAX_LEN, VOCAB_SIZE, EMBEDDING_DIM)(inputs) \\nx, attention_scores = TransformerBlock(\\n    N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM\\n)(x) \\noutputs = layers.Dense(VOCAB_SIZE, activation = \\'softmax\\')(x) \\ngpt = models.Model(inputs=inputs, outputs=[outputs, attention]) \\ngpt.compile(\"adam\", loss=[losses.SparseCategoricalCrossentropy(), None]) \\ngpt.fit(train_ds, epochs=5)\\nThe input is padded (with zeros).\\nThe text is encoded using a TokenAndPositionEmbedding layer.\\nGPT \\n| \\n251'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 279}, page_content='The encoding is passed through a TransformerBlock.\\nThe transformed output is passed through a Dense layer with softmax activation\\nto predict a distribution over the subsequent word.\\nThe Model takes a sequence of word tokens as input and outputs the predicted\\nsubsequent word distribution. The output from the Transformer block is also\\nreturned so that we can inspect how the model is directing its attention.\\nThe model is compiled with SparseCategoricalCrossentropy loss over the pre‐\\ndicted word distribution.\\nAnalysis of GPT\\nNow that we have compiled and trained our GPT model, we can start to use it to gen‐\\nerate long strings of text. We can also interrogate the attention weights that are output\\nfrom the TransformerBlock, to understand where the Transformer is looking for\\ninformation at different points in the generation process.\\nGenerating text\\nWe can generate new text by applying the following process:\\n1. Feed the network with an existing sequence of words and ask it to predict the fol‐\\nlowing word.\\n2. Append this word to the existing sequence and repeat.\\nThe network will output a set of probabilities for each word that we can sample from,\\nso we can make the text generation stochastic, rather than deterministic.\\nWe will use the same TextGenerator class introduced in Chapter 5 for LSTM text\\ngeneration, including the temperature parameter that specifies how deterministic we\\nwould like the sampling process to be. Let’s take a look at this in action, at two differ‐\\nent temperature values (Figure 9-10).\\n252 \\n| \\nChapter 9: Transformers'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 280}, page_content='Figure 9-10. Generated outputs at temperature = 1.0 and temperature = 0.5.\\nThere are a few things to note about these two passages. First, both are stylistically\\nsimilar to a wine review from the original training set. They both open with the\\nregion and type of wine, and the wine type stays consistent throughout the passage\\n(for example, it doesn’t switch color halfway through). As we saw in Chapter 5, the\\ngenerated text with temperature 1.0 is more adventurous and therefore less accurate\\nthan the example with temperature 0.5. Generating multiple samples with tempera‐\\nture 1.0 will therefore lead to more variety as the model is sampling from a probabil‐\\nity distribution with greater variance.\\nViewing the attention scores\\nWe can also ask the model to tell us how much attention is being placed on each\\nword, when deciding on the next word in the sentence. The TransformerBlock out‐\\nputs the attention weights for each head, which are a softmax distribution over the\\npreceding words in the sentence.\\nTo demonstrate this, Figure 9-11 shows the top five tokens with the highest probabili‐\\nties for three different input prompts, as well as the average attention across both\\nheads, against each preceding word. The preceding words are colored according to\\ntheir attention score, averaged across the two attention heads. Darker blue indicates\\nmore attention is being placed on the word.\\nGPT \\n| \\n253'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 281}, page_content='Figure 9-11. Distribution of word probabilities following various sequences\\nIn the first example, the model attends closely to the country (germany) in order to\\ndecide on the word that relates to the region. This makes sense! To pick a region, it\\nneeds to take lots of information from the words that relate to the country, to ensure\\nthey match. It doesn’t need to pay as much attention to the first two tokens (wine\\nreview) because they don’t hold any useful information regarding the region.\\nIn the second example, it needs to refer back to the grape (riesling), so it pays atten‐\\ntion to the first time that it was mentioned. It can pull this information by directly\\nattending to the word, no matter how far back it is in the sentence (within the upper\\nlimit of 80 words). Notice that this is very different from a recurrent neural network,\\nwhich relies on a hidden state to maintain all interesting information over the length\\nof the sequence so that it can be drawn upon if required—a much less efficient\\napproach.\\nThe final sequence shows an example of how our GPT model can choose an appro‐\\npriate adjective based on a combination of information. Here the attention is again on\\nthe grape (riesling), but also on the fact that it contains residual sugar. As Riesling is\\ntypically a sweet wine, and sugar is already mentioned, it makes sense that it should\\nbe described as slightly sweet rather than slightly earthy, for example.\\n254 \\n| \\nChapter 9: Transformers'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 282}, page_content='It is incredibly informative to be able to interrogate the network in this way, to under‐\\nstand exactly where it is pulling information from in order to make accurate deci‐\\nsions about each subsequent word. I highly recommend playing around with the\\ninput prompts to see if you can get the model to attend to words really far back in the\\nsentence, to convince yourself of the power of attention-based models over more tra‐\\nditional recurrent models!\\nOther Transformers\\nOur GPT model is a decoder Transformer—it generates a text string one token at a\\ntime and uses causal masking to only attend to previous words in the input string.\\nThere are also encoder Transformers, which do not use causal masking—instead, they\\nattend to the entire input string in order to extract a meaningful contextual represen‐\\ntation of the input. For other tasks, such as language translation, there are also\\nencoder-decoder Transformers that can translate from one text string to another; this\\ntype of model contains both encoder Transformer blocks and decoder Transformer\\nblocks.\\nTable 9-1 summarizes the three types of Transformers, with the best examples of each\\narchitecture and typical use cases.\\nTable 9-1. The three Transformer architectures\\nType\\nExamples\\nUse cases\\nEncoder\\nBERT (Google)\\nSentence classification, named entity recognition, extractive question answering\\nEncoder-decoder T5 (Google)\\nSummarization, translation, question answering\\nDecoder\\nGPT-3 (OpenAI)\\nText generation\\nA well-known example of an encoder Transformer is the Bidirectional Encoder Repre‐\\nsentations from Transformers (BERT) model, developed by Google (Devlin et al.,\\n2018) that predicts missing words from a sentence, given context from both before\\nand after the missing word in all layers.\\nEncoder Transformers\\nEncoder Transformers are typically used for tasks that require an\\nunderstanding of the input as a whole, such as sentence classifica‐\\ntion, named entity recognition, and extractive question answering.\\nThey are not used for text generation tasks, so we will not explore\\nthem in detail in this book—see Lewis Tunstall et al.’s Natural Lan‐\\nguage \\nProcessing \\nwith \\nTransformers \\n(O’Reilly) \\nfor \\nmore\\ninformation.\\nOther Transformers \\n| \\n255'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 283}, page_content='In the following sections we will explore how encoder-decoder transformers work\\nand discuss extensions of the original GPT model architecture released by OpenAI,\\nincluding ChatGPT, which has been specifically designed for conversational\\napplications.\\nT5\\nAn example of a modern Transformer that uses the encoder-decoder structure is the\\nT5 model from Google.5 This model reframes a range of tasks into a text-to-text\\nframework, including translation, linguistic acceptability, sentence similarity, and\\ndocument summarization, as shown in Figure 9-12.\\nFigure 9-12. Examples of how T5 reframes a range of tasks into a text-to-text frame‐\\nwork, including translation, linguistic acceptability, sentence similarity, and document\\nsummarization (source: Raffel et al., 2019)\\nThe T5 model architecture closely matches the encoder-decoder architecture used in\\nthe original Transformer paper, shown in Figure 9-13. The key difference is that T5 is\\ntrained on an enormous 750 GB corpus of text (the Colossal Clean Crawled Corpus,\\nor C4), whereas the original Transformer paper was focused only on language trans‐\\nlation, so it was trained on 1.4 GB of English–German sentence pairs.\\n256 \\n| \\nChapter 9: Transformers'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 284}, page_content='Figure 9-13. An encoder-decoder Transformer model: each gray box is a Transformer\\nblock (source: Vaswani et al., 2017)\\nMuch of this diagram is already familiar to us—we can see the Transformer blocks\\nbeing repeated and positional embedding being used to capture the ordering of the\\ninput sequences. The two key differences between this model and the GPT model that\\nwe built earlier in the chapter are as follows:\\n• On the lefthand side, a set of encoder Transformer blocks encode the sequence to\\nbe translated. Notice that there is no causal masking on the attention layer. This is\\nbecause we are not generating further text to extend the sequence to be trans‐\\nlated; we just want to learn a good representation of the sequence as a whole that\\ncan be fed to the decoder. Therefore, the attention layers in the encoder can be\\ncompletely unmasked to capture all the cross-dependencies between words, no\\nmatter the order.\\nOther Transformers \\n| \\n257'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 285}, page_content='• On the righthand side, a set of decoder Transformer blocks generate the trans‐\\nlated text. The initial attention layer is self-referential (i.e., the key, value, and\\nquery come from the same input) and causal masking is used to ensure informa‐\\ntion from future tokens is not leaked to the current word to be predicted. How‐\\never, we can then see that the subsequent attention layer pulls the key and value\\nfrom the encoder, leaving only the query passed through from the decoder itself.\\nThis is called cross-referential attention and means that the decoder can attend to\\nthe encoder representation of the input sequence to be translated. This is how the\\ndecoder knows what meaning the translation needs to convey!\\nFigure 9-14 shows an example of cross-referential attention. Two attention heads of\\nthe decoder layer are able to work together to provide the correct German translation\\nfor the word the, when used in the context of the street. In German, there are three\\ndefinite articles (der, die, das) depending on the gender of the noun, but the Trans‐\\nformer knows to choose die because one attention head is able to attend to the word\\nstreet (a feminine word in German), while another attends to the word to translate\\n(the).\\nFigure 9-14. An example of how one attention head attends to the word “the” and\\nanother attends to the word “street” in order to correctly translate the word “the” to the\\nGerman word “die” as the feminine definite article of “Straße”\\n258 \\n| \\nChapter 9: Transformers'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 286}, page_content='This example is from the Tensor2Tensor GitHub repository, which\\ncontains a Colab notebook that allows you to play around with a\\ntrained encoder-decoder Transformer model and see how the\\nattention mechanisms of the encoder and decoder impact the\\ntranslation of a given sentence into German.\\nGPT-3 and GPT-4\\nSince the original 2018 publication of GPT, OpenAI has released multiple updated\\nversions that improve upon the original model, as shown in Table 9-2.\\nTable 9-2. The evolution of OpenAI’s GPT collection of models\\nModel\\nDate\\nLayers\\nAttention\\nheads\\nWord\\nembedding\\nsize\\nContext\\nwindow\\n# parameters\\nTraining data\\nGPT\\nJun\\n2018\\n12\\n12\\n768\\n512\\n120,000,000\\nBookCorpus: 4.5 GB of text\\nfrom unpublished books\\nGPT-2\\nFeb\\n2019\\n48\\n48\\n1,600\\n1,024\\n1,500,000,000\\nWebText: 40 GB of text from\\noutbound Reddit links\\nGPT-3\\nMay\\n2020\\n96\\n96\\n12,888\\n2,048\\n175,000,000,000\\nCommonCrawl, WebText,\\nEnglish Wikipedia, book\\ncorpora and others: 570 GB\\nGPT-4\\nMar\\n2023\\n-\\n-\\n-\\n-\\n-\\n-\\nThe model architecture of GPT-3 is fairly similar to the original GPT model, except it\\nis much larger and trained on much more data. At the time of writing, GPT-4 is in\\nlimited beta—OpenAI has not publicly released details of the model’s structure and\\nsize, though we do know that it is able to accept images as input, so crosses over into\\nbeing a multimodal model for the first time. The model weights of GPT-3 and GPT-4\\nare not open source, though the models are available through a commercial tool and\\nAPI.\\nGPT-3 can also be fine-tuned to your own training data—this allows you to provide\\nmultiple examples of how it should react to a given style of prompt by physically\\nupdating the weights of the network. In many cases this may not be necessary, as\\nGPT-3 can be told how to react to a given style of prompt simply by providing a few\\nexamples in the prompt itself (this is known as few-shot learning). The benefit of fine-\\ntuning is that you do not need to provide these examples as part of every single input\\nprompt, saving costs in the long run.\\nOther Transformers \\n| \\n259'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 287}, page_content='An example of the output from GPT-3, given a system prompt sentence, is shown in\\nFigure 9-15.\\nFigure 9-15. An example of how GPT-3 can extend a given system prompt\\nLanguage models such as GPT benefit hugely from scaling—both in terms of number\\nof model weights and dataset size. The ceiling of large language model capability has\\nyet to be reached, with researchers continuing to push the boundaries of what is pos‐\\nsible with increasingly larger models and datasets.\\nChatGPT\\nA few months before the beta release of GPT-4, OpenAI announced ChatGPT—a tool\\nthat allows users to interact with their suite of large language models through a con‐\\nversational interface. The original release in November 2022 was powered by\\nGPT-3.5, a version of the model that was more powerful that GPT-3 and was fine-\\ntuned to conversational responses.\\nExample dialogue is shown in Figure 9-16. Notice how the agent is able to maintain\\nstate between inputs, understanding that the attention mentioned in the second ques‐\\ntion refers to attention in the context of Transformers, rather than a person’s ability to\\nfocus.\\n260 \\n| \\nChapter 9: Transformers'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 288}, page_content='Figure 9-16. An example of ChatGPT answering questions about Transformers\\nOther Transformers \\n| \\n261'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 289}, page_content='At the time of writing, there is no official paper that describes how ChatGPT works in\\ndetail, but from the official blog post we know that it uses a technique called reinforce‐\\nment learning from human feedback (RLHF) to fine-tune the GPT-3.5 model. This\\ntechnique was also used in the ChatGPT group’s earlier paper6 that introduced the\\nInstructGPT model, a fine-tuned GPT-3 model that is specifically designed to more\\naccurately follow written instructions.\\nThe training process for ChatGPT is as follows:\\n1. Supervised fine-tuning: Collect a demonstration dataset of conversational inputs\\n(prompts) and desired outputs that have been written by humans. This is used to\\nfine-tune the underlying language model (GPT-3.5) using supervised learning.\\n2. Reward modeling: Present a human labeler with examples of prompts and several\\nsampled model outputs and ask them to rank the outputs from best to worst.\\nTrain a reward model that predicts the score given to each output, given the con‐\\nversation history.\\n3. Reinforcement learning: Treat the conversation as a reinforcement learning envi‐\\nronment where the policy is the underlying language model, initialized to the\\nfine-tuned model from step 1. Given the current state (the conversation history)\\nthe policy outputs an action (a sequence of tokens), which is scored by the\\nreward model trained in step 2. A reinforcement learning algorithm—proximal\\npolicy optimization (PPO)—can then be trained to maximize the reward, by\\nadjusting the weights of the language model.\\nReinforcement Learning\\nFor an introduction to reinforcement learning see Chapter 12,\\nwhere we explore how generative models can be used in a rein‐\\nforcement learning setting.\\nThe RLHF process is shown in Figure 9-17.\\n262 \\n| \\nChapter 9: Transformers'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 290}, page_content='Figure 9-17. The reinforcement learning from human feedback fine-tuning process used\\nin ChatGPT (source: OpenAI)\\nOther Transformers \\n| \\n263'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 291}, page_content='While ChatGPT still has many limitations (such as sometimes “hallucinating” factu‐\\nally incorrect information), it is a powerful example of how Transformers can be used\\nto build generative models that can produce complex, long-ranging, and novel output\\nthat is often indistinguishable from human-generated text. The progress made thus\\nfar by models like ChatGPT serves as a testament to the potential of AI and its trans‐\\nformative impact on the world.\\nMoreover, it is evident that AI-driven communication and interaction will continue\\nto rapidly evolve in the future. Projects like Visual ChatGPT7 are now combining the\\nlinguistic power of ChatGPT with visual foundation models such as Stable Diffusion,\\nenabling users to interact with ChatGPT not only through text, but also images. The\\nfusion of linguistic and visual capabilities in projects like Visual ChatGPT and GPT-4\\nhave the potential to herald a new era in human–computer interaction.\\nSummary\\nIn this chapter, we explored the Transformer model architecture and built a version\\nof GPT—a model for state-of-the-art text generation.\\nGPT makes use of a mechanism known as attention, which removes the need for\\nrecurrent layers (e.g., LSTMs). It works like an information retrieval system, utilizing\\nqueries, keys, and values to decide how much information it wants to extract from\\neach input token.\\nAttention heads can be grouped together to form what is known as a multihead atten‐\\ntion layer. These are then wrapped up inside a Transformer block, which includes\\nlayer normalization and skip connections around the attention layer. Transformer\\nblocks can be stacked to create very deep neural networks.\\nCausal masking is used to ensure that GPT cannot leak information from down‐\\nstream tokens into the current prediction. Also, a technique known as positional\\nencoding is used to ensure that the ordering of the input sequence is not lost, but\\ninstead is baked into the input alongside the traditional word embedding.\\nWhen analyzing the output from GPT, we saw it was possible not only to generate\\nnew text passages, but also to interrogate the attention layer of the network to under‐\\nstand where in the sentence it is looking to gather information to improve its predic‐\\ntion. GPT can access information at a distance without loss of signal, because the\\nattention scores are calculated in parallel and do not rely on a hidden state that is car‐\\nried through the network sequentially, as is the case with recurrent neural networks.\\nWe saw how there are three families of Transformers (encoder, decoder, and encoder-\\ndecoder) and the different tasks that can be accomplished with each. Finally, we\\nexplored the structure and training process of other large language models such as\\nGoogle’s T5 and OpenAI’s ChatGPT.\\n264 \\n| \\nChapter 9: Transformers'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 292}, page_content='References\\n1. Ashish Vaswani et al., “Attention Is All You Need,” June 12, 2017, https://\\narxiv.org/abs/1706.03762.\\n2. Alec Radford et al., “Improving Language Understanding by Generative Pre-\\nTraining,” June 11, 2018, https://openai.com/research/language-unsupervised.\\n3. Jacob Devlin et al., “BERT: Pre-Training of Deep Bidirectional Transformers for\\nLanguage Understanding,” October 11, 2018, https://arxiv.org/abs/1810.04805.\\n4. Sheng Shen et al., “PowerNorm: Rethinking Batch Normalization in Transformers,”\\nJune 28, 2020, https://arxiv.org/abs/2003.07845.\\n5. Colin Raffel et al., “Exploring the Limits of Transfer Learning with a Unified Text-\\nto-Text Transformer,” October 23, 2019, https://arxiv.org/abs/1910.10683.\\n6. Long Ouyang et al., “Training Language Models to Follow Instructions with\\nHuman Feedback,” March 4, 2022, https://arxiv.org/abs/2203.02155.\\n7. Chenfei Wu et al., “Visual ChatGPT: Talking, Drawing and Editing with Visual\\nFoundation Models,” March 8, 2023, https://arxiv.org/abs/2303.04671.\\nSummary \\n| \\n265'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 293}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 294}, page_content='CHAPTER 10\\nAdvanced GANs\\nChapter Goals\\nIn this chapter you will:\\n• See how a ProGAN model progressively trains a GAN to generate high-\\nresolution images.\\n• Understand how ProGAN was adapted to build StyleGAN, a high-performing\\nGAN for image synthesis.\\n• Explore how StyleGAN was adjusted to create StyleGAN2, a state-of-the-art\\nmodel that improves further upon the original work.\\n• Learn about the key contributions of these models, including progressive train‐\\ning, adaptive instance normalization, weight modulation and demodulation, and\\npath length regularization.\\n• Walk through the architecture of the Self-Attention GAN (SAGAN), which\\nincorporates the attention mechanism into the GAN framework.\\n• See how BigGAN expands upon the ideas in the SAGAN paper to produce high-\\nquality images.\\n• Learn how VQ-GAN uses a codebook to encode images into a discrete sequence\\nof tokens that can be modeled using a Transformer.\\n• See how ViT VQ-GAN adapts the VQ-GAN architecture to use Transformers\\ninstead of convolutional layers in the encoder and decoder.\\n267'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 295}, page_content='Chapter 4 introduced generative adversarial networks (GANs), a class of generative\\nmodel that has produced state-of-the-art results across a wide variety of image gener‐\\nation tasks. The flexibility in the model architecture and training process has led aca‐\\ndemics and deep learning practitioners to find new ways to design and train GANs,\\nleading to many different advanced flavors of the architecture that we shall explore in\\nthis chapter.\\nIntroduction\\nExplaining all GAN developments and their repercussions in detail could easily fill\\nanother book. The GAN Zoo repository on GitHub contains over 500 distinct exam‐\\nples of GANs with linked papers, ranging from ABC-GAN to ZipNet-GAN!\\nIn this chapter we will cover the main GANs that have been influential in the field,\\nincluding a detailed explanation of the model architecture and training process for\\neach.\\nWe will first explore three important models from NVIDIA that have pushed the\\nboundaries of image generation: ProGAN, StyleGAN, and StyleGAN2. We will ana‐\\nlyze each of these models in enough detail to understand the fundamental concepts\\nthat underpin the architectures and see how they have each built on ideas from earlier\\npapers.\\nWe will also explore two other important GAN architectures that incorporate atten‐\\ntion: the Self-Attention GAN (SAGAN) and BigGAN, which built on many of the\\nideas in the SAGAN paper. We have already seen the power of the attention mecha‐\\nnism in the context of Transformers in Chapter 9.\\nLastly, we will cover VQ-GAN and ViT VQ-GAN, which incorporate a blend of ideas\\nfrom variational autoencoders, Transformers, and GANs. VQ-GAN is a key compo‐\\nnent of Google’s state-of-the-art text-to-image generation model Muse.1 We will\\nexplore so-called multimodal models in more detail in Chapter 13.\\nTraining Your Own Models\\nFor conciseness I have chosen not to include code to directly build\\nthese models in the code repository for this book, but instead will\\npoint to publicly available implementations where possible, so that\\nyou can train your own versions if you wish.\\n268 \\n| \\nChapter 10: Advanced GANs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 296}, page_content='ProGAN\\nProGAN is a technique developed by NVIDIA Labs in 20172 to improve both the\\nspeed and stability of GAN training. Instead of immediately training a GAN on full-\\nresolution images, the ProGAN paper suggests first training the generator and dis‐\\ncriminator on low-resolution images of, say, 4 × 4 pixels and then incrementally\\nadding layers throughout the training process to increase the resolution.\\nLet’s take a look at the concept of progressive training in more detail.\\nTraining Your Own ProGAN\\nThere is an excellent tutorial by Bharath K on training your own\\nProGAN using Keras available on the Paperspace blog. Bear in\\nmind that training a ProGAN to achieve the results from the paper\\nrequires a significant amount of computing power.\\nProgressive Training\\nAs always with GANs, we build two independent networks, the generator and dis‐\\ncriminator, with a fight for dominance taking place during the training process.\\nIn a normal GAN, the generator always outputs full-resolution images, even in the\\nearly stages of training. It is reasonable to think that this strategy might not be opti‐\\nmal—the generator might be slow to learn high-level structures in the early stages of\\ntraining, because it is immediately operating over complex, high-resolution images.\\nWouldn’t it be better to first train a lightweight GAN to output accurate low-\\nresolution images and then see if we can build on this to gradually increase the reso‐\\nlution?\\nThis simple idea leads us to progressive training, one of the key contributions of the\\nProGAN paper. The ProGAN is trained in stages, starting with a training set that has\\nbeen condensed down to 4 × 4–pixel images using interpolation, as shown in\\nFigure 10-1.\\nProGAN \\n| \\n269'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 297}, page_content='Figure 10-1. Images in the dataset can be compressed to lower resolution using\\ninterpolation\\nWe can then initially train the generator to transform a latent input noise vector z\\n(say, of length 512) into an image of shape 4 × 4 × 3. The matching discriminator will\\nneed to transform an input image of size 4 × 4 × 3 into a single scalar prediction. The\\nnetwork architectures for this first step are shown in Figure 10-2.\\nThe blue box in the generator represents the convolutional layer that converts the set\\nof feature maps into an RGB image (toRGB), and the blue box in the discriminator\\nrepresents the convolutional layer that converts the RGB images into a set of feature\\nmaps (fromRGB).\\n270 \\n| \\nChapter 10: Advanced GANs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 298}, page_content='Figure 10-2. The generator and discriminator architectures for the first stage of the Pro‐\\nGAN training process\\nIn the paper, the authors train this pair of networks until the discriminator has seen\\n800,000 real images. We now need to understand how the generator and discrimina‐\\ntor are expanded to work with 8 × 8–pixel images.\\nTo expand the generator and discriminator, we need to blend in additional layers.\\nThis is managed in two phases, transition and stabilization, as shown in Figure 10-3.\\nProGAN \\n| \\n271'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 299}, page_content='Figure 10-3. The ProGAN generator training process, expanding the network from 4 × 4\\nimages to 8 × 8 (dotted lines represent the rest of the network, not shown)\\nLet’s first look at the generator. During the transition phase, new upsampling and con‐\\nvolutional layers are appended to the existing network, with a residual connection set\\nup to maintain the output from the existing trained toRGB layer. Crucially, the new\\nlayers are initially masked using a parameter α that is gradually increased from 0 to 1\\nthroughout the transition phase to allow more of the new toRGB output through and\\nless of the existing toRGB layer. This is to avoid a shock to the network as the new lay‐\\ners take over.\\nEventually, there is no flow through the old toRGB layer and the network enters the\\nstabilization phase—a further period of training where the network can fine-tune the\\noutput, without any flow through the old toRGB layer.\\n272 \\n| \\nChapter 10: Advanced GANs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 300}, page_content='The discriminator uses a similar process, as shown in Figure 10-4.\\nFigure 10-4. The ProGAN discriminator training process, expanding the network from\\n4 × 4 images to 8 × 8 (dotted lines represent the rest of the network, not shown)\\nHere, we need to blend in additional downscaling and convolutional layers. Again,\\nthe layers are injected into the network—this time at the start of the network, just\\nafter the input image. The existing fromRGB layer is connected via a residual connec‐\\ntion and gradually phased out as the new layers take over during the transition phase.\\nThe stabilization phase allows the discriminator to fine-tune using the new layers.\\nProGAN \\n| \\n273'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 301}, page_content='All transition and stabilization phases last until the discriminator has been shown\\n800,000 real images. Note that even through the network is trained progressively, no\\nlayers are frozen. Throughout the training process, all layers remain fully trainable.\\nThis process continues, growing the GAN from 4 × 4 images to 8 × 8, then 16 × 16,\\n32 × 32, and so on, until it reaches full resolution (1,024 × 1,024), as shown in\\nFigure 10-5.\\nFigure 10-5. The ProGAN training mechanism, and some example generated faces\\n(source: Karras et al., 2017)\\nThe overall structure of the generator and discriminator after the full progressive\\ntraining process is complete is shown in Figure 10-6.\\n274 \\n| \\nChapter 10: Advanced GANs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 302}, page_content='Figure 10-6. The ProGAN generator and discriminator used to generate 1,024 × 1,024–\\npixel CelebA faces (source: Karras et al., 2018)\\nThe paper also makes several other important contributions, namely minibatch stan‐\\ndard deviation, equalized learning rates, and pixelwise normalization, which are\\ndescribed briefly in the following sections.\\nMinibatch standard deviation\\nThe minibatch standard deviation layer is an extra layer in the discriminator that\\nappends the standard deviation of the feature values, averaged across all pixels and\\nacross the minibatch as an additional (constant) feature. This helps to ensure the gen‐\\nerator creates more variety in its output—if variety is low across the minibatch, then\\nthe standard deviation will be small, and the discriminator can use this feature to dis‐\\ntinguish the fake batches from the real batches! Therefore, the generator is incentiv‐\\nized to ensure it generates a similar amount of variety as is present in the real training\\ndata.\\nProGAN \\n| \\n275'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 303}, page_content='Equalized learning rates\\nAll dense and convolutional layers in ProGAN use equalized learning rates. Usually,\\nweights in a neural network are initialized using a method such as He initialization—a\\nGaussian distribution where the standard deviation is scaled to be inversely propor‐\\ntional to the square root of the number of inputs to the layer. This way, layers with a\\ngreater number of inputs will be initialized with weights that have a smaller deviation\\nfrom zero, which generally improves the stability of the training process.\\nThe authors of the ProGAN paper found that this was causing problems when used\\nin combination with modern optimizers such as Adam or RMSProp. These methods\\nnormalize the gradient update for each weight, so that the size of the update is inde‐\\npendent of the scale (magnitude) of the weight. However, this means that weights\\nwith a larger dynamic range (i.e., layers with fewer inputs) will take comparatively\\nlonger to adjust than weights with a smaller dynamic range (i.e., layers with more\\ninputs). It was found that this causes an imbalance between the speed of training of\\nthe different layers of the generator and discriminator in ProGAN, so they used\\nequalized learning rates to solve this problem.\\nIn ProGAN, weights are initialized using a simple standard Gaussian, regardless of\\nthe number of inputs to the layer. The normalization is applied dynamically, as part of\\nthe call to the layer, rather than only at initialization. This way, the optimizer sees\\neach weight as having approximately the same dynamic range, so it applies the same\\nlearning rate. It is only when the layer is called that the weight is scaled by the factor\\nfrom the He initializer.\\nPixelwise normalization\\nLastly, in ProGAN pixelwise normalization is used in the generator, rather than batch\\nnormalization. This normalizes the feature vector in each pixel to a unit length and\\nhelps to prevent the signal from spiraling out of control as it propagates through the\\nnetwork. The pixelwise normalization layer has no trainable weights.\\nOutputs\\nIn addition to the CelebA dataset, ProGAN was also applied to images from the\\nLarge-scale Scene Understanding (LSUN) dataset with excellent results, as shown in\\nFigure 10-7. This demonstrated the power of ProGAN over earlier GAN architectures\\nand paved the way for future iterations such as StyleGAN and StyleGAN2, which we\\nshall explore in the next sections.\\n276 \\n| \\nChapter 10: Advanced GANs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 304}, page_content='Figure 10-7. Generated examples from a ProGAN trained progressively on the LSUN\\ndataset at 256 × 256 resolution (source: Karras et al., 2017)\\nStyleGAN\\nStyleGAN3 is a GAN architecture from 2018 that builds on the earlier ideas in the\\nProGAN paper. In fact, the discriminator is identical; only the generator is changed.\\nOften when training GANs it is difficult to separate out vectors in the latent space\\ncorresponding to high-level attributes—they are frequently entangled, meaning that\\nadjusting an image in the latent space to give a face more freckles, for example, might\\nalso inadvertently change the background color. While ProGAN generates fantasti‐\\ncally realistic images, it is no exception to this general rule. We would ideally like to\\nhave full control of the style of the image, and this requires a disentangled separation\\nof features in the latent space.\\nStyleGAN achieves this by explicitly injecting style vectors into the network at differ‐\\nent points: some that control high-level features (e.g., face orientation) and some that\\ncontrol low-level details (e.g., the way the hair falls across the forehead).\\nThe overall architecture of the StyleGAN generator is shown in Figure 10-8. Let’s\\nwalk through this architecture step by step, starting with the mapping network.\\nStyleGAN \\n| \\n277'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 305}, page_content='Figure 10-8. The StyleGAN generator architecture (source: Karras et al., 2018)\\nTraining Your Own StyleGAN\\nThere is an excellent tutorial by Soon-Yau Cheong on training your\\nown StyleGAN using Keras available on the Keras website. Bear in\\nmind that training a StyleGAN to achieve the results from the\\npaper requires a significant amount of computing power.\\nThe Mapping Network\\nThe mapping network f  is a simple feed-forward network that converts the input\\nnoise �∈� into a different latent space �∈�. This gives the generator the oppor‐\\ntunity to disentangle the noisy input vector into distinct factors of variation, which\\ncan be easily picked up by the downstream style-generating layers.\\nThe point of doing this is to separate out the process of choosing a style for the image\\n(the mapping network) from the generation of an image with a given style (the syn‐\\nthesis network).\\n278 \\n| \\nChapter 10: Advanced GANs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 306}, page_content='The Synthesis Network\\nThe synthesis network is the generator of the actual image with a given style, as pro‐\\nvided by the mapping network. As can be seen from Figure 10-8, the style vector � is\\ninjected into the synthesis network at different points, each time via a differently\\ndensely connected layer Ai, which generates two vectors: a bias vector �b, i and a scal‐\\ning vector �s, i. These vectors define the specific style that should be injected at this\\npoint in the network—that is, they tell the synthesis network how to adjust the feature\\nmaps to move the generated image in the direction of the specified style.\\nThis adjustment is achieved through adaptive instance normalization (AdaIN) layers.\\nAdaptive instance normalization\\nAn AdaIN layer is a type of neural network layer that adjusts the mean and variance\\nof each feature map �i with a reference style bias �b, i and scale �s, i, respectively.4 Both\\nvectors are of length equal to the number of channels output from the preceding con‐\\nvolutional layer in the synthesis network. The equation for adaptive instance normal‐\\nization is as follows:\\nAdaIN �i, �= �s, i\\n�i −μ �i\\nσ �i\\n+ �b, i\\nThe adaptive instance normalization layers ensure that the style vectors that are injec‐\\nted into each layer only affect features at that layer, by preventing any style informa‐\\ntion from leaking through between layers. The authors show that this results in the\\nlatent vectors � being significantly more disentangled than the original � vectors.\\nSince the synthesis network is based on the ProGAN architecture, it is trained pro‐\\ngressively. The style vectors at earlier layers in the synthesis network (when the reso‐\\nlution of the image is lowest—4 × 4, 8 × 8) will affect coarser features than those later\\nin the network (64 × 64 to 1,024 × 1,024–pixel resolution). This means that not only\\ndo we have complete control over the generated image through the latent vector �,\\nbut we can also switch the � vector at different points in the synthesis network to\\nchange the style at a variety of levels of detail.\\nStyle mixing\\nThe authors use a trick known as style mixing to ensure that the generator cannot uti‐\\nlize correlations between adjacent styles during training (i.e., the styles injected at\\neach layer are as disentangled as possible). Instead of sampling only a single latent\\nvector �, two are sampled �1, �2 , corresponding to two style vectors �1, �2 . Then,\\nat each layer, either �1 or �2  is chosen at random, to break any possible correlation\\nbetween the vectors.\\nStyleGAN \\n| \\n279'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 307}, page_content='Stochastic variation\\nThe synthesizer network adds noise (passed through a learned broadcasting layer B)\\nafter each convolution to account for stochastic details such as the placement of indi‐\\nvidual hairs, or the background behind the face. Again, the depth at which the noise\\nis injected affects the coarseness of the impact on the image.\\nThis also means that the initial input to the synthesis network can simply be a learned\\nconstant, rather than additional noise. There is enough stochasticity already present\\nin the style inputs and the noise inputs to generate sufficient variation in the images.\\nOutputs from StyleGAN\\nFigure 10-9 shows StyleGAN in action.\\nFigure 10-9. Merging styles between two generated images at different levels of detail\\n(source: Karras et al., 2018)\\n280 \\n| \\nChapter 10: Advanced GANs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 308}, page_content='Here, two images, source A and source B, are generated from two different � vectors.\\nTo generate a merged image, the source A � vector is passed through the synthesis\\nnetwork but, at some point, switched for the source B � vector. If this switch happens\\nearly on (4 × 4 or 8 × 8 resolution), coarse styles such as pose, face shape, and glasses\\nfrom source B are carried across onto source A. However, if the switch happens later,\\nonly fine-grained detail is carried across from source B, such as colors and micro‐\\nstructure of the face, while the coarse features from source A are preserved.\\nStyleGAN2\\nThe final contribution in this chain of important GAN papers is StyleGAN2.5 This\\nbuilds further upon the StyleGAN architecture, with some key changes that improve\\nthe quality of the generated output. In particular, StyleGAN2 generations do not suf‐\\nfer as greatly from artifacts—water droplet–like areas of the image that were found to\\nbe caused by the adaptive instance normalization layers in StyleGAN, as shown in\\nFigure 10-10.\\nFigure 10-10. An artifact in a StyleGAN-generated image of a face (source: Karras et al.,\\n2019)\\nBoth the generator and the discriminator in StyleGAN2 are different from the Style‐\\nGAN. In the next sections we will explore the key differences between the\\narchitectures.\\nStyleGAN2 \\n| \\n281'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 309}, page_content='Training Your Own StyleGAN2\\nThe official code for training your own StyleGAN using Tensor‐\\nFlow is available on GitHub. Bear in mind that training a Style‐\\nGAN2 to achieve the results from the paper requires a significant\\namount of computing power.\\nWeight Modulation and Demodulation\\nThe artifact problem is solved by removing the AdaIN layers in the generator and\\nreplacing them with weight modulation and demodulation steps, as shown in\\nFigure 10-11. � represents the weights of the convolutional layer, which are directly\\nupdated by the modulation and demodulation steps in StyleGAN2 at runtime. In\\ncomparison, the AdaIN layers of StyleGAN operate on the image tensor as it flows\\nthrough the network.\\nThe AdaIN layer in StyleGAN is simply an instance normalization followed by style\\nmodulation (scaling and bias). The idea in StyleGAN2 is to apply style modulation\\nand normalization (demodulation) directly to the weights of the convolutional layers\\nat runtime, rather than the output from the convolutional layers, as shown in\\nFigure 10-11. The authors show how this removes the artifact issue while retaining\\ncontrol of the image style.\\nFigure 10-11. A comparison between the StyleGAN and StyleGAN2 style blocks\\n282 \\n| \\nChapter 10: Advanced GANs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 310}, page_content='In StyleGAN2, each dense layer A outputs a single style vector si, where i indexes the\\nnumber of input channels in the corresponding convolutional layer. This style vector\\nis then applied to the weights of the convolutional layer as follows:\\nwi, j, k\\n′\\n= si · wi, j, k\\nHere, j indexes the output channels of the layer and k indexes the spatial dimensions.\\nThis is the modulation step of the process.\\nThen, we need to normalize the weights so that they again have a unit standard devia‐\\ntion, to ensure stability in the training process. This is the demodulation step:\\nwi, j, k\\n′′\\n=\\nwi, j, k\\n′\\n∑i, kwi, j, k\\n′\\n2 + ε\\nwhere � is a small constant value that prevents division by zero.\\nIn the paper, the authors show how this simple change is enough to prevent water-\\ndroplet artifacts, while retaining control over the generated images via the style vec‐\\ntors and ensuring the quality of the output remains high.\\nPath Length Regularization\\nAnother change made to the StyleGAN architecture is the inclusion of an additional\\npenalty term in the loss function—this is known as path length regularization.\\nWe would like the latent space to be as smooth and uniform as possible, so that a\\nfixed-size step in the latent space in any direction results in a fixed-magnitude change\\nin the image.\\nTo encourage this property, StyleGAN2 aims to minimize the following term, along‐\\nside the usual Wasserstein loss with gradient penalty:\\n��, �∥��\\n⊤�∥2 −a\\n2\\nHere, � is a set of style vectors created by the mapping network, � is a set of noisy\\nimages drawn from �0, �, and ��= ∂g\\n∂� is the Jacobian of the generator network\\nwith respect to the style vectors.\\nThe term ∥��\\n⊤�∥2 measures the magnitude of the images � after transformation by\\nthe gradients given in the Jacobian. We want this to be close to a constant a, which is\\nStyleGAN2 \\n| \\n283'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 311}, page_content='calculated dynamically as the exponential moving average of ∥��\\n⊤�∥2 as the training\\nprogresses.\\nThe authors find that this additional term makes exploring the latent space more reli‐\\nable and consistent. Moreover, the regularization terms in the loss function are only\\napplied once every 16 minibatches, for efficiency. This technique, called lazy regulari‐\\nzation, does not cause a measurable drop in performance.\\nNo Progressive Growing\\nAnother major update is in how StyleGAN2 is trained. Rather than adopting the\\nusual progressive training mechanism, StyleGAN2 utilizes skip connections in the\\ngenerator and residual connections in the discriminator to train the entire network as\\none. It no longer requires different resolutions to be trained independently and blen‐\\nded as part of the training process.\\nFigure 10-12 shows the generator and discriminator blocks in StyleGAN2.\\nFigure 10-12. The generator and discriminator blocks in StyleGAN2\\n284 \\n| \\nChapter 10: Advanced GANs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 312}, page_content='The crucial property that we would like to be able to preserve is that the StyleGAN2\\nstarts by learning low-resolution features and gradually refines the output as training\\nprogresses. The authors show that this property is indeed preserved using this archi‐\\ntecture. Each network benefits from refining the convolutional weights in the lower-\\nresolution layers in the earlier stages of training, with the skip and residual\\nconnections used to pass the output through the higher-resolution layers mostly\\nunaffected. As training progresses, the higher-resolution layers begin to dominate, as\\nthe generator discovers more intricate ways to improve the realism of the images in\\norder to fool the discriminator. This process is demonstrated in Figure 10-13.\\nFigure 10-13. The contribution of each resolution layer to the output of the generator, by\\ntraining time (adapted from Karras et al., 2019)\\nStyleGAN2 \\n| \\n285'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 313}, page_content='Outputs from StyleGAN2\\nSome examples of StyleGAN2 output are shown in Figure 10-14. To date, the Style‐\\nGAN2 architecture (and scaled variations such as StyleGAN-XL6) remain state of the\\nart for image generation on datasets such as Flickr-Faces-HQ (FFHQ) and CIFAR-10,\\naccording to the benchmarking website Papers with Code.\\nFigure 10-14. Uncurated StyleGAN2 output for the FFHQ face dataset and LSUN car\\ndataset (source: Karras et al., 2019)\\nOther Important GANs\\nIn this section, we will explore two more architectures that have also contributed sig‐\\nnificantly to the development of GANs—SAGAN and BigGAN.\\nSelf-Attention GAN (SAGAN)\\nThe Self-Attention GAN (SAGAN)7 is a key development for GANs as it shows how\\nthe attention mechanism that powers sequential models such as the Transformer can\\nalso be incorporated into GAN-based models for image generation. Figure 10-15\\nshows the self-attention mechanism from the paper introducing this architecture.\\n286 \\n| \\nChapter 10: Advanced GANs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 314}, page_content='Figure 10-15. The self-attention mechanism within the SAGAN model (source: Zhang et\\nal., 2018)\\nThe problem with GAN-based models that do not incorporate attention is that con‐\\nvolutional feature maps are only able to process information locally. Connecting pixel\\ninformation from one side of an image to the other requires multiple convolutional\\nlayers that reduce the size of the image, while increasing the number of channels. Pre‐\\ncise positional information is reduced throughout this process in favor of capturing\\nhigher-level features, making it computationally inefficient for the model to learn\\nlong-range dependencies between distantly connected pixels. SAGAN solves this\\nproblem by incorporating the attention mechanism that we explored earlier in this\\nchapter into the GAN. The effect of this inclusion is shown in Figure 10-16.\\nFigure 10-16. A SAGAN-generated image of a bird (leftmost cell) and the attention\\nmaps of the final attention-based generator layer for the pixels covered by the three col‐\\nored dots (rightmost cells) (source: Zhang et al., 2018)\\nThe red dot is a pixel that is part of the bird’s body, and so attention naturally falls on\\nthe surrounding body cells. The green dot is part of the background, and here the\\nattention actually falls on the other side of the bird’s head, on other background pix‐\\nels. The blue dot is part of the bird’s long tail and so attention falls on other tail pixels,\\nsome of which are distant from the blue dot. It would be difficult to maintain this\\nOther Important GANs \\n| \\n287'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 315}, page_content='long-range dependency for pixels without attention, especially for long, thin struc‐\\ntures in the image (such as the tail in this case).\\nTraining Your Own SAGAN\\nThe official code for training your own SAGAN using TensorFlow\\nis available on GitHub. Bear in mind that training a SAGAN to\\nachieve the results from the paper requires a significant amount of\\ncomputing power.\\nBigGAN\\nBigGAN,8 developed at DeepMind, extends the ideas from the SAGAN paper.\\nFigure 10-17 shows some of the images generated by BigGAN, trained on the Image‐\\nNet dataset at 128 × 128 resolution.\\nFigure 10-17. Examples of images generated by BigGAN (source: Brock et al., 2018)\\nAs well as some incremental changes to the base SAGAN model, there are also several\\ninnovations outlined in the paper that take the model to the next level of sophistica‐\\ntion. One such innovation is the so-called truncation trick. This is where the latent\\ndistribution used for sampling is different from the z ∼�0, � distribution used\\nduring training. Specifically, the distribution used during sampling is a truncated nor‐\\nmal distribution (resampling values of z that have magnitude greater than a certain\\nthreshold). The smaller the truncation threshold, the greater the believability of gen‐\\nerated samples, at the expense of reduced variability. This concept is shown in\\nFigure 10-18.\\n288 \\n| \\nChapter 10: Advanced GANs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 316}, page_content='Figure 10-18. The truncation trick: from left to right, the threshold is set to 2, 1, 0.5, and\\n0.04 (source: Brock et al., 2018)\\nAlso, as the name suggests, BigGAN is an improvement over SAGAN in part simply\\nby being bigger. BigGAN uses a batch size of 2,048—8 times larger than the batch size\\nof 256 used in SAGAN—and a channel size that is increased by 50% in each layer.\\nHowever, BigGAN additionally shows that SAGAN can be improved structurally by\\nthe inclusion of a shared embedding, by orthogonal regularization, and by incorpo‐\\nrating the latent vector z into each layer of the generator, rather than just the initial\\nlayer.\\nFor a full description of the innovations introduced by BigGAN, I recommend read‐\\ning the original paper and accompanying presentation material.\\nUsing BigGAN\\nA tutorial for generating images using a pre-trained BigGAN is\\navailable on the TensorFlow website.\\nVQ-GAN\\nAnother important type of GAN is the Vector Quantized GAN (VQ-GAN), intro‐\\nduced in 2020.9 This model architecture builds upon an idea introduced in the 2017\\npaper “Neural Discrete Representation Learning”10—namely, that the representations\\nlearned by a VAE can be discrete, rather than continuous. This new type of model,\\nthe Vector Quantized VAE (VQ-VAE), was shown to generate high-quality images\\nwhile avoiding some of the issues often seen with traditional continuous latent space\\nVAEs, such as posterior collapse (where the learned latent space becomes uninforma‐\\ntive due to an overly powerful decoder).\\nThe first version of DALL.E, a text-to-image model released by\\nOpenAI in 2021 (see Chapter 13), utilized a VAE with a discrete\\nlatent space, similar to VQ-VAE.\\nOther Important GANs \\n| \\n289'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 317}, page_content='By a discrete latent space, we mean a learned list of vectors (the codebook), each asso‐\\nciated with a corresponding index. The job of the encoder in a VQ-VAE is to collapse\\nthe input image to a smaller grid of vectors that can then be compared to the code‐\\nbook. The closest codebook vector to each grid square vector (by Euclidean distance)\\nis then taken forward to be decoded by the decoder, as shown in Figure 10-19. The\\ncodebook is a list of learned vectors of length d (the embedding size) that matches the\\nnumber of channels in the output of the encoder and input to the decoder. For exam‐\\nple, e1 is a vector that can be interpreted as background.\\nFigure 10-19. A diagram of a VQ-VAE\\nThe codebook can be thought of as a set of learned discrete concepts that are shared\\nby the encoder and decoder in order to describe the contents of a given image. The\\nVQ-VAE must find a way to make this set of discrete concepts as informative as pos‐\\nsible so that the encoder can accurately label each grid square with a particular code\\nvector that is meaningful to the decoder. The loss function for a VQ-VAE is therefore\\nthe reconstruction loss added to two terms (alignment and commitment loss) that\\nensure that the output vectors from the encoder are as close as possible to vectors in\\nthe codebook. These terms replace the the KL divergence term between the encoded\\ndistribution and the standard Gaussian prior in a typical VAE.\\nHowever, this architecture poses a question—how do we sample novel code grids to\\npass to the decoder to generate new images? Clearly, using a uniform prior (picking\\neach code with equal probability for each grid square) will not work. For example in\\nthe MNIST dataset, the top-left grid square is highly likely to be coded as background,\\nwhereas grid squares toward the center of the image are not as likely to be coded as\\nsuch. To solve this problem, the authors used another model, an autoregressive\\nPixelCNN (see Chapter 5), to predict the next code vector in the grid, given previous\\n290 \\n| \\nChapter 10: Advanced GANs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 318}, page_content='code vectors. In other words, the prior is learned by the model, rather than static as in\\nthe case of the vanilla VAE.\\nTraining Your Own VQ-VAE\\nThere is an excellent tutorial by Sayak Paul on training your own\\nVQ-VAE using Keras available on the Keras website.\\nThe VQ-GAN paper details several key changes to the VQ-VAE architecture, as\\nshown in Figure 10-20.\\nFigure 10-20. A diagram of a VQ-GAN: the GAN discriminator helps to encourage the\\nVAE to generate less blurry images through an additional adversarial loss term\\nFirstly, as the name suggests, the authors include a GAN discriminator that tries to\\ndistinguish between the output from the VAE decoder and real images, with an\\naccompanying adversarial term in the loss function. GANs are known to produce\\nsharper images than VAEs, so this addition improves the overall image quality. Notice\\nthat despite the name, the VAE is still present in a VQ-GAN model—the GAN dis‐\\ncriminator is an additional component rather than a replacement of the VAE. The\\nidea of combining a VAE with a GAN discriminator (VAE-GAN) was first introduced\\nby Larsen et al. in their 2015 paper.11\\nSecondly, the GAN discriminator predicts if small patches of the images are real or\\nfake, rather than the entire image at once. This idea (PatchGAN) was applied in the\\nsuccessful pix2pix image-to-image model introduced in 2016 by Isola et al.12 and was\\nalso successfully applied as part of CycleGAN,13 another image-to-image style transfer\\nOther Important GANs \\n| \\n291'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 319}, page_content='model. The PatchGAN discriminator outputs a prediction vector (a prediction for\\neach patch), rather than a single prediction for the overall image. The benefit of using\\na PatchGAN discriminator is that the loss function can then measure how good the\\ndiscriminator is at distinguishing images based on their style, rather than their con‐\\ntent. Since each individual element of the discriminator prediction is based on a small\\nsquare of the image, it must use the style of the patch, rather than its content, to make\\nits decision. This is useful as we know that VAEs produce images that are stylistically\\nmore blurry than real images, so the PatchGAN discriminator can encourage the\\nVAE decoder to generate sharper images than it would naturally produce.\\nThirdly, rather than use a single MSE reconstruction loss that compares the input\\nimage pixels with the output pixels from the VAE decoder, VQ-GAN uses a percep‐\\ntual loss term that calculates the difference between feature maps at intermediate lay‐\\ners of the encoder and corresponding layers of the decoder. This idea is from the 2016\\npaper by Hou et al.,14 where the authors show that this change to the loss function\\nresults in more realistic image generations.\\nLastly, instead of PixelCNN, a Transformer is used as the autoregressive part of the\\nmodel, trained to generate sequences of codes. The Transformer is trained in a sepa‐\\nrate phase, after the VQ-GAN has been fully trained. Rather than use all previous\\ntokens in a fully autoregressive manner, the authors choose to only use tokens that\\nfall within a sliding window around the token to be predicted. This ensures that the\\nmodel scales to larger images, which require a larger latent grid size and therefore\\nmore tokens to be generated by the Transformer.\\nViT VQ-GAN\\nOne final extension to the VQ-GAN was made by Yu et al. in their 2021 paper enti‐\\ntled “Vector-Quantized Image Modeling with Improved VQGAN.”15 Here, the\\nauthors show how the convolutional encoder and decoder of the VQ-GAN can be\\nreplaced with Transformers as shown in Figure 10-21.\\nFor the encoder, the authors use a Vision Transformer (ViT).16 A ViT is a neural net‐\\nwork architecture that applies the Transformer model, originally designed for natural\\nlanguage processing, to image data. Instead of using convolutional layers to extract\\nfeatures from an image, a ViT divides the image into a sequence of patches, which are\\ntokenized and then fed as input to an encoder Transformer.\\nSpecifically, in the ViT VQ-GAN, the nonoverlapping input patches (each of size 8 ×\\n8) are first flattened, then projected into a low-dimensional embedding space, where\\npositional embeddings are added. This sequence is then fed to a standard encoder\\nTransformer and the resulting embeddings are quantized according to a learned\\ncodebook. These integer codes are then processed by a decoder Transformer model,\\nwith the overall output being a sequence of patches that can be stitched back together\\n292 \\n| \\nChapter 10: Advanced GANs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 320}, page_content='to form the original image. The overall encoder-decoder model is trained end-to-end\\nas an autoencoder.\\nFigure 10-21. A diagram of a ViT VQ-GAN: the GAN discriminator helps to encourage\\nthe VAE to generate less blurry images through an additional adversarial loss term\\n(source: Yu and Koh, 2022)17\\nAs with the original VQ-GAN model, the second phase of training involves using an\\nautoregressive decoder Transformer to generate sequences of codes. Therefore in\\ntotal, there are three Transformers in a ViT VQ-GAN, in addition to the GAN dis‐\\ncriminator and learned codebook. Examples of images generated by the ViT VQ-\\nGAN from the paper are shown in Figure 10-22.\\nOther Important GANs \\n| \\n293'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 321}, page_content='Figure 10-22. Example images generated by a ViT VQ-GAN trained on ImageNet\\n(source: Yu et al., 2021)\\nSummary\\nIn this chapter, we have taken a tour of some of the most important and influential\\nGAN papers since 2017. In particular, we have explored ProGAN, StyleGAN, Style‐\\nGAN2, SAGAN, BigGAN, VQ-GAN, and ViT VQ-GAN.\\nWe started by exploring the concept of progressive training that was pioneered in the\\n2017 ProGAN paper. Several key changes were introduced in the 2018 StyleGAN\\npaper that gave greater control over the image output, such as the mapping network\\nfor creating a specific style vector and synthesis network that allowed the style to be\\ninjected at different resolutions. Finally, StyleGAN2 replaced the adaptive instance\\nnormalization of StyleGAN with weight modulation and demodulation steps, along‐\\nside additional enhancements such as path regularization. The paper also showed\\nhow the desirable property of gradual resolution refinement could be retained\\nwithout having to the train the network progressively.\\nWe also saw how the concept of attention could be built into a GAN, with the intro‐\\nduction of SAGAN in 2018. This allows the network to capture long-range depen‐\\ndencies, such as similar background colors over opposite sides of an image, without\\nrelying on deep convolutional maps to spread the information over the spatial\\ndimensions of the image. BigGAN was an extension of this idea that made several key\\nchanges and trained a larger network to improve the image quality further.\\nIn the VQ-GAN paper, the authors show how several different types of generative\\nmodels can be combined to great effect. Building on the original VQ-VAE paper that\\nintroduced the concept of a VAE with a discrete latent space, VQ-GAN additionally\\nincludes a discriminator that encourages the VAE to generate less blurry images\\nthrough an additional adversarial loss term. An autoregressive Transformer is used to\\nconstruct a novel sequence of code tokens that can be decoded by the VAE decoder to\\nproduce novel images. The ViT VQ-GAN paper extends this idea even further, by\\nreplacing the convolutional encoder and decoder of VQ-GAN with Transformers.\\n294 \\n| \\nChapter 10: Advanced GANs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 322}, page_content='References\\n1. Huiwen Chang et al., “Muse: Text-to-Image Generation via Masked Generative\\nTransformers,” January 2, 2023, https://arxiv.org/abs/2301.00704.\\n2. Tero Karras et al., “Progressive Growing of GANs for Improved Quality, Stability,\\nand Variation,” October 27, 2017, https://arxiv.org/abs/1710.10196.\\n3. Tero Karras et al., “A Style-Based Generator Architecture for Generative Adversa‐\\nrial Networks,” December 12, 2018, https://arxiv.org/abs/1812.04948.\\n4. Xun Huang and Serge Belongie, “Arbitrary Style Transfer in Real-Time with Adap‐\\ntive Instance Normalization,” March 20, 2017, https://arxiv.org/abs/1703.06868.\\n5. Tero Karras et al., “Analyzing and Improving the Image Quality of StyleGAN,”\\nDecember 3, 2019, https://arxiv.org/abs/1912.04958.\\n6. Axel Sauer et al., “StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets,” Feb‐\\nruary 1, 2022, https://arxiv.org/abs/2202.00273v2.\\n7. Han Zhang et al., “Self-Attention Generative Adversarial Networks,” May 21, 2018,\\nhttps://arxiv.org/abs/1805.08318.\\n8. Andrew Brock et al., “Large Scale GAN Training for High Fidelity Natural Image\\nSynthesis,” September 28, 2018, https://arxiv.org/abs/1809.11096.\\n9. Patrick Esser et al., “Taming Transformers for High-Resolution Image Synthesis,”\\nDecember 17, 2020, https://arxiv.org/abs/2012.09841.\\n10. Aaron van den Oord et al., “Neural Discrete Representation Learning,” November\\n2, 2017, https://arxiv.org/abs/1711.00937v2.\\n11. Anders Boesen Lindbo Larsen et al., “Autoencoding Beyond Pixels Using a\\nLearned Similarity Metric,” December 31, 2015, https://arxiv.org/abs/1512.09300.\\n12. Phillip Isola et al., “Image-to-Image Translation with Conditional Adversarial\\nNetworks,” November 21, 2016, https://arxiv.org/abs/1611.07004v3.\\n13. Jun-Yan Zhu et al., “Unpaired Image-to-Image Translation using Cycle-\\nConsistent Adversarial Networks,” March 30, 2017, https://arxiv.org/abs/1703.10593.\\n14. Xianxu Hou et al., “Deep Feature Consistent Variational Autoencoder,” October 2,\\n2016, https://arxiv.org/abs/1610.00291.\\n15. Jiahui Yu et al., “Vector-Quantized Image Modeling with Improved VQGAN,”\\nOctober 9, 2021, https://arxiv.org/abs/2110.04627.\\n16. Alexey Dosovitskiy et al., “An Image Is Worth 16x16 Words: Transformers for\\nImage Recognition at Scale,” October 22, 2020, https://arxiv.org/abs/2010.11929v2.\\nSummary \\n| \\n295'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 323}, page_content='17. Jiahui Yu and Jing Yu Koh, “Vector-Quantized Image Modeling with Improved\\nVQGAN,” May 18, 2022, https://ai.googleblog.com/2022/05/vector-quantized-image-\\nmodeling-with.html.\\n296 \\n| \\nChapter 10: Advanced GANs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 324}, page_content='CHAPTER 11\\nMusic Generation\\nChapter Goals\\nIn this chapter you will:\\n• Understand how we can treat music generation as a sequence prediction prob‐\\nlem, so we can apply autoregressive models such as Transformers.\\n• See how to parse and tokenize MIDI files using the music21 package to create a\\ntraining set.\\n• Learn how to use sine positional encoding.\\n• Train a music-generating Transformer, with multiple inputs and outputs to han‐\\ndle note and duration.\\n• Understand how to handle polyphonic music, including grid tokenization and\\nevent-based tokenization.\\n• Train a MuseGAN model to generate multitrack music.\\n• Use the MuseGAN to adjust different properties of the generated bars.\\nMusical composition is a complex and creative process that involves combining dif‐\\nferent musical elements such as melody, harmony, rhythm, and timbre. While this is\\ntraditionally seen as a uniquely human activity, recent advancements have made it\\npossible to generate music that both is pleasing to the ear and has long-term\\nstructure.\\nOne of the most popular techniques for music generation is the Transformer, as\\nmusic can be thought of as a sequence prediction problem. These models have been\\nadapted to generate music by treating musical notes as a sequence of tokens, similar\\n297'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 325}, page_content='to words in a sentence. The Transformer model learns to predict the next note in the\\nsequence based on the previous notes, resulting in a generated piece of music.\\nMuseGAN takes a totally different approach to generating music. Unlike Transform‐\\ners, which generate music note by note, MuseGAN generates entire musical tracks at\\nonce by treating music as an image, consisting of a pitch axis and a time axis. More‐\\nover, MuseGAN separates out different musical components such as chords, style,\\nmelody, and groove so that they can be controlled independently.\\nIn this chapter we will learn how to process music data and apply both a Transformer\\nand MuseGAN to generate music that is stylistically similar to a given training set.\\nIntroduction\\nFor a machine to compose music that is pleasing to our ear, it must master many of\\nthe same technical challenges that we saw in Chapter 9 in relation to text. In particu‐\\nlar, our model must be able to learn from and re-create the sequential structure of\\nmusic and be able to choose from a discrete set of possibilities for subsequent notes.\\nHowever, music generation presents additional challenges that are not present for text\\ngeneration, namely pitch and rhythm. Music is often polyphonic—that is, there are\\nseveral streams of notes played simultaneously on different instruments, which com‐\\nbine to create harmonies that are either dissonant (clashing) or consonant (harmo‐\\nnious). Text generation only requires us to handle a single stream of text, in contrast\\nto the parallel streams of chords that are present in music.\\nAlso, text generation can be handled one word at a time. Unlike text data, music is a\\nmultipart, interwoven tapestry of sounds that are not necessarily delivered at the\\nsame time—much of the interest that stems from listening to music is in the interplay\\nbetween different rhythms across the ensemble. For example, a guitarist might play a\\nflurry of quicker notes while the pianist holds a longer sustained chord. Therefore,\\ngenerating music note by note is complex, because we often do not want all the\\ninstruments to change notes simultaneously.\\nWe will start this chapter by simplifying the problem to focus on music generation for\\na single (monophonic) line of music. Many of the techniques from Chapter 9 for text\\ngeneration can also be used for music generation, as the two tasks share many com‐\\nmon themes. We will start by training a Transformer to generate music in the style of\\nthe J.S. Bach cello suites and see how the attention mechanism allows the model to\\nfocus on previous notes in order to determine the most natural subsequent note.\\nWe’ll then tackle the task of polyphonic music generation and explore how we can\\ndeploy an architecture based around GANs to create music for multiple voices.\\n298 \\n| \\nChapter 11: Music Generation'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 326}, page_content='Transformers for Music Generation\\nThe model we will be building here is a decoder Transformer, taking inspiration from\\nOpenAI’s MuseNet, which also utilizes a decoder Transformer (similar to GPT-3)\\ntrained to predict the next note given a sequence of previous notes.\\nIn music generation tasks, the length of the sequence N grows large as the music pro‐\\ngresses, and this means that the N × N attention matrix for each head becomes\\nexpensive to store and compute. We ideally do not want to clip the input sequence to\\na short number of tokens, as we would like the model to construct the piece around a\\nlong-term structure and repeat motifs and phrases from several minutes ago, as a\\nhuman composer would.\\nTo tackle this problem, MuseNet utilizes a form of Transformer known as a Sparse\\nTransformer. Each output position in the attention matrix only computes weights for\\na subset of input positions, thereby reducing the computational complexity and\\nmemory required to train the model. MuseNet can therefore operate with full atten‐\\ntion over 4,096 tokens and can learn long-term structure and melodic structure\\nacross a range of styles. (See, for example, OpenAI’s Chopin and Mozart recordings\\non SoundCloud.)\\nTo see how the continuation of a musical phrase is often influenced by notes from\\nseveral bars ago, take a look at the opening bars of the Prelude to Bach’s Cello Suite\\nNo. 1 (Figure 11-1).\\nFigure 11-1. The opening of Bach’s Cello Suite No. 1 (Prelude)\\nBars\\nBars (or measures) are small units of music that contain a fixed,\\nsmall number of beats and are marked out by vertical lines that\\ncross the staff. If you can count 1, 2, 1, 2 along to a piece of music,\\nthen there are two beats in each bar and you’re probably listening\\nto a march. If you can count 1, 2, 3, 1, 2, 3, then there are three\\nbeats to each bar and you may be listening to a waltz.\\nTransformers for Music Generation \\n| \\n299'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 327}, page_content='What note do you think comes next? Even if you have no musical training you may\\nstill be able to guess. If you said G (the same as the very first note of the piece), then\\nyou’d be correct. How did you know this? You may have been able to see that every\\nbar and half bar starts with the same note and used this information to inform your\\ndecision. We want our model to be able to perform the same trick—in particular, we\\nwant it to pay attention to a particular note from the previous half bar, when the pre‐\\nvious low G was registered. An attention-based model such as a Transformer will be\\nable to incorporate this long-term look-back without having to maintain a hidden\\nstate across many bars, as is the case with a recurrent neural network.\\nAnyone tackling the task of music generation must first have a basic understanding of\\nmusical theory. In the next section we’ll go through the essential knowledge required\\nto read music and how we can represent this numerically, in order to transform\\nmusic into the input data required to train our Transformer.\\nRunning the Code for This Example\\nThe code for this example can be found in the Jupyter notebook\\nlocated at notebooks/11_music/01_transformer/transformer.ipynb in\\nthe book repository.\\nThe Bach Cello Suite Dataset\\nThe raw dataset that we shall be using is a set of MIDI files for the Cello Suites by J.S.\\nBach. You can download the dataset by running the dataset downloader script in the\\nbook repository, as shown in Example 11-1. This will save the MIDI files locally to\\nthe /data folder.\\nExample 11-1. Downloading the J.S. Bach Cello Suites dataset\\nbash scripts/download_music_data.sh\\nTo view and listen to the music generated by the model, you’ll need some software\\nthat can produce musical notation. MuseScore is a great tool for this purpose and can\\nbe downloaded for free.\\nParsing MIDI Files\\nWe’ll be using the Python library music21 to load the MIDI files into Python for pro‐\\ncessing. Example 11-2 shows how to load a MIDI file and visualize it (Figure 11-2),\\nboth as a score and as structured data.\\n300 \\n| \\nChapter 11: Music Generation'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 328}, page_content='Figure 11-2. Musical notation\\nTransformers for Music Generation \\n| \\n301'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 329}, page_content='Example 11-2. Importing a MIDI file\\nimport music21\\nfile = \"/app/data/bach-cello/cs1-2all.mid\"\\nexample_score = music21.converter.parse(file).chordify()\\nOctaves\\nThe number after each note name indicates the octave that the note\\nis in—since the note names (A to G) repeat, this is needed to\\nuniquely identify the pitch of the note. For example, G2 is an octave\\nbelow G3.\\nNow it’s time to convert the scores into something that looks more like text! We start\\nby looping over each score and extracting the note and duration of each element in\\nthe piece into two separate text strings, with elements separated by spaces. We encode\\nthe key and time signature of the piece as special symbols, with zero duration.\\nMonophonic Versus Polyphonic Music\\nIn this first example, we will treat the music as monophonic (one\\nsingle line), taking just the top note of any chords. Sometimes we\\nmay wish to keep the parts separate to generate music that is poly‐\\nphonic in nature. This presents additional challenges that we shall\\ntackle later on in this chapter.\\nThe output from this process is shown in Figure 11-3—compare this to Figure 11-2\\nso that you can see how the raw music data has been transformed into the two strings.\\nFigure 11-3. Samples of the notes text string and the duration text string, corresponding\\nto Figure 11-2\\n302 \\n| \\nChapter 11: Music Generation'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 330}, page_content='This looks a lot more like the text data that we have dealt with previously. The words\\nare the note–duration combinations, and we should try to build a model that predicts\\nthe next note and duration, given a sequence of previous notes and durations. A key\\ndifference between music and text generation is that we need to build a model that\\ncan handle the note and duration prediction simultaneously—i.e., there are two\\nstreams of information that we need to handle, compared to the single streams of text\\nthat we saw in Chapter 9.\\nTokenization\\nTo create the dataset that will train the model, we first need to tokenize each note and\\nduration, exactly as we did previously for each word in a text corpus. We can achieve\\nthis by using a TextVectorization layer, applied to the notes and durations sepa‐\\nrately, as shown in Example 11-3.\\nExample 11-3. Tokenizing the notes and durations\\ndef create_dataset(elements):\\n    ds = (\\n        tf.data.Dataset.from_tensor_slices(elements)\\n        .batch(BATCH_SIZE, drop_remainder = True)\\n        .shuffle(1000)\\n    )\\n    vectorize_layer = layers.TextVectorization(\\n        standardize = None, output_mode=\"int\"\\n    )\\n    vectorize_layer.adapt(ds)\\n    vocab = vectorize_layer.get_vocabulary()\\n    return ds, vectorize_layer, vocab\\nnotes_seq_ds, notes_vectorize_layer, notes_vocab = create_dataset(notes)\\ndurations_seq_ds, durations_vectorize_layer, durations_vocab = create_dataset(\\n    durations\\n)\\nseq_ds = tf.data.Dataset.zip((notes_seq_ds, durations_seq_ds))\\nTransformers for Music Generation \\n| \\n303'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 331}, page_content='The full parsing and tokenization process is shown in Figure 11-4.\\nFigure 11-4. Parsing the MIDI files and tokenizing the notes and durations\\nCreating the Training Set\\nThe final step of preprocessing is to create the training set that we will feed to our\\nTransformer.\\nWe do this by splitting both the note and duration strings into chunks of 50 elements,\\nusing a sliding window technique. The output is simply the input window shifted by\\none note, so that the Transformer is trained to predict the note and duration of the\\nelement one timestep into the future, given previous elements in the window. An\\nexample of this (using a sliding window of only four elements for demonstration pur‐\\nposes) is shown in Figure 11-5.\\n304 \\n| \\nChapter 11: Music Generation'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 332}, page_content='Figure 11-5. The inputs and outputs for the musical Transformer model—in this exam‐\\nple, a sliding window of width 4 is used to create input chunks, which are then shifted by\\none element to create the target output\\nThe architecture we will be using for our Transformer is the same as we used for text\\ngeneration in Chapter 9, with a few key differences.\\nSine Position Encoding\\nFirstly, we will be introducing a different type of encoding for the token positions. In\\nChapter 9 we used a simple Embedding layer to encode the position of each token,\\neffectively mapping each integer position to a distinct vector that was learned by the\\nmodel. We therefore needed to define a maximum length (N) that the sequence could\\nbe and train on this length of sequence. The downside to this approach is that it is\\nthen impossible to extrapolate to sequences that are longer than this maximum\\nlength. You would have to clip the input to the last N tokens, which isn’t ideal if you\\nare trying to generate long-form content.\\nTransformers for Music Generation \\n| \\n305'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 333}, page_content='To circumvent this problem, we can switch to using a different type of embedding\\ncalled a sine position embedding. This is similar to the embedding that we used in\\nChapter 8 to encode the noise variances of the diffusion model. Specifically, the fol‐\\nlowing function is used to convert the position of the word (pos) in the input\\nsequence into a unique vector of length d:\\nPEpos, 2i = sin\\npos\\n10, 0002i/d\\nPEpos, 2i + 1 = cos\\npos\\n10, 000 2i + 1 /d\\nFor small i, the wavelength of this function is short and therefore the function value\\nchanges rapidly along the position axis. Larger values of i create a longer wavelength.\\nEach position thus has its own unique encoding, which is a specific combination of\\nthe different wavelengths.\\nNotice that this embedding is defined for all possible position val‐\\nues. It is a deterministic function (i.e., it isn’t learned by the model)\\nthat uses trigonometric functions to define a unique encoding for\\neach possible position.\\nThe Keras NLP module has a built-in layer that implements this embedding for us—\\nwe can therefore define our TokenAndPositionEmbedding layer as shown in\\nExample 11-4.\\nExample 11-4. Tokenizing the notes and durations\\nclass TokenAndPositionEmbedding(layers.Layer):\\n    def __init__(self, vocab_size, embed_dim):\\n        super(TokenAndPositionEmbedding, self).__init__()\\n        self.vocab_size = vocab_size\\n        self.embed_dim = embed_dim\\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\\n        self.pos_emb = keras_nlp.layers.SinePositionEncoding()\\n    def call(self, x):\\n        embedding = self.token_emb(x)\\n        positions = self.pos_emb(embedding)\\n        return embedding + positions\\nFigure 11-6 shows how the two embeddings (token and position) are added to pro‐\\nduce the overall embedding for the sequence.\\n306 \\n| \\nChapter 11: Music Generation'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 334}, page_content='Figure 11-6. The TokenAndPositionEmbedding layer adds the token embeddings to the\\nsinusoidal position embeddings to produce the overall embedding for the sequence\\nMultiple Inputs and Outputs\\nWe now have two input streams (notes and durations) and two output streams (pre‐\\ndicted notes and durations). We therefore need to adapt the architecture of our\\nTransformer to cater for this.\\nThere are many ways of handling the dual stream of inputs. We could create tokens\\nthat represent each note–duration pair and then treat the sequence as a single stream\\nof tokens. However, this has the downside of not being able to represent note–dura‐\\ntion pairs that have not been seen in the training set (for example, we may have seen a\\nG#2 note and a 1/3 duration independently, but never together, so there would be no\\ntoken for G#2:1/3.\\nInstead, we choose to embed the note and duration tokens separately and then use a\\nconcatenation layer to create a single representation of the input that can be used by\\nthe downstream Transformer block. Similarly, the output from the Transformer block\\nis passed to two separate dense layers, which represent the predicted note and dura‐\\ntion probabilities. The overall architecture is shown in Figure 11-7. Layer output\\nshapes are shown with batch size b and sequence length l.\\nTransformers for Music Generation \\n| \\n307'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 335}, page_content='Figure 11-7. The architecture of the music-generating Transformer\\nAn alternative approach would be to interleave the note and duration tokens into a\\nsingle stream of input and let the model learn that the output should be a single\\nstream where the note and duration tokens alternate. This comes with the added\\ncomplexity of ensuring that the output can still be parsed when the model has not yet\\nlearned how to interleave the tokens correctly.\\nThere is no right or wrong way to design your model—part of the\\nfun is experimenting with different setups and seeing which works\\nbest for you!\\n308 \\n| \\nChapter 11: Music Generation'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 336}, page_content='Analysis of the Music-Generating Transformer\\nWe’ll start by generating some music from scratch, by seeding the network with a\\nSTART note token and 0.0 duration token (i.e., we are telling the model to assume it is\\nstarting from the beginning of the piece). Then we can generate a musical passage\\nusing the same iterative technique we used in Chapter 9 for generating text sequen‐\\nces, as follows:\\n1. Given the current sequence (of notes and durations), the model predicts two dis‐\\ntributions, one for the next note and one for the next duration.\\n2. We sample from both of these distributions, using a temperature parameter to\\ncontrol how much variation we would like in the sampling process.\\n3. The chosen note and duration are appended to the respective input sequences.\\n4. The process repeats with the new input sequences for as many elements as we\\nwish to generate.\\nFigure 11-8 shows examples of music generated from scratch by the model at various\\nepochs of the training process. We use a temperature of 0.5 for the notes and\\ndurations.\\nFigure 11-8. Some examples of passages generated by the model when seeded only with a\\nSTART note token and 0.0 duration token\\nTransformers for Music Generation \\n| \\n309'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 337}, page_content='Most of our analysis in this section will focus on the note predictions, rather than\\ndurations, as for Bach’s Cello Suites the harmonic intricacies are more difficult to cap‐\\nture and therefore more worthy of investigation. However, you can also apply the\\nsame analysis to the rhythmic predictions of the model, which may be particularly\\nrelevant for other styles of music that you could use to train this model (such as a\\ndrum track).\\nThere are several points to note about the generated passages in Figure 11-8. First, see\\nhow the music is becoming more sophisticated as training progresses. To begin with,\\nthe model plays it safe by sticking to the same group of notes and rhythms. By epoch\\n10, the model has begun to generate small runs of notes, and by epoch 20 it is pro‐\\nducing interesting rhythms and is firmly established in a set key (E ♭ major).\\nSecond, we can analyze the distribution of notes over time by plotting the predicted\\ndistribution at each timestep as a heatmap. Figure 11-9 shows this heatmap for the\\nexample from epoch 20 in Figure 11-8.\\nFigure 11-9. The distribution of possible next notes over time (at epoch 20): the darker\\nthe square, the more certain the model is that the next note is at this pitch\\n310 \\n| \\nChapter 11: Music Generation'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 338}, page_content='An interesting point to note here is that the model has clearly learned which notes\\nbelong to particular keys, as there are gaps in the distribution at notes that do not\\nbelong to the key. For example, there is a gray gap along the row for note 54 (corre‐\\nsponding to G ♭/F ♯). This note is highly unlikely to appear in a piece of music in the\\nkey of E ♭ major. The model establishes the key early on in the generation process,\\nand as the piece progresses, the model chooses notes that are more likely to feature in\\nthat key by attending to the token that represents it.\\nIt is also worth pointing out that the model has learned Bach’s characteristic style of\\ndropping to a low note on the cello to end a phrase and bouncing back up again to\\nstart the next. See how around note 20, the phrase ends on a low E ♭—it is common\\nin the Bach Cello Suites to then return to a higher, more sonorous range of the instru‐\\nment for the start of next phrase, which is exactly what the model predicts. There is a\\nlarge gray gap between the low E ♭ (pitch number 39) and the next note, which is\\npredicted to be around pitch number 50, rather than continuing to rumble around\\nthe depths of the instrument.\\nLastly, we should check to see if our attention mechanism is working as expected. The\\nhorizontal axis in Figure 11-10 shows the generated sequence of notes; the vertical\\naxis shows where the attention of the network was aimed when predicting each note\\nalong the horizontal axis. The color of each square shows the maximum attention\\nweight across all heads at each point in the generated sequence. The darker the\\nsquare, the more attention is being applied to this position in the sequence. For sim‐\\nplicity, we only show the notes in this diagram, but the durations of each note are also\\nbeing attended to by the network.\\nWe can see that for the initial key signature, time signature, and rest, the network\\nchose to place almost all of its attention on the START token. This makes sense, as\\nthese artifacts always appear at the start of a piece of music—once the notes start\\nflowing the START token essentially stops being attended to.\\nAs we move beyond the initial few notes, we can see that the network places most\\nattention on approximately the last two to four notes and rarely places significant\\nweight on notes more than four notes ago. Again, this makes sense; there is probably\\nenough information contained in the previous four notes to understand how the\\nphrase might continue. Additionally, some notes attend more strongly back to the key\\nsignature of D minor—for example, the E3 (7th note of the piece) and B-2 (B ♭–14th\\nnote of the piece). This is fascinating, because these are the exact notes that rely on\\nthe key of D minor to relieve any ambiguity. The network must look back at the key\\nsignature in order to tell that there is a B ♭ in the key signature (rather than a B natu‐\\nral) but there isn’t an E ♭ in the key signature (E natural must be used instead).\\nTransformers for Music Generation \\n| \\n311'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 339}, page_content='Figure 11-10. The color of each square in the matrix indicates the amount of attention\\ngiven to each position on the vertical axis, at the point of predicting the note on the hori‐\\nzontal axis\\nThere are also examples of where the network has chosen to ignore a certain note or\\nrest nearby, as it doesn’t add any additional information to its understanding of the\\nphrase. For example, the penultimate note (A2) is not particularly attentive to the B-2\\nthree notes back, but is slightly more attentive to the A2 four notes back. It is more\\ninteresting for the model to look at the A2 that falls on the beat, rather than the B-2 off\\nthe beat, which is just a passing note.\\nRemember we haven’t told the model anything about which notes are related or\\nwhich notes belong to which key signatures—it has worked this out for itself just by\\nstudying the music of J.S. Bach.\\n312 \\n| \\nChapter 11: Music Generation'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 340}, page_content='Tokenization of Polyphonic Music\\nThe Transformer we’ve been exploring in this section works well for single-line\\n(monophonic) music, but could it be adapted to multiline (polyphonic) music?\\nThe challenge lies in how to represent the different lines of music as a single sequence\\nof tokens. In the previous section we decided to split the notes and durations of the\\nnotes into two distinct inputs and outputs of the network, but we also saw that we\\ncould have interleaved these tokens into a single stream. We can use the same idea to\\nhandle polyphonic music. Two different approaches will be introduced here: grid\\ntokenization and event-based tokenization, as discussed in the 2018 paper “Music\\nTransformer: Generating Music with Long-Term Structure.”1\\nGrid tokenization\\nConsider the two bars of music from a J.S. Bach chorale in Figure 11-11. There are\\nfour distinct parts (soprano [S], alto [A], tenor [T], bass [B]), written on different\\nstaffs.\\nFigure 11-11. The first two bars of a J.S. Bach chorale\\nWe can imagine drawing this music on a grid, where the y-axis represents the pitch of\\nthe note and the x-axis represents the number of 16th-notes (semiquavers) that have\\npassed since the start of the piece. If the grid square is filled, then there is a note\\nTransformers for Music Generation \\n| \\n313'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 341}, page_content='playing at that point in time. All four parts are drawn on the same grid. This grid is\\nknown as a piano roll because it resembles a physical roll of paper with holes punched\\ninto it, which was used as a recording mechanism before digital systems were\\ninvented.\\nWe can serialize the grid into a stream of tokens by moving first through the four voi‐\\nces, then along the timesteps in sequence. This produces a sequence of tokens\\nS1, A1, T1, B1, S2, A2, T2, B2, ..., where the subscript denotes the timestep, as shown in\\nFigure 11-12.\\nFigure 11-12. Creating the grid tokenization for the first two bars of the Bach chorale\\n314 \\n| \\nChapter 11: Music Generation'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 342}, page_content='We would then train our Transformer on this sequence of tokens, to predict the next\\ntoken given the previous tokens. We can decode the generated sequence back into a\\ngrid structure by rolling the sequence back out over time in groups of four notes (one\\nfor each voice). This technique works surprisingly well, despite the same note often\\nbeing split across multiple tokens with tokens from other voices in between.\\nHowever, there are some disadvantages. Firstly, notice that there is no way for the\\nmodel to tell the difference between one long note and two shorter adjacent notes of\\nthe same pitch. This is because the tokenization does not explicitly encode the dura‐\\ntion of notes, only whether a note is present at each timestep.\\nSecondly, this method requires the music to have a regular beat that is divisible into\\nreasonably sized chunks. For example, using the current system, we cannot encode\\ntriplets (a group of three notes played across a single beat). We could divide the music\\ninto 12 steps per quarter-note (crotchet) instead of 4, that would triple the number of\\ntokens required to represent the same passage of music, adding overhead on the\\ntraining process and affecting the lookback capacity of the model.\\nLastly, it is not obvious how we might add other components to the tokenization,\\nsuch as dynamics (how loud or quiet the music is in each part) or tempo changes. We\\nare locked into the two-dimensional grid structure of the piano roll, which provides a\\nconvenient way to represent pitch and timing, but not necessarily an easy way to\\nincorporate other components that make music interesting to listen to.\\nEvent-based tokenization\\nA more flexible approach is to use event-based tokenization. This can be thought of\\nas a vocabulary that literally describes how the music is created as a sequence of\\nevents, using a rich set of tokens.\\nFor example in Figure 11-13, we use three types of tokens:\\n• NOTE_ON<pitch> (start playing a note of a given pitch)\\n• NOTE_OFF<pitch> (stop playing a note of a given pitch)\\n• TIME_SHIFT<step> (shift forward in time by a given step)\\nThis vocabulary can be used to create a sequence that describes the construction of\\nthe music as a set of instructions.\\nTransformers for Music Generation \\n| \\n315'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 343}, page_content='Figure 11-13. An event tokenization for the first bar of the Bach chorale\\nWe could easily incorporate other types of tokens into this vocabulary, to represent\\ndynamic and tempo changes for subsequent notes. This method also provides a way\\nto generate triplets against a backdrop of quarter-notes, by separating the notes of the\\ntriplets with TIME_SHIFT<0.33> tokens. Overall, it is a more expressive framework for\\ntokenization, though it is also potentially more complex for the Transformer to learn\\ninherent patterns in the training set music, as it is by definition less structured than\\nthe grid method.\\nI encourage you to try implementing these polyphonic techniques\\nand train a Transformer on the new tokenized dataset using all\\nthe knowledge you have built up so far in this book. I would also\\nrecommend checking our Dr. Tristan Behrens’s guide to music gen‐\\neration research, available on GitHub, which provides a compre‐\\nhensive overview of different papers on the topic of music\\ngeneration using deep learning.\\nIn the next section we will take a completely different approach to music generation,\\nusing GANs.\\n316 \\n| \\nChapter 11: Music Generation'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 344}, page_content='MuseGAN\\nYou may have thought that the piano roll shown in Figure 11-12 looks a bit like a\\npiece of modern art. This begs the question—could we in fact treat this piano roll as a\\npicture and utilize image generation methods instead of sequence generation\\ntechniques?\\nAs we shall see, the answer to this question is yes, we can treat music generation\\ndirectly as an image generation problem. This means that instead of using Trans‐\\nformers, we can apply the same convolutional-based techniques that work so well for\\nimage generation problems—in particular, GANs.\\nMuseGAN was introduced in the 2017 paper “MuseGAN: Multi-Track Sequential\\nGenerative Adversarial Networks for Symbolic Music Generation and Accompani‐\\nment.”2 The authors show how it is possible to train a model to generate polyphonic,\\nmultitrack, multibar music through a novel GAN framework. Moreover, they show\\nhow, by dividing up the responsibilities of the noise vectors that feed the generator,\\nthey are able to maintain fine-grained control over the high-level temporal and track-\\nbased features of the music.\\nLet’s start by introducing the the J.S. Bach chorale dataset.\\nRunning the Code for This Example\\nThe code for this example can be found in the Jupyter notebook\\nlocated at notebooks/11_music/02_musegan/musegan.ipynb in the\\nbook repository.\\nThe Bach Chorale Dataset\\nTo begin this project, you’ll first need to download the MIDI files that we’ll be using\\nto train the MuseGAN. We’ll use a dataset of 229 J.S. Bach chorales for four voices.\\nYou can download the dataset by running the Bach chorale dataset downloader script\\nin the book repository, as shown in Example 11-5. This will save the MIDI files locally\\nto the /data folder.\\nExample 11-5. Downloading the Bach chorale dataset\\nbash scripts/download_bach_chorale_data.sh\\nMuseGAN \\n| \\n317'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 345}, page_content='The dataset consists of an array of four numbers for each timestep: the MIDI note\\npitches of each of the four voices. A timestep in this dataset is equal to a 16th note (a\\nsemiquaver). So, for example, in a single bar of 4 quarter (crotchet) beats, there are 16\\ntimesteps. The dataset is automatically split into train, validation, and test sets. We\\nwill be using the train dataset to train the MuseGAN.\\nTo start, we need to get the data into the correct shape to feed the GAN. In this exam‐\\nple we’ll generate two bars of music, so we’ll extract only the first two bars of each\\nchorale. Each bar consists of 16 timesteps and there are a potential 84 pitches across\\nthe 4 voices.\\nVoices will be referred to as tracks from here on, to keep the termi‐\\nnology in line with the original paper.\\nTherefore, the transformed data will have the following shape:\\n[BATCH_SIZE, N_BARS, N_STEPS_PER_BAR, N_PITCHES, N_TRACKS]\\nwhere:\\nBATCH_SIZE = 64\\nN_BARS = 2\\nN_STEPS_PER_BAR = 16\\nN_PITCHES = 84\\nN_TRACKS = 4\\nTo get the data into this shape, we one-hot encode the pitch numbers into a vector of\\nlength 84 and split each sequence of notes into two bars of 16 timesteps each. We are\\nmaking the assumption here that each chorale in the dataset has four beats in each\\nbar, which is reasonable, and even if this were not the case it would not adversely\\naffect the training of the model.\\nFigure 11-14 shows how two bars of raw data are converted into the transformed\\npiano roll dataset that we will use to train the GAN.\\n318 \\n| \\nChapter 11: Music Generation'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 346}, page_content='Figure 11-14. Processing two bars of raw data into piano roll data that we can use to\\ntrain the GAN\\nMuseGAN \\n| \\n319'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 347}, page_content='The MuseGAN Generator\\nLike all GANs, MuseGAN consists of a generator and a critic. The generator tries to\\nfool the critic with its musical creations, and the critic tries to prevent this from hap‐\\npening by ensuring it is able to tell the difference between the generator’s forged Bach\\nchorales and the real thing.\\nWhere MuseGAN differs is in the fact that the generator doesn’t just accept a single\\nnoise vector as input, but instead has four separate inputs, which correspond to four\\ndifferent characteristics of the music: chords, style, melody, and groove. By manipu‐\\nlating each of these inputs independently we can change high-level properties of the\\ngenerated music.\\nA high-level view of the generator is shown in Figure 11-15.\\nFigure 11-15. High-level diagram of the MuseGAN generator\\nThe diagram shows how the chords and melody inputs are first passed through a\\ntemporal network that outputs a tensor with one of the dimensions equal to the num‐\\nber of bars to be generated. The style and groove inputs are not stretched temporally\\nin this way, as they remain constant through the piece.\\nThen, to generate a particular bar for a particular track, the relevant outputs from the\\nchords, style, melody, and groove parts of the network are concatenated to form a\\nlonger vector. This is then passed to a bar generator, which ultimately outputs the\\nspecified bar for the specified track.\\n320 \\n| \\nChapter 11: Music Generation'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 348}, page_content=\"By concatenating the generated bars for all tracks, we create a score that can be com‐\\npared with real scores by the critic.\\nLet’s first take a look at how to build a temporal network.\\nThe temporal network\\nThe job of a temporal network—a neural network consisting of convolutional trans‐\\npose layers—is to transform a single input noise vector of length Z_DIM = 32 into a\\ndifferent noise vector for every bar (also of length 32). The Keras code to build this is\\nshown in Example 11-6.\\nExample 11-6. Building the temporal network\\ndef conv_t(x, f, k, s, a, p, bn):\\n    x = layers.Conv2DTranspose(\\n                filters = f\\n                , kernel_size = k\\n                , padding = p\\n                , strides = s\\n                , kernel_initializer = initializer\\n                )(x)\\n    if bn:\\n        x = layers.BatchNormalization(momentum = 0.9)(x)\\n    x = layers.Activation(a)(x)\\n    return x\\ndef TemporalNetwork():\\n    input_layer = layers.Input(shape=(Z_DIM,), name='temporal_input') \\n    x = layers.Reshape([1,1,Z_DIM])(input_layer) \\n    x = conv_t(\\n        x, f=1024, k=(2,1), s=(1,1), a = 'relu', p = 'valid', bn = True\\n    ) \\n    x = conv_t(\\n        x, f=Z_DIM, k=(N_BARS - 1,1), s=(1,1), a = 'relu', p = 'valid', bn = True\\n    )\\n    output_layer = layers.Reshape([N_BARS, Z_DIM])(x) \\n    return models.Model(input_layer, output_layer)\\nThe input to the temporal network is a vector of length 32 (Z_DIM).\\nWe reshape this vector to a 1 × 1 tensor with 32 channels, so that we can apply\\nconvolutional 2D transpose operations to it.\\nWe apply Conv2DTranspose layers to expand the size of the tensor along one axis,\\nso that it is the same length as N_BARS.\\nMuseGAN \\n| \\n321\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 349}, page_content='We remove the unnecessary extra dimension with a Reshape layer.\\nThe reason we use convolutional operations rather than requiring two independent\\nvectors into the network is because we would like the network to learn how one bar\\nshould follow on from another in a consistent way. Using a neural network to expand\\nthe input vector along the time axis means the model has a chance to learn how\\nmusic flows across bars, rather than treating each bar as completely independent of\\nthe last.\\nChords, style, melody, and groove\\nLet’s now take a closer look at the four different inputs that feed the generator:\\nChords\\nThe chords input is a single noise vector of length Z_DIM. This vector’s job is to\\ncontrol the general progression of the music over time, shared across tracks, so\\nwe use a TemporalNetwork to transform this single vector into a different latent\\nvector for every bar. Note that while we call this input chords, it really could con‐\\ntrol anything about the music that changes per bar, such as general rhythmic\\nstyle, without being specific to any particular track.\\nStyle\\nThe style input is also a vector of length Z_DIM. This is carried forward without\\ntransformation, so it is the same across all bars and tracks. It can be thought of as\\nthe vector that controls the overall style of the piece (i.e., it affects all bars and\\ntracks consistently).\\nMelody\\nThe melody input is an array of shape [N_TRACKS, Z_DIM]—that is, we provide\\nthe model with a random noise vector of length Z_DIM for each track.\\nEach of these vectors is passed through a track-specific TemporalNetwork, where\\nthe weights are not shared between tracks. The output is a vector of length Z_DIM\\nfor every bar of every track. The model can therefore use these input vectors to\\nfine-tune the content of every single bar and track independently.\\nGroove\\nThe groove input is also an array of shape [N_TRACKS, Z_DIM]—a random noise\\nvector of length Z_DIM for each track. Unlike the melody input, these vectors are\\nnot passed through the temporal network but instead are fed straight through,\\njust like the style vector. Therefore, each groove vector will affect the overall\\nproperties of a track, across all bars.\\n322 \\n| \\nChapter 11: Music Generation'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 350}, page_content=\"We can summarize the responsibilities of each component of the MuseGAN genera‐\\ntor as shown in Table 11-1.\\nTable 11-1. Components of the MuseGAN generator\\nOutput differs across bars? Output differs across parts?\\nStyle\\nＸ\\nＸ\\nGroove\\nＸ\\n✓\\nChords\\n✓\\nＸ\\nMelody\\n✓\\n✓\\nThe final piece of the MuseGAN generator is the bar generator—let’s see how we can\\nuse this to glue together the outputs from the chord, style, melody, and groove\\ncomponents.\\nThe bar generator\\nThe bar generator receives four latent vectors—one from each of the chord, style,\\nmelody, and groove components. These are concatenated to produce a vector of\\nlength 4 * Z_DIM as input. The output is a piano roll representation of a single bar\\nfor a single track—i.e., a tensor of shape [1, n_steps_per_bar, n_pitches, 1].\\nThe bar generator is just a neural network that uses convolutional transpose layers to\\nexpand the time and pitch dimensions of the input vector. We create one bar genera‐\\ntor for every track, and weights are not shared between tracks. The Keras code to\\nbuild a BarGenerator is given in Example 11-7.\\nExample 11-7. Building the BarGenerator\\ndef BarGenerator():\\n    input_layer = layers.Input(shape=(Z_DIM * 4,), name='bar_generator_input') \\n    x = layers.Dense(1024)(input_layer) \\n    x = layers.BatchNormalization(momentum = 0.9)(x)\\n    x = layers.Activation('relu')(x)\\n    x = layers.Reshape([2,1,512])(x)\\n    x = conv_t(x, f=512, k=(2,1), s=(2,1), a= 'relu',  p = 'same', bn = True) \\n    x = conv_t(x, f=256, k=(2,1), s=(2,1), a= 'relu', p = 'same', bn = True)\\n    x = conv_t(x, f=256, k=(2,1), s=(2,1), a= 'relu', p = 'same', bn = True)\\n    x = conv_t(x, f=256, k=(1,7), s=(1,7), a= 'relu', p = 'same', bn = True) \\n    x = conv_t(x, f=1, k=(1,12), s=(1,12), a= 'tanh', p = 'same', bn = False) \\n    output_layer = layers.Reshape([1, N_STEPS_PER_BAR , N_PITCHES ,1])(x) \\n    return models.Model(input_layer, output_layer)\\nMuseGAN \\n| \\n323\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 351}, page_content=\"The input to the bar generator is a vector of length 4 * Z_DIM.\\nAfter passing it through a Dense layer, we reshape the tensor to prepare it for the\\nconvolutional transpose operations.\\nFirst we expand the tensor along the timestep axis…\\n…then along the pitch axis.\\nThe final layer has a tanh activation applied, as we will be using a WGAN-GP\\n(which requires tanh output activation) to train the network.\\nThe tensor is reshaped to add two extra dimensions of size 1, to prepare it for\\nconcatenation with other bars and tracks.\\nPutting it all together\\nUltimately, the MuseGAN generator takes the four input noise tensors (chords, style,\\nmelody, and groove) and converts them into a multitrack, multibar score. The Keras\\ncode to build the MuseGAN generator is provided in Example 11-8.\\nExample 11-8. Building the MuseGAN generator\\ndef Generator():\\n    chords_input = layers.Input(shape=(Z_DIM,), name='chords_input') \\n    style_input = layers.Input(shape=(Z_DIM,), name='style_input')\\n    melody_input = layers.Input(shape=(N_TRACKS, Z_DIM), name='melody_input')\\n    groove_input = layers.Input(shape=(N_TRACKS, Z_DIM), name='groove_input')\\n    chords_tempNetwork = TemporalNetwork() \\n    chords_over_time = chords_tempNetwork(chords_input)\\n    melody_over_time = [None] * N_TRACKS\\n    melody_tempNetwork = [None] * N_TRACKS\\n    for track in range(N_TRACKS):\\n        melody_tempNetwork[track] = TemporalNetwork() \\n        melody_track = layers.Lambda(lambda x, track = track: x[:,track,:])(\\n            melody_input\\n        )\\n        melody_over_time[track] = melody_tempNetwork[track](melody_track)\\n    barGen = [None] * N_TRACKS\\n    for track in range(N_TRACKS):\\n        barGen[track] = BarGenerator() \\n    bars_output = [None] * N_BARS\\n    c = [None] * N_BARS\\n    for bar in range(N_BARS): \\n324 \\n| \\nChapter 11: Music Generation\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 352}, page_content=\"track_output = [None] * N_TRACKS\\n        c[bar] = layers.Lambda(lambda x, bar = bar: x[:,bar,:])(chords_over_time)\\n        s = style_input\\n        for track in range(N_TRACKS):\\n            m = layers.Lambda(lambda x, bar = bar: x[:,bar,:])(\\n                melody_over_time[track]\\n            )\\n            g = layers.Lambda(lambda x, track = track: x[:,track,:])(\\n                groove_input\\n            )\\n            z_input = layers.Concatenate(\\n                axis = 1, name = 'total_input_bar_{}_track_{}'.format(bar, track)\\n            )([c[bar],s,m,g])\\n            track_output[track] = barGen[track](z_input)\\n        bars_output[bar] = layers.Concatenate(axis = -1)(track_output)\\n    generator_output = layers.Concatenate(axis = 1, name = 'concat_bars')(\\n        bars_output\\n    ) \\n    return models.Model(\\n        [chords_input, style_input, melody_input, groove_input], generator_output\\n    ) \\ngenerator = Generator()\\nDefine the inputs to the generator.\\nPass the chords input through the temporal network.\\nPass the melody input through the temporal network.\\nCreate an independent bar generator network for every track.\\nLoop over the tracks and bars, creating a generated bar for each combination.\\nConcatenate everything together to form a single output tensor.\\nThe MuseGAN model takes four distinct noise tensors as input and outputs a\\ngenerated multitrack, multibar score.\\nMuseGAN \\n| \\n325\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 353}, page_content=\"The MuseGAN Critic\\nIn comparison to the generator, the critic architecture is much more straightforward\\n(as is often the case with GANs).\\nThe critic tries to distinguish full multitrack, multibar scores created by the generator\\nfrom real excerpts from the Bach chorales. It is a convolutional neural network, con‐\\nsisting mostly of Conv3D layers that collapse the score into a single output prediction.\\nConv3D Layers\\nSo far in this book, we have only worked with Conv2D layers, appli‐\\ncable to three-dimensional input images (width, height, channels).\\nHere we have to use Conv3D layers, which are analogous to Conv2D\\nlayers but accept four-dimensional input tensors (n_bars,\\nn_steps_per_bar, n_pitches, n_tracks).\\nWe do not use batch normalization layers in the critic as we will be using the WGAN-\\nGP framework for training the GAN, which forbids this.\\nThe Keras code to build the critic is given in Example 11-9.\\nExample 11-9. Building the MuseGAN critic\\ndef conv(x, f, k, s, p):\\n    x = layers.Conv3D(filters = f\\n                , kernel_size = k\\n                , padding = p\\n                , strides = s\\n                , kernel_initializer = initializer\\n                )(x)\\n    x = layers.LeakyReLU()(x)\\n    return x\\ndef Critic():\\n    critic_input = layers.Input(\\n        shape=(N_BARS, N_STEPS_PER_BAR, N_PITCHES, N_TRACKS),\\n        name='critic_input'\\n    ) \\n    x = critic_input\\n    x = conv(x, f=128, k = (2,1,1), s = (1,1,1), p = 'valid') \\n    x = conv(x, f=128, k = (N_BARS - 1,1,1), s = (1,1,1), p = 'valid')\\n    x = conv(x, f=128, k = (1,1,12), s = (1,1,12), p = 'same') \\n    x = conv(x, f=128, k = (1,1,7), s = (1,1,7), p = 'same')\\n    x = conv(x, f=128, k = (1,2,1), s = (1,2,1), p = 'same') \\n    x = conv(x, f=128, k = (1,2,1), s = (1,2,1), p = 'same')\\n326 \\n| \\nChapter 11: Music Generation\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 354}, page_content=\"x = conv(x, f=256, k = (1,4,1), s = (1,2,1), p = 'same')\\n    x = conv(x, f=512, k = (1,3,1), s = (1,2,1), p = 'same')\\n    x = layers.Flatten()(x)\\n    x = layers.Dense(1024, kernel_initializer = initializer)(x)\\n    x = layers.LeakyReLU()(x)\\n    critic_output = layers.Dense(\\n        1, activation=None, kernel_initializer = initializer\\n    )(x) \\n    return models.Model(critic_input, critic_output)\\ncritic = Critic()\\nThe input to the critic is an array of multitrack, multibar scores, each of shape\\n[N_BARS, N_STEPS_PER_BAR, N_PITCHES, N_TRACKS].\\nFirst, we collapse the tensor along the bar axis. We apply Conv3D layers through‐\\nout the critic as we are working with 4D tensors.\\nNext, we collapse the tensor along the pitch axis.\\nFinally, we collapse the tensor along the timesteps axis.\\nThe output is a Dense layer with a single unit and no activation function, as\\nrequired by the WGAN-GP framework.\\nAnalysis of the MuseGAN\\nWe can perform some experiments with our MuseGAN by generating a score, then\\ntweaking some of the input noise parameters to see the effect on the output.\\nThe output from the generator is an array of values in the range [–1, 1] (due to the\\ntanh activation function of the final layer). To convert this to a single note for each\\ntrack, we choose the note with the maximum value over all 84 pitches for each time‐\\nstep. In the original MuseGAN paper the authors use a threshold of 0, as each track\\ncan contain multiple notes; however, in this setting we can simply take the maximum\\nto guarantee exactly one note per timestep per track, as is the case for the Bach\\nchorales.\\nFigure 11-16 shows a score that has been generated by the model from random nor‐\\nmally distributed noise vectors (top left). We can find the closest score in the dataset\\n(by Euclidean distance) and check that our generated score isn’t a copy of a piece of\\nmusic that already exists in the dataset—the closest score is shown just below it, and\\nwe can see that it does not resemble our generated score.\\nMuseGAN \\n| \\n327\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 355}, page_content='Figure 11-16. Example of a MuseGAN predicted score, showing the closest real score in\\nthe training data and how the generated score is affected by changing the input noise\\nLet’s now play around with the input noise to tweak our generated score. First, we can\\ntry changing the chord noise vector—the bottom-left score in Figure 11-16 shows the\\nresult. We can see that every track has changed, as expected, and also that the two\\nbars exhibit different properties. In the second bar, the baseline is more dynamic and\\nthe top line is higher in pitch than in the first bar. This is because the latent vectors\\nthat affect the two bars are different, as the input chord vector was passed through a\\ntemporal network.\\nWhen we change the style vector (top right), both bars change in a similar way. The\\nwhole passage has changed style from the original generated score, in a consistent\\nway (i.e., the same latent vector is being used to adjust all tracks and bars).\\nWe can also alter tracks individually, through the melody and groove inputs. In the\\ncenter-right score in Figure 11-16 we can see the effect of changing just the melody\\nnoise input for the top line of music. All other parts remain unaffected, but the top-\\nline notes change significantly. Also, we can see a rhythmic change between the two\\n328 \\n| \\nChapter 11: Music Generation'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 356}, page_content='bars in the top line: the second bar is more dynamic, containing faster notes than the\\nfirst bar.\\nLastly, the bottom-right score in the diagram shows the predicted score when we alter\\nthe groove input parameter for only the baseline. Again, all other parts remain unaf‐\\nfected, but the baseline is different. Moreover, the overall pattern of the baseline\\nremains similar between bars, as we would expect.\\nThis shows how each of the input parameters can be used to directly influence high-\\nlevel features of the generated musical sequence, in much the same way as we were\\nable to adjust the latent vectors of VAEs and GANs in previous chapters to alter the\\nappearance of a generated image. One drawback to the model is that the number of\\nbars to generate must be specified up front. To tackle this, the authors show an exten‐\\nsion to the model that allows previous bars to be fed in as input, allowing the model\\nto generate long-form scores by continually feeding the most recent predicted bars\\nback in as additional input.\\nSummary\\nIn this chapter we have explored two different kinds of models for music generation:\\na Transformer and a MuseGAN.\\nThe Transformer is similar in design to the networks we saw in Chapter 9 for text\\ngeneration. Music and text generation share a lot of features in common, and often\\nsimilar techniques can be used for both. We extended the Transformer architecture\\nby incorporating two input and output streams, for note and duration. We saw how\\nthe model was able to learn about concepts such as keys and scales, simply by learn‐\\ning to accurately generate the music of Bach.\\nWe also explored how we can adapt the tokenization process to handle polyphonic\\n(multitrack) music generation. Grid tokenization serializes a piano roll representation\\nof the score, allowing us to train a Transformer on a single stream of tokens that\\ndescribe which note is present in each voice, at discrete, equally spaced timestep\\nintervals. Event-based tokenization produces a recipe that describes how to create the\\nmultiple lines of music in a sequential fashion, through a single stream of instruc‐\\ntions. Both methods have advantages and disadvantages—the success or failure of a\\nTransformer-based approach to music generation is often heavily dependent on the\\nchoice of tokenization method.\\nWe also saw that generating music does not always require a sequential approach—\\nMuseGAN uses convolutions to generate polyphonic musical scores with multiple\\ntracks, by treating the score as an image where the tracks are individual channels of\\nthe image. The novelty of MuseGAN lies in the way the four input noise vectors\\n(chords, style, melody, and groove) are organized so that it is possible to maintain full\\ncontrol over high-level features of the music. While the underlying harmonization is\\nSummary \\n| \\n329'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 357}, page_content='still not as perfect or varied as Bach’s, it is a good attempt at what is an extremely diffi‐\\ncult problem to master and highlights the power of GANs to tackle a wide variety of\\nproblems.\\nReferences\\n1. Cheng-Zhi Anna Huang et al., “Music Transformer: Generating Music with Long-\\nTerm Structure,” September 12, 2018, https://arxiv.org/abs/1809.04281.\\n2. Hao-Wen Dong et al., “MuseGAN: Multi-Track Sequential Generative Adversarial\\nNetworks for Symbolic Music Generation and Accompaniment,” September 19, 2017,\\nhttps://arxiv.org/abs/1709.06298.\\n330 \\n| \\nChapter 11: Music Generation'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 358}, page_content='CHAPTER 12\\nWorld Models\\nChapter Goals\\nIn this chapter you will:\\n• Walk through the basics of reinforcement learning (RL).\\n• Understand how generative modeling can be used within a world model approach\\nto RL.\\n• See how to train a variational autoencoder (VAE) to capture environment obser‐\\nvations in a low-dimensional latent space.\\n• Walk through the training process of a mixture density network–recurrent neu‐\\nral network (MDN-RNN) that predicts the latent variable.\\n• Use the covariance matrix adaptation evolution strategy (CMA-ES) to train a\\ncontroller that can take intelligent actions in the environment.\\n• Understand how the trained MDN-RNN can itself be used as an environment,\\nallowing the agent to train the controller within its own hallucinated dreams,\\nrather than the real environment.\\nThis chapter introduces one of the most interesting applications of generative models\\nin recent years, namely their use within so-called world models.\\nIntroduction\\nIn March 2018, David Ha and Jürgen Schmidhuber published their “World Models”\\npaper.1 The paper showed how it is possible to train a model that can learn how to\\nperform a particular task through experimentation within its own generated dream\\nenvironment, rather than inside the real environment. It is an excellent example of\\n331'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 359}, page_content='how generative modeling can be used to solve practical problems, when applied\\nalongside other machine learning techniques such as reinforcement learning.\\nA key component of the architecture is a generative model that can construct a prob‐\\nability distribution for the next possible state, given the current state and action. Hav‐\\ning built up an understanding of the underlying physics of the environment through\\nrandom movements, the model is then able to train itself from scratch on a new task,\\nentirely within its own internal representation of the environment. This approach led\\nto world-best scores for both of the tasks on which it was tested.\\nIn this chapter we will explore the model from the paper in detail, with particular\\nfocus on a task that requires the agent to learn how to drive a car around a virtual\\nracetrack as fast as possible. While we will be using a 2D computer simulation as our\\nenvironment, the same technique could also be applied to real-world scenarios where\\ntesting strategies in the live environment is expensive or infeasible.\\nIn this chapter we will reference the excellent TensorFlow imple‐\\nmentation of the “World Models” paper available publicly on\\nGitHub, which I encourage you to clone and run yourself!\\nBefore we start exploring the model, we need to take a closer look at the concept of\\nreinforcement learning.\\nReinforcement Learning\\nReinforcement learning can be defined as follows:\\nReinforcement learning (RL) is a field of machine learning that aims to train an agent\\nto perform optimally within a given environment, with respect to a particular goal.\\nWhile both discriminative modeling and generative modeling aim to minimize a loss\\nfunction over a dataset of observations, reinforcement learning aims to maximize the\\nlong-term reward of an agent in a given environment. It is often described as one of\\nthe three major branches of machine learning, alongside supervised learning (predict‐\\ning using labeled data) and unsupervised learning (learning structure from unlabeled\\ndata).\\nLet’s first introduce some key terminology related to reinforcement learning:\\nEnvironment\\nThe world in which the agent operates. It defines the set of rules that govern the\\ngame state update process and reward allocation, given the agent’s previous\\naction and current game state. For example, if we were teaching a reinforcement\\nlearning algorithm to play chess, the environment would consist of the rules that\\n332 \\n| \\nChapter 12: World Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 360}, page_content='govern how a given action (e.g., the pawn move e2e4) affects the next game state\\n(the new positions of the pieces on the board) and would also specify how to\\nassess if a given position is checkmate and allocate the winning player a reward of\\n1 after the winning move.\\nAgent\\nThe entity that takes actions in the environment.\\nGame state\\nThe data that represents a particular situation that the agent may encounter (also\\njust called a state). For example, a particular chessboard configuration with\\naccompanying game information such as which player will make the next move.\\nAction\\nA feasible move that an agent can make.\\nReward\\nThe value given back to the agent by the environment after an action has been\\ntaken. The agent aims to maximize the long-term sum of its rewards. For exam‐\\nple, in a game of chess, checkmating the opponent’s king has a reward of 1 and\\nevery other move has a reward of 0. Other games have rewards constantly awar‐\\nded throughout the episode (e.g., points in a game of Space Invaders).\\nEpisode\\nOne run of an agent in the environment; this is also called a rollout.\\nTimestep\\nFor a discrete event environment, all states, actions, and rewards are subscripted\\nto show their value at timestep t.\\nThe relationship between these concepts is shown in Figure 12-1.\\nFigure 12-1. Reinforcement learning diagram\\nThe environment is first initialized with a current game state, s0. At timestep t, the\\nagent receives the current game state st and uses this to decide on its next best action\\nat, which it then performs. Given this action, the environment then calculates the\\nnext state st + 1 and reward rt + 1 and passes these back to the agent, for the cycle to\\nReinforcement Learning \\n| \\n333'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 361}, page_content='begin again. The cycle continues until the end criterion of the episode is met (e.g., a\\ngiven number of timesteps elapse or the agent wins/loses).\\nHow can we design an agent to maximize the sum of rewards in a given environ‐\\nment? We could build an agent that contains a set of rules for how to respond to any\\ngiven game state. However, this quickly becomes infeasible as the environment\\nbecomes more complex and doesn’t ever allow us to build an agent that has\\nsuperhuman ability in a particular task, as we are hardcoding the rules. Reinforce‐\\nment learning involves creating an agent that can learn optimal strategies by itself in\\ncomplex environments through repeated play.\\nLet’s now take a look at the CarRacing environment that simulates a car driving\\naround a track.\\nThe CarRacing Environment\\nCarRacing is an environment that is available through the Gymnasium package.\\nGymnasium is a Python library for developing reinforcement learning algorithms\\nthat contains several classic reinforcement learning environments, such as CartPole\\nand Pong, as well as environments that present more complex challenges, such as\\ntraining an agent to walk on uneven terrain or win an Atari game.\\nGymnasium\\nGymnasium is a maintained fork of OpenAI’s Gym library—since\\n2021, further development of Gym has shifted to Gymnasium. In\\nthis book, we therefore refer to Gymnasium environments as Gym\\nenvironments.\\nAll of the environments provide a step method through which you can submit a given\\naction; the environment will return the next state and the reward. By repeatedly\\ncalling the step method with the actions chosen by the agent, you can play out an epi‐\\nsode in the environment. There is also a reset method for returning the environment\\nto its initial state and a render method that allows you to watch your agent perform in\\na given environment. This is useful for debugging and finding areas where your agent\\ncould improve.\\n334 \\n| \\nChapter 12: World Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 362}, page_content='Let’s see how the game state, action, reward, and episode are defined for the\\nCarRacing environment:\\nGame state\\nA 64 × 64–pixel RGB image depicting an overhead view of the track and car.\\nAction\\nA set of three values: the steering direction (–1 to 1), acceleration (0 to 1), and\\nbraking (0 to 1). The agent must set all three values at each timestep.\\nReward\\nA negative penalty of –0.1 for each timestep taken and a positive reward of 1,000/\\nN if a new track tile is visited, where N is the total number of tiles that make up\\nthe track.\\nEpisode\\nThe episode ends when the car completes the track or drives off the edge of the\\nenvironment, or when 3,000 timesteps have elapsed.\\nThese concepts are shown on a graphical representation of a game state in\\nFigure 12-2.\\nFigure 12-2. A graphical representation of one game state in the CarRacing environment\\nPerspective\\nWe should imagine the agent floating above the track and control‐\\nling the car from a bird’s-eye view, rather than viewing the track\\nfrom the driver’s perspective.\\nReinforcement Learning \\n| \\n335'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 363}, page_content='World Model Overview\\nWe’ll now cover a high-level overview of the entire world model architecture and\\ntraining process, before diving into each component in more detail.\\nArchitecture\\nThe solution consists of three distinct parts, as shown in Figure 12-3, that are trained\\nseparately:\\nV\\nA variational autoencoder (VAE)\\nM\\nA recurrent neural network with a mixture density network (MDN-RNN)\\nC\\nA controller\\nFigure 12-3. World model architecture diagram\\nThe VAE\\nWhen you make decisions while driving, you don’t actively analyze every single pixel\\nin your view—instead, you condense the visual information into a smaller number of\\nlatent entities, such as the straightness of the road, upcoming bends, and your posi‐\\ntion relative to the road, to inform your next action.\\nWe saw in Chapter 3 how a VAE can take a high-dimensional input image and con‐\\ndense it into a latent random variable that approximately follows a standard Gaussian\\n336 \\n| \\nChapter 12: World Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 364}, page_content='distribution, through minimization of the reconstruction error and KL divergence.\\nThis ensures that the latent space is continuous and that we are able to easily sample\\nfrom it to generate meaningful new observations.\\nIn the car racing example, the VAE condenses the 64 × 64 × 3 (RGB) input image into\\na 32-dimensional normally distributed random variable, parameterized by two vari‐\\nables, mu and logvar. Here, logvar is the logarithm of the variance of the distribu‐\\ntion. We can sample from this distribution to produce a latent vector z that represents\\nthe current state. This is passed on to the next part of the network, the MDN-RNN.\\nThe MDN-RNN\\nAs you drive, each subsequent observation isn’t a complete surprise to you. If the cur‐\\nrent observation suggests a left turn in the road ahead and you turn the wheel to the\\nleft, you expect the next observation to show that you are still in line with the road.\\nIf you didn’t have this ability, your car would probably snake all over the road as you\\nwouldn’t be able to see that a slight deviation from the center is going to be worse in\\nthe next timestep unless you do something about it now.\\nThis forward thinking is the job of the MDN-RNN, a network that tries to predict the\\ndistribution of the next latent state based on the previous latent state and the previous\\naction.\\nSpecifically, the MDN-RNN is an LSTM layer with 256 hidden units followed by a\\nmixture density network (MDN) output layer that allows for the fact that the next\\nlatent state could actually be drawn from any one of several normal distributions.\\nThe same technique was applied by one of the authors of the “World Models” paper,\\nDavid Ha, to a handwriting generation task, as shown in Figure 12-4, to describe the\\nfact that the next pen point could land in any one of the distinct red areas.\\nFigure 12-4. MDN for handwriting generation\\nIn the car racing example, we allow for each element of the next observed latent state\\nto be drawn from any one of five normal distributions.\\nWorld Model Overview \\n| \\n337'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 365}, page_content='The controller\\nUntil this point, we haven’t mentioned anything about choosing an action. That\\nresponsibility lies with the controller. The controller is a densely connected neural\\nnetwork, where the input is a concatenation of z (the current latent state sampled\\nfrom the distribution encoded by the VAE) and the hidden state of the RNN. The\\nthree output neurons correspond to the three actions (turn, accelerate, brake) and are\\nscaled to fall in the appropriate ranges.\\nThe controller is trained using reinforcement learning as there is no training dataset\\nthat will tell us that a certain action is good and another is bad. Instead, the agent dis‐\\ncovers this for itself through repeated experimentation.\\nAs we shall see later in the chapter, the crux of the “World Models” paper is that it\\ndemonstrates how this reinforcement learning can take place within the agent’s own\\ngenerative model of the environment, rather than the Gym environment. In other\\nwords, it takes place in the agent’s hallucinated version of how the environment\\nbehaves, rather than the real thing.\\nTo understand the different roles of the three components and how they work\\ntogether, we can imagine a dialogue between them:\\nVAE (looking at latest 64 × 64 × 3 observation): This looks like a straight road, with a\\nslight left bend approaching, with the car facing in the direction of the road (z).\\nRNN: Based on that description (z) and the fact that the controller chose to accelerate\\nhard at the last timestep (action), I will update my hidden state (h) so that the next\\nobservation is predicted to still be a straight road, but with slightly more left turn in\\nview.\\nController: Based on the description from the VAE (z) and the current hidden state\\nfrom the RNN (h), my neural network outputs [0.34, 0.8, 0] as the next action.\\nThe action from the controller is then passed to the environment, which returns an\\nupdated observation, and the cycle begins again.\\nTraining\\nThe training process consists of five steps, run in sequence, which are outlined here:\\n1. Collect random rollout data. Here, the agent does not care about the given task,\\nbut instead simply explores the environment using random actions. Multiple epi‐\\nsodes are simulated and the observed states, actions, and rewards at each time‐\\nstep are stored. The idea is to build up a dataset of how the physics of the\\nenvironment works, which the VAE can then learn from to capture the states\\nefficiently as latent vectors. The MDN-RNN can then subsequently learn how the\\nlatent vectors evolve over time.\\n338 \\n| \\nChapter 12: World Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 366}, page_content='2. Train the VAE. Using the randomly collected data, we train a VAE on the obser‐\\nvation images.\\n3. Collect data to train the MDN-RNN. Once we have a trained VAE, we use it to\\nencode each of the collected observations into mu and logvar vectors, which are\\nsaved alongside the current action and reward.\\n4. Train the MDN-RNN. We take batches of episodes and load the corresponding\\nmu, logvar, action, and reward variables at each timestep that were generated in\\nstep 3. We then sample a z vector from the mu and logvar vectors. Given the cur‐\\nrent z vector, action, and reward, the MDN-RNN is then trained to predict the\\nsubsequent z vector and reward.\\n5. Train the controller. With a trained VAE and RNN, we can now train the control‐\\nler to output an action given the current z and hidden state, h, of the RNN. The\\ncontroller uses an evolutionary algorithm, CMA-ES, as its optimizer. The algo‐\\nrithm rewards matrix weightings that generate actions that lead to overall high\\nscores on the task, so that future generations are also likely to inherit this desired\\nbehavior.\\nLet’s now take look at each of these steps in more detail.\\nCollecting Random Rollout Data\\nThe first step is to collect rollout data from the environment, using an agent taking\\nrandom actions. This may seem strange, given we ultimately want our agent to learn\\nhow to take intelligent actions, but this step will provide the data that the agent will\\nuse to learn how the world operates and how its actions (albeit random at first) influ‐\\nence subsequent observations.\\nWe can capture multiple episodes in parallel by spinning up multiple Python pro‐\\ncesses, each running a separate instance of the environment. Each process will run on\\na separate core, so if your machine has lots of cores you can collect data much faster\\nthan if you only have a few cores.\\nThe hyperparameters used by this step are as follows:\\nparallel_processes\\nThe number of parallel processes to run (e.g., 8 if your machine has ≥8 cores)\\nmax_trials\\nHow many episodes each process should run in total (e.g., 125, so 8 processes\\nwould create 1,000 episodes overall)\\nmax_frames\\nThe maximum number of timesteps per episode (e.g., 300)\\nCollecting Random Rollout Data \\n| \\n339'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 367}, page_content='Figure 12-5 shows an excerpt from frames 40 to 59 of one episode, as the car\\napproaches a corner, alongside the randomly chosen action and reward. Note how\\nthe reward changes to 3.22 as the car rolls over new track tiles but is otherwise –0.1.\\nFigure 12-5. Frames 40 to 59 of one episode\\nTraining the VAE\\nWe now build a generative model (a VAE) on this collected data. Remember, the aim\\nof the VAE is to allow us to collapse one 64 × 64 × 3 image into a normally distributed\\nrandom variable z, whose distribution is parameterized by two vectors, mu and\\n340 \\n| \\nChapter 12: World Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 368}, page_content='logvar. Each of these vectors is of length 32. The hyperparameters of this step are as\\nfollows:\\nvae_batch_size\\nThe batch size to use when training the VAE (how many observations per batch)\\n(e.g., 100)\\nz_size\\nThe length of latent z vector (and therefore mu and logvar variables) (e.g., 32)\\nvae_num_epoch\\nThe number of training epochs (e.g., 10)\\nThe VAE Architecture\\nAs we have seen previously, Keras allows us to not only define the VAE model that\\nwill be trained end-to-end, but also additional submodels that define the encoder and\\ndecoder of the trained network separately. These will be useful when we want to\\nencode a specific image or decode a given z vector, for example. We’ll define the VAE\\nmodel and three submodels, as follows:\\nvae\\nThis is the end-to-end VAE that is trained. It accepts a 64 × 64 × 3 image as input\\nand outputs a reconstructed 64 × 64 × 3 image.\\nencode_mu_logvar\\nThis accepts a 64 × 64 × 3 image as input and outputs the mu and logvar vectors\\ncorresponding to this input. Running the same input image through this model\\nmultiple times will produce the same mu and logvar vectors each time.\\nencode\\nThis accepts a 64 × 64 × 3 image as input and outputs a sampled z vector. Run‐\\nning the same input image through this model multiple times will produce a dif‐\\nferent z vector each time, using the calculated mu and logvar values to define the\\nsampling distribution.\\ndecode\\nThis accepts a z vector as input and returns the reconstructed 64 × 64 × 3 image.\\nA diagram of the model and submodels is shown in Figure 12-6.\\nTraining the VAE \\n| \\n341'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 369}, page_content='Figure 12-6. The VAE architecture from the “World Models” paper\\n342 \\n| \\nChapter 12: World Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 370}, page_content='Exploring the VAE\\nWe’ll now take a look at the output from the VAE and each submodel and then see\\nhow the VAE can be used to generate completely new track observations.\\nThe VAE model\\nIf we feed the VAE with an observation, it is able to accurately reconstruct the origi‐\\nnal image, as shown in Figure 12-7. This is useful to visually check that the VAE is\\nworking correctly.\\nFigure 12-7. The input and output from the VAE model\\nThe encoder models\\nIf we feed the encode_mu_logvar model with an observation, the output is the gener‐\\nated mu and logvar vectors describing a multivariate normal distribution. The encode\\nmodel goes one step further by sampling a particular z vector from this distribution.\\nThe diagram showing the output from the two encoder models is shown in\\nFigure 12-8.\\nFigure 12-8. The output from the encoder models\\nTraining the VAE \\n| \\n343'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 371}, page_content='The latent variable z is sampled from the Gaussian defined by mu and logvar by sam‐\\npling from a standard Gaussian and then scaling and shifting the sampled vector\\n(Example 12-1).\\nExample 12-1. Sampling z from the multivariate normal distribution defined by mu and\\nlogvar\\neps = tf.random_normal(shape=tf.shape(mu))\\nsigma = tf.exp(logvar * 0.5)\\nz = mu + eps * sigma\\nThe decoder model\\nThe decode model accepts a z vector as input and reconstructs the original image. In\\nFigure 12-9 we linearly interpolate two of the dimensions of z to show how each\\ndimension appears to encode a particular aspect of the track—in this example z[4]\\ncontrols the immediate left/right direction of the track nearest the car and z[7] con‐\\ntrols the sharpness of the approaching left turn.\\nThis shows that the latent space that the VAE has learned is continuous and can be\\nused to generate new track segments that have never before been observed by the\\nagent.\\n344 \\n| \\nChapter 12: World Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 372}, page_content='Figure 12-9. A linear interpolation of two dimensions of z\\nTraining the VAE \\n| \\n345'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 373}, page_content='Collecting Data to Train the MDN-RNN\\nNow that we have a trained VAE, we can use this to generate training data for our\\nMDN-RNN.\\nIn this step, we pass all of the random rollout observations through the\\nencode_mu_logvar model and store the mu and logvar vectors corresponding to each\\nobservation. This encoded data, along with the already collected action, reward, and\\ndone variables, will be used to train the MDN-RNN. This process is shown in\\nFigure 12-10.\\nFigure 12-10. Creating the MDN-RNN training dataset\\nTraining the MDN-RNN\\nWe can now train the MDN-RNN to predict the distribution of the next z vector and\\nreward one timestep ahead into the future, given the current z vector, current action,\\nand previous reward. We can then use the internal hidden state of the RNN (which\\ncan be thought of as the model’s current understanding of the environment dynam‐\\nics) as part of the input into the controller, which will ultimately decide on the best\\nnext action to take.\\nThe hyperparameters of this step of the process are as follows:\\nrnn_batch_size\\nThe batch size to use when training the MDN-RNN (how many sequences per\\nbatch) (e.g., 100)\\nrnn_num_steps\\nThe total number of training iterations (e.g., 4000)\\n346 \\n| \\nChapter 12: World Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 374}, page_content='The MDN-RNN Architecture\\nThe architecture of the MDN-RNN is shown in Figure 12-11.\\nFigure 12-11. The MDN-RNN architecture\\nThe MDN-RNN consists of an LSTM layer (the RNN), followed by a densely connec‐\\nted layer (the MDN) that transforms the hidden state of the LSTM into the parame‐\\nters of a mixture distribution. Let’s walk through the network step by step.\\nThe input to the LSTM layer is a vector of length 36—a concatenation of the encoded\\nz vector (length 32) from the VAE, the current action (length 3), and the previous\\nreward (length 1).\\nThe output from the LSTM layer is a vector of length 256—one value for each LSTM\\ncell in the layer. This is passed to the MDN, which is just a densely connected layer\\nthat transforms the vector of length 256 into a vector of length 481.\\nWhy 481? Figure 12-12 explains the composition of the output from the MDN-RNN.\\nThe aim of a mixture density network is to model the fact that our next z could be\\ndrawn from one of several possible distributions with a certain probability. In the car\\nracing example, we choose five normal distributions. How many parameters do we\\nneed to define these distributions? For each of the 5 mixtures, we need a mu and a\\nlogvar (to define the distribution) and a log-probability of this mixture being chosen\\n(logpi), for each of the 32 dimensions of z. This makes 5 × 3 × 32 = 480 parameters.\\nThe one extra parameter is for the reward prediction.\\nFigure 12-12. The output from the mixture density network\\nTraining the MDN-RNN \\n| \\n347'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 375}, page_content='Sampling from the MDN-RNN\\nWe can sample from the MDN output to generate a prediction for the next z and\\nreward at the following timestep, through the following process:\\n1. Split the 481-dimensional output vector into the 3 variables (logpi, mu, logvar)\\nand the reward value.\\n2. Exponentiate and scale logpi so that it can be interpreted as 32 probability distri‐\\nbutions over the 5 mixture indices.\\n3. For each of the 32 dimensions of z, sample from the distributions created from\\nlogpi (i.e., choose which of the 5 distributions should be used for each dimen‐\\nsion of z).\\n4. Fetch the corresponding values of mu and logvar for this distribution.\\n5. Sample a value for each dimension of z from the normal distribution parameter‐\\nized by the chosen parameters of mu and logvar for this dimension.\\nThe loss function for the MDN-RNN is the sum of the z vector reconstruction loss\\nand the reward loss. The z vector reconstruction loss is the negative log-likelihood of\\nthe distribution predicted by the MDN-RNN, given the true value of z, and the\\nreward loss is the mean squared error between the predicted reward and the true\\nreward.\\nTraining the Controller\\nThe final step is to train the controller (the network that outputs the chosen action)\\nusing an evolutionary algorithm called the covariance matrix adaptation evolution\\nstrategy (CMA-ES).\\nThe hyperparameters of this step of the process are as follows:\\ncontroller_num_worker\\nThe number of workers that will test solutions in parallel\\ncontroller_num_worker_trial\\nThe number of solutions that each worker will be given to test at each generation\\ncontroller_num_episode\\nThe number of episodes that each solution will be tested against to calculate the\\naverage reward\\ncontroller_eval_steps\\nThe number of generations between evaluations of the current best parameter set\\n348 \\n| \\nChapter 12: World Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 376}, page_content='The Controller Architecture\\nThe architecture of the controller is very simple. It is a densely connected neural net‐\\nwork with no hidden layers. It connects the input vector directly to the action vector.\\nThe input vector is a concatenation of the current z vector (length 32) and the current\\nhidden state of the LSTM (length 256), giving a vector of length 288. Since we are\\nconnecting each input unit directly to the 3 output action units, the total number of\\nweights to tune is 288 × 3 = 864, plus 3 bias weights, giving 867 in total.\\nHow should we train this network? Notice that this is not a supervised learning prob‐\\nlem—we are not trying to predict the correct action. There is no training set of cor‐\\nrect actions, as we do not know what the optimal action is for a given state of the\\nenvironment. This is what distinguishes this as a reinforcement learning problem. We\\nneed the agent to discover the optimal values for the weights itself by experimenting\\nwithin the environment and updating its weights based on received feedback.\\nEvolutionary strategies are a popular choice for solving reinforcement learning prob‐\\nlems, due to their simplicity, efficiency, and scalability. We shall use one particular\\nstrategy, known as CMA-ES.\\nCMA-ES\\nEvolutionary strategies generally adhere to the following process:\\n1. Create a population of agents and randomly initialize the parameters to be opti‐\\nmized for each agent.\\n2. Loop over the following:\\na. Evaluate each agent in the environment, returning the average reward over\\nmultiple episodes.\\nb. Breed the agents with the best scores to create new members of the\\npopulation.\\nc. Add randomness to the parameters of the new members.\\nd. Update the population pool by adding the newly created agents and removing\\npoorly performing agents.\\nThis is similar to the process through which animals evolve in nature—hence the\\nname evolutionary strategies. “Breeding” in this context simply means combining the\\nexisting best-scoring agents such that the next generation are more likely to produce\\nhigh-quality results, similar to their parents. As with all reinforcement learning solu‐\\ntions, there is a balance to be found between greedily searching for locally optimal\\nsolutions and exploring unknown areas of the parameter space for potentially better\\nTraining the Controller \\n| \\n349'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 377}, page_content='solutions. This is why it is important to add randomness to the population, to ensure\\nwe are not too narrow in our search field.\\nCMA-ES is just one form of evolutionary strategy. In short, it works by maintaining a\\nnormal distribution from which it can sample the parameters of new agents. At each\\ngeneration, it updates the mean of the distribution to maximize the likelihood of\\nsampling the high-scoring agents from the previous timestep. At the same time, it\\nupdates the covariance matrix of the distribution to maximize the likelihood of sam‐\\npling the high-scoring agents, given the previous mean. It can be thought of as a form\\nof naturally arising gradient descent, but with the added benefit that it is derivative-\\nfree, meaning that we do not need to calculate or estimate costly gradients.\\nOne generation of the algorithm demonstrated on a toy example is shown in\\nFigure 12-13. Here we are trying to find the minimum point of a highly nonlinear\\nfunction in two dimensions—the value of the function in the red/black areas of the\\nimage is greater than the value of the function in the white/yellow parts of the image.\\nFigure 12-13. One update step from the CMA-ES algorithm (source: Ha, 2017)2\\nThe steps are as follows:\\n1. We start with a randomly generated 2D normal distribution and sample a popu‐\\nlation of candidates, shown in blue in Figure 12-13.\\n2. We then calculate the value of the function for each candidate and isolate the best\\n25%, shown in purple in Figure 12-13—we’ll call this set of points P.\\n3. We set the mean of the new normal distribution to be the mean of the points in P.\\nThis can be thought of as the breeding stage, wherein we only use the best candi‐\\ndates to generate a new mean for the distribution. We also set the covariance\\nmatrix of the new normal distribution to be the covariance matrix of the points\\nin P, but use the existing mean in the covariance calculation rather than the cur‐\\nrent mean of the points in P. The larger the difference between the existing mean\\nand the mean of the points in P, the wider the variance of the next normal distri‐\\nbution. This has the effect of naturally creating momentum in the search for the\\noptimal parameters.\\n350 \\n| \\nChapter 12: World Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 378}, page_content='4. We can then sample a new population of candidates from our new normal distri‐\\nbution with an updated mean and covariance matrix.\\nFigure 12-14 shows several generations of the process. See how the covariance widens\\nas the mean moves in large steps toward the minimum, but narrows as the mean set‐\\ntles into the true minimum.\\nFigure 12-14. CMA-ES (source: Wikipedia)\\nFor the car racing task, we do not have a well-defined function to maximize, but\\ninstead an environment where the 867 parameters to be optimized determine how\\nwell the agent scores. Initially, some sets of parameters will, by random chance, gen‐\\nerate scores that are higher than others and the algorithm will gradually move the\\nnormal distribution in the direction of those parameters that score highest in the\\nenvironment.\\nParallelizing CMA-ES\\nOne of the great benefits of CMA-ES is that it can be easily parallelized. The most\\ntime-consuming part of the algorithm is calculating the score for a given set of\\nparameters, since it needs to simulate an agent with these parameters in the environ‐\\nment. However, this process can be parallelized, since there are no dependencies\\nbetween individual simulations. There is a orchestrator process that sends out param‐\\neter sets to be tested to many node processes in parallel. The nodes return the results\\nto the orchestrator, which accumulates the results and then passes the overall result of\\nthe generation to the CMA-ES object. This object updates the mean and covariance\\nTraining the Controller \\n| \\n351'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 379}, page_content='matrix of the normal distribution as per Figure 12-13 and provides the orchestrator\\nwith a new population to test. The loop then starts again. Figure 12-15 explains this\\nin a diagram.\\nFigure 12-15. Parallelizing CMA-ES—here there is a population size of eight and four\\nnodes (so t = 2, the number of trials that each node is responsible for)\\nThe orchestrator asks the CMA-ES object (es) for a set of parameters to trial.\\nThe orchestrator divides the parameters into the number of nodes available.\\nHere, each of the four node processes gets two parameter sets to trial.\\nThe nodes run a worker process that loops over each set of parameters and runs\\nseveral episodes for each. Here we run three episodes for each set of parameters.\\nThe rewards from each episode are averaged to give a single score for each set of\\nparameters.\\nEach node returns its list of scores to the orchestrator.\\n352 \\n| \\nChapter 12: World Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 380}, page_content='The orchestrator groups all the scores together and sends this list to the es object.\\nThe es object uses this list of rewards to calculate the new normal distribution as\\nper Figure 12-13.\\nAfter around 200 generations, the training process achieves an average reward score\\nof around 840 for the car racing task, as shown in Figure 12-16.\\nFigure 12-16. Average episode reward of the controller training process, by generation\\n(source: Zac Wellmer, “World Models”)\\nIn-Dream Training\\nSo far, the controller training has been conducted using the Gym CarRacing environ‐\\nment to implement the step method that moves the simulation from one state to the\\nnext. This function calculates the next state and reward, given the current state of the\\nenvironment and chosen action.\\nNotice how the step method performs a very similar function to the MDN-RNN in\\nour model. Sampling from the MDN-RNN outputs a prediction for the next z and\\nreward, given the current z and chosen action.\\nIn fact, the MDN-RNN can be thought of as an environment in its own right, but\\noperating in z-space rather than in the original image space. Incredibly, this means\\nthat we can actually substitute the real environment with a copy of the MDN-RNN\\nand train the controller entirely within an MDN-RNN-inspired dream of how the\\nenvironment should behave.\\nIn other words, the MDN-RNN has learned enough about the general physics of\\nthe real environment from the original random movement dataset that it can be used\\nas a proxy for the real environment when training the controller. This is quite\\nIn-Dream Training \\n| \\n353'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 381}, page_content='remarkable—it means that the agent can train itself to learn a new task by thinking\\nabout how it can maximize reward in its dream environment, without ever having to\\ntest out strategies in the real world. It can then perform well at the task the first time,\\nhaving never attempted the task in reality.\\nA comparison of the architectures for training in the real environment and the dream\\nenvironment follows: the real-world architecture is shown in Figure 12-17 and the in-\\ndream training setup is illustrated in Figure 12-18.\\nFigure 12-17. Training the controller in the Gym environment\\nNotice how in the dream architecture, the training of the controller is performed\\nentirely in z-space without the need to ever decode the z vectors back into recogniza‐\\nble track images. We can of course do so, in order to visually inspect the performance\\nof the agent, but it is not required for training.\\n354 \\n| \\nChapter 12: World Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 382}, page_content='Figure 12-18. Training the controller in the MDN-RNN dream environment\\nOne of the challenges of training agents entirely within the MDN-RNN dream envi‐\\nronment is overfitting. This occurs when the agent finds a strategy that is rewarding\\nin the dream environment but does not generalize well to the real environment, due\\nto the MDN-RNN not fully capturing how the true environment behaves under cer‐\\ntain conditions.\\nThe authors of the original paper highlight this challenge and show how including a\\ntemperature parameter to control model uncertainty can help alleviate the problem.\\nIncreasing this parameter magnifies the variance when sampling z through the\\nMDN-RNN, leading to more volatile rollouts when training in the dream environ‐\\nment. The controller receives higher rewards for safer strategies that encounter well-\\nunderstood states and therefore tend to generalize better to the real environment.\\nIncreased temperature, however, needs to be balanced against not making the envi‐\\nronment so volatile that the controller cannot learn any strategy, as there is not\\nenough consistency in how the dream environment evolves over time.\\nIn the original paper, the authors show this technique successfully applied to a differ‐\\nent environment: DoomTakeCover, based around the computer game Doom.\\nIn-Dream Training \\n| \\n355'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 383}, page_content='Figure 12-19 shows how changing the temperature parameter affects both the virtual\\n(dream) score and the actual score in the real environment.\\nFigure 12-19. Using temperature to control dream environment volatility (source: Ha\\nand Schmidhuber, 2018)\\nThe optimal temperature setting of 1.15 achieves a score of 1,092 in the real environ‐\\nment, surpassing the current Gym leader at the time of publication. This is an amaz‐\\ning achievement—remember, the controller has never attempted the task in the real\\nenvironment. It has only ever taken random steps in the real environment (to train\\nthe VAE and MDN-RNN dream model) and then used the dream environment to\\ntrain the controller.\\nA key benefit of using generative world models as an approach to reinforcement\\nlearning is that each generation of training in the dream environment is much faster\\nthan training in the real environment. This is because the z and reward prediction by\\nthe MDN-RNN is faster than the z and reward calculation by the Gym environment.\\nSummary\\nIn this chapter we have seen how a generative model (a VAE) can be utilized within a\\nreinforcement learning setting to enable an agent to learn an effective strategy by test‐\\ning policies within its own generated dreams, rather than within the real\\nenvironment.\\nThe VAE is trained to learn a latent representation of the environment, which is then\\nused as input to a recurrent neural network that forecasts future trajectories within\\nthe latent space. Amazingly, the agent can then use this generative model as a pseudo-\\nenvironment to iteratively test policies, using an evolutionary methodology, that gen‐\\neralize well to the real environment.\\nFor further information on the model, there is an excellent interactive explanation\\navailable online, written by the authors of the original paper.\\n356 \\n| \\nChapter 12: World Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 384}, page_content='References\\n1. David Ha and Jürgen Schmidhuber, “World Models,” March 27, 2018, https://\\narxiv.org/abs/1803.10122.\\n2. David Ha, “A Visual Guide to Evolution Strategies,” October 29, 2017, https://\\nblog.otoro.net/2017/10/29/visual-evolution-strategies.\\nSummary \\n| \\n357'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 385}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 386}, page_content='CHAPTER 13\\nMultimodal Models\\nChapter Goals\\nIn this chapter you will:\\n• Learn what is meant by a multimodal model.\\n• Explore the inner workings of DALL.E 2, a large-scale text-to-image model from\\nOpenAI.\\n• Understand how CLIP and diffusion models such as GLIDE play an integral role\\nin the overall DALL.E 2 architecture.\\n• Analyze the limitations of DALL.E 2, as highlighted by the authors of the paper.\\n• Explore the architecture of Imagen, a large-scale text-to-image model from Goo‐\\ngle Brain.\\n• Learn about the latent diffusion process used by Stable Diffusion, an open source\\ntext-to-image model.\\n• Understand the similarities and differences between DALL.E 2, Imagen, and Sta‐\\nble Diffusion.\\n• Investigate DrawBench, a benchmarking suite for evaluating text-to-image\\nmodels.\\n• Learn the architectural design of Flamingo, a novel visual language model from\\nDeepMind.\\n• Unpick the different components of Flamingo and learn how they each contrib‐\\nute to the model as a whole.\\n• Explore some of the capabilities of Flamingo, including conversational\\nprompting.\\n359'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 387}, page_content='So far, we have analyzed generative learning problems that focus solely on one modal‐\\nity of data: either text, images, or music. We have seen how GANs and diffusion mod‐\\nels can generate state-of-the-art images and how Transformers are pioneering the way\\nfor both text and image generation. However, as humans, we have no difficulties\\ncrossing modalities—for example, writing a description of what is happening in a\\ngiven photograph, creating digital art to depict a fictional fantasy world in a book, or\\nmatching a film score to the emotions of a given scene. Can we train machines to do\\nthe same?\\nIntroduction\\nMultimodal learning involves training generative models to convert between two or\\nmore different kinds of data. Some of the most impressive generative models intro‐\\nduced in the last two years have been multimodal in nature. In this chapter we will\\nexplore how they work in detail and consider how the future of generative modeling\\nwill be shaped by large multimodal models.\\nWe’ll explore four different vision-language models: DALL.E 2 from OpenAI; Imagen\\nfrom Google Brain; Stable Diffusion from Stability AI, CompVis, and Runway; and\\nFlamingo from DeepMind.\\nThe aim of this chapter is to concisely explain how each model\\nworks, without going into the fine detail of every design decision.\\nFor more information, refer to the individual papers for each\\nmodel, which explain all of the design choices and architecture\\ndecisions in detail.\\nText-to-image generation focuses on producing state-of-the-art images from a given\\ntext prompt. For example, given the input “A head of broccoli made out of modeling\\nclay, smiling in the sun,” we would like the model to be able to output a image that\\naccurately matches the text prompt, as shown in Figure 13-1.\\nThis is clearly a highly challenging problem. Text understanding and image genera‐\\ntion are difficult to solve in their own right, as we have seen in previous chapters of\\nthis book. Multimodal modeling such as this presents an additional challenge,\\nbecause the model must also learn how to cross the bridge between the two domains\\nand learn a shared representation that allows it to accurately convert from a block of\\ntext to a high-fidelity image without loss of information.\\n360 \\n| \\nChapter 13: Multimodal Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 388}, page_content='Figure 13-1. An example of text-to-image generation by DALL.E 2\\nMoreover, in order to be successful the model must be able to combine concepts and\\nstyles that it may never have seen before. For example, there are no Michelangelo\\nfrescos containing people wearing virtual reality headsets, but we would like our\\nmodel to be able to create such an image if we ask it to. Equally, it would be desirable\\nfor the model to accurately infer how objects in the generated image relate to each\\nother, based on the text prompt. For example, a picture of “an astronaut riding a\\ndoughnut through space” should look very different from one of “an astronaut eating\\na doughnut in a crowded space.” The model must learn how words are given meaning\\nthrough context and how to convert explicit textual relationships between entities to\\nimages that imply the same meaning.\\nDALL.E 2\\nThe first model we shall explore is DALL.E 2, a model designed by OpenAI for text-\\nto-image generation. The first version of this model, DALL.E,1 was released in Febru‐\\nary 2021 and sparked a new wave of interest in generative multimodal models. In this\\nsection, we shall investigate the workings of the second iteration of the model,\\nDALL.E 2,2 released just over a year later in April 2022.\\nDALL.E 2 is an extremely impressive model that has furthered our understanding of\\nAI’s ability to solve these types of multimodal problems. It not only has ramifications\\nacademically, but also forces us to ask big questions relating to the role of AI in crea‐\\ntive processes that previously were thought to be unique to humans. We will start by\\nexploring how DALL.E 2 works, building on key foundational ideas that we have\\nalready explored earlier in this book.\\nDALL.E 2 \\n| \\n361'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 389}, page_content='Architecture\\nTo understand how DALL.E 2 works, we must first survey its overall architecture, as\\nshown in Figure 13-2.\\nFigure 13-2. The DALL.E 2 architecture\\nThere are three distinct parts to consider: the text encoder, the prior, and the decoder.\\nText is first passed through the text encoder to produce a text embedding vector. This\\nvector is then transformed by the prior to produce an image embedding vector.\\nFinally, this is passed through the decoder, along with the original text, to produce the\\ngenerated image. We will step through each component in turn, to get a complete pic‐\\nture of how DALL.E 2 works in practice.\\nThe Text Encoder\\nThe aim of the text encoder is to convert the text prompt into an embedding vector\\nthat represents the conceptual meaning of the text prompt within a latent space. As\\nwe have seen in previous chapters, converting discrete text to a continuous latent\\nspace vector is essential for all downstream tasks, because we can continue to manip‐\\nulate the vector further depending on our particular goal.\\nIn DALL.E 2, the authors do not train the text encoder from scratch, but instead\\nmake use of an existing model called Contrastive Language–Image Pre-training\\n(CLIP), also produced by OpenAI. Therefore, to understand the text encoder, we\\nmust first understand how CLIP works.\\nCLIP\\nCLIP3 was unveiled in a paper published by OpenAI in February 2021 (just a few days\\nafter the first DALL.E paper) that described it as “a neural network that efficiently\\nlearns visual concepts from natural language supervision.”\\nIt uses a technique called contrastive learning to match images with text descriptions.\\nThe model is trained on a dataset of 400 million text–image pairs scraped from the\\ninternet—some example pairs are shown in Figure 13-3. For comparison, there are 14\\n362 \\n| \\nChapter 13: Multimodal Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 390}, page_content='million hand-annotated images in ImageNet. Given an image and a list of possible\\ntext descriptions, its task is to find the one that actually matches the image.\\nFigure 13-3. Examples of text–image pairs\\nThe key idea behind contrastive learning is simple. We train two neural networks: a\\ntext encoder that converts text to a text embedding and an image encoder that converts\\nan image to an image embedding. Then, given a batch of text–image pairs, we com‐\\npare all text and image embedding combinations using cosine similarity and train the\\nnetworks to maximize the score between matching text–image pairs and minimize\\nthe score between incorrect text–image pairs. This process is shown in Figure 13-4.\\nCLIP Is Not Generative\\nNote that CLIP is not itself a generative model—it cannot produce\\nimages or text. Is it closer to a discriminative model, because the\\nfinal output is a prediction about which text description from a\\ngiven set most closely matches a given image (or the other way\\naround, which image most closely matches a given text\\ndescription).\\nDALL.E 2 \\n| \\n363'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 391}, page_content='Figure 13-4. The CLIP training process\\nBoth the text encoder and the image encoder are Transformers—the image encoder is\\na Vision Transformer (ViT), introduced in “ViT VQ-GAN” on page 292, which\\napplies the same concept of attention to images. The authors tested other model\\narchitectures, but found this combination to produce the best results.\\nWhat makes CLIP especially interesting is the way it can be used for zero-shot predic‐\\ntion on tasks that it has never been exposed to. For example, suppose we want to use\\nCLIP to predict the label of a given image in the ImageNet dataset. We can first con‐\\nvert the ImageNet labels into sentences by using a template (e.g., “a photo of a\\n<label>”), as shown in Figure 13-5.\\n364 \\n| \\nChapter 13: Multimodal Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 392}, page_content='Figure 13-5. Converting labels in a new dataset to captions, in order to produce CLIP\\ntext embeddings\\nTo predict the label of a given image, we can pass it through the CLIP image encoder\\nand calculate the cosine similarity between the image embedding and all possible text\\nembeddings in order to find the label with the maximum score, as shown in\\nFigure 13-6.\\nFigure 13-6. Using CLIP to predict the content of an image\\nNotice that we do not need to retrain either of the CLIP neural networks for it to be\\nreadily applicable to new tasks. It uses language as the common domain through\\nwhich any set of labels can be expressed.\\nDALL.E 2 \\n| \\n365'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 393}, page_content='Using this approach, it is possible to show that CLIP performs well across a wide\\nrange of image dataset labeling challenges (Figure 13-7). Other models that have been\\ntrained on a specific dataset to predict a given set of labels often fail when applied to\\ndifferent datasets with the same labels because they are highly optimized to the indi‐\\nvidual datasets on which they were trained. CLIP is much more robust, as it has\\nlearned a deep conceptual understanding of full text descriptions and images, rather\\nthan just excelling at the narrow task of assigning a single label to a given image in a\\ndataset.\\nFigure 13-7. CLIP performs well on a wide range of image labeling datasets (source:\\nRadford et al., 2021)\\nAs mentioned, CLIP is measured on its discriminative ability, so how does it help us\\nto build generative models such as DALL.E 2?\\n366 \\n| \\nChapter 13: Multimodal Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 394}, page_content='The answer is that we can take the trained text encoder and use it as one part of a\\nlarger model such as DALL.E 2, with frozen weights. The trained encoder is simply a\\ngeneralized model for converting text to a text embedding, which should be useful for\\ndownstream tasks such as generating images. The text encoder is able to capture a\\nrich conceptual understanding of the text, as it has been trained to be as similar as\\npossible to its matching image embedding counterpart, which is produced only from\\nthe paired image. It is therefore the first part of the bridge that we need to be able to\\ncross over from the text domain to the image domain.\\nThe Prior\\nThe next stage of the process involves converting the text embedding into a CLIP\\nimage embedding. The DALL.E 2 authors tried two different methods for training the\\nprior model:\\n• An autoregressive model\\n• A diffusion model\\nThey found that the diffusion approach outperformed the autoregressive model and\\nwas more computationally efficient. In this section, we’ll look at both and see how\\nthey differ.\\nAutoregressive prior\\nAn autoregressive model generates output sequentially, by placing an ordering on the\\noutput tokens (e.g., words, pixels) and conditioning the next token on previous\\ntokens. We have seen in previous chapters how this is used in recurrent neural net‐\\nworks (e.g., LSTMs), Transformers, and PixelCNN.\\nThe autoregressive prior of DALL.E 2 is an encoder-decoder Transformer. It is\\ntrained to reproduce the CLIP image embedding given a CLIP text embedding, as\\nshown in Figure 13-8. Note that there are some additional components to the autore‐\\ngressive model mentioned in the original paper that we omit here for conciseness.\\nDALL.E 2 \\n| \\n367'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 395}, page_content='Figure 13-8. A simplified diagram of the autoregressive prior of DALL.E 2\\nThe model is trained on the CLIP text–image pair dataset. You can think of it as the\\nsecond part of the bridge that we need in order to jump from the text domain to the\\nimage domain: we are converting a vector from the text embedding latent space to\\nthe image embedding latent space.\\nThe input text embedding is processed by the encoder of the Transformer to produce\\nanother representation that is fed to the decoder, alongside the current generated out‐\\nput image embedding. The output is generated one element at a time, using teacher\\nforcing to compare the predicted next element to the actual CLIP image embedding.\\nThe sequential nature of the generation means that the autoregressive model is less\\ncomputationally efficient than the other method tried by the authors, which we’ll look\\nat next.\\nDiffusion prior\\nAs we saw in Chapter 8, diffusion models are fast becoming the go-to choice for gen‐\\nerative modeling practitioners, alongside Transformers. In DALL.E 2 a decoder-only\\nTransformer is used as the prior, trained using a diffusion process.\\n368 \\n| \\nChapter 13: Multimodal Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 396}, page_content='The training and generation process is shown in Figure 13-9. Again, this is a simpli‐\\nfied version; the original paper contains full details of how the diffusion model is\\nstructured.\\nFigure 13-9. A simplified diagram of the diffusion prior training and generation process\\nof DALL.E 2\\nDuring training, each CLIP text and image embedding pair are first concatenated\\ninto a single vector. Then, the image embedding is noised over 1,000 timesteps until it\\nis indistinguishable from random noise. The diffusion prior is then trained to predict\\nthe denoised image embedding at the previous timestep. The prior has access to the\\ntext embedding throughout, so it is able to condition its predictions on this informa‐\\ntion, gradually transforming the random noise into a predicted CLIP image embed‐\\nding. The loss function is the average mean-squared error across denoising steps.\\nTo generate new image embeddings, we sample a random vector, prepend the rele‐\\nvant text embedding, and pass it through the trained diffusion prior multiple times.\\nThe Decoder\\nThe final part of DALL.E 2 is the decoder. This is the part of the model that generates\\nthe final image conditioned on the text prompt and the predicted image embedding\\noutput by the prior.\\nThe architecture and training process of the decoder borrows from an earlier OpenAI\\npaper, published in December 2021, which presented a generative model called Gui‐\\nded Language to Image Diffusion for Generation and Editing (GLIDE).4\\nDALL.E 2 \\n| \\n369'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 397}, page_content='GLIDE is able to generate realistic images from text prompts, in much the same way\\nthat DALL.E 2 can. The difference is that GLIDE does not make use of CLIP embed‐\\ndings, but instead works directly with the raw text prompt, training the entire model\\nfrom scratch, as shown in Figure 13-10.\\nFigure 13-10. A comparison between DALL.E 2 and GLIDE—GLIDE trains the entire\\ngenerative model from scratch, whereas DALL.E 2 makes use of CLIP embeddings to\\ncarry information forward from the initial text prompt\\nLet’s see how GLIDE works first.\\nGLIDE\\nGLIDE is trained as a diffusion model, with U-Net architecture for the denoiser and\\nTransformer architecture for the text encoder. It learns to undo the noise added to an\\nimage, guided by the text prompt. Finally, an Upsampler is trained to scale the gener‐\\nated image to 1,024 × 1,024 pixels.\\nGLIDE trains the 3.5 billion (B) parameter model from scratch—2.3B parameters for\\nthe visual part of the model (U-Net and Upsampler) and 1.2B for the Transformer. It\\nis trained on 250 million text–image pairs.\\n370 \\n| \\nChapter 13: Multimodal Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 398}, page_content='The diffusion process is shown in Figure 13-11. A Transformer is used to create an\\nembedding of the input text prompt, which is then used to guide the U-Net through‐\\nout the denoising process. We explored the U-Net architecture in Chapter 8; it’s a per‐\\nfect model choice when the overall size of the image should stay the same (e.g., for\\nstyle transfer, denoising, etc.).\\nFigure 13-11. The GLIDE diffusion process\\nDALL.E 2 \\n| \\n371'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 399}, page_content='The DALL.E 2 decoder still uses the U-Net denoiser and Transformer text encoder\\narchitectures, but additionally has the predicted CLIP image embeddings to condi‐\\ntion on. This is the key difference between GLIDE and DALL.E 2, as shown in\\nFigure 13-12.\\nFigure 13-12. The DALL.E 2 decoder additionally conditions on the image embedding\\nproduced by the prior\\nAs with all diffusion models, to generate a new image, we simply sample some ran‐\\ndom noise and run this through the U-Net denoiser multiple times, conditioned on\\nthe Transformer text encoding and image embedding. The output is a 64 × 64–pixel\\nimage.\\nUpsampler\\nThe final part of the decoder is the Upsampler (two separate diffusion models). The\\nfirst diffusion model transforms the image from 64 × 64 to 256 × 256 pixels. The sec‐\\nond transforms it again, from 256 × 256 to 1,024 × 1,024 pixels, as shown in\\nFigure 13-13.\\nUpsampling is useful because it means we do not have to build large upstream models\\nto handle high-dimensional images. We can work with small images until the final\\n372 \\n| \\nChapter 13: Multimodal Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 400}, page_content='stages of the process, when we apply the Upsamplers. This saves on model parameters\\nand ensures a more efficient upstream training process.\\nFigure 13-13. The first Upsampler diffusion model converts the image from 64 × 64 pix‐\\nels to 256 × 256 pixels while the second converts from 256 × 256 pixels to 1,024 × 1,024\\npixels\\nThis concludes the DALL.E 2 model explanation! In summary, DALL.E 2 makes use\\nof the pre-trained CLIP model to immediately produce a text embedding of the input\\nprompt. Then it converts this into an image embedding using a diffusion model\\ncalled the prior. Lastly, it implements a GLIDE-style diffusion model to generate the\\noutput image, conditioned on the predicted image embedding and Transformer-\\nencoded input prompt.\\nExamples from DALL.E 2\\nExamples of more images generated by DALL.E 2 can be found on the official web‐\\nsite. The way that the model is able to combine complex, disparate concepts in a real‐\\nistic, believable way is astonishing and represents a significant leap forward for AI\\nand generative modeling.\\nIn the paper, the authors show how the model can be used for additional purposes\\nother than text-to-image generation. One of these applications is creating variations\\nof a given image, which we explore in the following section.\\nDALL.E 2 \\n| \\n373'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 401}, page_content='Image variations\\nAs discussed previously, to generate images using the DALL.E 2 decoder we sample\\nan image consisting of pure random noise and then gradually reduce the amount of\\nnoise using the denoising diffusion model, conditioned on the provided image\\nembedding. Selecting different initial random noise samples will result in different\\nimages.\\nIn order to generate variations of a given image, we therefore just need to establish its\\nimage embedding to feed to the decoder. We can obtain this using the original CLIP\\nimage encoder, which is explicitly designed to convert an image into its CLIP image\\nembedding. This process is shown in Figure 13-14.\\nFigure 13-14. DALL.E 2 can be used for generating variations of a given image\\nImportance of the prior\\nAnother avenue explored by the authors is establishing the importance of the prior.\\nThe purpose of the prior is to provide the decoder with a useful representation of the\\nimage to be generated, making use of the pre-trained CLIP model. However, it is fea‐\\nsible that this step isn’t necessary—perhaps we could just pass the text embedding\\ndirectly to the decoder instead of the image embedding, or ignore the CLIP embed‐\\ndings completely and condition only on the text prompt. Would this impact the qual‐\\nity of the generations?\\nTo test this, the authors tried three different approaches:\\n1. Feed the decoder only with the text prompt (and a zero vector for the image\\nembedding).\\n2. Feed the decoder with the text prompt and the text embedding (as if it were an\\nimage embedding).\\n3. Feed the decoder with the text prompt and the image embedding (i.e., the full\\nmodel).\\nExample results are shown in Figure 13-15. We can see that when the decoder is\\nstarved of image embedding information, it can only produce a rough approximation\\n374 \\n| \\nChapter 13: Multimodal Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 402}, page_content='of the text prompt, missing key information such as the calculator. Using the text\\nembedding as if it were an image embedding performs slightly better, though it is not\\nable to capture the relationship between the hedgehog and the calculator. Only the\\nfull model with the prior produces an image that accurately reflects all of the infor‐\\nmation contained within the prompt.\\nFigure 13-15. The prior provides the model with additional context and helps the\\ndecoder to produce more accurate generations (source: Ramesh et al., 2022)\\nLimitations\\nIn the DALL.E 2 paper, the authors also highlight several known limitations of the\\nmodel. Two of these (attribute binding and text generation) are shown in\\nFigure 13-16.\\nDALL.E 2 \\n| \\n375'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 403}, page_content='Figure 13-16. Two limitations of DALL.E 2 lie in its ability to bind attributes to objects\\nand reproduce textual information—top prompt: “A red cube on top of a blue cube”; bot‐\\ntom prompt: “A sign that says deep learning” (source: Ramesh et al., 2022)\\nAttribute binding is the ability of a model to understand the relationship between\\nwords in a given text prompt, and in particular how attributes relate to objects. For\\nexample, the prompt “A red cube on top of a blue cube” must appear visually distinct\\nfrom “A blue cube on top of a red cube.” DALL.E struggles somewhat with this, com‐\\npared to earlier models such as GLIDE, though the overall quality of generations is\\nbetter and more diverse.\\nAlso, DALL.E 2 is not able to accurately reproduce text—this is probably due to the\\nfact that the CLIP embeddings do not capture spellings, but instead only contain a\\nhigher-level representation of the text. These representations can be decoded into text\\nwith partial success (e.g., individual letters are mostly correct), but not with enough\\ncompositional understanding to form full words.\\n376 \\n| \\nChapter 13: Multimodal Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 404}, page_content='Imagen\\nJust over a month after OpenAI released DALL.E 2, the Google Brain team released\\ntheir own text-to-image model called Imagen.5 Many of the core themes that we have\\nalready explored in this chapter are also relevant to Imagen: for example, it uses a text\\nencoder and a diffusion model decoder.\\nIn the next section, we’ll explore the overall architecture of Imagen and compare it\\nwith DALL.E 2.\\nArchitecture\\nAn overview of the Imagen architecture is shown in Figure 13-17.\\nFigure 13-17. The Imagen architecture (source: Saharia et al., 2022)\\nThe frozen text encoder is the pre-trained T5-XXL model, a large encoder-decoder\\nTransformer. Unlike CLIP, this was trained only on text and not images, so it is not a\\nmultimodal model. However, the authors found that it still functions extremely well\\nas a text encoder for Imagen and that scaling this model has more impact on overall\\nperformance than scaling the diffusion model decoder.\\nImagen \\n| \\n377'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 405}, page_content='Like DALL.E 2’s, Imagen’s the decoding diffusion model is based on a U-Net architec‐\\nture, conditioned on text embeddings. There are several architectural improvements\\nmade to the standard U-Net architecture, to produce what the authors call the Effi‐\\ncient U-Net. This model uses less memory, converges faster, and has better sample\\nquality than previous U-Net models.\\nThe Upsampler super-resolution models that take the generated image from 64 × 64\\nto 1,024 × 1,024 pixels are also diffusion models that continue to use the text embed‐\\ndings to guide the upsampling process.\\nDrawBench\\nAn additional contribution of the Imagen paper is DrawBench—a suite of 200 text\\nprompts for text-to-image evaluation. The text prompts cover 11 categories, such as\\nCounting (ability to generate a specified number of objects), Description (ability to\\ngenerate complex and long text prompts describing objects), and Text (ability to gen‐\\nerate quoted text). To compare two models, the DrawBench text prompts are passed\\nthrough each model and the outputs given to a panel of human raters for evaluation\\nacross two metrics:\\nAlignment\\nWhich image more accurately describes the caption?\\nFidelity\\nWhich image is more photorealistic (looks more real)?\\nThe results from the DrawBench human evaluation are shown in Figure 13-18.\\nBoth DALL.E 2 and Imagen are remarkable models that have made significant contri‐\\nbutions to the field of text-to-image generation. Whilst Imagen outperforms DALL.E\\n2 on many of the DrawBench benchmarks, DALL.E 2 provides additional functionali‐\\nties that are not present in Imagen. For example, because DALL.E 2 utilizes CLIP (a\\nmultimodal text–image model), it is able to accept images as input to generate image\\nembeddings. This means DALL.E 2 is able to provide image editing and image varia‐\\ntion capabilities. This is not possible with Imagen; the text encoder is a pure text\\nmodel, so there is no way to input an image.\\n378 \\n| \\nChapter 13: Multimodal Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 406}, page_content='Figure 13-18. Comparison of Imagen and DALL.E 2 on DrawBench across alignment\\nand image fidelity (source: Saharia et al., 2022)\\nExamples from Imagen\\nExample Imagen generations are shown in Figure 13-19.\\nFigure 13-19. Example Imagen generations (source: Saharia et al., 2022)\\nImagen \\n| \\n379'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 407}, page_content='Stable Diffusion\\nThe last text-to-image diffusion model that we shall explore is Stable Diffusion,\\nreleased in August 2022 by Stability AI, in collaboration with the Computer Vision\\nand Learning research group at Ludwig Maximilian University of Munich and Run‐\\nway. It is different from DALL.E 2 and Imagen in that its code and model weights\\nhave been released publicly, through Hugging Face. This means that anyone can\\ninteract with the model on their own hardware, without having to use proprietary\\nAPIs.\\nArchitecture\\nThe main architectural difference between Stable Diffusion and the text-to-image\\nmodels discussed previously is that it uses latent diffusion as its underlying generative\\nmodel. Latent diffusion models (LDMs) were introduced by Rombach et al. in\\nDecember 2021, in the paper “High-Resolution Image Synthesis with Latent Diffu‐\\nsion Models.”6 The key idea from the paper is to wrap the diffusion model within an\\nautoencoder, so that the diffusion process operates on a latent space representation of\\nthe image rather than the image itself, as shown in Figure 13-20.\\nFigure 13-20. The Stable Diffusion architecture\\nThis breakthrough means that the denoising U-Net model can be kept relatively\\nlightweight, in comparison to U-Net models that operate on full images. The autoen‐\\ncoder handles the heavy lifting of encoding the image detail into latent space and\\ndecoding the latent space back to a high-resolution image, leaving the diffusion\\n380 \\n| \\nChapter 13: Multimodal Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 408}, page_content='model to work purely in a latent, conceptual space. This gives a significant speed and\\nperformance boost to the training process.\\nThe denoising process can also optionally be guided by a text prompt that has been\\npassed through a text encoder. The first version of Stable Diffusion utilized the pre-\\ntrained CLIP model from OpenAI (the same as in DALL.E 2), but Stable Diffusion 2\\nhas a custom trained CLIP model called OpenCLIP, which has been trained from\\nscratch.\\nExamples from Stable Diffusion\\nFigure 13-21 shows some example outputs from Stable Diffusion 2.1—you can try\\nyour own prompts through the model hosted on Hugging Face.\\nFigure 13-21. Example outputs from Stable Diffusion 2.1\\nExploring the Latent Space\\nIf you’d like to explore the latent space of the Stable Diffusion\\nmodel, I highly recommended the walkthrough on the Keras web‐\\nsite.\\nFlamingo\\nSo far we have looked at three different kinds of text-to-image models. In this section,\\nwe’ll explore a multimodal model that generates text given a stream of text and visual\\ndata. Flamingo, introduced in a paper by DeepMind in April 2022,7 is a family of vis‐\\nual language models (VLMs) that act as a bridge between pre-trained vision-only and\\nlanguage-only models.\\nIn this section, we’ll run through the architecture of Flamingo models and compare\\nthem to the text-to-image models we have seen so far.\\nFlamingo \\n| \\n381'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 409}, page_content='Architecture\\nThe overall architecture of Flamingo is shown in Figure 13-22. For conciseness, we\\nshall explore the core components of this model—the Vision Encoder, the Perceiver\\nResampler, and the Language Mode—in just enough detail to highlight the key ideas\\nthat make Flamingo unique. I highly recommend reading the original research paper\\nfor a thorough review of each part of the model.\\nFigure 13-22. The Flamingo architecture (source: Alayrac et al., 2022)\\nThe Vision Encoder\\nThe first difference between a Flamingo model and pure text-to-image models such\\nas DALL.E 2 and Imagen is that Flamingo can accept a combination of text and visual\\ndata interleaved. Here, visual data includes videos as well as images.\\n382 \\n| \\nChapter 13: Multimodal Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 410}, page_content='The job of the Vision Encoder is to convert the vision data within the input into\\nembedding vectors (similar to the image encoder in CLIP). The Vision Encoder in\\nFlamingo is a pre-trained Normalizer-Free ResNet (NFNet), as introduced by Brock\\net al. in 20218—in particular, an NFNet-F6 (the NFNet models range from F0 to F6,\\nincreasing in size and power). This is one key difference between the CLIP image\\nencoder and the Flamingo Vision Encoder: the former uses a ViT architecture,\\nwhereas the latter uses a ResNet architecture.\\nThe Vision Encoder is trained on image-text pairs using the same contrastive objec‐\\ntive as introduced in the CLIP paper. After training, the weights are frozen so that any\\nfurther training of the Flamingo model does not affect the weights of the Vision\\nEncoder.\\nThe output from the Vision Encoder is a 2D grid of features that then gets flattened\\nto a 1D vector before being passed to the Perceiver Resampler. Video is handled by\\nsampling at 1 frame per second and passing each snapshot through the Vision\\nEncoder independently to produce several feature grids; learned temporal encodings\\nare then added in before flattening the features and concatenating the results into a\\nsingle vector.\\nThe Perceiver Resampler\\nMemory requirements in a traditional encoder Transformer (e.g., BERT) scale quad‐\\nratically with input sequence length, which is why input sequences are normally cap‐\\nped at a set number of tokens (e.g., 512 in BERT). However, the output from the\\nVision Encoder is a vector of variable length (due to the variable input image resolu‐\\ntion and the variable number of video frames) and is therefore potentially very long.\\nThe Perceiver architecture is specifically designed to efficiently handle long input\\nsequences. Instead of performing self-attention on the full input sequence, it works\\nwith a fixed-length latent vector and only uses the input sequence for cross-attention.\\nSpecifically, in the Flamingo Perceiver Resampler, the key and value are a concatena‐\\ntion of the input sequence and latent vector and the query is the latent vector alone. A\\ndiagram of the Vision Encoder and Perceiver Resampler process for video data is\\nshown in Figure 13-23.\\nFlamingo \\n| \\n383'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 411}, page_content='Figure 13-23. The Perceiver Resampler applied to video input (source: Alayrac et al.,\\n2022)\\n384 \\n| \\nChapter 13: Multimodal Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 412}, page_content='The output of the Perceiver Resampler is a fixed-length latent vector that gets passed\\nto the Language Model.\\nThe Language Model\\nThe Language Model consists of several stacked blocks, in the style of a decoder\\nTransformer, that output a predicted text continuation. In fact, the majority of the\\nLanguage Model is from a pre-trained DeepMind model called Chinchilla. The Chin‐\\nchilla paper, published in March 2022,9 showcases a language model that is designed\\nto be considerably smaller than its peers (e.g., 70B parameters for Chinchilla com‐\\npared to 170B for GPT-3), while using significantly more tokens for training. The\\nauthors show that the model outperforms larger models on a range of tasks, high‐\\nlighting the importance of optimizing the trade-off between training a larger model\\nand using a larger number of tokens during training.\\nA key contribution of the Flamingo paper is to show how Chinchilla can be adapted\\nto work with additional vision data (X) that is interspersed with the language data (Y).\\nLet’s first explore how the language and vision input are combined to produce the\\ninput to the Language Model (Figure 13-24).\\nFirst the text is processed by replacing vision data (e.g., images) with an <image> tag\\nand the text is divided into chunks using the <EOC> (end of chunk) tag. Each chunk\\ncontains at most one image, which is always at the start of the chunk—i.e., the subse‐\\nquent text is assumed to relate only to that image. The beginning of the sequence is\\nalso marked with the <BOS> (beginning of sentence) tag.\\nNext, the sequence is tokenized and each token is given an index (phi) corresponding\\nto the preceding image index (or 0 if there is no preceding image in the chunk). This\\nway, the text tokens (Y) can be forced to only cross-attend to the image tokens (X) that\\ncorrespond to their particular chunk, through masking. For example, in Figure 13-24\\nthe first chunk contains no images, so all image tokens from the Perceiver Resampler\\nare masked. The second chunk contains image 1, so these tokens are allowed to inter‐\\nact with the image tokens from image 1. Likewise, the final chunk contains image 2,\\nso these tokens are allowed to interact with the image tokens from image 2.\\nFlamingo \\n| \\n385'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 413}, page_content='Figure 13-24. Masked cross-attention (XATTN), combining vision and text data—light\\nblue entries are masked and dark blue entries are nonmasked (source: Alayrac et al.,\\n2022)\\nWe can now see how this masked cross-attention component fits into the overall\\narchitecture of the Language Model (Figure 13-25).\\nThe blue LM layer components are frozen layers from Chinchilla—these are not\\nupdated during the training process. The purple GATED XATTN-DENSE layers are\\ntrained as part of Flamingo and include the masked cross-attention components that\\nblend the language and vision information, as well as subsequent feed-forward\\n(dense) layers.\\nThe layer is gated because it passes the output from the cross-attention and feed-\\nforward components through two distinct tanh gates, which are both initialized to\\nzero. Therefore, when the network is initialized, there is no contribution from the\\nGATED XATTN-DENSE layers—the language information is just passed straight through.\\n386 \\n| \\nChapter 13: Multimodal Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 414}, page_content='The alpha gating parameters are learned by the network, to gradually blend in infor‐\\nmation from the vision data as training progresses.\\nFigure 13-25. A Flamingo Language Model block, comprising a frozen language model\\nlayer from Chinchilla and a GATED XATTN-DENSE layer (source: Alayrac et al., 2022)\\nFlamingo \\n| \\n387'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 415}, page_content='Examples from Flamingo\\nFlamingo can be used for a variety of purposes, including image and video under‐\\nstanding, conversational prompting, and visual dialogue. In Figure 13-26 we can see a\\nfew examples of what Flamingo is capable of.\\nFigure 13-26. Examples of inputs and outputs obtained from the 80B parameter Fla‐\\nmingo model (source: Alayrac et al., 2022)\\nNotice how in each example, Flamingo is blending information from the text and the\\nimages in true multimodal style. The first example uses images in place of words and\\nis able to suggest an appropriate book to continue the prompt. The second example\\n388 \\n| \\nChapter 13: Multimodal Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 416}, page_content='shows frames from a video, and Flamingo correctly identifies the consequence of the\\naction. The last three examples all demonstrate how Flamingo can be used interac‐\\ntively, to provide additional information through dialogue or probe with further\\nquestioning.\\nIt is astonishing to see a machine being able to answer complex questions across such\\na wide range of modalities and input tasks. In the paper, the authors quantify Flamin‐\\ngo’s ability across a set of benchmark tasks and find that across many benchmarks,\\nFlamingo is able to surpass the performance of models that have been tailored to\\nspecifically tackle the one task in question. This highlights how large multimodal\\nmodels can be rapidly adapted to a wide range of tasks and paves the way for the\\ndevelopment of AI agents that aren’t just tied to a single task, but instead are truly\\ngeneral agents that can be guided by the user at inference time.\\nSummary\\nIn this chapter we have explored four different state-of-the-art multimodal models:\\nDALL.E 2, Imagen, Stable Diffusion, and Flamingo.\\nDALL.E 2 is a large-scale text-to-image model from OpenAI that can generate realis‐\\ntic images across a range of styles given a text prompt. It works by combining pre-\\ntrained models (e.g., CLIP) with diffusion model architectures from previous works\\n(GLIDE). It also has additional capabilities, such as being able to edit images through\\ntext prompting and provide variations of a given image. While it does have some lim‐\\nitations, such as inconsistent text rendering and attribute binding, DALL.E 2 is an\\nincredibly powerful AI model that has helped to propel the field of generative model‐\\ning into a new era.\\nAnother model that has surpassed previous benchmarks is Imagen from Google\\nBrain. This model shares many similarities with DALL.E 2, such as a text encoder and\\na diffusion model decoder. One of the key differences between the two models is that\\nthe Imagen text encoder is trained on pure text data, whereas the training process for\\nthe DALL.E 2 text encoder involves image data (through the contrastive CLIP learn‐\\ning objective). The authors show that this approach leads to state-of-the-art perfor‐\\nmance across a range of tasks, through their DrawBench evaluation suite.\\nStable Diffusion is an open source offering from Stability AI, CompVis, and Runway.\\nIt is a text-to-image model whose model weights and code are freely available, so you\\ncan run it on your own hardware. Stable Diffusion is particularly fast and lightweight\\ndue to the use of a latent diffusion model that operates on the latent space of an\\nautoencoder, rather than the images themselves.\\nFinally, DeepMind’s Flamingo is a visual language model—that is, it accepts a stream\\nof interleaved text and visual data (images and video) and is able to continue\\nthe prompt with additional text, in the style of a decoder Transformer. The key\\nSummary \\n| \\n389'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 417}, page_content='contribution is showing how the visual information can be fed to the Transformer via\\na Visual Encoder and Perceiver Resampler that encode the visual input features into a\\nsmall number of visual tokens. The Language Model itself is an extension of Deep‐\\nMind’s earlier Chinchilla model, adapted to blend in visual information.\\nAll four are remarkable examples of the power of multimodal models. In the future, it\\nis highly likely that generative modeling will become more multimodal and AI mod‐\\nels will be able to easily cross modalities and tasks through interactive language\\nprompting.\\nReferences\\n1. Aditya Ramesh et al., “Zero-Shot Text-to-Image Generation,” February 24, 2021,\\nhttps://arxiv.org/abs/2102.12092.\\n2. Aditya Ramesh et al., “Hierarchical Text-Conditional Image Generation with CLIP\\nLatents,” April 13, 2022, https://arxiv.org/abs/2204.06125.\\n3. Alec Radford et al., “Learning Transferable Visual Models From Natural Language\\nSupervision,” February 26, 2021, https://arxiv.org/abs/2103.00020.\\n4. Alex Nichol et al., “GLIDE: Towards Photorealistic Image Generation and Editing\\nwith Text-Guided Diffusion Models,” December 20, 2021, https://arxiv.org/abs/\\n2112.10741.\\n5. Chitwan Saharia et al., “Photorealistic Text-to-Image Diffusion Models with Deep\\nLanguage Understanding,” May 23, 2022, https://arxiv.org/abs/2205.11487.\\n6. Robin Rombach et al., “High Resolution Image Synthesis with Latent Diffusion\\nModels,” December 20, 2021, https://arxiv.org/abs/2112.10752.\\n7. Jean-Baptiste Alayrac et al., “Flamingo: A Visual Language Model for Few-Shot\\nLearning,” April 29, 2022, https://arxiv.org/abs/2204.14198.\\n8. Andrew Brock et al., “High-Performance Large-Scale Image Recognition Without\\nNormalization,” February 11, 2021, https://arxiv.org/abs/2102.06171.\\n9. Jordan Hoffmann et al., “Training Compute-Optimal Large Language Models,”\\nMarch 29, 2022, https://arxiv.org/abs/2203.15556v1.\\n390 \\n| \\nChapter 13: Multimodal Models'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 418}, page_content='CHAPTER 14\\nConclusion\\nChapter Goals\\nIn this chapter you will:\\n• Review the history of generative AI from 2014 to the present day, including a\\ntimeline of key models and developments.\\n• Understand the current state of generative AI, including the broad themes that\\nare dominating the landscape.\\n• See my predictions for the future of generative AI and how it will impact every‐\\nday life, the workplace, and education.\\n• Learn about the important ethical and practical challenges faced by generative AI\\ngoing forward.\\n• Read my final thoughts on the deeper meaning of generative AI and how it has\\nthe potential to revolutionize our quest for artificial general intelligence.\\nIn May 2018, I began work on the first edition of this book. Five years later, I am\\nmore excited than ever about the endless possibilities and potential impact of genera‐\\ntive AI.\\nIn this time we have seen incredible progress in this field, with seemingly limitless\\npotential for real-world applications. I am filled with a sense of awe and wonder at\\nwhat we have been able to achieve so far and eagerly anticipate witnessing the effect\\nthat generative AI will have on the world in the coming years. Generative deep learn‐\\ning has the power to shape the future in ways we can’t even begin to imagine.\\n391'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 419}, page_content='What’s more, as I have been researching content for this book, it has become ever\\nclearer to me that this field isn’t just about creating images, text, or music. I believe\\nthat at the core of generative deep learning lies the secret of intelligence itself.\\nThe first section of this chapter summarizes how we have reached this point in our\\ngenerative AI journey. We will walk through a timeline of generative AI develop‐\\nments since 2014 in chronological order, so that you can see where each technique\\nfits into the history of generative AI to date. The second section explains where we\\ncurrently stand in terms of state-of-the-art generative AI. We will discuss current\\ntrends in the approach to generative deep learning and the current off-the-shelf mod‐\\nels available to the general public. Next, we will explore the future of generative AI\\nand the opportunities and challenges that lie ahead. We will consider what generative\\nAI might look like five years in the future and its potential impact on society and\\nbusiness, and address some of the main ethical and practical concerns.\\nTimeline of Generative AI\\nFigure 14-1 is a timeline of the key developments in generative modeling that we have\\nexplored together in this book. The colors represent different model types.\\nTo field of generative AI stands on the shoulders of earlier developments in deep\\nlearning, such as backpropagation and convolutional neural networks, which\\nunlocked the possibility for models to learn complex relationships across large data‐\\nsets at scale. In this section, we will study the modern history of generative AI, from\\n2014 onwards, that has moved at such breathtaking speed.\\nTo help us understand how everything fits together, we can loosely break down this\\nhistory into three main eras:\\n1. 2014–2017: The VAE and GAN era\\n2. 2018–2019: The Transformer era\\n3. 2020–2022: The Big Model era\\n392 \\n| \\nChapter 14: Conclusion'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 420}, page_content='Figure 14-1. A brief history of generative AI from 2014 to 2023 (note: some important\\ndevelopments such as LSTMs and early energy-based models [e.g., Boltzmann machines]\\nprecede this timeline)\\nTimeline of Generative AI \\n| \\n393'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 421}, page_content='2014–2017: The VAE and GAN Era\\nThe invention of the VAE in December 2013 can perhaps be thought of as the spark\\nthat lit the generative AI touchpaper. This paper showed how it was possible to gener‐\\nate not only simple images such as MNIST digits but also more complex images such\\nas faces in a latent space that could be smoothly traversed. It was followed in 2014 by\\nthe introduction of the GAN, an entirely new adversarial framework for tackling gen‐\\nerative modeling problems.\\nThe following three years were dominated by progressively more impressive exten‐\\nsions of the GAN portfolio. In addition to fundamental changes to the GAN model\\narchitecture (DCGAN, 2015), loss function (Wasserstein GAN, 2017), and training\\nprocess (ProGAN, 2017), new domains were tackled using GANs, such as image-to-\\nimage translation (pix2pix, 2016, and CycleGAN, 2017) and music generation (Muse‐\\nGAN, 2017).\\nDuring this era, important VAE improvements were also introduced, such as VAE-\\nGAN (2015) and later VQ-VAE (2017), and applications to reinforcement learning\\nwere seen in the “World Models” paper (2018).\\nEstablished autoregressive models such as LSTMs and GRUs remained the dominant\\nforce in text generation over this time. The same autoregressive ideas were also being\\nused to generate images, with PixelRNN (2016) and PixelCNN (2016) introduced as\\nnew ways to think about image generation. Other approaches to image generation\\nwere also being tested, such as the RealNVP model (2016) that paved the way for later\\ntypes of normalizing flow models.\\nIn June 2017, a groundbreaking paper entitled “Attention Is All You Need” was\\npublished that would usher in the next era of generative AI, focused around\\nTransformers.\\n2018–2019: The Transformer Era\\nAt the heart of a Transformer is the attention mechanism that negates the need for\\nthe recurrent layers present in older autoregressive models such as LSTMs. The\\nTransformer quickly rose to prominence with the introduction of GPT (a decoder-\\nonly Transformer) and BERT (an encoder-only Transformer) in 2018. The following\\nyear saw progressively larger language models being built that excelled at a wide\\nrange of tasks by treating them as pure text-to-text generation problems, with GPT-2\\n(2018, 1.5B parameters) and T5 (2019, 11B parameters) being standout examples.\\nTransformers were also starting to be successfully applied to music generation, with\\nthe introduction of, for example, the Music Transformer (2018) and MuseNet (2019)\\nmodels.\\n394 \\n| \\nChapter 14: Conclusion'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 422}, page_content='Over these two years, several impressive GANs were also released that cemented the\\ntechnique’s place as the state-of-the-art approach for image generation. In particular,\\nSAGAN (2018) and the larger BigGAN (2018) incorporated the attention mechanism\\ninto the GAN framework with incredible results, and StyleGAN (2018) and later\\nStyleGAN2 (2019) showed how images could be generated with amazing fine-grained\\ncontrol over the style and content of a particular image.\\nAnother field of generative AI that was gathering momentum was score-based mod‐\\nels (NCSN, 2019), which would eventually pave the way for the next seismic shift in\\nthe generative AI landscape—diffusion models.\\n2020–2022: The Big Model Era\\nThis era saw the introduction of several models that merged ideas across different\\ngenerative modeling families and turbo-charged existing architectures. For example,\\nthe VQ-GAN (2020) brought the GAN discriminator into the VQ-VAE architecture\\nand the Vision Transformer (2020) showed how it was possible to train a Trans‐\\nformer to operate over images. 2022 saw the release of StyleGAN-XL, a further\\nupdate to the StyleGAN architecture that enables 1,024 × 1,024–pixel images to be\\ngenerated.\\nTwo models were introduced in 2020 that would lay the foundations for all future\\nlarge image generation models: DDPM and DDIM. Suddenly, diffusion models were\\na rival for GANs in terms of image generation quality, as explicitly stated in the title\\nof the 2021 paper “Diffusion Models Beat GANs on Image Synthesis.” The image\\nquality of diffusion models is unbelievably good and they only require a single U-Net\\nnetwork to be trained, rather than the dual-network setup of a GAN, making the\\ntraining process much more stable.\\nAround the same time, GPT-3 (2020) was released—an enormous 175B parameter\\nTransformer that can generate text on just about any topic in a way that seems almost\\nimpossible to comprehend. The model was released through a web application and\\nAPI, allowing companies to build products and services on top of it. ChatGPT (2022)\\nis a web application and API wrapper around the latest version of GPT from OpenAI\\nthat allows users to have natural conversations with the AI about any topic.\\nOver 2021 and 2022, a flurry of other large language models were released to rival\\nGPT-3, including Megatron-Turing NLG (2021) by Microsoft and NVIDIA, Gopher\\n(2021) and Chinchilla by DeepMind (2022), LaMDA (2022) and PaLM (2022) by\\nGoogle, and Luminous (2022) by Aleph Alpha. Some open source models were also\\nreleased, such as GPT-Neo (2021), GPT-J (2021), and GPT-NeoX (2022) by Eleu‐\\ntherAI; the 66B parameter OPT model (2022) by Meta; the fine-tuned Flan-T5 model\\n(2022) by Google, BLOOM (2022) by Hugging Face; and others. Each of these models\\nis a variation of a Transformer, trained on a huge corpus of data.\\nTimeline of Generative AI \\n| \\n395'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 423}, page_content='The rapid rise of powerful Transformers for text generation and state-of-the-art diffu‐\\nsion models for image generation has meant that much of the focus of the last two\\nyears of generative AI development has been on multimodal models—that is, models\\nthat operate over more than one domain (for example, text-to-image models).\\nThis trend was established in 2021 when OpenAI released DALL.E, a text-to-image\\nmodel based upon a discrete VAE (similar to VQ-VAE) and CLIP (a Transformer\\nmodel that predicts image/text pairs). This was followed by GLIDE (2021) and\\nDALL.E 2 (2022), which updated the generative part of the model to use a diffusion\\nmodel rather than a discrete VAE, with truly impressive results. This era also saw the\\nrelease of three text-to-image models from Google: Imagen (2022, using Transformer\\nand diffusion models), Parti (2022, using Transformers and a ViT-VQGAN model),\\nand later MUSE (2023, using Transformers and VQ-GANs). DeepMind also released\\nFlamingo (2022), a visual language model that builds upon their large language\\nmodel Chinchilla by allowing images to be used as part of the prompt data.\\nAnother important diffusion advancement introduced in 2021 was latent diffusion,\\nwhere a diffusion model is trained within the latent space of an autoencoder. This\\ntechnique powers the Stable Diffusion model, released as a joint collaboration\\nbetween Stability AI, CompVis, and Runway in 2022. Unlike with DALL.E 2, Imagen,\\nand Flamingo, the code and model weights of Stable Diffusion are open source,\\nmeaning anyone can run the model on their own hardware.\\nThe Current State of Generative AI\\nAs we come to end of our journey through the history of generative AI, it is impor‐\\ntant to now reflect on where we stand in terms of current state-of-the-art applications\\nand models. Let’s take a moment to assess our progress and key accomplishments in\\nthe field to date.\\nLarge Language Models\\nGenerative AI for text is now almost entirely focused on building large language\\nmodels (LLMs), whose sole purpose is to directly model language from a huge corpus\\nof text—that is, they are trained to predict the next word, in the style of a decoder\\nTransformer.\\nThe large language model approach has been adopted so widely because of its flexibil‐\\nity and ability to excel at a wide range of tasks. The same model can be used for ques‐\\ntion answering, text summarization, content creation, and many other examples\\nbecause ultimately each use case can be framed as a text-to-text problem, where the\\nspecific task instructions (the prompt) are given as part of the input to the model.\\nLet’s take GPT-3 as an example. Figure 14-2 shows how the same model can be used\\nfor text summarization and content creation.\\n396 \\n| \\nChapter 14: Conclusion'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 424}, page_content='Figure 14-2. Output from GPT-3—the non-highlighted text is the prompt and the green\\nhighlighted text is the output from GPT-3\\nNotice how in both cases, the prompt contains the relevant instructions. The job of\\nGPT-3 is just to continue the prompt, one token at a time. It doesn’t have a database\\nof facts from which it can look up information, or snippets of text that it can copy\\ninto its answers. It is only asked to predict what token is most likely to follow the\\nexisting tokens and then append this prediction to the prompt to generate the next\\ntoken, and so on.\\nIncredibly, this simple design is enough for the language model to excel at a range of\\ntasks, as shown in Figure 14-2. Moreover, it gives the language model incredible flexi‐\\nbility to generate realistic text as a response to any prompt—imagination is often the\\nlimiting factor!\\nFigure 14-3 shows how large language models have grown in size since the original\\nGPT model was published in 2018. The number of parameters grew exponentially\\nThe Current State of Generative AI \\n| \\n397'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 425}, page_content='until late 2021, with Megatron-Turing NLG reaching 530B parameters. Recently,\\nmore emphasis has been placed on building more efficient language models that use\\nfewer parameters, as larger models are more costly and slower to serve in a produc‐\\ntion environment.\\nFigure 14-3. The size of large language models (orange) and multimodal models (pink)\\nin number of parameters over time\\nOpenAI’s GPT collection (GPT-3, GPT-3.5, GPT-4, etc.) is still considered by many\\nto be the most powerful state-of-the-art suite of language models available for per‐\\nsonal and commercial use. They are each available through a web application and\\nAPI.\\nAnother recent addition to the large language model family is Large Language Model\\nMeta AI (LLaMA) from Meta,1 a suite of models ranging from 7B to 65B parameters\\nin size that are trained purely on publicly available datasets.\\nA summary of some of the most powerful LLMs in existence today is shown in\\nTable 14-1. Some, like LLaMA, are families of models of different sizes—in this case,\\nthe size of the largest model is shown here. Pre-trained weights are fully open source\\nfor some of the models, meaning that they are free for anyone to use and build upon.\\n398 \\n| \\nChapter 14: Conclusion'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 426}, page_content='Table 14-1. Large language models\\nModel\\nDate\\nDeveloper\\n# parameters\\nOpen source\\nGPT-3\\nMay 2020\\nOpenAI\\n175,000,000,000\\nNo\\nGPT-Neo\\nMar 2021\\nEleutherAI\\n2,700,000,000\\nYes\\nGPT-J\\nJun 2021\\nEleutherAI\\n6,000,000,000\\nYes\\nMegatron-Turing NLG\\nOct 2021\\nMicrosoft & NVIDIA\\n530,000,000,000\\nNo\\nGopher\\nDec 2021\\nDeepMind\\n280,000,000,000\\nNo\\nLaMDA\\nJan 2022\\nGoogle\\n137,000,000,000\\nNo\\nGPT-NeoX\\nFeb 2022\\nEleutherAI\\n20,000,000,000\\nYes\\nChinchilla\\nMar 2022\\nDeepMind\\n70,000,000,000\\nNo\\nPaLM\\nApr 2022\\nGoogle\\n540,000,000,000\\nNo\\nLuminous\\nApr 2022\\nAleph Alpha\\n70,000,000,000\\nNo\\nOPT\\nMay 2022\\nMeta\\n175,000,000,000\\nYes (66B)\\nBLOOM\\nJul 2022\\nHugging Face collaboration 175,000,000,000\\nYes\\nFlan-T5\\nOct 2022\\nGoogle\\n11,000,000,000\\nYes\\nGPT-3.5\\nNov 2022\\nOpenAI\\nUnknown\\nNo\\nLLaMA\\nFeb 2023\\nMeta\\n65,000,000,000\\nNo\\nGPT-4\\nMar 2023\\nOpenAI\\nUnknown\\nNo\\nDespite the impressive applications of large language models, there remain significant\\nchallenges to overcome. Most notably, they are prone to inventing facts and cannot\\nreliably apply logical thought processes, as shown in Figure 14-4.\\nFigure 14-4. While large language models excel at some tasks, they are also prone to mis‐\\ntakes related to factual or logical reasoning (GPT-3 output shown)\\nThe Current State of Generative AI \\n| \\n399'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 427}, page_content='It is important to remember that LLMs are trained only to predict the next word.\\nThey have no other connection to reality that would allow them to reliably identify\\nfactual or logical fallacies. Therefore, we must be extremely cautious about how we\\nuse these powerful text prediction models in production—they cannot yet be reliably\\nutilized for anything that requires precise reasoning.\\nText-to-Code Models\\nAnother application of large language models is code generation. In July 2021,\\nOpenAI introduced a model called Codex, a GPT language model that had been fine-\\ntuned on code from GitHub.2 The model was able to successfully write novel coded\\nsolutions to a range of problems, prompted only with a comment on the problem to\\nbe solved, or a function name. The technology today powers GitHub Copilot, an AI\\npair programmer that can be used to suggest code in real time as you type. Copilot is\\na paid subscription-based service, with a free trial period.\\nFigure 14-5 shows two examples of autogenerated completions. The first example is a\\nfunction that fetches tweets from a given user, using the Twitter API. Given the func‐\\ntion name and parameter, Copilot is able to autocomplete the rest of the function def‐\\ninition. The second example asks Copilot to parse a list of expenses, by additionally\\nincluding a free text description in the docstring that explains the format of the input\\nparameter and specific instructions related to the task. Copilot is able to autocom‐\\nplete the entire function from the description alone.\\nThis remarkable technology is already beginning to change how programmers\\napproach a given task. A significant proportion of a programmer’s time is usually\\nspent searching for examples of existing solutions, reading community Q&A forums\\nsuch as Stack Overflow, and looking up syntax in package documentation. This\\nmeans leaving the interactive development environment (IDE) through which you\\nare coding, switching to a web browser, and copying and pasting code snippets from\\nthe web to see if they solve your specific problem. Copilot removes the need to do this\\nin many cases, because you can simply tab through potential solutions generated by\\nthe AI from within the IDE, after writing a brief description of what you are looking\\nto achieve.\\n400 \\n| \\nChapter 14: Conclusion'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 428}, page_content='Figure 14-5. Two examples of GitHub Copilot capabilities (source: GitHub Copilot)\\nThe Current State of Generative AI \\n| \\n401'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 429}, page_content='Text-to-Image Models\\nState-of-the-art image generation is currently dominated by large multimodal models\\nthat convert a given text prompt into an image. Text-to-image models are highly use‐\\nful as they allow users to easily manipulate generated images via natural language.\\nThis is in contrast to models such as StyleGAN, which, while extremely impressive,\\ndoes not have a text interface through which you can describe the image that you\\nwant to be generated.\\nThree important text-to-image generation models that are currently available for\\ncommercial and personal use are DALL.E 2, Midjourney, and Stable Diffusion.\\nDALL.E 2 by OpenAI is a pay-as-you-go service that is available through a web appli‐\\ncation and API. Midjourney provides a subscription-based text-to-image service\\nthrough its Discord channel. Both DALL.E 2 and Midjourney offer free credits to\\nthose joining the platform for early experimentation.\\nMidjourney\\nMidjourney is the service used to create the illustrations for the sto‐\\nries in Part II of this book!\\nStable Diffusion is different because it is fully open source. The model weights and\\ncode to train the model are available on GitHub, so anyone can run the model on\\ntheir own hardware. The dataset used to train Stable Diffusion is also open source.\\nThis dataset, called LAION-5B, contains 5.85 billion image-text pairs and is currently\\nthe largest openly accessible image-text dataset in the world.\\nAn important corollary of this approach is that the baseline Stable Diffusion model\\ncan be built upon and adapted to different use cases. An excellent demonstration of\\nthis is ControlNet, a neural network structure that allows fine-grained control of the\\noutput from Stable Diffusion by adding extra conditions.3 For example, output\\nimages can be conditioned on a Canny edge map of a given input image, as shown in\\nFigure 14-6.\\n402 \\n| \\nChapter 14: Conclusion'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 430}, page_content='Figure 14-6. Conditioning the output of Stable Diffusion using a Canny edge map and\\nControlNet (source: Lvmin Zhang, ControlNet)\\nControlNet contains a trainable copy of the Stable Diffusion encoder, alongside a\\nlocked copy of the full Stable Diffusion model. The job of this trainable encoder is to\\nlearn how to handle the input condition (e.g., the Canny edge map), whilst the locked\\ncopy retains the power of the original model. This way, Stable Diffusion can be fine-\\ntuned using only a small number of image pairs. Zero convolutions are simply 1 × 1\\nconvolutions where all weights and biases are zero, so that before training, Control‐\\nNet does not have any effect.\\nThe Current State of Generative AI \\n| \\n403'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 431}, page_content='Figure 14-7. The ControlNet architecture, with the trainable copies of the Stable Diffu‐\\nsion encoder blocks highlighted in blue (source: Lvmin Zhang, ControlNet)\\nAnother advantage of Stable Diffusion is that it is able to run on a single modestly\\nsized GPU with only 8 GB of VRAM, making it possible to run on edge devices,\\nrather than through calls to a cloud service. As text-to-image services are included in\\ndownstream products, the speed of generation is becoming increasingly more impor‐\\ntant. This is one reason why the size of multimodal models is generally trending\\ndownward (see Figure 14-3).\\n404 \\n| \\nChapter 14: Conclusion'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 432}, page_content='Example outputs for all three models can be seen in Figure 14-8. All of these models\\nare exceptional and are able to capture the content and style of the given description.\\nFigure 14-8. Outputs from Stable Diffusion v2.1, Midjourney, and DALL.E 2 for the\\nsame prompt\\nA summary of some of the most powerful text-to-image models in existence today is\\nshown in Table 14-2.\\nTable 14-2. Text-to-image models\\nModel\\nDate\\nDeveloper\\n# parameters\\nOpen source\\nDALL.E 2\\nApr 2022\\nOpenAI\\n3,500,000,000\\nNo\\nImagen\\nMay 2022\\nGoogle\\n4,600,000,000\\nNo\\nParti\\nJun 2022\\nGoogle\\n20,000,000,000\\nNo\\nStable Diffusion\\nAug 2022\\nStability AI, CompVis, and Runway\\n890,000,000\\nYes\\nMUSE\\nJan 2023\\nGoogle\\n3,000,000,000\\nNo\\nPart of the skill of working with text-to-image models is creating a prompt that both\\ndescribes the content of the image you want to generate and uses keywords that\\nencourage the model to produce a particular style or type of image. For example,\\nadjectives such as stunning or award-winning can often be used to improve the quality\\nof the generation. However, it is not always the case that the same prompt will work\\nwell across different models—it depends on the contents of the specific text-image\\ndataset used to train the model. The art of uncovering prompts that work well for a\\nparticular model is known as prompt engineering.\\nOther Applications\\nGenerative AI is rapidly finding applications across a variety of novel domains, from\\nreinforcement learning to other kinds of text-to-X multimodal models.\\nThe Current State of Generative AI \\n| \\n405'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 433}, page_content='For example, in November 2022 Meta published a paper on CICERO, an AI agent\\ntrained to play the board game Diplomacy. In this game, players represent different\\ncountries in Europe before World War I and must negotiate with and deceive each\\nother in order to gain control of the continent. It is a highly complex game for an AI\\nagent to master, not least because there is a communicative element where players\\nmust discuss their plans with other players in order to gain allies, coordinate maneu‐\\nvers, and suggest strategic goals. To achieve this, CICERO contains a language model\\nthat is able to initiate dialogue and respond to messages from other players. Crucially,\\nthe dialogue is consistent with the agent’s strategic plans, which are generated by\\nanother part of the model to adapt to the constantly evolving scenario. This includes\\nthe ability for the agent to bluff when conversing with other players—that is, convince\\nanother player to co-operate with the agent’s plans, only to then enact an aggressive\\nmaneuver against the player in a later turn. Remarkably, in an anonymous online\\nDiplomacy league featuring 40 games, CICERO’s score was more than double the\\naverage of the human players and it ranked in the top 10% of participants who played\\nmultiple games. This is an excellent example of how generative AI can be successfully\\nblended with reinforcement learning.\\nThe development of embodied large language models is an exciting area of research,\\nfurther exemplified by Google’s PaLM-E. This model combines the powerful language\\nmodel PaLM with a Vision Transformer to convert visual and sensor data into tokens\\nthat can be interleaved with text instructions, allowing robots to execute tasks based\\non text prompts and continuous feedback from other sensory modalities. The PaLM-\\nE website showcases the model’s abilities, including controlling a robot to arrange\\nblocks and fetch objects based on text descriptions.\\nText-to-video models involve the creation of videos from text input. This field, which\\nbuilds on the concept of text-to-image modeling, has the additional challenge of\\nincorporating a time dimension. For example, in September 2022 Meta published\\nMake-A-Video, a generative model that is able to create a short video given only a text\\nprompt as input. The model is also able to add motion between two static images and\\nproduce variations of a given input video. Interestingly, it is trained only on paired\\ntext–image data and unsupervised video footage, rather than text–video pairs\\ndirectly. The unsupervised video data is enough for the model to learn how the world\\nmoves; it then uses the text–image pairs to learn how to map between text image\\nmodalities, which are then animated. The Dreamix model is able to perform video\\nediting, where an input video is transformed based on a given text prompt while\\nretaining other stylistic attributes. For example, a video of a glass of milk being\\npoured could be converted to a cup of coffee being poured, while retaining the cam‐\\nera angle, background, and lighting elements of the original video.\\n406 \\n| \\nChapter 14: Conclusion'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 434}, page_content='Similarly, text-to-3D models extend traditional text-to-image approaches into a third\\ndimension. In September 2022 Google published DreamFusion, a diffusion model\\nthat generates 3D assets given an input text prompt. Crucially, the model does not\\nrequire labeled 3D assets to train on. Instead, the authors use a pre-trained 2D text-\\nto-image model (Imagen) as a prior and then train a 3D Neural Radiance Field\\n(NeRF), such that it is able to produce good images when rendered from random\\nangles. Another example is OpenAI’s Point-E, published in December 2022. Point-E is\\na pure diffusion-based system that is able to generate a 3D point cloud from a given\\ntext prompt. While the output produced is not as high quality as DreamFusion’s, the\\nadvantage of this approach is that is much faster than NeRF-based methods—it can\\nproduce output in just one to two minutes on a single GPU, rather than requiring\\nmultiple GPU-hours.\\nGiven the similarities between text and music, it is not surprising that there have also\\nbeen attempts to create text-to-music models. MusicLM, released by Google in Janu‐\\nary 2023, is a language model that is able to convert a text description of a piece of\\nmusic (e.g., “a calming violin melody backed by a distorted guitar riff”) into audio\\nspanning several minutes that accurately reflects the description. It builds upon the\\nearlier work AudioLM by adding the ability for the model to be guided by a text\\nprompt; examples that you can listen to are available on the Google Research website.\\nThe Future of Generative AI\\nIn this final section, we will explore the potential impact that powerful generative AI\\nsystems may have on the world we live in—across our everyday lives, in the work‐\\nplace, and within the field of education. We will also lay down the key practical and\\nethical challenges generative AI will face if it is to become a ubiquitous tool that\\nmakes a significant net positive contribution to society.\\nGenerative AI in Everyday Life\\nThere is no doubt that in the future generative AI will play an increasingly important\\nrole in people’s everyday lives—particularly large language models. With OpenAI’s\\nChatGPT, it is already possible to generate a perfect cover letter for a job application,\\na professional email response to a colleague, or a funny social media post on a given\\ntopic using generative AI. This technology is truly interactive: it is able to include\\nspecific details that you request, respond to feedback, and ask its own questions back\\nif something isn’t clear. This style of personal assistant AI should be the stuff of sci‐\\nence fiction, but it isn’t—it’s here right now, for anyone who chooses to use it.\\nWhat are the repercussions of this kind of application becoming mainstream? It is\\nlikely that the most immediate effect will be an increase in the quality of written com‐\\nmunication. Access to large language models with a user-friendly interface will enable\\npeople to translate a sketch of an idea into coherent, high-quality paragraphs in\\nThe Future of Generative AI \\n| \\n407'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 435}, page_content='seconds. Email writing, social media posts, and even short-form instant messaging\\nwill be transformed by this technology. It goes beyond removing the common barri‐\\ners associated with spelling, grammar, and readability—it directly links our thought\\nprocesses to usable output, often removing the need to engage with the process of\\nconstructing sentences at all.\\nProduction of well-formed text is only one use of large language models. People will\\nstart using these models for idea generation, advice, and information retrieval. I\\nbelieve we can see this as the fourth stage of our ability as a species to acquire, share,\\nretrieve, and synthesize information. We started by acquiring information from those\\naround us, or physically traveling to new locations to transfer knowledge. The inven‐\\ntion of the printing press allowed the book to become the primary vessel through\\nwhich ideas were shared. Finally, the birth of the internet allowed us to instantane‐\\nously search for and retrieve information at the touch of a button. Generative AI\\nunlocks a new era of information synthesis that I believe will replace many of the cur‐\\nrent uses of today’s search engines.\\nFor example, OpenAI’s GPT suite of models can provide bespoke holiday destination\\nrecommendations, as shown in Figure 14-9, or advice on how to respond to a diffi‐\\ncult situation, or a detailed explanation of an obscure concept. Using this technology\\nfeels more like asking a friend than typing a query into a search engine, and for that\\nreason, people are flocking to it extremely quickly. ChatGPT is the fastest-growing\\ntech platform ever; it acquired 1 million users within 5 days of its launch. For context,\\nit took Instagram 2.5 months to reach the same number of users and Facebook 10\\nmonths.\\nFigure 14-9. Output from GPT-3, giving bespoke holiday recommendations\\n408 \\n| \\nChapter 14: Conclusion'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 436}, page_content='Generative AI in the Workplace\\nAs well as general use, generative AI will find applications in specific jobs where crea‐\\ntivity is required. A nonexhaustive list of occupations that may benefit follows:\\nAdvertising\\nGenerative AI can be used to create personalized ad campaigns that target spe‐\\ncific demographics based on their browsing and purchase history.\\nMusic production\\nGenerative AI can be used to compose and produce original music tracks, allow‐\\ning for a limitless range of possibilities.\\nArchitecture\\nGenerative AI can be used to design buildings and structures, taking into account\\nfactors such as style and constraints around layout.\\nFashion design\\nGenerative AI can be used to create unique and diverse clothing designs, taking\\ninto account trends and wearer preferences.\\nAutomotive design\\nGenerative AI can be used to design and develop new vehicle models and auto‐\\nmatically find interesting variations on a particular design.\\nFilm and video production\\nGenerative AI can be used to create special effects and animations, as well as to\\ngenerate dialogue for entire scenes or storylines.\\nPharmaceutical research\\nGenerative AI can be used to generate new drug compounds, which can aid in\\nthe development of new treatments.\\nCreative writing\\nGenerative AI can be used to generate written content, such as fiction stories,\\npoetry, news articles, and more.\\nGame design\\nGenerative AI can be used to design and develop new game levels and content,\\ncreating an infinite variety of gameplay experiences.\\nDigital design\\nGenerative AI can be used to create original digital art and animations, as well as\\nto design and develop new user interfaces and web designs.\\nAI is often said to pose an existential threat to jobs in fields such as these, but I do not\\nbelieve that this is actually the case. For me, AI is just another tool in the toolbox of\\nthese creative roles (albeit a very powerful one), rather than a replacement for the role\\nThe Future of Generative AI \\n| \\n409'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 437}, page_content='itself. Those who choose to embrace this new technology will find that they are able\\nto explore new ideas much faster and iterate over concepts in a way that previously\\nwas not possible.\\nGenerative AI in Education\\nOne final area of everyday life that I believe will be significantly impacted is educa‐\\ntion. Generative AI challenges the fundamental axioms of education in a way that we\\nhaven’t seen since the dawn of the internet. The internet gave students the ability to\\nretrieve information instantaneously and unambiguously, making exams that purely\\ntested memorization and recall seem old-fashioned and irrelevant. This prompted a\\nshift in approach, focused on testing students’ ability to synthesize ideas in a novel\\nway instead of only testing factual knowledge.\\nI believe that generative AI will cause another transformative shift in the field of edu‐\\ncation, necessitating a reevaluation and adjustment of current teaching methods and\\nassessment criteria. If every student now has access to an essay-writing machine in\\ntheir pocket that can generate novel responses to questions, what is the purpose of\\nessay-based coursework?\\nMany would call for the use of such AI tools to be banned, in the same way that plagi‐\\narism is banned. However, it’s not that simple, as detecting AI-generated text is much\\nharder than detecting plagiarism and even harder to prove beyond doubt. Moreover,\\nstudents could use AI tools to generate a skeleton draft for the essay and then add\\nextra detail or update factually incorrect information as required. In this case, is it the\\nstudent’s original work, or the AI’s?\\nClearly, these are huge questions that need to be addressed in order for education and\\ncertifications to maintain their integrity. In my opinion, there is no sense in resisting\\nthe proliferation of AI tools within education—any such approach is doomed to fail,\\nas they will become so widespread in everyday life that trying to restrict their use will\\nbe futile. Instead, we need to find ways to embrace the technology and ask how we\\ncan design open-AI coursework, in the same way that we allow open-book course‐\\nwork, and encourage students to openly research material using the internet and AI\\ntools.\\nThe potential for generative AI to assist with the learning process itself is also\\nimmense and deeply profound. An AI-powered tutor could help a student learn a\\nnew topic (as shown in Figure 14-10), overcome a misunderstanding, or generate an\\nentirely personalized study plan. The challenge of filtering truth from generated fic‐\\ntion is no different from what we currently have with information available on the\\ninternet and is a life skill that needs further attention across the curriculum.\\n410 \\n| \\nChapter 14: Conclusion'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 438}, page_content='Figure 14-10. Output from GPT-3—an example of how large language models can be\\nused for learning\\nGenerative AI can be an incredibly powerful tool to level the playing field between\\nthose who have access to excellent teachers and the best learning materials and those\\nwho do not. I am excited to see the progress in this space, as I believe it could unlock\\nmassive amounts of potential across the globe.\\nGenerative AI Ethics and Challenges\\nDespite the incredible progress that has been made in the field of generative AI, there\\nremain many challenges to overcome. Some of these challenges are practical and oth‐\\ners ethical.\\nFor example, a major criticism of large language models is that they are prone to gen‐\\nerate misinformation when asked about a topic that is unfamiliar or contradictory, as\\nshown in Figure 14-4. The danger with this is that it is difficult to know if the infor‐\\nmation that is contained within a generated response is truly accurate. Even if you ask\\nthe LLM to explain its reasoning or cite sources, it might make up references or spout\\na series of statements that do not logically follow on from one another. This is not an\\neasy problem to solve, as the LLM is nothing more than a set of weights that accu‐\\nrately capture the most likely next word given a set of input tokens—it does not have\\na bank of true information that it can use as a reference.\\nA potential solution to this problem is to provide large language models with the abil‐\\nity to call upon structured tools such as calculators, code compilers, and online infor‐\\nmation sources for tasks that require precise execution or facts. For example,\\nFigure 14-11 shows output from a model called Toolformer, published by Meta in\\nFebruary 2023.4\\nThe Future of Generative AI \\n| \\n411'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 439}, page_content='Figure 14-11. An example of how Toolformer is able to autonomously call different APIs\\nin order to obtain precise information where necessary (source: Schick et al., 2023)\\nToolformer is able to explicitly call APIs for information, as part of its generative\\nresponse. For example, it might use the Wikipedia API to retrieve information about\\na particular person, rather than relying on this information being embedded in its\\nmodel weights. This approach is particularly useful for precise mathematical opera‐\\ntions, where Toolformer can state which operations it would like to enter into the cal‐\\nculator API instead of trying to generate the answer autoregressively in the useful\\nfashion.\\nAnother prominent ethical concern with generative AI centers on the fact that large\\ncompanies have used huge amounts of data scraped from the web to train their mod‐\\nels, when consent was not explicitly given by the original creators to do so. Often this\\ndata is not even publicly released, so it is impossible to know if your data is being\\nused to train large language models or multimodal text-to-image models. Clearly this\\nis a valid concern, particularly for artists, who may argue that it is usage of their art‐\\nwork for which they are not being paid any royalties or commission. Moreover, an\\nartist’s name may be used as a prompt in order to generate more artwork that is simi‐\\nlar in style to the originals, thereby degrading the uniqueness of the content and com‐\\nmoditizing the style.\\nA solution to this problem is being pioneered by Stability AI, whose multimodal\\nmodel Stable Diffusion is trained on a subset of the open source LAION-5B dataset.\\n412 \\n| \\nChapter 14: Conclusion'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 440}, page_content='They have also launched the website Have I Been Trained? where anyone can search\\nfor a particular image or text passage within the training dataset and opt out of future\\ninclusion in the model training process. This puts control back in the hands of the\\noriginal creators and ensures that there is transparency in the data that is being used\\nto create powerful tools like this one. However, this practice is not commonplace, and\\nmany commercially available generative AI models do not make their datasets or\\nmodel weights open source or provide any option to opt out of the training process.\\nIn conclusion, while generative AI is a powerful tool for communication, productiv‐\\nity, and learning across everyday life, in the workplace, and in the field of education,\\nthere are both advantages and disadvantages to its widespread use. It is important to\\nbe aware of the potential risks of using the output from a generative AI model and to\\nalways be sure to use it responsibly. Nevertheless, I remain optimistic about the future\\nof generative AI and am eager to see how businesses and people adapt to this new and\\nexciting technology.\\nFinal Thoughts\\nIn this book we have taken a journey through the last decade of generative modeling\\nresearch, starting out with the basic ideas behind VAEs, GANs, autoregressive mod‐\\nels, normalizing flow models, energy-based models, and diffusion models and build‐\\ning upon these foundations to understand how state-of-the-art techniques such as\\nVQ-GAN, Transformers, world models, and multimodal models are now pushing the\\nboundaries of what generative models are capable of achieving, across a variety of\\ntasks.\\nI believe that in the future, generative modeling may be the key to a deeper form of\\nartificial intelligence that transcends any one particular task and allows machines to\\norganically formulate their own rewards, strategies, and perhaps awareness within\\ntheir environment. My beliefs are closely aligned to the principle of active inference,\\noriginally pioneered by Karl Friston. The theory behind active inference could easily\\nfill another entire book—and does, in Thomas Parr et al.’s excellent Active Inference:\\nThe Free Energy Principle in Mind, Brain, and Behavior (MIT Press), which I highly\\nrecommend—so I will only attempt a short explanation here.\\nAs babies, we are constantly exploring our surroundings, building up a mental model\\nof possible futures with no apparent aim other than to develop a deeper understand‐\\ning of the world. There are no labels on the data that we receive—a seemingly ran‐\\ndom stream of light and sound waves that bombard our senses from the moment we\\nare born. Even when our someone points to an apple and says apple, there is no rea‐\\nson for our young brains to associate the two inputs and learn that the way in which\\nlight entered our eye at that particular moment is in some way related to the way the\\nsound waves entered our ear. There is no training set of sounds and images, no train‐\\nFinal Thoughts \\n| \\n413'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 441}, page_content='ing set of smells and tastes, and no training set of actions and rewards; there’s just an\\nendless stream of extremely noisy data.\\nAnd yet here you are now, reading this sentence, perhaps enjoying the taste of a cup\\nof coffee in a noisy cafe. You pay no attention to the background noise as you concen‐\\ntrate on converting the absence of light on a tiny portion of your retina into a\\nsequence of abstract concepts that convey almost no meaning individually but, when\\ncombined, trigger a wave of parallel representations in your mind’s eye—images,\\nemotions, ideas, beliefs, and potential actions all flood your consciousness, awaiting\\nyour recognition. The same noisy stream of data that was essentially meaningless to\\nyour infant brain is not so noisy anymore. Everything makes sense to you. You see\\nstructure everywhere. You are never surprised by the physics of everyday life. The\\nworld is the way that it is because your brain decided it should be that way. In this\\nsense, your brain is an extremely sophisticated generative model, equipped with the\\nability to attend to particular parts of the input data, form representations of concepts\\nwithin a latent space of neural pathways, and process sequential data over time.\\nActive inference is a framework that builds upon this idea to explain how the brain\\nprocesses and integrates sensory information to make decisions and actions. It states\\nthat an organism has a generative model of the world it inhabits, and uses this model\\nto make predictions about future events. In order to reduce the surprise caused by\\ndiscrepancies between the model and reality, the organism adjusts its actions and\\nbeliefs accordingly. Friston’s key idea is that action and perception optimization can\\nbe framed as two sides of the same coin, with both seeking to minimize a single quan‐\\ntity known as free energy.\\nAt the heart of this framework is a generative model of the environment (captured\\nwithin the brain) that is constantly being compared to reality. Crucially, the brain is\\nnot a passive observer of events. In humans, it is attached to a neck and a set of legs\\nthat can put its core input sensors in a myriad of positions relative to the source of the\\ninput data. Therefore, the generated sequence of possible futures is not only depen‐\\ndent on its understanding of the physics of the environment, but also on its under‐\\nstanding of itself and how it acts. This feedback loop of action and perception is\\nextremely interesting to me, and I believe we have only scratched the surface of what\\nis possible with embodied generative models that are able to take actions within a\\ngiven environment according to the principles of active inference.\\nThis is the core idea that I believe will continue to propel generative modeling into\\nthe spotlight in the next decade, as one of the keys to unlocking artificial general\\nintelligence.\\nWith that in mind, I encourage you to continue learning more about generative mod‐\\nels from all the great material that is available online and in other books. Thank you\\nfor taking the time to read to the end of this book—I hope you have enjoyed reading\\nit as much as I have enjoyed generating it!\\n414 \\n| \\nChapter 14: Conclusion'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 442}, page_content='References\\n1. Hugo Touvron et al., “LLaMA: Open and Efficient Foundation Language Models,”\\nFebruary 27, 2023, https://arxiv.org/abs/2302.13971.\\n2. Mark Chen et al., “Evaluating Large Language Models Trained on Code,” July 7,\\n2021, https://arxiv.org/abs/2107.03374.\\n3. Lvmin Zhang and Maneesh Agrawala, “Adding Conditional Control to Text-to-\\nImage Diffusion Models,” February 10, 2023, https://arxiv.org/abs/2302.05543.\\n4. Timo Schick et al., “Toolformer: Language Models Can Teach Themselves to Use\\nTools,” February 9, 2023, https://arxiv.org/abs/2302.04761.\\nFinal Thoughts \\n| \\n415'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 443}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 444}, page_content='Index\\nSymbols\\n1-Lipschitz continuous function, 115\\nA\\naccuracy, determining, 99, 264, 399, 411\\naction, in reinforcement learning, 333\\nactivation functions, 32\\nactive inference, 414\\nAdam (Adaptive Moment Estimation) opti‐\\nmizer, 36\\nadaptive instance normalization (AdaIN), 279,\\n282\\nagent, in reinforcement learning, 333\\nAI (artificial intelligence), 8, 413\\nAI ethics, 411-413\\napproximate density models, 20\\nartifacts, 103, 281\\nartificial intelligence (AI), 8, 413\\nartificial neural networks (ANNs), 25\\narXiv, xxii\\n“Attention Is All You Need” (Vaswani), 236, 394\\nattention mechanisms\\nattention equation, 241\\nattention head, 239\\nattention scores, 253\\nattention weights, 241\\ngenerating polyphonic music, 313\\npaper popularizing, 236\\nself- versus cross-referential, 258\\nunderstanding, 238\\nattribute binding, 376\\nattributes, entangled, 277\\nAudioLM, 407\\n“Auto-Encoding Variational Bayes” (Kingma\\nand Welling), 59\\nautoencoders (see also variational autoencod‐\\ners)\\narchitecture of, 63\\ndecoder architecture, 65-67\\ndiagram of process, 61\\nencoder architecture, 64\\nFashion-MNIST dataset, 62\\ngenerating new images, 71-74\\njoining encoder to decoder, 67\\nreconstructing images, 69\\nuses for, 64\\nvisualizing latent space, 70\\nautoregressive models\\nautoregressive prior of DALL.E 2, 367, 374\\nbidirectional cells, 153\\ndescription of, 130\\ngated recurrent units (GRUs), 151-153\\ngenerative model taxonomy, 19\\nhistory of, 394\\nhow LSTMs work, 130\\nlong short-term memory (LSTM) networks,\\n131-149\\nmasked convolutional layers, 154-156\\nstacked recurrent networks, 149-151\\nB\\nBach chorale dataset, 317\\nbackpropagation, 26\\nbatch normalization, 46-49, 51, 120\\nbatches, 37\\nBERT (Bidirectional Encoder Representations\\nfrom Transformers), 255, 394\\n417'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 445}, page_content='bidirectional cells, 153\\nbig model era, 395\\nBigGAN, 288, 395\\nbinary cross-entropy loss, 36\\nBLOOM, 395, 399\\nBoltzmann distribution, 191\\nBoltzmann machine, 203\\nBookCorpus, 236\\nBricks dataset, 98\\nC\\nCanny edge maps, 402\\ncategorical cross-entropy loss, 36\\ncausal masking, 242-244\\nCelebFaces Attributes (CelebA) dataset, 85, 275\\nCGAN (see conditional GAN)\\nchallenges, of generative AI, 264, 399, 411-413\\nchange of variables equation, 173\\nchange of variables technique, 170-172\\ncharacter tokens, 135\\nChatGPT, 260-264, 395, 407\\nChinchilla, 395\\nCICERO, 406\\nCIFAR-10 dataset, 28\\nCLIP (Contrastive Language-Image Pre-\\ntraining)\\ndescription of, 362\\nhistory of, 396\\nkey concepts behind, 363\\ntraining process, 363-367\\nCMA-ES (covariance matrix adaptation evolu‐\\ntion strategy), 348-353\\nCNN (see convolutional neural networks)\\ncode examples, obtaining and using, xxiii, 20\\ncodebook, 290\\nCodex, 400\\ncomments and questions, xxv\\ncompile method, 36\\nconditional GAN (CGAN)\\nanalysis of, 126\\narchitecture of, 123\\ntraining, 124\\n“Conditional Generative Adversarial Nets”\\n(Mirza and Osindero), 122\\ncontext vector, 241\\ncontrastive divergence, 191, 197-201\\nContrastive Language-Image Pre-training (see\\nCLIP)\\ncontrastive learning, 362\\nControlNet, 402\\nconvolutional neural networks (CNNs)\\nbatch normalization, 46-49\\nbenefits of, 40\\nbuilding, 51-53\\nconvolutional layers, 41-46\\ndropout, 49-51\\nmasked convolutional layers, 154-156\\ntraining and evaluating, 53\\nconvolutional transpose layers, 65\\nCopilot, 400\\ncosine diffusion schedule, 212\\ncosine similarity, 363\\ncoupling layers, 175-177, 180\\ncovariate shift, 47\\ncross-referential attention, 258\\nCycleGAN, 291, 394\\nD\\nDALL.E, 289, 361, 376, 396\\nDALL.E 2\\narchitecture, 362\\navailability of, 402\\ndecoder, 369-373\\nexamples generated by, 373-375, 405\\nhistory of, 361, 396\\nlimitations of, 375\\ntext encoder, 362-367\\ntraining the prior model, 367-369\\nDCGAN (see deep convolutional GAN)\\nDDIM (see Denoising Diffusion Implicit\\nModel)\\nDDM (see denoising diffusion models)\\nDDPM (see Denoising Diffusion Probabilistic\\nModel)\\ndecoder Transformers, 244, 255\\ndeep convolutional GAN (DCGAN)\\nanalysis of, 109\\ndataset used, 98\\ndiscriminator in, 99-101\\ngenerator in, 101-103\\nhistory of, 394\\npublished paper on, 97\\ntraining, 104-109\\ntraining tips and tricks, 110-113\\ndeep learning\\ndeep neural networks, 25-27, 54\\ndefined, 23\\nKeras and TensorFlow for, 27\\n418 \\n| \\nIndex'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 446}, page_content='model creation, 28-40\\nmodel improvement, 40-54\\nstructured versus unstructured data, 24\\ndemodulation step, 283\\nDenoising Diffusion Implicit Model (DDIM)\\ndescription of, 226\\nhistory of, 395\\ndenoising diffusion models (DDMs)\\nanalysis of, 228-230\\ndataset used, 208-209\\ndescription of, 208\\ndiffusion schedules, 211\\nforward diffusion process, 209\\nreparameterization trick, 210\\nreverse diffusion process, 214-216\\nsampling from, 225-227\\ntraining, 224-225\\nU-Net denoising model, 217-224, 370\\nDenoising Diffusion Probabilistic Model\\n(DDPM)\\ndevelopment of, 203\\nhistory of, 395\\ndenoising models, 64 (see also denoising diffu‐\\nsion models)\\ndense layers, 25\\ndensity function, 16, 18\\ndeterminants, 172\\nDhariwal, Prafulla, 395\\ndiffusion models (see also denoising diffusion\\nmodels)\\ndescription of, 205\\ndiffusion prior of DALL.E 2, 368\\ngenerative model taxonomy, 20\\nhistory of, 395\\nkey ideas underpinning, 206-207\\nlatent diffusion, 396\\n“Diffusion Models Beat GANs on Image Syn‐\\nthesis ” (Dhariwal and Nichol), 395\\ndiffusion schedules, 211\\nDinh, Laurent, 174\\nDiplomacy board game, 406\\ndiscrete latent space, 290\\ndiscriminative modeling, 5\\ndiscriminators, 97, 99-101, 110\\nDong, Hae-Wen, 317\\nDrawBench, 378\\nDreamFusion, 407\\nDreamix, 406\\ndropout layers, 49\\nDu, Yilun, 191\\nE\\neducational applications, 410\\nEMA (exponential moving average), 214\\nembedding space, 63, 70, 85-93, 219\\nembodied large language models, 406\\nencoder Transformers, 244, 255\\nencoder-decoder Transformers, 255\\nencoders, 64\\nenergy function/energy score, 191, 193\\nenergy-based models (EBMs)\\nanalysis of, 201-202\\nBoltzmann distribution, 191\\nBoltzmann machine, 203\\ndataset used, 192\\ndescription of, 189\\nenergy function, 193\\ngenerative model taxonomy, 20\\nkey concepts behind, 189\\nRBM (restricted Boltzmann machine), 203\\nsampling using Langevin dynamics, 194-196\\nscore-based generative models, 206\\ntraining with contrastive divergence,\\n197-201\\nentangled attributes, 277\\nenvironment, in reinforcement learning, 332\\nepisode, in reinforcement learning, 333\\nepochs, 38\\nequalized learning rates, 276\\nethical concerns, of generative AI, 411-413\\nevaluate method, 38\\nevent-based tokenization, 315\\nevolutionary strategies, 349\\nexplicit density models, 19\\nexploding gradient problem, 46\\nexponential moving average (EMA), 214\\nF\\nfacial image generation\\ndataset used, 85\\ngenerating new faces, 90\\nlatent space arithmetic, 91\\nmorphing between faces, 92\\nprogress in, 7\\nVAE analysis, 89\\nVAE training, 87\\nFashion-MNIST dataset, 62, 70\\nfast-track route, 156\\nIndex \\n| \\n419'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 447}, page_content='feature engineering, 26\\nfeatures\\ndescriptions of, 4\\nlearning high-level, 26\\nFFJORD (Free-Form Continuous Dynamics for\\nScalable Reversible Generative Models), 187\\nfilters, 41\\nfit method, 37\\nFlamingo\\narchitecture, 382-387\\nexamples generated by, 388-389\\nhistory of, 396\\nFlan-T5, 395\\nFlowers dataset, 208\\nforward diffusion process, 209\\nforward pass, 26\\nFree-Form Continuous Dynamics for Scalable\\nReversible Generative Models (FFJORD),\\n187\\nfully connected layers, 25\\nfunctional API (Keras), 30-35\\nG\\ngame state, in reinforcement learning, 333\\nGAN (see generative adversarial networks)\\ngated recurrent units (GRUs), 132, 151, 394\\nGaussian distribution (normal distribution), 75\\nGenAI (see generative AI)\\n“Generative Adversarial Nets” (Goodfellow), 95\\ngenerative adversarial networks (GANs)\\nBigGAN, 288\\nchallenges of, 110, 113\\nconditional GAN (CGAN), 122-127\\ndeep convolutional GANs (DCGANs),\\n97-113\\nfundamental training concepts, 96\\ngenerative model taxonomy, 19\\nhistory of, 95, 394\\nProGAN, 269-276\\nStyleGAN, 277-281\\nStyleGAN2, 281-286\\nWasserstein GAN with Gradient Penalty\\n(WGAN-GP), 113-122\\nversus WGAN-GPs, 121\\ngenerative AI (GenAI)\\ncurrent state of, 396-407\\nethics and challenges related to, 264, 411\\nfuture of, 407-411, 413\\nhistory of, 392-396\\ngenerative deep learning (see also models)\\nadditional resources, xxii\\ncore probability theory, 15-18\\ncreating something that is creative, xvii\\ngenerative modeling framework, 10\\nintroduction to, 4-9\\nlearning objectives and approach, xviii\\nprerequisites to learning, xix\\ngenerative modeling (see models)\\ngenerative pre-trained transformer (see GPT)\\ngenerators\\nattention-based, 287\\nbar generator, 323\\nDCGAN generator, 101-103, 110\\nin GANs, 97, 101-103\\nMuseGAN generator, 320\\nStyleGAN generator, 277\\nGitHub Copilot, 400\\nGLIDE (Guided Language-to-Image Diffusion\\nfor Generation and Editing), 369-373, 396\\nGLOW model, 186\\nGoodfellow, Ian, 95\\nGopher, 395\\nGPT (generative pre-trained transformer)\\nanalysis of, 252-255\\napplications in everyday life, 407\\nattention mechanism, 238\\ncausal masking, 242-244\\ndataset used, 237\\ndescription of, 236\\nevolution of, 259\\nhistory of, 236, 394\\nimprovements to, 237\\nmultihead attention, 241\\npositional encoding, 248-250\\nqueries, keys, and values, 239-241\\nTransformer block, 245-248\\nGPT-2, 237, 394\\nGPT-3\\navailability of, 398\\nbenefits of, 237\\nevolution of, 259\\nexample generated by, 260, 396, 411\\nhistory of, 395\\nGPT-3.5, 237, 262\\nGPT-4, 237, 259\\ngradient descent using Langevin dynamics, 195\\ngradient penalty loss, 117\\nGradient Tape, 83\\n420 \\n| \\nIndex'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 448}, page_content='grid tokenization, 313-315\\nGRU (see gated recurrent units)\\nGuided Language-to-Image Diffusion for Gen‐\\neration and Editing (GLIDE), 369-373\\nH\\nHa, David, 331, 337, 394\\nhallucinations, 264\\nHe initialization, 276\\nhidden layers, 27\\nhidden state, 140, 142\\nHinton, Geoffrey, 49\\nHochreiter, Sepp, 132\\nHuang, Cheng-Zhi Anna, 313\\nHui, Jonathan, 116\\nhyperparameters, 112\\nI\\nimage generation (see also facial image genera‐\\ntion; PixelCNN)\\nbenefits of diffusion models for, 205\\nBigGAN, 288\\nCIFAR-10 dataset for, 28\\nDDM analysis, 228-230\\ngenerating new images, 71-74\\ngenerative modeling process, 4\\ngenerative versus discriminative modeling,\\n5\\nhistory of, 395\\nProGAN, 269-276\\nprogress in facial image generation, 7\\nreconstructing images, 69\\nrepresentation learning for, 13\\nSelf-Attention GAN (SAGAN), 286\\nStyleGAN2, 281-286\\nvisualizing latent space, 70\\nimage-to-image models, 291, 394\\nImagen\\narchitecture, 377\\nDrawBench, 378\\nexamples generated by, 379\\nhistory of, 377, 396\\noverview of, 405\\nImages of LEGO Bricks dataset, 98\\nimplicit density models, 19\\n“Implicit Generation and Modeling with\\nEnergy-Based Models” (Du and Mordatch),\\n191\\n“Improving Language Understanding by Gen‐\\nerative Pre-Training” (Radford), 236\\nin-dream training, 353-356\\nInstructGPT model, 262\\nisotropic multivariate normal distributions, 76\\nJ\\nJacobian determinant, 172\\njoint token/position encoding, 249\\nK\\nKaggle, 86, 237\\nKeras (see also models)\\nautoencoder creation in, 65\\nbenefits of, 27\\nConv2DTranspose layer, 66\\ncreating new layers in, 79\\ndata loading, 28\\ndataset creation, 98\\ndecoder creation in, 67\\ndocumentation, 36\\nGAN discriminator creation in, 100\\nmodel building, 30-35\\nmodel compilation, 35\\nmodel evaluation, 38-40\\nmodel improvement, 40-54\\nmodel training, 37\\nMuseGAN generator in, 324\\nresources, 20\\nStyleGAN tutorial, 278\\nVAE creation in, 78\\nKeras layers\\nActivation, 52\\nBatch Normalization, 46\\nBidirectional, 153\\nConv2D, 42\\nConv2DTranspose, 66, 103\\nConv3D, 326\\nDense, 32\\nDropout, 49\\nEmbedding, 138\\nFlatten, 32\\nGRU, 132\\nInput, 32\\nLeakyReLU, 52\\nLSTM, 140\\nMultiHeadAttention, 241\\nUpSampling2D, 103\\nKeras NLP module, 306\\nIndex \\n| \\n421'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 449}, page_content='kernels, 41\\nkey vectors, 239\\nKingma, Diederik, 59\\nKullback–Leibler (KL) divergence, 80\\nL\\nlabel smoothing, 108\\nLAION-5B dataset, 402\\nLaMDA, 395\\nLangevin dynamics, 191, 194-196\\nlanguage modeling, 236\\nLarge Language Model Meta AI (LLaMA), 398\\nlarge language models (LLMs), 396-400\\nLarge-scale Scene Understanding (LSUN) data‐\\nset, 276\\nlatent diffusion, 380, 396\\nlatent space, 63, 70, 85-93\\nlayer normalization, 245\\nlayers, 25, 79 (see also Keras layers)\\nlazy regularization, 284\\nLeakyReLU, 33\\nlearning rate, 36\\nlikelihood, 17\\nlinear diffusion schedule, 211\\nLipschitz constraint, 115, 116\\nLLaMA (Large Language Model Meta AI), 398\\nLLMs (large language models), 396-400\\nlogarithm of the variance, 77\\nlong short-term memory networks (see LSTM\\nnetworks)\\nloss functions, 35, 68, 80, 195\\nlower triangle matrix, 178\\nLSTM (long short-term memory) networks\\nembedding layer, 138\\ngenerating datasets, 137\\ngenerating new text, 146\\nhistory of, 131, 394\\nLSTM architecture, 138\\nLSTM cell, 142-144\\nLSTM layer, 140-142\\npublished paper on, 132\\ntokenizing the text, 134\\nLSUN (Large-scale Scene Understanding) data‐\\nset, 276\\nM\\nmachine learning\\nbenefits of, 13\\ndata for, 24-25\\ndropout principle, 49\\ngenerative modeling and, 4-7\\nlibraries for, 27\\nmajor branches of, 23, 28, 332\\nresources, xxii\\nMake-A-Video, 406\\nmapping network f, 278\\nmasked convolutional layers, 154-156\\nmasking, causal, 242-244\\nmatrix determinants, 172\\nmaximum likelihood estimation, 18\\nMDN (mixture density network), 337, 346\\nmean squared error loss, 35\\nMegatron-Turing NLG, 395, 398\\nmetrics parameter, 36\\nMIDI files, 300\\nMidjourney, 60, 96, 402, 405\\nMildenhall, Ben, 219\\nminibatch standard deviation layer, 275\\nMirza, Mehdi, 122\\nmixture distributions, 162-164\\nMLP (see multilayer perceptrons)\\nMNIST dataset, 192\\nmode collapse, 111\\nmodel.summary() method, 34, 45\\nmodels (see also generative deep learning;\\nKeras)\\ncore probability theory, 15-18\\ndeep neural networks, 25-28\\ngenerative model taxonomy, 18\\ngenerative modeling, 4\\ngenerative versus discriminative modeling,\\n5\\nhistory of, 395\\nimproving, 40-54\\nparametric modeling, 16\\nprobabilistic versus deterministic, 4\\nvariational autoencoders (VAEs), 59\\nWorld Model architecture, 336-356\\nmodulation step, 283\\nMordatch\\nIgor, 191\\nmultihead attention, 241\\nmultilayer perceptrons (MLPs)\\ndata preparation, 28\\nexample of, 25\\nmodel building, 30-35\\nsupervised learning and, 28\\nmultilayer RNNs, 149\\n422 \\n| \\nIndex'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 450}, page_content='multimodal models\\nchallenges of text-to-image generation, 360\\nDALL.E 2, 361-376\\nFlamingo, 381-389\\nhistory of, 396\\nImagen, 377-380\\nStable Diffusion, 380\\nmultivariate normal distribution, 75\\nMUSE, 396, 405\\nMuseGAN, 317-329\\nanalysis of, 327\\ndataset used, 317\\nMuseGAN critic, 326\\nMuseGAN generator, 320\\n“MuseGAN: Multi-Track Sequential Generative\\nAdversarial Networks for Symbolic Music\\nGeneration and Accompaniment” (Dong),\\n317\\nMuseNet, 394\\nMuseScore, 300\\nmusic generation\\nanalysis of music generation Transformer,\\n309-312\\ndataset used, 300\\ngenerating polyphonic music, 313\\nimporting MIDI files, 300\\ninputs and outputs, 307\\nMuseGAN, 317-329\\nmusic versus text generation, 298\\nprerequisites to, 300\\nsine position encoding, 305-306\\ntokenization, 303\\ntraining set for, 304\\nTransformers applied to, 394\\nMusic Transformer, 394\\n“Music Transformer: Generating Music with\\nLong-Term Structure” (Huang), 313\\nmusic21 library, 300\\nMusicLM, 407\\nN\\nNain, Aakash Kumar, 113\\nnatural language processing (NLP), 255\\nNCSN (Noise Conditional Score Network), 395\\n“NeRF: Representing Scenes as Neural Radi‐\\nance Fields for View Synthesis” (Milden‐\\nhall), 219\\n“Neural Discrete Representation Learning”\\n(van den Oord), 289\\nneural networks (see also convolutional neural\\nnetworks; deep learning)\\ndeep neural networks, 54\\ndefined, 25-27\\nloss functions and, 18\\nrole in deep learning, 20\\nusing Keras to build, 27, 30-35\\nNichol, Alex, 395\\nNLP (natural language processing), 255\\nNoise Conditional Score Network (NCSN), 395\\nnoise, adding to labels, 108\\nnontrainable parameters, 48\\nnormal distribution (Gaussian distribution), 75\\nnormalizing flow models\\nchange of variables equation, 173\\nchange of variables technique, 170-172\\ndescription of, 167\\nFFJORD (Free-Form Continuous Dynamics\\nfor Scalable Reversible Generative Mod‐\\nels), 187\\ngenerative model taxonomy, 19\\nGLOW, 186\\nJacobian determinant, 172\\nkey concepts behind, 168\\nmotivation of, 169\\nRealNVP model, 174-185\\nO\\nobservations, 4\\nOPT, 395, 399\\noptimizers, 35\\nOsindero, Simon, 122\\noverfitting, 49\\nOxford 102 Flower dataset, 208\\nP\\npadding, 43\\nPaLM-E, 406\\nPapers with Code, xxii\\nparameters, trainable and nontrainable, 48\\nparametric modeling, 16\\nParti, 396, 405\\nPatchGAN, 291\\npath length regularization, 283\\nperceptual loss term, 292\\npersonal assistants, 407\\npiano roll grid, 314\\npix2pix, 291, 394\\nPixelCNN\\nIndex \\n| \\n423'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 451}, page_content='analysis of, 159-162\\nhistory of, 153, 394\\nmasked convolutional layers, 154-156\\nmixture distributions, 162\\nresidual blocks, 156-158\\ntraining, 158\\nPixelRNN, 394\\npixelwise normalization, 276\\npoetry, 397\\nPoint-E, 407\\npositional embedding, 248\\npositional encoding, 248-250\\nposterior collapse, 289\\nprediction, using batch normalization, 48\\nprobability density function, 16, 18, 75\\nprobability distributions, 162\\nprobability theory, 15-18\\nProGAN\\nconcept of progressive training, 269-276\\ndescription of, 269\\nhistory of, 394\\noutputs, 276\\nprogressive training, 269-276\\nprompt engineering, 405\\nprompts, 396\\nQ\\nquery, 239\\nquestions and comments, xxv\\nR\\nRadford, Alec, 97, 236\\nrandom (stochastic) elements, 4\\nrandom noise, 108, 195\\nRBM (restricted Boltzmann machine), 203\\nRealNVP\\nanalysis of, 184\\ncoupling layers, 175-177\\ndataset used, 174\\ndescription of, 174\\nhistory of, 394\\npassing data through coupling layers,\\n177-180\\nstacking coupling layers, 180\\ntraining, 181-183\\nRecipes dataset, 132\\nrecurrent neural networks (see RNNs)\\nregularization techniques, 49\\nreinforcement learning (RL)\\nChatGPT and, 262\\ndefined, 332\\nkey terminology, 332\\nprocess of, 333-335\\nReinforcement Learning from Human Feed‐\\nback (RLHF), 262\\nReLU (rectified linear unit), 33\\nreparameterization trick, 79, 210\\nrepresentation learning, 13-14\\nresidual blocks, 156-158, 221-222\\nrestricted Boltzmann machine (RBM), 203\\nreverse diffusion process, 214-216\\nreward modeling, 262\\nreward, in reinforcement learning, 333\\nRLHF (Reinforcement Learning from Human\\nFeedback), 262\\nRMSE (root mean squared error), 68\\nRMSProp (Root Mean Squared Propagation)\\noptimizer, 36\\nRNNs (recurrent neural networks)\\nbidirectional cells, 153\\ngated recurrent units (GRUs), 151\\nhistory of, 131\\nLSTM (long short-term memory) networks,\\n131-149\\nMDN-RNN World Model architecture, 337\\nmultilayer, 149\\nstacked recurrent networks, 149\\nroot mean squared error (RMSE), 68\\nRoot Mean Squared Propagation (RMSProp)\\noptimizer, 36\\nS\\nSAGAN (self-attention GAN), 286, 395\\nsample space, 16\\nscaling streams, 177\\nSchmidhuber, Jurgen, 132, 331, 394\\nscore matching technique, 203\\nscore-based generative models, 206\\nSelf-Attention GAN (SAGAN), 286, 395\\nself-referential layers, 258\\nSequential models (Keras), 30-35\\nsigmoid activation, 33\\nsine position embedding, 305\\nsinusoidal embedding, 219\\nskip connections, 156, 217, 245, 284\\nsoftmax activation, 33\\nSparse Transformers, 299\\nstabilization phase, 272\\n424 \\n| \\nIndex'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 452}, page_content='Stable Diffusion\\nadvantages of, 402-405\\narchitecture, 380\\nexamples generated by, 381\\nhistory of, 380, 396\\nstacked recurrent networks, 149\\nstandard deviation, 75\\nstandard normal curves, 75\\nstemming, 135\\nstochastic (random) elements, 4\\nstochastic gradient Langevin dynamics, 195\\nstochastic variation, 280\\nstrides parameter (Keras), 43\\nstructured data, 24\\nstyle mixing, 279\\nStyleGAN, 277-281, 395\\nStyleGAN-XL, 286, 395\\nStyleGAN2, 281-286, 395\\nsubclassing layers, 79\\nsummary method, 35\\nsupervised fine-tuning, 262\\nsupervised learning, 28, 332\\nswish activation, 193\\nsynthesis network, 279\\nT\\nT5, 256-259, 394\\ntape.gradient() method, 83\\ntaxonomy, 18\\ntemperature parameter, 146\\ntemporal networks, 320\\nTensorFlow, 27\\ntext data generation (see also GPT)\\nLSTM (long short-term memory) networks,\\n131-149\\nRNN (recurrent neural network) exten‐\\nsions, 149-153\\nshort story generation example, 130\\ntext versus image data, 133\\ntext versus music generation, 298\\ntext-to-3D models, 407\\ntext-to-code models, 400-400\\ntext-to-image models, 360, 402-405\\ntext-to-music models, 407\\ntext-to-video models, 406\\ntext-to-X multimodal models, 405\\nthermodynamic diffusion, 206\\ntimeline of AI, 393\\ntimestep, in reinforcement learning, 333\\ntoken embedding, 248\\ntokenization\\nevent-based, 315\\ngrid, 313-315\\nof notes for music generation, 303\\nprocess of, 134-137\\nToolformer, 411\\ntractable models, 19\\ntrainable parameters, 48\\ntraining data, 4\\ntraining process, 26\\nTransformer block, 245-248\\nTransformers (see also GPT; music generation)\\narchitectures for, 255\\nBERT (Bidirectional Encoder Representa‐\\ntions from Transformers), 255\\nChatGPT, 260-264\\ndecoder versus encoder, 244\\ndescription of, 236\\nGPT-3 and GPT-4, 259\\nhistory of, 394\\nSparse Transformers, 299\\nT5, 256-259\\ntransition phase, 272\\ntranslation streams, 177\\ntruncated normal distribution, 288\\ntruncation trick, 288\\ntruth, filtering from generated fiction, 99, 264,\\n399, 410\\ntwo moons dataset, 174\\nU\\nU-Net denoising model, 217-224, 370\\nuninformative loss, 112\\nunit normal curves, 75\\nunits, 25, 140\\nunstructured data, 24\\nunsupervised learning, 332\\n“Unsupervised Representation Learning with\\nDeep Convolutional Generative Adversarial\\nNetwork” (Radford), 97\\nupsampling, 103, 372\\nV\\nVAE (see variational autoencoders)\\nVAE with a GAN discriminator (VAE-GAN),\\n394\\nvalue vectors, 241\\nvan den Oord, Aaron, 153, 289\\nIndex \\n| \\n425'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 453}, page_content='vanishing gradient problem, 132\\nvariance, 75\\nvariational autoencoders (VAEs) (see also\\nautoencoders)\\nanalysis of, 84\\nautoencoder architecture, 61-74\\ndecoders, 77\\nencoder adjustments, 75-78\\nfacial image generation using, 85-93\\ngenerative model taxonomy, 20\\nhistory of, 394\\nintroduction to, 60, 74\\npublished paper on, 59\\ntraining, 82\\nVAE build in Keras, 78\\nVAE loss function, 80\\nVAE model summary, 80\\nWorld Model architecture, 336\\nWorld Model training, 340-344\\nVaswani, Ashish, 219, 236, 394\\nVector Quantized Generative Adversarial Net‐\\nwork (VQ-GAN), 289-292, 395\\nVector Quantized VAE (VQ-VAE), 394\\n“Vector-quantized Image Modeling with\\nImproved VQGAN” (Yu), 292\\nVision Transformer (ViT), 292, 364, 395\\nVisual ChatGPT, 264\\nvocabulary, 135\\nVQ-GAN ( Vector Quantized Generative\\nAdversarial Network), 289-292, 395\\nVQ-VAE (Vector Quantized VAE), 394\\nW\\nWasserstein GAN with Gradient Penalty\\n(WGAN-GP)\\nanalysis of, 121\\ngradient penalty loss, 117\\nLipschitz constraint, 115\\nversus standard GANs, 121\\ntraining, 119\\ntutorial on, 113\\nWasserstein loss, 114\\nweight clipping, 116\\nWasserstein GANs (WGANs)\\nbenefits of, 113\\nhistory of, 394\\nweight clipping, 116\\nweight modulation and demodulation, 282-283\\nweights, 25\\nWelling, Max, 59\\nWGAN (see Wasserstein GANs)\\nWine Reviews dataset, 237\\nworkplace applications, 409\\nWorld Models\\narchitecture, 336-338\\ncollecting MDN-RNN training data, 346\\ncollecting random rollout data, 339\\npublished paper on, 331, 337, 394\\ntraining in-dream, 353-356\\ntraining process, 338\\ntraining the controller, 348-353\\ntraining the MDN-RNN, 346-348\\ntraining the VAE, 340-344\\nWorld Model architecture, 336-356\\nY\\nYu, Jiahui, 292\\nZ\\nzero-shot prediction, 364\\n426 \\n| \\nIndex'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 454}, page_content='About the Author\\nDavid Foster is a data scientist, entrepreneur, and educator specializing in AI appli‐\\ncations within creative domains. As cofounder of Applied Data Science Partners\\n(ADSP), he inspires and empowers organizations to harness the transformative\\npower of data and AI. He holds an MA in Mathematics from Trinity College, Cam‐\\nbridge, an MSc in Operational Research from the University of Warwick, and is a fac‐\\nulty member of the Machine Learning Institute, with a focus on the practical\\napplications of AI and real-world problem solving. His research interests include\\nenhancing the transparency and interpretability of AI algorithms, and he has pub‐\\nlished literature on explainable machine learning within healthcare.\\nColophon\\nThe animal on the cover of Generative Deep Learning is a painted parakeet (Pyrrhura\\npicta). The Pyrrhura genus falls under the family Psittacidae, one of three families of\\nparrots. Within its subfamily Arinae are several macaw and parakeet species of the\\nWestern Hemisphere. The painted parakeet inhabits the coastal forests and moun‐\\ntains of northeastern South America.\\nBright green feathers cover most of a painted parakeet, but they are blue above the\\nbeak, brown in the face, and reddish in the breast and tail. Most strikingly, the feath‐\\ners on the painted parakeet’s neck look like scales; the brown center is outlined in off-\\nwhite. This combination of colors camouflages the birds in the rainforest.\\nPainted parakeets tend to feed in the forest canopy, where their green plumage masks\\nthem best. They forage in flocks of 5 to 12 birds for a wide variety of fruits, seeds, and\\nflowers. Occasionally, when feeding below the canopy, painted parakeets will eat algae\\nfrom forest pools. They grow to about 9 inches in length and live for 13 to 15 years. A\\nclutch of painted parakeet chicks—each of which are less than an inch wide at hatch‐\\ning—is usually around five eggs.\\nMany of the animals on O’Reilly’s covers are endangered; all of them are important to\\nthe world.\\nThe cover illustration is by Karen Montgomery, based on a black and white engraving\\nfrom Shaw’s Zoology. The cover fonts are Gilroy Semibold and Guardian Sans. The\\ntext font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the\\ncode font is Dalton Maag’s Ubuntu Mono.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2023-04-28T15:37:09+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Generative Deep Learning Teaching Machines to Paint Write Compose and Play 2nd ed.pdf', 'total_pages': 456, 'format': 'PDF 1.6', 'title': 'Generative Deep Learning', 'author': 'David Foster;', 'subject': '', 'keywords': '', 'moddate': '2023-04-28T11:49:28-04:00', 'trapped': '', 'modDate': \"D:20230428114928-04'00'\", 'creationDate': 'D:20230428153709Z', 'page': 455}, page_content='Learn from experts. \\nBecome one yourself.\\nBooks | Live online courses  \\nInstant Answers | Virtual events\\nVideos | Interactive learning\\nGet started at oreilly.com. \\n©2022 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc. | 175'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 0}, page_content='DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\\nReinforcement Learning\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\\nreasoning behaviors. However, it encounters challenges such as poor readability, and language\\nmixing. To address these issues and further enhance reasoning performance, we introduce\\nDeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\\nR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\\nresearch community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\\n(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nGPQA Diamond\\n(Pass@1)\\nMATH-500\\n(Pass@1)\\nMMLU\\n(Pass@1)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100\\nAccuracy / Percentile (%)\\n79.8\\n96.3\\n71.5\\n97.3\\n90.8\\n49.2\\n79.2\\n96.6\\n75.7\\n96.4\\n91.8\\n48.9\\n72.6\\n90.6\\n62.1\\n94.3\\n87.4\\n36.8\\n63.6\\n93.4\\n60.0\\n90.0\\n85.2\\n41.6\\n39.2\\n58.7\\n59.1\\n90.2\\n88.5\\n42.0\\nDeepSeek-R1\\nOpenAI-o1-1217\\nDeepSeek-R1-32B\\nOpenAI-o1-mini\\nDeepSeek-V3\\nFigure 1 | Benchmark performance of DeepSeek-R1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 1}, page_content='Contents\\n1\\nIntroduction\\n3\\n1.1\\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n4\\n1.2\\nSummary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n4\\n2\\nApproach\\n5\\n2.1\\nOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5\\n2.2\\nDeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . .\\n5\\n2.2.1\\nReinforcement Learning Algorithm\\n. . . . . . . . . . . . . . . . . . . . . .\\n5\\n2.2.2\\nReward Modeling\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.2.3\\nTraining Template\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n2.2.4\\nPerformance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\\n6\\n2.3\\nDeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . .\\n9\\n2.3.1\\nCold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n9\\n2.3.2\\nReasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . .\\n10\\n2.3.3\\nRejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . .\\n10\\n2.3.4\\nReinforcement Learning for all Scenarios . . . . . . . . . . . . . . . . . . .\\n11\\n2.4\\nDistillation: Empower Small Models with Reasoning Capability . . . . . . . . . .\\n11\\n3\\nExperiment\\n11\\n3.1\\nDeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n13\\n3.2\\nDistilled Model Evaluation\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n4\\nDiscussion\\n14\\n4.1\\nDistillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n4.2\\nUnsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n15\\n5\\nConclusion, Limitations, and Future Work\\n16\\nA Contributions and Acknowledgments\\n20\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 2}, page_content='1. Introduction\\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and\\nevolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap\\ntowards Artificial General Intelligence (AGI).\\nRecently, post-training has emerged as an important component of the full training pipeline.\\nIt has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt\\nto user preferences, all while requiring relatively minimal computational resources against\\npre-training. In the context of reasoning capabilities, OpenAI’s o1 (OpenAI, 2024b) series models\\nwere the first to introduce inference-time scaling by increasing the length of the Chain-of-\\nThought reasoning process. This approach has achieved significant improvements in various\\nreasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge\\nof effective test-time scaling remains an open question for the research community. Several prior\\nworks have explored various approaches, including process-based reward models (Lightman\\net al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),\\nand search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh\\net al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning\\nperformance comparable to OpenAI’s o1 series models.\\nIn this paper, we take the first step toward improving language model reasoning capabilities\\nusing pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop\\nreasoning capabilities without any supervised data, focusing on their self-evolution through\\na pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ\\nGRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.\\nDuring training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting\\nreasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance\\non reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to\\n71.0%, and with majority voting, the score further improves to 86.7%, matching the performance\\nof OpenAI-o1-0912.\\nHowever, DeepSeek-R1-Zero encounters challenges such as poor readability, and language\\nmixing. To address these issues and further enhance reasoning performance, we introduce\\nDeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training\\npipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the\\nDeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-\\nZero. Upon nearing convergence in the RL process, we create new SFT data through rejection\\nsampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains\\nsuch as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.\\nAfter fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking\\ninto account prompts from all scenarios. After these steps, we obtained a checkpoint referred to\\nas DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\\nWe further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-\\n32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying\\nRL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru-\\ncial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey\\net al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source\\nQwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a\\nnew record on the reasoning benchmarks among dense models.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 3}, page_content='1.1. Contributions\\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\\n• We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as\\na preliminary step. This approach allows the model to explore chain-of-thought (CoT) for\\nsolving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-\\nR1-Zero demonstrates capabilities such as self-verification, reflection, and generating\\nlong CoTs, marking a significant milestone for the research community. Notably, it is the\\nfirst open research to validate that reasoning capabilities of LLMs can be incentivized\\npurely through RL, without the need for SFT. This breakthrough paves the way for future\\nadvancements in this area.\\n• We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL\\nstages aimed at discovering improved reasoning patterns and aligning with human pref-\\nerences, as well as two SFT stages that serve as the seed for the model’s reasoning and\\nnon-reasoning capabilities. We believe the pipeline will benefit the industry by creating\\nbetter models.\\nDistillation: Smaller Models Can Be Powerful Too\\n• We demonstrate that the reasoning patterns of larger models can be distilled into smaller\\nmodels, resulting in better performance compared to the reasoning patterns discovered\\nthrough RL on small models. The open source DeepSeek-R1, as well as its API, will benefit\\nthe research community to distill better smaller models in the future.\\n• Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models\\nthat are widely used in the research community. The evaluation results demonstrate that\\nthe distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-\\nR1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi-\\ntionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500,\\nand 57.2% on LiveCodeBench. These results significantly outperform previous open-\\nsource models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B,\\n32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\\n1.2. Summary of Evaluation Results\\n• Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly\\nsurpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%,\\nperforming on par with OpenAI-o1-1217 and significantly outperforming other models. (2)\\nOn coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks,\\nas it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in\\nthe competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than\\nDeepSeek-V3, which could help developers in real world tasks.\\n• Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-\\nR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\\nperformance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1\\nsurpasses other closed-source models, demonstrating its competitive edge in educational\\ntasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed\\nwhere OpenAI-o1 surpasses 4o on this benchmark.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 4}, page_content='• Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing,\\ngeneral question answering, editing, summarization, and more. It achieves an impressive\\nlength-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-\\nnaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.\\nAdditionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring\\nlong-context understanding, substantially outperforming DeepSeek-V3 on long-context\\nbenchmarks.\\n2. Approach\\n2.1. Overview\\nPrevious work has heavily relied on large amounts of supervised data to enhance model\\nperformance. In this study, we demonstrate that reasoning capabilities can be significantly\\nimproved through large-scale reinforcement learning (RL), even without using supervised\\nfine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with\\nthe inclusion of a small amount of cold-start data. In the following sections, we present: (1)\\nDeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and\\n(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of\\nlong Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to\\nsmall dense models.\\n2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\\nReinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev-\\nidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works\\nheavily depended on supervised data, which are time-intensive to gather. In this section, we\\nexplore the potential of LLMs to develop reasoning capabilities without any supervised data,\\nfocusing on their self-evolution through a pure reinforcement learning process. We start with a\\nbrief overview of our RL algorithm, followed by the presentation of some exciting results, and\\nhope this provides the community with valuable insights.\\n2.2.1. Reinforcement Learning Algorithm\\nGroup Relative Policy Optimization\\nIn order to save the training costs of RL, we adopt Group\\nRelative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is\\ntypically the same size as the policy model, and estimates the baseline from group scores instead.\\nSpecifically, for each question 𝑞, GRPO samples a group of outputs {𝑜1, 𝑜2, · · · , 𝑜𝐺} from the old\\npolicy 𝜋𝜃𝑜𝑙𝑑and then optimizes the policy model 𝜋𝜃by maximizing the following objective:\\nJ𝐺𝑅𝑃𝑂(𝜃) = E[𝑞∼𝑃(𝑄), {𝑜𝑖}𝐺\\n𝑖=1 ∼𝜋𝜃𝑜𝑙𝑑(𝑂|𝑞)]\\n1\\n𝐺\\n𝐺\\n∑︁\\n𝑖=1\\n\\x12\\nmin\\n\\x12 𝜋𝜃(𝑜𝑖|𝑞)\\n𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞) 𝐴𝑖, clip\\n\\x12 𝜋𝜃(𝑜𝑖|𝑞)\\n𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞) , 1 −𝜀, 1 + 𝜀\\n\\x13\\n𝐴𝑖\\n\\x13\\n−𝛽D𝐾𝐿\\n\\x00𝜋𝜃||𝜋𝑟𝑒𝑓\\n\\x01\\x13\\n,\\n(1)\\nD𝐾𝐿\\n\\x00𝜋𝜃||𝜋𝑟𝑒𝑓\\n\\x01 =\\n𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)\\n𝜋𝜃(𝑜𝑖|𝑞) −log\\n𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)\\n𝜋𝜃(𝑜𝑖|𝑞) −1,\\n(2)\\nwhere 𝜀and 𝛽are hyper-parameters, and 𝐴𝑖is the advantage, computed using a group of\\nrewards {𝑟1, 𝑟2, . . . , 𝑟𝐺} corresponding to the outputs within each group:\\n𝐴𝑖= 𝑟𝑖−m𝑒𝑎𝑛({𝑟1, 𝑟2, · · · , 𝑟𝐺})\\ns𝑡𝑑({𝑟1, 𝑟2, · · · , 𝑟𝐺})\\n.\\n(3)\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 5}, page_content='A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\\nThe assistant first thinks about the reasoning process in the mind and then provides the user\\nwith the answer. The reasoning process and answer are enclosed within <think> </think> and\\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\\n<answer> answer here </answer>. User: prompt. Assistant:\\nTable 1 | Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning\\nquestion during training.\\n2.2.2. Reward Modeling\\nThe reward is the source of the training signal, which decides the optimization direction of RL.\\nTo train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two\\ntypes of rewards:\\n• Accuracy rewards: The accuracy reward model evaluates whether the response is correct.\\nFor example, in the case of math problems with deterministic results, the model is required\\nto provide the final answer in a specified format (e.g., within a box), enabling reliable\\nrule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be\\nused to generate feedback based on predefined test cases.\\n• Format rewards: In addition to the accuracy reward model, we employ a format reward\\nmodel that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’\\ntags.\\nWe do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,\\nbecause we find that the neural reward model may suffer from reward hacking in the large-scale\\nreinforcement learning process, and retraining the reward model needs additional training\\nresources and it complicates the whole training pipeline.\\n2.2.3. Training Template\\nTo train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides\\nthe base model to adhere to our specified instructions. As depicted in Table 1, this template\\nrequires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer.\\nWe intentionally limit our constraints to this structural format, avoiding any content-specific\\nbiases—such as mandating reflective reasoning or promoting particular problem-solving strate-\\ngies—to ensure that we can accurately observe the model’s natural progression during the RL\\nprocess.\\n2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\\nPerformance of DeepSeek-R1-Zero\\nFigure 2 depicts the performance trajectory of DeepSeek-\\nR1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated,\\nDeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the\\nRL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant\\nincrease, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels\\ncomparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL\\nalgorithm in optimizing the model’s performance over time.\\nTable 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI’s o1-0912\\nmodels across a variety of reasoning-related benchmarks. The findings reveal that RL empowers\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 6}, page_content='Model\\nAIME 2024\\nMATH-500\\nGPQA\\nLiveCode\\nCodeForces\\nDiamond\\nBench\\npass@1\\ncons@64\\npass@1\\npass@1\\npass@1\\nrating\\nOpenAI-o1-mini\\n63.6\\n80.0\\n90.0\\n60.0\\n53.8\\n1820\\nOpenAI-o1-0912\\n74.4\\n83.3\\n94.8\\n77.3\\n63.4\\n1843\\nDeepSeek-R1-Zero\\n71.0\\n86.7\\n95.9\\n73.3\\n50.0\\n1444\\nTable 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related\\nbenchmarks.\\nFigure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample\\n16 responses and calculate the overall average accuracy to ensure a stable evaluation.\\nDeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised\\nfine-tuning data. This is a noteworthy achievement, as it underscores the model’s ability to\\nlearn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-\\nR1-Zero can be further augmented through the application of majority voting. For example,\\nwhen majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero’s performance\\nescalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The\\nability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without\\nmajority voting, highlights its strong foundational capabilities and its potential for further\\nadvancements in reasoning tasks.\\nSelf-evolution Process of DeepSeek-R1-Zero\\nThe self-evolution process of DeepSeek-R1-Zero\\nis a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities\\nautonomously. By initiating RL directly from the base model, we can closely monitor the model’s\\nprogression without the influence of the supervised fine-tuning stage. This approach provides\\na clear view of how the model evolves over time, particularly in terms of its ability to handle\\ncomplex reasoning tasks.\\nAs depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 7}, page_content='Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL\\nprocess. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\\nment throughout the training process. This improvement is not the result of external adjustments\\nbut rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the\\nability to solve increasingly complex reasoning tasks by leveraging extended test-time compu-\\ntation. This computation ranges from generating hundreds to thousands of reasoning tokens,\\nallowing the model to explore and refine its thought processes in greater depth.\\nOne of the most remarkable aspects of this self-evolution is the emergence of sophisticated\\nbehaviors as the test-time computation increases. Behaviors such as reflection—where the model\\nrevisits and reevaluates its previous steps—and the exploration of alternative approaches to\\nproblem-solving arise spontaneously. These behaviors are not explicitly programmed but instead\\nemerge as a result of the model’s interaction with the reinforcement learning environment. This\\nspontaneous development significantly enhances DeepSeek-R1-Zero’s reasoning capabilities,\\nenabling it to tackle more challenging tasks with greater efficiency and accuracy.\\nAha Moment of DeepSeek-R1-Zero\\nA particularly intriguing phenomenon observed during\\nthe training of DeepSeek-R1-Zero is the occurrence of an “aha moment”. This moment, as\\nillustrated in Table 3, occurs in an intermediate version of the model. During this phase,\\nDeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial\\napproach. This behavior is not only a testament to the model’s growing reasoning abilities\\nbut also a captivating example of how reinforcement learning can lead to unexpected and\\nsophisticated outcomes.\\nThis moment is not only an “aha moment” for the model but also for the researchers\\nobserving its behavior. It underscores the power and beauty of reinforcement learning: rather\\nthan explicitly teaching the model on how to solve a problem, we simply provide it with the\\nright incentives, and it autonomously develops advanced problem-solving strategies. The\\n“aha moment” serves as a powerful reminder of the potential of RL to unlock new levels of\\nintelligence in artificial systems, paving the way for more autonomous and adaptive models in\\nthe future.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 8}, page_content='Question: If 𝑎> 1, then the sum of the real solutions of\\n√︁\\n𝑎−√\\n𝑎+ 𝑥= 𝑥is equal to\\nResponse: <think>\\nTo solve the equation\\n√︁\\n𝑎−√\\n𝑎+ 𝑥= 𝑥, let’s start by squaring both · · ·\\n\\x10√︁\\n𝑎−√\\n𝑎+ 𝑥\\n\\x112\\n= 𝑥2 =⇒𝑎−√\\n𝑎+ 𝑥= 𝑥2.\\nRearrange to isolate the inner square root term:\\n(𝑎−𝑥2)2 = 𝑎+ 𝑥=⇒𝑎2 −2𝑎𝑥2 + (𝑥2)2 = 𝑎+ 𝑥=⇒𝑥4 −2𝑎𝑥2 −𝑥+ (𝑎2 −𝑎) = 0\\n. . .\\nWait, wait. Wait. That’s an aha moment I can flag here.\\nLet’s reevaluate this step-by-step to identify if the correct sum can be · · ·\\nWe started with the equation:\\n√︁\\n𝑎−√\\n𝑎+ 𝑥= 𝑥\\nFirst, let’s square both sides:\\n𝑎−√\\n𝑎+ 𝑥= 𝑥2 =⇒√\\n𝑎+ 𝑥= 𝑎−𝑥2\\nNext, I could square both sides again, treating the equation: · · ·\\n. . .\\nTable 3 | An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. The\\nmodel learns to rethink using an anthropomorphic tone. This is also an aha moment for us,\\nallowing us to witness the power and beauty of reinforcement learning.\\nDrawback of DeepSeek-R1-Zero\\nAlthough DeepSeek-R1-Zero exhibits strong reasoning\\ncapabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces\\nseveral issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability,\\nand language mixing. To make reasoning processes more readable and share them with the\\nopen community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly\\ncold-start data.\\n2.3. DeepSeek-R1: Reinforcement Learning with Cold Start\\nInspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can\\nreasoning performance be further improved or convergence accelerated by incorporating a small\\namount of high-quality data as a cold start? 2) How can we train a user-friendly model that\\nnot only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong\\ngeneral capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The\\npipeline consists of four stages, outlined as follows.\\n2.3.1. Cold Start\\nUnlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from\\nthe base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data\\nto fine-tune the model as the initial RL actor. To collect such data, we have explored several\\napproaches: using few-shot prompting with a long CoT as an example, directly prompting\\nmodels to generate detailed answers with reflection and verification, gathering DeepSeek-R1-\\nZero outputs in a readable format, and refining the results through post-processing by human\\nannotators.\\nIn this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as\\nthe starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 9}, page_content='include:\\n• Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable\\nfor reading. Responses may mix multiple languages or lack markdown formatting to\\nhighlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1,\\nwe design a readable pattern that includes a summary at the end of each response and\\nfilters out responses that are not reader-friendly. Here, we define the output format as\\n|special_token|<reasoning_process>|special_token|<summary>, where the reasoning\\nprocess is the CoT for the query, and the summary is used to summarize the reasoning\\nresults.\\n• Potential: By carefully designing the pattern for cold-start data with human priors, we\\nobserve better performance against DeepSeek-R1-Zero. We believe the iterative training is\\na better way for reasoning models.\\n2.3.2. Reasoning-oriented Reinforcement Learning\\nAfter fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale\\nreinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses\\non enhancing the model’s reasoning capabilities, particularly in reasoning-intensive tasks such\\nas coding, mathematics, science, and logic reasoning, which involve well-defined problems with\\nclear solutions. During the training process, we observe that CoT often exhibits language mixing,\\nparticularly when RL prompts involve multiple languages. To mitigate the issue of language\\nmixing, we introduce a language consistency reward during RL training, which is calculated\\nas the proportion of target language words in the CoT. Although ablation experiments show\\nthat such alignment results in a slight degradation in the model’s performance, this reward\\naligns with human preferences, making it more readable. Finally, we combine the accuracy of\\nreasoning tasks and the reward for language consistency by directly summing them to form the\\nfinal reward. We then apply RL training on the fine-tuned model until it achieves convergence\\non reasoning tasks.\\n2.3.3. Rejection Sampling and Supervised Fine-Tuning\\nWhen reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT\\n(Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which\\nprimarily focuses on reasoning, this stage incorporates data from other domains to enhance the\\nmodel’s capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we\\ngenerate the data and fine-tune the model as described below.\\nReasoning data\\nWe curate reasoning prompts and generate reasoning trajectories by perform-\\ning rejection sampling from the checkpoint from the above RL training. In the previous stage,\\nwe only included data that could be evaluated using rule-based rewards. However, in this stage,\\nwe expand the dataset by incorporating additional data, some of which use a generative reward\\nmodel by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.\\nAdditionally, because the model output is sometimes chaotic and difficult to read, we have\\nfiltered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For\\neach prompt, we sample multiple responses and retain only the correct ones. In total, we collect\\nabout 600k reasoning related training samples.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 10}, page_content='Non-Reasoning data\\nFor non-reasoning data, such as writing, factual QA, self-cognition,\\nand translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of\\nDeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential\\nchain-of-thought before answering the question by prompting. However, for simpler queries,\\nsuch as “hello” we do not provide a CoT in response. In the end, we collected a total of\\napproximately 200k training samples that are unrelated to reasoning.\\nWe fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about\\n800k samples.\\n2.3.4. Reinforcement Learning for all Scenarios\\nTo further align the model with human preferences, we implement a secondary reinforcement\\nlearning stage aimed at improving the model’s helpfulness and harmlessness while simultane-\\nously refining its reasoning capabilities. Specifically, we train the model using a combination\\nof reward signals and diverse prompt distributions. For reasoning data, we adhere to the\\nmethodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the\\nlearning process in math, code, and logical reasoning domains. For general data, we resort to\\nreward models to capture human preferences in complex and nuanced scenarios. We build\\nupon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train-\\ning prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the\\nassessment emphasizes the utility and relevance of the response to the user while minimizing\\ninterference with the underlying reasoning process. For harmlessness, we evaluate the entire\\nresponse of the model, including both the reasoning process and the summary, to identify and\\nmitigate any potential risks, biases, or harmful content that may arise during the generation\\nprocess. Ultimately, the integration of reward signals and diverse data distributions enables us\\nto train a model that excels in reasoning while prioritizing helpfulness and harmlessness.\\n2.4. Distillation: Empower Small Models with Reasoning Capability\\nTo equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly\\nfine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using\\nthe 800k samples curated with DeepSeek-R1, as detailed in §2.3.3. Our findings indicate that\\nthis straightforward distillation method significantly enhances the reasoning abilities of smaller\\nmodels. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-\\n14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its\\nreasoning capability is slightly better than that of Llama-3.1.\\nFor distilled models, we apply only SFT and do not include an RL stage, even though\\nincorporating RL could substantially boost model performance. Our primary goal here is to\\ndemonstrate the effectiveness of the distillation technique, leaving the exploration of the RL\\nstage to the broader research community.\\n3. Experiment\\nBenchmarks\\nWe evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema\\net al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al.,\\n2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al.,\\n2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI,\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 11}, page_content='2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08 – 2025-01), Codeforces 2, Chinese\\nNational High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Math-\\nematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we\\nalso evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we\\nadhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li\\net al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we\\nonly feed the final summary to evaluation to avoid the length bias. For distilled models, we\\nreport representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and\\nLiveCodeBench.\\nEvaluation Prompts\\nFollowing the setup in DeepSeek-V3, standard benchmarks such as\\nMMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-\\nevals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a\\nzero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts\\nare few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot\\nmay hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation\\nprotocols with default prompts provided by their creators. For code and math benchmarks, the\\nHumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++,\\nC#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated\\nusing CoT format, with data collected between August 2024 and January 2025. The Codeforces\\ndataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases,\\nafter which the expected ratings and percentages of competitors are calculated. SWE-Bench\\nverified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related\\nbenchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum\\nof 32,768 tokens for each benchmark.\\nBaselines\\nWe conduct comprehensive evaluations against several strong baselines, including\\nDeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217.\\nSince accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor-\\nmance based on official reports. For distilled models, we also compare the open-source model\\nQwQ-32B-Preview (Qwen, 2024a).\\nEvaluation Setup\\nWe set the maximum generation length to 32,768 tokens for the models.\\nWe found that using greedy decoding to evaluate long-output reasoning models results in\\nhigher repetition rates and significant variability across different checkpoints. Therefore, we\\ndefault to pass@𝑘evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature.\\nSpecifically, we use a sampling temperature of 0.6 and a top-𝑝value of 0.95 to generate 𝑘\\nresponses (typically between 4 and 64, depending on the test set size) for each question. Pass@1\\nis then calculated as\\npass@1 = 1\\n𝑘\\n𝑘\\n∑︁\\n𝑖=1\\n𝑝𝑖,\\nwhere 𝑝𝑖denotes the correctness of the 𝑖-th response. This method provides more reliable\\nperformance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang\\net al., 2022) using 64 samples, denoted as cons@64.\\n1https://aider.chat\\n2https://codeforces.com\\n3https://www.cms.org.cn/Home/comp/comp/cid/12.html\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 12}, page_content='3.1. DeepSeek-R1 Evaluation\\nBenchmark (Metric)\\nClaude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek\\nSonnet-1022\\n0513\\nV3\\no1-mini o1-1217\\nR1\\nArchitecture\\n-\\n-\\nMoE\\n-\\n-\\nMoE\\n# Activated Params\\n-\\n-\\n37B\\n-\\n-\\n37B\\n# Total Params\\n-\\n-\\n671B\\n-\\n-\\n671B\\nEnglish\\nMMLU (Pass@1)\\n88.3\\n87.2\\n88.5\\n85.2\\n91.8\\n90.8\\nMMLU-Redux (EM)\\n88.9\\n88.0\\n89.1\\n86.7\\n-\\n92.9\\nMMLU-Pro (EM)\\n78.0\\n72.6\\n75.9\\n80.3\\n-\\n84.0\\nDROP (3-shot F1)\\n88.3\\n83.7\\n91.6\\n83.9\\n90.2\\n92.2\\nIF-Eval (Prompt Strict)\\n86.5\\n84.3\\n86.1\\n84.8\\n-\\n83.3\\nGPQA Diamond (Pass@1)\\n65.0\\n49.9\\n59.1\\n60.0\\n75.7\\n71.5\\nSimpleQA (Correct)\\n28.4\\n38.2\\n24.9\\n7.0\\n47.0\\n30.1\\nFRAMES (Acc.)\\n72.5\\n80.5\\n73.3\\n76.9\\n-\\n82.5\\nAlpacaEval2.0 (LC-winrate)\\n52.0\\n51.1\\n70.0\\n57.8\\n-\\n87.6\\nArenaHard (GPT-4-1106)\\n85.2\\n80.4\\n85.5\\n92.0\\n-\\n92.3\\nCode\\nLiveCodeBench (Pass@1-COT)\\n38.9\\n32.9\\n36.2\\n53.8\\n63.4\\n65.9\\nCodeforces (Percentile)\\n20.3\\n23.6\\n58.7\\n93.4\\n96.6\\n96.3\\nCodeforces (Rating)\\n717\\n759\\n1134\\n1820\\n2061\\n2029\\nSWE Verified (Resolved)\\n50.8\\n38.8\\n42.0\\n41.6\\n48.9\\n49.2\\nAider-Polyglot (Acc.)\\n45.3\\n16.0\\n49.6\\n32.9\\n61.7\\n53.3\\nMath\\nAIME 2024 (Pass@1)\\n16.0\\n9.3\\n39.2\\n63.6\\n79.2\\n79.8\\nMATH-500 (Pass@1)\\n78.3\\n74.6\\n90.2\\n90.0\\n96.4\\n97.3\\nCNMO 2024 (Pass@1)\\n13.1\\n10.8\\n43.2\\n67.6\\n-\\n78.8\\nChinese\\nCLUEWSC (EM)\\n85.4\\n87.9\\n90.9\\n89.9\\n-\\n92.8\\nC-Eval (EM)\\n76.7\\n76.0\\n86.5\\n68.9\\n-\\n91.8\\nC-SimpleQA (Correct)\\n55.4\\n58.7\\n68.0\\n40.3\\n-\\n63.7\\nTable 4 | Comparison between DeepSeek-R1 and other representative models.\\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA\\nDiamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im-\\nprovement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-\\nicant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1\\nexcels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis\\ncapabilities. This highlights the potential of reasoning models in AI-driven search and data\\nanalysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed where\\nOpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than\\nDeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\\nanswering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an\\naccuracy of over 70%.\\nDeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\\nmodel’s ability to follow format instructions. These improvements can be linked to the inclusion\\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\\ntraining. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,\\nindicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering. Its\\nsignificant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale\\nRL, which not only boosts reasoning capabilities but also improves performance across diverse\\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 13}, page_content='DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying\\nits robustness across multiple tasks.\\nOn math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,\\nsurpassing other models by a large margin. A similar trend is observed on coding algorithm\\ntasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these\\nbenchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1\\non Aider but achieves comparable performance on SWE Verified. We believe the engineering\\nperformance of DeepSeek-R1 will improve in the next version, as the amount of related RL\\ntraining data currently remains very limited.\\n3.2. Distilled Model Evaluation\\nModel\\nAIME 2024\\nMATH-500\\nGPQA\\nLiveCode\\nCodeForces\\nDiamond\\nBench\\npass@1\\ncons@64\\npass@1\\npass@1\\npass@1\\nrating\\nGPT-4o-0513\\n9.3\\n13.4\\n74.6\\n49.9\\n32.9\\n759\\nClaude-3.5-Sonnet-1022\\n16.0\\n26.7\\n78.3\\n65.0\\n38.9\\n717\\nOpenAI-o1-mini\\n63.6\\n80.0\\n90.0\\n60.0\\n53.8\\n1820\\nQwQ-32B-Preview\\n50.0\\n60.0\\n90.6\\n54.5\\n41.9\\n1316\\nDeepSeek-R1-Distill-Qwen-1.5B\\n28.9\\n52.7\\n83.9\\n33.8\\n16.9\\n954\\nDeepSeek-R1-Distill-Qwen-7B\\n55.5\\n83.3\\n92.8\\n49.1\\n37.6\\n1189\\nDeepSeek-R1-Distill-Qwen-14B\\n69.7\\n80.0\\n93.9\\n59.1\\n53.1\\n1481\\nDeepSeek-R1-Distill-Qwen-32B\\n72.6\\n83.3\\n94.3\\n62.1\\n57.2\\n1691\\nDeepSeek-R1-Distill-Llama-8B\\n50.4\\n80.0\\n89.1\\n49.0\\n39.6\\n1205\\nDeepSeek-R1-Distill-Llama-70B\\n70.0\\n86.7\\n94.5\\n65.2\\n57.5\\n1633\\nTable 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on\\nreasoning-related benchmarks.\\nAs shown in Table 5, simply distilling DeepSeek-R1’s outputs enables the efficient DeepSeek-\\nR1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-\\nreasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-\\nPreview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly\\nexceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla-\\ntion. Additionally, we found that applying RL to these distilled models yields significant further\\ngains. We believe this warrants further exploration and therefore present only the results of the\\nsimple SFT-distilled models here.\\n4. Discussion\\n4.1. Distillation v.s. Reinforcement Learning\\nIn Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive\\nresults. However, there is still one question left: can the model achieve comparable performance\\nthrough the large-scale RL training discussed in the paper without distillation?\\nTo answer this question, we conduct large-scale RL training on Qwen-32B-Base using math,\\ncode, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The\\nexperimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 14}, page_content='Model\\nAIME 2024\\nMATH-500\\nGPQA Diamond\\nLiveCodeBench\\npass@1\\ncons@64\\npass@1\\npass@1\\npass@1\\nQwQ-32B-Preview\\n50.0\\n60.0\\n90.6\\n54.5\\n41.9\\nDeepSeek-R1-Zero-Qwen-32B\\n47.0\\n60.0\\n91.6\\n55.0\\n40.2\\nDeepSeek-R1-Distill-Qwen-32B\\n72.6\\n83.3\\n94.3\\n62.1\\n57.2\\nTable 6 | Comparison of distilled and RL Models on Reasoning-Related Benchmarks.\\nRL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-\\nDistill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than\\nDeepSeek-R1-Zero-Qwen-32B across all benchmarks.\\nTherefore, we can draw two conclusions: First, distilling more powerful models into smaller\\nones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in\\nthis paper require enormous computational power and may not even achieve the performance\\nof distillation. Second, while distillation strategies are both economical and effective, advancing\\nbeyond the boundaries of intelligence may still require more powerful base models and larger-\\nscale reinforcement learning.\\n4.2. Unsuccessful Attempts\\nIn the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along\\nthe way. We share our failure experiences here to provide insights, but this does not imply that\\nthese approaches are incapable of developing effective reasoning models.\\nProcess Reward Model (PRM)\\nPRM is a reasonable method to guide the model toward better\\napproaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al.,\\n2023). However, in practice, PRM has three main limitations that may hinder its ultimate suc-\\ncess. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second,\\ndetermining whether the current intermediate step is correct is a challenging task. Automated\\nannotation using models may not yield satisfactory results, while manual annotation is not con-\\nducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward\\nhacking (Gao et al., 2022), and retraining the reward model needs additional training resources\\nand it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good\\nability to rerank the top-N responses generated by the model or assist in guided search (Snell\\net al., 2024), its advantages are limited compared to the additional computational overhead it\\nintroduces during the large-scale reinforcement learning process in our experiments.\\nMonte Carlo Tree Search (MCTS)\\nInspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil-\\nver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time\\ncompute scalability. This approach involves breaking answers into smaller parts to allow the\\nmodel to explore the solution space systematically. To facilitate this, we prompt the model to\\ngenerate multiple tags that correspond to specific reasoning steps necessary for the search. For\\ntraining, we first use collected prompts to find answers via MCTS guided by a pre-trained value\\nmodel. Subsequently, we use the resulting question-answer pairs to train both the actor model\\nand the value model, iteratively refining the process.\\nHowever, this approach encounters several challenges when scaling up the training. First,\\nunlike chess, where the search space is relatively well-defined, token generation presents an\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 15}, page_content='exponentially larger search space. To address this, we set a maximum extension limit for each\\nnode, but this can lead to the model getting stuck in local optima. Second, the value model\\ndirectly influences the quality of generation since it guides each step of the search process.\\nTraining a fine-grained value model is inherently difficult, which makes it challenging for the\\nmodel to iteratively improve. While AlphaGo’s core success relied on training a value model to\\nprogressively enhance its performance, this principle proves difficult to replicate in our setup\\ndue to the complexities of token generation.\\nIn conclusion, while MCTS can improve performance during inference when paired with a\\npre-trained value model, iteratively boosting model performance through self-search remains a\\nsignificant challenge.\\n5. Conclusion, Limitations, and Future Work\\nIn this work, we share our journey in enhancing model reasoning abilities through reinforcement\\nlearning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start\\ndata, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,\\nleveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves\\nperformance comparable to OpenAI-o1-1217 on a range of tasks.\\nWe further explore distillation the reasoning capability to small dense models. We use\\nDeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small\\ndense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o\\nand Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other\\ndense models also achieve impressive results, significantly outperforming other instruction-\\ntuned models based on the same underlying checkpoints.\\nIn the future, we plan to invest in research across the following directions for DeepSeek-R1.\\n• General Capability: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3\\nin tasks such as function calling, multi-turn, complex role-playing, and JSON output.\\nMoving forward, we plan to explore how long CoT can be leveraged to enhance tasks in\\nthese fields.\\n• Language Mixing: DeepSeek-R1 is currently optimized for Chinese and English, which\\nmay result in language mixing issues when handling queries in other languages. For\\ninstance, DeepSeek-R1 might use English for reasoning and responses, even if the query is\\nin a language other than English or Chinese. We aim to address this limitation in future\\nupdates.\\n• Prompting Engineering: When evaluating DeepSeek-R1, we observe that it is sensitive\\nto prompts. Few-shot prompting consistently degrades its performance. Therefore, we\\nrecommend users directly describe the problem and specify the output format using a\\nzero-shot setting for optimal results.\\n• Software Engineering Tasks: Due to the long evaluation times, which impact the effi-\\nciency of the RL process, large-scale RL has not been applied extensively in software\\nengineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement\\nover DeepSeek-V3 on software engineering benchmarks. Future versions will address\\nthis by implementing rejection sampling on software engineering data or incorporating\\nasynchronous evaluations during the RL process to improve efficiency.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 16}, page_content='References\\nAI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-m\\nodels/blob/main/models/llama3_1/MODEL_CARD.md.\\nAnthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3\\n-5-sonnet.\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,\\nN. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,\\nB. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet,\\nF. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,\\nA. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,\\nA. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,\\nM. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and\\nW. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.\\nURL https://arxiv.org/abs/2107.03374.\\nA. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten,\\nA. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\\nY. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple\\nway to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.\\nX. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like\\ntree-search can guide large language model decoding and training, 2024. URL https:\\n//arxiv.org/abs/2309.17179.\\nL. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization, 2022. URL\\nhttps://arxiv.org/abs/2210.10760.\\nA. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao,\\nX. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and\\nP. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or\\ng/10.48550/arXiv.2406.04127.\\nGoogle. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno\\nlogy/ai/google-gemini-next-generation-model-february-2024.\\nY. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi-\\nnese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint\\narXiv:2411.07140, 2024.\\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\\nY. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A\\nmulti-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint\\narXiv:2305.08322, 2023.\\nN. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica.\\nLivecodebench: Holistic and contamination free evaluation of large language models for code.\\nCoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 17}, page_content='S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui.\\nFact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR,\\nabs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485\\n50/arXiv.2409.12941.\\nA. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop,\\nR. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv\\npreprint arXiv:2409.12917, 2024.\\nH. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measur-\\ning massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212,\\n2023.\\nT. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From\\ncrowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv\\npreprint arXiv:2406.11939, 2024.\\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,\\nI. Sutskever, and K. Cobbe. Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023.\\nB. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL\\nhttps://github.com/WildEval/ZeroEval.\\nMAA.\\nAmerican invitational mathematics examination - aime.\\nIn American Invitational\\nMathematics Examination - AIME 2024, February 2024. URL https://maa.org/math\\n-competitions/american-invitational-mathematics-examination-aime.\\nOpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/.\\nOpenAI. Learning to reason with llms, 2024b. URL https://openai.com/index/learnin\\ng-to-reason-with-llms/.\\nOpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing\\n-simpleqa/.\\nOpenAI. Introducing SWE-bench verified we’re releasing a human-validated subset of swe-\\nbench that more, 2024d. URL https://openai.com/index/introducing-swe-bench\\n-verified/.\\nQwen. Qwq: Reflect deeply on the boundaries of the unknown, 2024a. URL https://qwenlm\\n.github.io/blog/qwq-32b-preview/.\\nQwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b\\nlog/qwen2.5.\\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman.\\nGPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.\\nZ. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath:\\nPushing the limits of mathematical reasoning in open language models. arXiv preprint\\narXiv:2402.03300, 2024.\\nD. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,\\nD. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and\\nshogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815,\\n2017a. URL http://arxiv.org/abs/1712.01815.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 18}, page_content='D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,\\nM. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and\\nD. Hassabis. Mastering the game of go without human knowledge. Nat., 550(7676):354–359,\\n2017b. doi: 10.1038/NATURE24270. URL https://doi.org/10.1038/nature24270.\\nC. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more\\neffective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.033\\n14.\\nT. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human\\ndemonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5.\\nJ. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and\\nI. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv\\npreprint arXiv:2211.14275, 2022.\\nP. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A label-\\nfree step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935,\\n2023.\\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou.\\nSelf-consistency improves chain of thought reasoning in language models. arXiv preprint\\narXiv:2203.11171, 2022.\\nY. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li,\\nM. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and\\nchallenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024.\\nURL https://doi.org/10.48550/arXiv.2406.01574.\\nC. S. Xia, Y. Deng, S. Dunn, and L. Zhang.\\nAgentless: Demystifying llm-based software\\nengineering agents. arXiv preprint, 2024.\\nH. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao,\\nQ. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing\\nproof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL\\nhttps://arxiv.org/abs/2408.08152.\\nJ. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following\\nevaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 19}, page_content='Appendix\\nA. Contributions and Acknowledgments\\nCore Contributors\\nDaya Guo\\nDejian Yang\\nHaowei Zhang\\nJunxiao Song\\nRuoyu Zhang\\nRunxin Xu\\nQihao Zhu\\nShirong Ma\\nPeiyi Wang\\nXiao Bi\\nXiaokang Zhang\\nXingkai Yu\\nYu Wu\\nZ.F. Wu\\nZhibin Gou\\nZhihong Shao\\nZhuoshu Li\\nZiyi Gao\\nContributors\\nAixin Liu\\nBing Xue\\nBingxuan Wang\\nBochao Wu\\nBei Feng\\nChengda Lu\\nChenggang Zhao\\nChengqi Deng\\nChong Ruan\\nDamai Dai\\nDeli Chen\\nDongjie Ji\\nErhang Li\\nFangyun Lin\\nFucong Dai\\nFuli Luo*\\nGuangbo Hao\\nGuanting Chen\\nGuowei Li\\nH. Zhang\\nHanwei Xu\\nHonghui Ding\\nHuazuo Gao\\nHui Qu\\nHui Li\\nJianzhong Guo\\nJiashi Li\\nJingchang Chen\\nJingyang Yuan\\nJinhao Tu\\nJunjie Qiu\\nJunlong Li\\nJ.L. Cai\\nJiaqi Ni\\nJian Liang\\nJin Chen\\nKai Dong\\nKai Hu*\\nKaichao You\\nKaige Gao\\nKang Guan\\nKexin Huang\\nKuai Yu\\nLean Wang\\nLecong Zhang\\nLiang Zhao\\nLitong Wang\\nLiyue Zhang\\nLei Xu\\nLeyi Xia\\nMingchuan Zhang\\nMinghua Zhang\\nMinghui Tang\\nMingxu Zhou\\nMeng Li\\nMiaojun Wang\\nMingming Li\\nNing Tian\\nPanpan Huang\\nPeng Zhang\\nQiancheng Wang\\nQinyu Chen\\nQiushi Du\\nRuiqi Ge*\\nRuisong Zhang\\nRuizhe Pan\\nRunji Wang\\nR.J. Chen\\nR.L. Jin\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 20}, page_content='Ruyi Chen\\nShanghao Lu\\nShangyan Zhou\\nShanhuang Chen\\nShengfeng Ye\\nShiyu Wang\\nShuiping Yu\\nShunfeng Zhou\\nShuting Pan\\nS.S. Li\\nShuang Zhou\\nShaoqing Wu\\nShengfeng Ye\\nTao Yun\\nTian Pei\\nTianyu Sun\\nT. Wang\\nWangding Zeng\\nWen Liu\\nWenfeng Liang\\nWenjun Gao\\nWenqin Yu*\\nWentao Zhang\\nW.L. Xiao\\nWei An\\nXiaodong Liu\\nXiaohan Wang\\nXiaokang Chen\\nXiaotao Nie\\nXin Cheng\\nXin Liu\\nXin Xie\\nXingchao Liu\\nXinyu Yang\\nXinyuan Li\\nXuecheng Su\\nXuheng Lin\\nX.Q. Li\\nXiangyue Jin\\nXiaojin Shen\\nXiaosha Chen\\nXiaowen Sun\\nXiaoxiang Wang\\nXinnan Song\\nXinyi Zhou\\nXianzu Wang\\nXinxia Shan\\nY.K. Li\\nY.Q. Wang\\nY.X. Wei\\nYang Zhang\\nYanhong Xu\\nYao Li\\nYao Zhao\\nYaofeng Sun\\nYaohui Wang\\nYi Yu\\nYichao Zhang\\nYifan Shi\\nYiliang Xiong\\nYing He\\nYishi Piao\\nYisong Wang\\nYixuan Tan\\nYiyang Ma*\\nYiyuan Liu\\nYongqiang Guo\\nYuan Ou\\nYuduan Wang\\nYue Gong\\nYuheng Zou\\nYujia He\\nYunfan Xiong\\nYuxiang Luo\\nYuxiang You\\nYuxuan Liu\\nYuyang Zhou\\nY.X. Zhu\\nYanping Huang\\nYaohui Li\\nYi Zheng\\nYuchen Zhu\\nYunxian Ma\\nYing Tang\\nYukun Zha\\nYuting Yan\\nZ.Z. Ren\\nZehui Ren\\nZhangli Sha\\nZhe Fu\\nZhean Xu\\nZhenda Xie\\nZhengyan Zhang\\nZhewen Hao\\nZhicheng Ma\\nZhigang Yan\\nZhiyu Wu\\nZihui Gu\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T07:53:55+00:00', 'source': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/DeepSeek_R1.pdf', 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-01-23T07:53:55+00:00', 'trapped': '', 'modDate': 'D:20250123075355Z', 'creationDate': 'D:20250123075355Z', 'page': 21}, page_content='Zijia Zhu\\nZijun Liu*\\nZilin Li\\nZiwei Xie\\nZiyang Song\\nZizheng Pan\\nZhen Huang\\nZhipeng Xu\\nZhongyu Zhang\\nZhen Zhang\\nWithin each role, authors are listed alphabetically by the first name. Names marked with *\\ndenote individuals who have departed from our team.\\n22'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 0}, page_content='M A N N I N G\\nSebastian Raschka\\nFROM\\nSCRATCH\\nBUILD A'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 1}, page_content='1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2\\nSTAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\nImplements the data sampling and\\nunderstand the basic mechanism\\nPretrains the LLM on unlabeled\\ndata to obtain a foundation\\nmodel for further ﬁne-tuning\\nFine-tunes the pretrained\\nLLM to create a\\nclassiﬁcation model\\nFine-tunes the pretrained\\nLLM to create a personal\\nassistant or chat model\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights\\n8) Fine-tuning\\n3) LLM\\narchitecture\\nThe three main stages of coding a large language model (LLM) are implementing the LLM architecture and data \\npreparation process (stage 1), pretraining an LLM to create a foundation model (stage 2), and fine-tuning the \\nfoundation model to become a personal assistant or text classifier (stage 3). Each of these stages is explored \\nand implemented in this book.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 2}, page_content='Build a Large Language Model (From Scratch)\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 3}, page_content='Licensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 4}, page_content='Build a Large\\nLanguage Model\\n(From Scratch)\\nSEBASTIAN RASCHKA\\nM A N N I N G\\nSHELTER ISLAND\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 5}, page_content='For online information and ordering of this and other Manning books, please visit\\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \\nFor more information, please contact\\nSpecial Sales Department\\nManning Publications Co.\\n20 Baldwin Road\\nPO Box 761\\nShelter Island, NY 11964\\nEmail: orders@manning.com\\n©2025 by Manning Publications Co. All rights reserved.\\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \\nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \\npermission of the publisher.\\nMany of the designations used by manufacturers and sellers to distinguish their products are \\nclaimed as trademarks. Where those designations appear in the book, and Manning Publications \\nwas aware of a trademark claim, the designations have been printed in initial caps or all caps.\\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \\nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \\nRecognizing also our responsibility to conserve the resources of our planet, Manning books\\nare printed on paper that is at least 15 percent recycled and processed without the use of \\nelemental chlorine.\\nThe authors and publisher have made every effort to ensure that the information in this book \\nwas correct at press time. The authors and publisher do not assume and hereby disclaim any \\nliability to any party for any loss, damage, or disruption caused by errors or omissions, whether \\nsuch errors or omissions result from negligence, accident, or any other cause, or from any usage \\nof the information herein.\\nManning Publications Co.\\nDevelopment editor: Dustin Archibald\\n20 Baldwin Road\\nTechnical editor: David Caswell\\nPO Box 761\\nReview editor: Kishor Rit\\nShelter Island, NY 11964\\nProduction editor: Aleksandar Dragosavljevic´\\nCopy editors: Kari Lucke and Alisa Larson\\nProofreader: Mike Beady\\nTechnical proofreader: Jerry Kuch\\nTypesetter: Dennis Dalinnik\\nCover designer: Marija Tudor\\nISBN: 9781633437166\\nPrinted in the United States of America\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 6}, page_content='v\\nbrief contents\\n1\\n■\\nUnderstanding large language models\\n1\\n2\\n■\\nWorking with text data\\n17\\n3\\n■\\nCoding attention mechanisms\\n50\\n4\\n■\\nImplementing a GPT model from scratch to \\ngenerate text\\n92\\n5\\n■\\nPretraining on unlabeled data\\n128\\n6\\n■\\nFine-tuning for classification\\n169\\n7\\n■\\nFine-tuning to follow instructions\\n204\\nA\\n■\\nIntroduction to PyTorch\\n251\\nB\\n■\\nReferences and further reading\\n289\\nC\\n■\\nExercise solutions\\n300\\nD\\n■\\nAdding bells and whistles to the training loop\\n313\\nE\\n■\\nParameter-efficient fine-tuning with LoRA\\n322\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 7}, page_content='Licensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 8}, page_content='vii\\ncontents\\npreface\\nxi\\nacknowledgments\\nxiii\\nabout this book\\nxv\\nabout the author\\nxix\\nabout the cover illustration\\nxx\\n1 Understanding large language models\\n1\\n1.1\\nWhat is an LLM?\\n2\\n1.2\\nApplications of LLMs\\n4\\n1.3\\nStages of building and using LLMs\\n5\\n1.4\\nIntroducing the transformer architecture\\n7\\n1.5\\nUtilizing large datasets\\n10\\n1.6\\nA closer look at the GPT architecture\\n12\\n1.7\\nBuilding a large language model\\n14\\n2 Working with text data\\n17\\n2.1\\nUnderstanding word embeddings\\n18\\n2.2\\nTokenizing text\\n21\\n2.3\\nConverting tokens into token IDs\\n24\\n2.4\\nAdding special context tokens\\n29\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 9}, page_content='CONTENTS\\nviii\\n2.5\\nByte pair encoding\\n33\\n2.6\\nData sampling with a sliding window\\n35\\n2.7\\nCreating token embeddings\\n41\\n2.8\\nEncoding word positions\\n43\\n3 Coding attention mechanisms\\n50\\n3.1\\nThe problem with modeling long sequences\\n52\\n3.2\\nCapturing data dependencies with attention \\nmechanisms\\n54\\n3.3\\nAttending to different parts of the input with \\nself-attention\\n55\\nA simple self-attention mechanism without trainable weights\\n56\\nComputing attention weights for all input tokens\\n61\\n3.4\\nImplementing self-attention with trainable weights\\n64\\nComputing the attention weights step by step\\n65\\n■Implementing a \\ncompact self-attention Python class\\n70\\n3.5\\nHiding future words with causal attention\\n74\\nApplying a causal attention mask\\n75\\n■Masking additional \\nattention weights with dropout\\n78\\n■Implementing a compact \\ncausal attention class\\n80\\n3.6\\nExtending single-head attention to multi-head \\nattention\\n82\\nStacking multiple single-head attention layers\\n82\\n■Implementing \\nmulti-head attention with weight splits\\n86\\n4 Implementing a GPT model from scratch to generate text\\n92\\n4.1\\nCoding an LLM architecture\\n93\\n4.2\\nNormalizing activations with layer normalization\\n99\\n4.3\\nImplementing a feed forward network with GELU \\nactivations\\n105\\n4.4\\nAdding shortcut connections\\n109\\n4.5\\nConnecting attention and linear layers in a transformer \\nblock\\n113\\n4.6\\nCoding the GPT model\\n117\\n4.7\\nGenerating text\\n122\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 10}, page_content='CONTENTS\\nix\\n5 Pretraining on unlabeled data\\n128\\n5.1\\nEvaluating generative text models\\n129\\nUsing GPT to generate text\\n130\\n■Calculating the text \\ngeneration loss\\n132\\n■Calculating the training and validation \\nset losses\\n140\\n5.2\\nTraining an LLM\\n146\\n5.3\\nDecoding strategies to control randomness\\n151\\nTemperature scaling\\n152\\n■Top-k sampling\\n155\\nModifying the text generation function\\n157\\n5.4\\nLoading and saving model weights in PyTorch\\n159\\n5.5\\nLoading pretrained weights from OpenAI\\n160\\n6 Fine-tuning for classification\\n169\\n6.1\\nDifferent categories of fine-tuning\\n170\\n6.2\\nPreparing the dataset\\n172\\n6.3\\nCreating data loaders\\n175\\n6.4\\nInitializing a model with pretrained weights\\n181\\n6.5\\nAdding a classification head\\n183\\n6.6\\nCalculating the classification loss and accuracy\\n190\\n6.7\\nFine-tuning the model on supervised data\\n195\\n6.8\\nUsing the LLM as a spam classifier\\n200\\n7 Fine-tuning to follow instructions\\n204\\n7.1\\nIntroduction to instruction fine-tuning\\n205\\n7.2\\nPreparing a dataset for supervised instruction \\nfine-tuning\\n207\\n7.3\\nOrganizing data into training batches\\n211\\n7.4\\nCreating data loaders for an instruction dataset\\n223\\n7.5\\nLoading a pretrained LLM\\n226\\n7.6\\nFine-tuning the LLM on instruction data\\n229\\n7.7\\nExtracting and saving responses\\n233\\n7.8\\nEvaluating the fine-tuned LLM\\n238\\n7.9\\nConclusions\\n247\\nWhat’s next?\\n247\\n■Staying up to date in a fast-moving \\nfield\\n248\\n■Final words\\n248\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 11}, page_content='CONTENTS\\nx\\nappendix A\\nIntroduction to PyTorch\\n251\\nappendix B\\nReferences and further reading\\n289\\nappendix C\\nExercise solutions\\n300\\nappendix D\\nAdding bells and whistles to the training loop\\n313\\nappendix E\\nParameter-efficient fine-tuning with LoRA\\n322\\nindex\\n337\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 12}, page_content='xi\\npreface\\nI’ve always been fascinated with language models. More than a decade ago, my jour-\\nney into AI began with a statistical pattern classification class, which led to my first\\nindependent project: developing a model and web application to detect the mood of\\na song based on its lyrics.\\n Fast forward to 2022, with the release of ChatGPT, large language models (LLMs)\\nhave taken the world by storm and have revolutionized how many of us work. These\\nmodels are incredibly versatile, aiding in tasks such as checking grammar, composing\\nemails, summarizing lengthy documents, and much more. This is owed to their ability\\nto parse and generate human-like text, which is important in various fields, from cus-\\ntomer service to content creation, and even in more technical domains like coding\\nand data analysis. \\n As their name implies, a hallmark of LLMs is that they are “large”—very large—\\nencompassing millions to billions of parameters. (For comparison, using more tradi-\\ntional machine learning or statistical methods, the Iris flower dataset can be classified\\nwith more than 90% accuracy using a small model with only two parameters.) How-\\never, despite the large size of LLMs compared to more traditional methods, LLMs\\ndon’t have to be a black box. \\n In this book, you will learn how to build an LLM one step at a time. By the end,\\nyou will have a solid understanding of how an LLM, like the ones used in ChatGPT,\\nworks on a fundamental level. I believe that developing confidence with each part of\\nthe fundamental concepts and underlying code is crucial for success. This not only\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 13}, page_content='PREFACE\\nxii\\nhelps in fixing bugs and improving performance but also enables experimentation\\nwith new ideas.\\n Several years ago, when I started working with LLMs, I had to learn how to imple-\\nment them the hard way, sifting through many research papers and incomplete code\\nrepositories to develop a general understanding. With this book, I hope to make\\nLLMs more accessible by developing and sharing a step-by-step implementation tuto-\\nrial detailing all the major components and development phases of an LLM.\\n I strongly believe that the best way to understand LLMs is to code one from\\nscratch—and you’ll see that this can be fun too! \\n Happy reading and coding!\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 14}, page_content='xiii\\nacknowledgments\\nWriting a book is a significant undertaking, and I would like to express my sincere\\ngratitude to my wife, Liza, for her patience and support throughout this process. Her\\nunconditional love and constant encouragement have been absolutely essential.\\n I am incredibly grateful to Daniel Kleine, whose invaluable feedback on the in-\\nprogress chapters and code went above and beyond. With his keen eye for detail and\\ninsightful suggestions, Daniel’s contributions have undoubtedly made this book a\\nsmoother and more enjoyable reading experience.\\n I would also like to thank the wonderful staff at Manning Publications, including\\nMichael Stephens, for the many productive discussions that helped shape the direc-\\ntion of this book, and Dustin Archibald, whose constructive feedback and guidance in\\nadhering to the Manning guidelines have been crucial. I also appreciate your flexibil-\\nity in accommodating the unique requirements of this unconventional from-scratch\\napproach. A special thanks to Aleksandar Dragosavljevic´, Kari Lucke, and Mike Beady\\nfor their work on the professional layouts and to Susan Honeywell and her team for\\nrefining and polishing the graphics.\\n I want to express my heartfelt gratitude to Robin Campbell and her outstanding\\nmarketing team for their invaluable support throughout the writing process.\\n Finally, I extend my thanks to the reviewers: Anandaganesh Balakrishnan, Anto\\nAravinth, Ayush Bihani, Bassam Ismail, Benjamin Muskalla, Bruno Sonnino, Christian\\nProkopp, Daniel Kleine, David Curran, Dibyendu Roy Chowdhury, Gary Pass, Georg\\nSommer, Giovanni Alzetta, Guillermo Alcántara, Jonathan Reeves, Kunal Ghosh,\\nNicolas Modrzyk, Paul Silisteanu, Raul Ciotescu, Scott Ling, Sriram Macharla, Sumit\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 15}, page_content='ACKNOWLEDGMENTS\\nxiv\\nPal, Vahid Mirjalili, Vaijanath Rao, and Walter Reade for their thorough feedback on\\nthe drafts. Your keen eyes and insightful comments have been essential in improving\\nthe quality of this book.\\n To everyone who has contributed to this journey, I am sincerely grateful. Your sup-\\nport, expertise, and dedication have been instrumental in bringing this book to frui-\\ntion. Thank you!\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 16}, page_content='xv\\nabout this book\\nBuild a Large Language Model (From Scratch) was written to help you understand and\\ncreate your own GPT-like large language models (LLMs) from the ground up. It\\nbegins by focusing on the fundamentals of working with text data and coding atten-\\ntion mechanisms and then guides you through implementing a complete GPT\\nmodel from scratch. The book then covers the pretraining mechanism as well as\\nfine-tuning for specific tasks such as text classification and following instructions. By\\nthe end of this book, you’ll have a deep understanding of how LLMs work and the\\nskills to build your own models. While the models you’ll create are smaller in scale\\ncompared to the large foundational models, they use the same concepts and serve\\nas powerful educational tools to grasp the core mechanisms and techniques used in\\nbuilding state-of-the-art LLMs.\\nWho should read this book\\nBuild a Large Language Model (From Scratch) is for machine learning enthusiasts, engi-\\nneers, researchers, students, and practitioners who want to gain a deep understand-\\ning of how LLMs work and learn to build their own models from scratch. Both\\nbeginners and experienced developers will be able to use their existing skills and\\nknowledge to grasp the concepts and techniques used in creating LLMs.\\n What sets this book apart is its comprehensive coverage of the entire process of\\nbuilding LLMs, from working with datasets to implementing the model architecture,\\npretraining on unlabeled data, and fine-tuning for specific tasks. As of this writing, no\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 17}, page_content='ABOUT THIS BOOK\\nxvi\\nother resource provides such a complete and hands-on approach to building LLMs\\nfrom the ground up.\\n To understand the code examples in this book, you should have a solid grasp of\\nPython programming. While some familiarity with machine learning, deep learning,\\nand artificial intelligence can be beneficial, an extensive background in these areas is\\nnot required. LLMs are a unique subset of AI, so even if you’re relatively new to the\\nfield, you’ll be able to follow along.\\n If you have some experience with deep neural networks, you may find certain con-\\ncepts more familiar, as LLMs are built upon these architectures. However, proficiency\\nin PyTorch is not a prerequisite. Appendix A provides a concise introduction to\\nPyTorch, equipping you with the necessary skills to comprehend the code examples\\nthroughout the book.\\n A high school–level understanding of mathematics, particularly working with vec-\\ntors and matrices, can be helpful as we explore the inner workings of LLMs. However,\\nadvanced mathematical knowledge is not necessary to grasp the key concepts and\\nideas presented in this book.\\n The most important prerequisite is a strong foundation in Python programming.\\nWith this knowledge, you’ll be well prepared to explore the fascinating world of LLMs\\nand understand the concepts and code examples presented in this book.\\nHow this book is organized: A roadmap\\nThis book is designed to be read sequentially, as each chapter builds upon the con-\\ncepts and techniques introduced in the previous ones. The book is divided into seven\\nchapters that cover the essential aspects of LLMs and their implementation.\\n Chapter 1 provides a high-level introduction to the fundamental concepts behind\\nLLMs. It explores the transformer architecture, which forms the basis for LLMs such\\nas those used on the ChatGPT platform.\\n Chapter 2 lays out a plan for building an LLM from scratch. It covers the process of\\npreparing text for LLM training, including splitting text into word and subword\\ntokens, using byte pair encoding for advanced tokenization, sampling training exam-\\nples with a sliding window approach, and converting tokens into vectors that feed into\\nthe LLM.\\n Chapter 3 focuses on the attention mechanisms used in LLMs. It introduces a basic\\nself-attention framework and progresses to an enhanced self-attention mechanism.\\nThe chapter also covers the implementation of a causal attention module that enables\\nLLMs to generate one token at a time, masking randomly selected attention weights\\nwith dropout to reduce overfitting and stacking multiple causal attention modules\\ninto a multihead attention module.\\n Chapter 4 focuses on coding a GPT-like LLM that can be trained to generate\\nhuman-like text. It covers techniques such as normalizing layer activations to stabilize\\nneural network training, adding shortcut connections in deep neural networks to\\ntrain models more effectively, implementing transformer blocks to create GPT models\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 18}, page_content='ABOUT THIS BOOK\\nxvii\\nof various sizes, and computing the number of parameters and storage requirements\\nof GPT models.\\n Chapter 5 implements the pretraining process of LLMs. It covers computing the\\ntraining and validation set losses to assess the quality of LLM-generated text, imple-\\nmenting a training function and pretraining the LLM, saving and loading model\\nweights to continue training an LLM, and loading pretrained weights from OpenAI.\\n Chapter 6 introduces different LLM fine-tuning approaches. It covers preparing a\\ndataset for text classification, modifying a pretrained LLM for fine-tuning, fine-tuning\\nan LLM to identify spam messages, and evaluating the accuracy of a fine-tuned LLM\\nclassifier.\\n Chapter 7 explores the instruction fine-tuning process of LLMs. It covers prepar-\\ning a dataset for supervised instruction fine-tuning, organizing instruction data in\\ntraining batches, loading a pretrained LLM and fine-tuning it to follow human\\ninstructions, extracting LLM-generated instruction responses for evaluation, and eval-\\nuating an instruction-fine-tuned LLM.\\nAbout the code\\nTo make it as easy as possible to follow along, all code examples in this book are con-\\nveniently available on the Manning website at https://www.manning.com/books/\\nbuild-a-large-language-model-from-scratch, as well as in Jupyter notebook format on\\nGitHub at https://github.com/rasbt/LLMs-from-scratch. And don’t worry about get-\\nting stuck—solutions to all the code exercises can be found in appendix C.\\n This book contains many examples of source code both in numbered listings and\\nin line with normal text. In both cases, source code is formatted in a fixed-width\\nfont like this to separate it from ordinary text.\\n In many cases, the original source code has been reformatted; we’ve added line\\nbreaks and reworked indentation to accommodate the available page space in the\\nbook. In rare cases, even this was not enough, and listings include line-continuation\\nmarkers (➥). Additionally, comments in the source code have often been removed\\nfrom the listings when the code is described in the text. Code annotations accompany\\nmany of the listings, highlighting important concepts.\\n One of the key goals of this book is accessibility, so the code examples have been\\ncarefully designed to run efficiently on a regular laptop, without the need for any spe-\\ncial hardware. But if you do have access to a GPU, certain sections provide helpful tips\\non scaling up the datasets and models to take advantage of that extra power.\\n Throughout the book, we’ll be using PyTorch as our go-to tensor and a deep learn-\\ning library to implement LLMs from the ground up. If PyTorch is new to you, I recom-\\nmend you start with appendix A, which provides an in-depth introduction, complete\\nwith setup recommendations.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 19}, page_content='ABOUT THIS BOOK\\nxviii\\nliveBook discussion forum\\nPurchase of Build a Large Language Model (From Scratch) includes free access to live-\\nBook, Manning’s online reading platform. Using liveBook’s exclusive discussion fea-\\ntures, you can attach comments to the book globally or to specific sections or\\nparagraphs. It’s a snap to make notes for yourself, ask and answer technical questions,\\nand receive help from the author and other users. To access the forum, go to https://\\nlivebook.manning.com/book/build-a-large-language-model-from-scratch/discussion.\\nYou can also learn more about Manning’s forums and the rules of conduct at https://\\nlivebook.manning.com/discussion.\\n Manning’s commitment to readers is to provide a venue where a meaningful dia-\\nlogue between individual readers and between readers and the author can take place.\\nIt is not a commitment to any specific amount of participation on the part of the\\nauthor, whose contribution to the forum remains voluntary (and unpaid). We suggest\\nyou try asking the author some challenging questions lest his interest stray! The forum\\nand the archives of previous discussions will be accessible from the publisher’s website\\nas long as the book is in print.\\nOther online resources\\nInterested in the latest AI and LLM research trends?\\n\\uf0a1Check out my blog at https://magazine.sebastianraschka.com, where I regularly\\ndiscusses the latest AI research with a focus on LLMs.\\nNeed help getting up to speed with deep learning and PyTorch?\\n\\uf0a1I offer several free courses on my website at https://sebastianraschka.com/\\nteaching. These resources can help you quickly get up to speed with the latest\\ntechniques.\\nLooking for bonus materials related to the book?\\n\\uf0a1Visit the book’s GitHub repository at https://github.com/rasbt/LLMs-from\\n-scratch to find additional resources and examples to supplement your learning.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 20}, page_content='xix\\nabout the author\\nSEBASTIAN RASCHKA, PhD, has been working in machine learn-\\ning and AI for more than a decade. In addition to being a\\nresearcher, Sebastian has a strong passion for education. He is\\nknown for his bestselling books on machine learning with\\nPython and his contributions to open source.\\n      Sebastian is a staff research engineer at Lightning AI, focus-\\ning on implementing and training LLMs. Before his industry\\nexperience, Sebastian was an assistant professor in the Depart-\\nment of Statistics at the University of Wisconsin-Madison, where\\nhe focused on deep learning research. You can learn more about Sebastian at https://\\nsebastianraschka.com.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 21}, page_content='xx\\nabout the cover illustration\\nThe figure on the cover of Build a Large Language Model (From Scratch), titled “Le duch-\\nesse,” or “The duchess,” is taken from a book by Louis Curmer published in 1841.\\nEach illustration is finely drawn and colored by hand. \\n In those days, it was easy to identify where people lived and what their trade or sta-\\ntion in life was just by their dress. Manning celebrates the inventiveness and initiative\\nof the computer business with book covers based on the rich diversity of regional cul-\\nture centuries ago, brought back to life by pictures from collections such as this one.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 22}, page_content='1\\nUnderstanding large\\nlanguage models\\nLarge language models (LLMs), such as those offered in OpenAI’s ChatGPT, are\\ndeep neural network models that have been developed over the past few years.\\nThey ushered in a new era for natural language processing (NLP). Before the\\nadvent of LLMs, traditional methods excelled at categorization tasks such as email\\nspam classification and straightforward pattern recognition that could be captured\\nwith handcrafted rules or simpler models. However, they typically underperformed\\nin language tasks that demanded complex understanding and generation abilities,\\nsuch as parsing detailed instructions, conducting contextual analysis, and creating\\ncoherent and contextually appropriate original text. For example, previous genera-\\ntions of language models could not write an email from a list of keywords—a task\\nthat is trivial for contemporary LLMs.\\nThis chapter covers\\n\\uf0a1High-level explanations of the fundamental \\nconcepts behind large language models (LLMs)\\n\\uf0a1Insights into the transformer architecture from \\nwhich LLMs are derived\\n\\uf0a1A plan for building an LLM from scratch\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 23}, page_content='2\\nCHAPTER 1\\nUnderstanding large language models\\n LLMs have remarkable capabilities to understand, generate, and interpret human\\nlanguage. However, it’s important to clarify that when we say language models “under-\\nstand,” we mean that they can process and generate text in ways that appear coher-\\nent and contextually relevant, not that they possess human-like consciousness or\\ncomprehension. \\n Enabled by advancements in deep learning, which is a subset of machine learn-\\ning and artificial intelligence (AI) focused on neural networks, LLMs are trained on\\nvast quantities of text data. This large-scale training allows LLMs to capture deeper\\ncontextual information and subtleties of human language compared to previous\\napproaches. As a result, LLMs have significantly improved performance in a wide\\nrange of NLP tasks, including text translation, sentiment analysis, question answer-\\ning, and many more. \\n Another important distinction between contemporary LLMs and earlier NLP mod-\\nels is that earlier NLP models were typically designed for specific tasks, such as text\\ncategorization, language translation, etc. While those earlier NLP models excelled in\\ntheir narrow applications, LLMs demonstrate a broader proficiency across a wide\\nrange of NLP tasks. \\n The success behind LLMs can be attributed to the transformer architecture that\\nunderpins many LLMs and the vast amounts of data on which LLMs are trained,\\nallowing them to capture a wide variety of linguistic nuances, contexts, and patterns\\nthat would be challenging to encode manually. \\n This shift toward implementing models based on the transformer architecture and\\nusing large training datasets to train LLMs has fundamentally transformed NLP, pro-\\nviding more capable tools for understanding and interacting with human language. \\n The following discussion sets a foundation to accomplish the primary objective of\\nthis book: understanding LLMs by implementing a ChatGPT-like LLM based on the\\ntransformer architecture step by step in code.\\n1.1\\nWhat is an LLM?\\nAn LLM is a neural network designed to understand, generate, and respond to human-\\nlike text. These models are deep neural networks trained on massive amounts of text\\ndata, sometimes encompassing large portions of the entire publicly available text on\\nthe internet.\\n The “large” in “large language model” refers to both the model’s size in terms of\\nparameters and the immense dataset on which it’s trained. Models like this often have\\ntens or even hundreds of billions of parameters, which are the adjustable weights in\\nthe network that are optimized during training to predict the next word in a sequence.\\nNext-word prediction is sensible because it harnesses the inherent sequential nature\\nof language to train models on understanding context, structure, and relationships\\nwithin text. Yet, it is a very simple task, and so it is surprising to many researchers that\\nit can produce such capable models. In later chapters, we will discuss and implement\\nthe next-word training procedure step by step.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 24}, page_content='3\\n1.1\\nWhat is an LLM?\\n LLMs utilize an architecture called the transformer, which allows them to pay selec-\\ntive attention to different parts of the input when making predictions, making them\\nespecially adept at handling the nuances and complexities of human language. \\n Since LLMs are capable of generating text, LLMs are also often referred to as a form\\nof generative artificial intelligence, often abbreviated as generative AI or GenAI. As illus-\\ntrated in figure 1.1, AI encompasses the broader field of creating machines that can\\nperform tasks requiring human-like intelligence, including understanding lan-\\nguage, recognizing patterns, and making decisions, and includes subfields like machine\\nlearning and deep learning. \\nThe algorithms used to implement AI are the focus of the field of machine learning.\\nSpecifically, machine learning involves the development of algorithms that can learn\\nfrom and make predictions or decisions based on data without being explicitly pro-\\ngrammed. To illustrate this, imagine a spam filter as a practical application of\\nmachine learning. Instead of manually writing rules to identify spam emails, a\\nmachine learning algorithm is fed examples of emails labeled as spam and legitimate\\nemails. By minimizing the error in its predictions on a training dataset, the model\\nthen learns to recognize patterns and characteristics indicative of spam, enabling it to\\nclassify new emails as either spam or not spam.\\n As illustrated in figure 1.1, deep learning is a subset of machine learning that focuses\\non utilizing neural networks with three or more layers (also called deep neural net-\\nworks) to model complex patterns and abstractions in data. In contrast to deep learn-\\ning, traditional machine learning requires manual feature extraction. This means that\\nhuman experts need to identify and select the most relevant features for the model.\\nArtiﬁcial intelligence\\nMachine learning\\nDeep learning\\nLarge language models\\nSystems with\\nhuman-like intelligence\\nAlgorithms that learn rules\\nautomatically from data\\nMachine learning with\\nneural networks consisting\\nof many layers\\nDeep neural network for\\nparsing and generating\\nhuman-like text\\nGenAI\\nGenAI involves the use of\\ndeep neural networks to\\ncreate new content, such\\nas text, images, or various\\nforms of media\\nFigure 1.1\\nAs this hierarchical depiction of the relationship between the different fields suggests, LLMs \\nrepresent a specific application of deep learning techniques, using their ability to process and generate human-\\nlike text. Deep learning is a specialized branch of machine learning that focuses on using multilayer neural \\nnetworks. Machine learning and deep learning are fields aimed at implementing algorithms that enable computers \\nto learn from data and perform tasks that typically require human intelligence.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 25}, page_content='4\\nCHAPTER 1\\nUnderstanding large language models\\n While the field of AI is now dominated by machine learning and deep learning, it\\nalso includes other approaches—for example, using rule-based systems, genetic algo-\\nrithms, expert systems, fuzzy logic, or symbolic reasoning.\\n Returning to the spam classification example, in traditional machine learning,\\nhuman experts might manually extract features from email text such as the fre-\\nquency of certain trigger words (for example, “prize,” “win,” “free”), the number of\\nexclamation marks, use of all uppercase words, or the presence of suspicious links.\\nThis dataset, created based on these expert-defined features, would then be used to\\ntrain the model. In contrast to traditional machine learning, deep learning does not\\nrequire manual feature extraction. This means that human experts do not need to\\nidentify and select the most relevant features for a deep learning model. (However,\\nboth traditional machine learning and deep learning for spam classification still\\nrequire the collection of labels, such as spam or non-spam, which need to be gath-\\nered either by an expert or users.)\\n Let’s look at some of the problems LLMs can solve today, the challenges that LLMs\\naddress, and the general LLM architecture we will implement later.\\n1.2\\nApplications of LLMs\\nOwing to their advanced capabilities to parse and understand unstructured text data,\\nLLMs have a broad range of applications across various domains. Today, LLMs are\\nemployed for machine translation, generation of novel texts (see figure 1.2), senti-\\nment analysis, text summarization, and many other tasks. LLMs have recently been\\nused for content creation, such as writing fiction, articles, and even computer code. \\n LLMs can also power sophisticated chatbots and virtual assistants, such as OpenAI’s\\nChatGPT or Google’s Gemini (formerly called Bard), which can answer user queries\\nand augment traditional search engines such as Google Search or Microsoft Bing.\\n Moreover, LLMs may be used for effective knowledge retrieval from vast volumes\\nof text in specialized areas such as medicine or law. This includes sifting through doc-\\numents, summarizing lengthy passages, and answering technical questions.\\n In short, LLMs are invaluable for automating almost any task that involves parsing\\nand generating text. Their applications are virtually endless, and as we continue to\\ninnovate and explore new ways to use these models, it’s clear that LLMs have the\\npotential to redefine our relationship with technology, making it more conversational,\\nintuitive, and accessible.\\n We will focus on understanding how LLMs work from the ground up, coding an\\nLLM that can generate texts. You will also learn about techniques that allow LLMs to\\ncarry out queries, ranging from answering questions to summarizing text, translating\\ntext into different languages, and more. In other words, you will learn how complex\\nLLM assistants such as ChatGPT work by building one step by step.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 26}, page_content='5\\n1.3\\nStages of building and using LLMs\\n1.3\\nStages of building and using LLMs\\nWhy should we build our own LLMs? Coding an LLM from the ground up is an excel-\\nlent exercise to understand its mechanics and limitations. Also, it equips us with the\\nrequired knowledge for pretraining or fine-tuning existing open source LLM architec-\\ntures to our own domain-specific datasets or tasks. \\nNOTE\\nMost LLMs today are implemented using the PyTorch deep learning\\nlibrary, which is what we will use. Readers can find a comprehensive introduc-\\ntion to PyTorch in appendix A.\\nResearch has shown that when it comes to modeling performance, custom-built\\nLLMs—those tailored for specific tasks or domains—can outperform general-purpose\\nLLMs, such as those provided by ChatGPT, which are designed for a wide array of\\napplications. Examples of these include BloombergGPT (specialized for finance) and\\nLLMs tailored for medical question answering (see appendix B for more details).\\n Using custom-built LLMs offers several advantages, particularly regarding data pri-\\nvacy. For instance, companies may prefer not to share sensitive data with third-party\\nLLM providers like OpenAI due to confidentiality concerns. Additionally, developing\\nsmaller custom LLMs enables deployment directly on customer devices, such as laptops\\nand smartphones, which is something companies like Apple are currently exploring.\\nUser input\\n(instructions)\\nModel output\\nFigure 1.2\\nLLM interfaces enable natural language communication between users and AI systems. This \\nscreenshot shows ChatGPT writing a poem according to a user’s specifications.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 27}, page_content='6\\nCHAPTER 1\\nUnderstanding large language models\\nThis local implementation can significantly decrease latency and reduce server-related\\ncosts. Furthermore, custom LLMs grant developers complete autonomy, allowing\\nthem to control updates and modifications to the model as needed.\\n The general process of creating an LLM includes pretraining and fine-tuning. The\\n“pre” in “pretraining” refers to the initial phase where a model like an LLM is trained\\non a large, diverse dataset to develop a broad understanding of language. This pre-\\ntrained model then serves as a foundational resource that can be further refined\\nthrough fine-tuning, a process where the model is specifically trained on a narrower\\ndataset that is more specific to particular tasks or domains. This two-stage training\\napproach consisting of pretraining and fine-tuning is depicted in figure 1.3.\\nThe first step in creating an LLM is to train it on a large corpus of text data, sometimes\\nreferred to as raw text. Here, “raw” refers to the fact that this data is just regular text\\nwithout any labeling information. (Filtering may be applied, such as removing format-\\nting characters or documents in unknown languages.)\\nNOTE\\nReaders with a background in machine learning may note that label-\\ning information is typically required for traditional machine learning models\\nand deep neural networks trained via the conventional supervised learning\\nparadigm. However, this is not the case for the pretraining stage of LLMs. In\\nthis phase, LLMs use self-supervised learning, where the model generates its\\nown labels from the input data.\\n• Internet texts\\n• Books\\n• Wikipedia\\n• Research articles\\nTrain\\nPretrained LLM\\n(foundation model)\\nRaw, unlabeled text\\n(trillions of words)\\nTrain\\nLabeled dataset\\nFine-tuned LLM\\n• Text completion\\n• Few-shot capabilities\\n• Classiﬁcation\\n• Summarization\\n• Translation\\n• Personal assistant\\n• …\\nAn LLM is pretrained\\non unlabeled text data.\\nThe LLM has a few\\nbasic capabilities\\nafter pretraining.\\nA pretrained LLM can be\\nfurther trained on a labeled\\ndataset to obtain a ﬁne-tuned\\nLLM for speciﬁc tasks.\\nFigure 1.3\\nPretraining an LLM involves next-word prediction on large text datasets. A pretrained LLM \\ncan then be fine-tuned using a smaller labeled dataset.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 28}, page_content='7\\n1.4\\nIntroducing the transformer architecture\\nThis first training stage of an LLM is also known as pretraining, creating an initial pre-\\ntrained LLM, often called a base or foundation model. A typical example of such a model\\nis the GPT-3 model (the precursor of the original model offered in ChatGPT). This\\nmodel is capable of text completion—that is, finishing a half-written sentence pro-\\nvided by a user. It also has limited few-shot capabilities, which means it can learn to\\nperform new tasks based on only a few examples instead of needing extensive train-\\ning data.\\n After obtaining a pretrained LLM from training on large text datasets, where the\\nLLM is trained to predict the next word in the text, we can further train the LLM on\\nlabeled data, also known as fine-tuning.\\n The two most popular categories of fine-tuning LLMs are instruction fine-tuning and\\nclassification fine-tuning. In instruction fine-tuning, the labeled dataset consists of\\ninstruction and answer pairs, such as a query to translate a text accompanied by the\\ncorrectly translated text. In classification fine-tuning, the labeled dataset consists of\\ntexts and associated class labels—for example, emails associated with “spam” and “not\\nspam” labels.\\n We will cover code implementations for pretraining and fine-tuning an LLM, and\\nwe will delve deeper into the specifics of both instruction and classification fine-tuning\\nafter pretraining a base LLM.\\n1.4\\nIntroducing the transformer architecture\\nMost modern LLMs rely on the transformer architecture, which is a deep neural net-\\nwork architecture introduced in the 2017 paper “Attention Is All You Need” (https://\\narxiv.org/abs/1706.03762). To understand LLMs, we must understand the original\\ntransformer, which was developed for machine translation, translating English texts to\\nGerman and French. A simplified version of the transformer architecture is depicted\\nin figure 1.4. \\n The transformer architecture consists of two submodules: an encoder and a\\ndecoder. The encoder module processes the input text and encodes it into a series of\\nnumerical representations or vectors that capture the contextual information of the\\ninput. Then, the decoder module takes these encoded vectors and generates the out-\\nput text. In a translation task, for example, the encoder would encode the text from\\nthe source language into vectors, and the decoder would decode these vectors to gen-\\nerate text in the target language. Both the encoder and decoder consist of many layers\\nconnected by a so-called self-attention mechanism. You may have many questions\\nregarding how the inputs are preprocessed and encoded. These will be addressed in a\\nstep-by-step implementation in subsequent chapters.\\n A key component of transformers and LLMs is the self-attention mechanism (not\\nshown), which allows the model to weigh the importance of different words or tokens\\nin a sequence relative to each other. This mechanism enables the model to capture\\nlong-range dependencies and contextual relationships within the input data, enhanc-\\ning its ability to generate coherent and contextually relevant output. However, due to\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 29}, page_content='8\\nCHAPTER 1\\nUnderstanding large language models\\nits complexity, we will defer further explanation to chapter 3, where we will discuss\\nand implement it step by step.\\n Later variants of the transformer architecture, such as BERT (short for bidirectional\\nencoder representations from transformers) and the various GPT models (short for genera-\\ntive pretrained transformers), built on this concept to adapt this architecture for different\\ntasks. If interested, refer to appendix B for further reading suggestions.\\n BERT, which is built upon the original transformer’s encoder submodule, differs\\nin its training approach from GPT. While GPT is designed for generative tasks, BERT\\nand its variants specialize in masked word prediction, where the model predicts masked\\nInput text\\nOutput layers\\nEncoder\\nEmbeddings\\nDecoder\\n1. The input text to\\nbe translated.\\n8. The complete output\\n(translation)\\n5. A partial output\\ntext: the model\\ncompletes the\\ntranslation one\\nword at a time.\\n3. The encoder has\\naccess to the\\ncomplete input\\ntext to produce\\ntext encodings\\nused by the\\ndecoder.\\n7. The decoder\\ngenerates the\\ntranslated text\\none word at a\\ntime.\\nPreprocessing steps\\nInput text\\nPreprocessing steps\\n2. The input text is\\nprepared for the\\nencoder.\\n4. The encoder returns\\nembedding vectors as\\ninput to the decoder.\\n6. The input text is\\nprepared for the\\ndecoder.\\nFigure 1.4\\nA simplified depiction of the original transformer architecture, which is a deep learning model for \\nlanguage translation. The transformer consists of two parts: (a) an encoder that processes the input text and \\nproduces an embedding representation (a numerical representation that captures many different factors in \\ndifferent dimensions) of the text that the (b) decoder can use to generate the translated text one word at a time. \\nThis figure shows the final stage of the translation process where the decoder has to generate only the final word \\n(“Beispiel”), given the original input text (“This is an example”) and a partially translated sentence (“Das ist \\nein”), to complete the translation.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 30}, page_content='9\\n1.4\\nIntroducing the transformer architecture\\nor hidden words in a given sentence, as shown in figure 1.5. This unique training strategy\\nequips BERT with strengths in text classification tasks, including sentiment prediction\\nand document categorization. As an application of its capabilities, as of this writing, X\\n(formerly Twitter) uses BERT to detect toxic content.\\nGPT, on the other hand, focuses on the decoder portion of the original transformer\\narchitecture and is designed for tasks that require generating texts. This includes\\nmachine translation, text summarization, fiction writing, writing computer code,\\nand more. \\n GPT models, primarily designed and trained to perform text completion tasks,\\nalso show remarkable versatility in their capabilities. These models are adept at exe-\\ncuting both zero-shot and few-shot learning tasks. Zero-shot learning refers to the abil-\\nity to generalize to completely unseen tasks without any prior specific examples. On\\nthe other hand, few-shot learning involves learning from a minimal number of exam-\\nples the user provides as input, as shown in figure 1.6.\\nInput text\\nEncoder\\nDecoder\\nPreprocessing steps\\nInput text\\nPreprocessing steps\\nBERT\\nGPT\\nReceives inputs where words\\nare randomly masked during\\ntraining\\nLearns to\\ngenerate one\\nword at a\\ntime\\nThis is an __ of how concise I __ be\\nThis is an example of how concise I can be\\nThis is an example of how concise I can be\\nThis is an example of how concise I can\\nFills in the\\nmissing\\nwords to\\ngenerate\\nthe original\\nsentence\\nReceives incomplete texts\\nFigure 1.5\\nA visual representation of the transformer’s encoder and decoder submodules. On the left, the \\nencoder segment exemplifies BERT-like LLMs, which focus on masked word prediction and are primarily used for \\ntasks like text classification. On the right, the decoder segment showcases GPT-like LLMs, designed for \\ngenerative tasks and producing coherent text sequences.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 31}, page_content='10\\nCHAPTER 1\\nUnderstanding large language models\\n1.5\\nUtilizing large datasets\\nThe large training datasets for popular GPT- and BERT-like models represent diverse\\nand comprehensive text corpora encompassing billions of words, which include a vast\\narray of topics and natural and computer languages. To provide a concrete example,\\ntable 1.1 summarizes the dataset used for pretraining GPT-3, which served as the base\\nmodel for the first version of ChatGPT.\\n \\nTransformers vs. LLMs\\nToday’s LLMs are based on the transformer architecture. Hence, transformers and\\nLLMs are terms that are often used synonymously in the literature. However, note\\nthat not all transformers are LLMs since transformers can also be used for com-\\nputer vision. Also, not all LLMs are transformers, as there are LLMs based on recur-\\nrent and convolutional architectures. The main motivation behind these alternative\\napproaches is to improve the computational efficiency of LLMs. Whether these alter-\\nnative LLM architectures can compete with the capabilities of transformer-based\\nLLMs and whether they are going to be adopted in practice remains to be seen. For\\nsimplicity, I use the term “LLM” to refer to transformer-based LLMs similar to GPT.\\n(Interested readers can find literature references describing these architectures in\\nappendix B.) \\nBreakfast is the\\nInput\\nOutput\\nmost important meal of the day.\\nTEXT COMPLETION\\nZERO-SHOT\\nFEW-SHOT\\nTranslate English to German:\\nbreakfast =>\\ngaot => goat\\nsheo => shoe\\npohne =>\\nphone\\nCreates plausible text\\ngiven a partial input text\\nCompletes a task given a\\nfew examples of the task\\nCompletes a\\ntask without an\\nexplicit example\\nFigure 1.6\\nIn addition to text completion, GPT-like LLMs can solve various tasks based on their inputs without \\nneeding retraining, fine-tuning, or task-specific model architecture changes. Sometimes it is helpful to provide \\nexamples of the target within the input, which is known as a few-shot setting. However, GPT-like LLMs are also \\ncapable of carrying out tasks without a specific example, which is called zero-shot setting.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 32}, page_content='11\\n1.5\\nUtilizing large datasets\\nTable 1.1 reports the number of tokens, where a token is a unit of text that a model\\nreads and the number of tokens in a dataset is roughly equivalent to the number of\\nwords and punctuation characters in the text. Chapter 2 addresses tokenization, the\\nprocess of converting text into tokens.\\n The main takeaway is that the scale and diversity of this training dataset allow these\\nmodels to perform well on diverse tasks, including language syntax, semantics, and\\ncontext—even some requiring general knowledge. \\nThe pretrained nature of these models makes them incredibly versatile for further\\nfine-tuning on downstream tasks, which is why they are also known as base or founda-\\ntion models. Pretraining LLMs requires access to significant resources and is very\\nexpensive. For example, the GPT-3 pretraining cost is estimated to be $4.6 million in\\nterms of cloud computing credits (https://mng.bz/VxEW). \\nTable 1.1\\nThe pretraining dataset of the popular GPT-3 LLM\\nDataset name\\nDataset description\\nNumber of tokens\\nProportion \\nin training data\\nCommonCrawl (filtered)\\nWeb crawl data\\n410 billion\\n60%\\nWebText2\\nWeb crawl data\\n19 billion\\n22%\\nBooks1\\nInternet-based book corpus\\n12 billion\\n8%\\nBooks2\\nInternet-based book corpus\\n55 billion\\n8%\\nWikipedia\\nHigh-quality text\\n3 billion\\n3%\\nGPT-3 dataset details\\nTable 1.1 displays the dataset used for GPT-3. The proportions column in the table\\nsums up to 100% of the sampled data, adjusted for rounding errors. Although the\\nsubsets in the Number of Tokens column total 499 billion, the model was trained on\\nonly 300 billion tokens. The authors of the GPT-3 paper did not specify why the model\\nwas not trained on all 499 billion tokens.\\nFor context, consider the size of the CommonCrawl dataset, which alone consists of\\n410 billion tokens and requires about 570 GB of storage. In comparison, later itera-\\ntions of models like GPT-3, such as Meta’s LLaMA, have expanded their training\\nscope to include additional data sources like Arxiv research papers (92 GB) and\\nStackExchange’s code-related Q&As (78 GB). \\nThe authors of the GPT-3 paper did not share the training dataset, but a comparable\\ndataset that is publicly available is Dolma: An Open Corpus of Three Trillion Tokens for\\nLLM Pretraining Research by Soldaini et al. 2024 (https://arxiv.org/abs/2402.00159).\\nHowever, the collection may contain copyrighted works, and the exact usage terms\\nmay depend on the intended use case and country.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 33}, page_content='12\\nCHAPTER 1\\nUnderstanding large language models\\n The good news is that many pretrained LLMs, available as open source models,\\ncan be used as general-purpose tools to write, extract, and edit texts that were not\\npart of the training data. Also, LLMs can be fine-tuned on specific tasks with rela-\\ntively smaller datasets, reducing the computational resources needed and improving\\nperformance.\\n We will implement the code for pretraining and use it to pretrain an LLM for educa-\\ntional purposes. All computations are executable on consumer hardware. After imple-\\nmenting the pretraining code, we will learn how to reuse openly available model weights\\nand load them into the architecture we will implement, allowing us to skip the expen-\\nsive pretraining stage when we fine-tune our LLM.\\n1.6\\nA closer look at the GPT architecture\\nGPT was originally introduced in the paper “Improving Language Understanding by\\nGenerative Pre-Training” (https://mng.bz/x2qg) by Radford et al. from OpenAI.\\nGPT-3 is a scaled-up version of this model that has more parameters and was trained\\non a larger dataset. In addition, the original model offered in ChatGPT was created by\\nfine-tuning GPT-3 on a large instruction dataset using a method from OpenAI’s\\nInstructGPT paper (https://arxiv.org/abs/2203.02155). As figure 1.6 shows, these\\nmodels are competent text completion models and can carry out other tasks such as\\nspelling correction, classification, or language translation. This is actually very remark-\\nable given that GPT models are pretrained on a relatively simple next-word prediction\\ntask, as depicted in figure 1.7.\\nThe next-word prediction task is a form of self-supervised learning, which is a form of\\nself-labeling. This means that we don’t need to collect labels for the training data\\nexplicitly but can use the structure of the data itself: we can use the next word in a sen-\\ntence or document as the label that the model is supposed to predict. Since this next-\\nword prediction task allows us to create labels “on the fly,” it is possible to use massive\\nunlabeled text datasets to train LLMs.\\n Compared to the original transformer architecture we covered in section 1.4, the\\ngeneral GPT architecture is relatively simple. Essentially, it’s just the decoder part\\nwithout the encoder (figure 1.8). Since decoder-style models like GPT generate text\\nby predicting text one word at a time, they are considered a type of autoregressive\\nmodel. Autoregressive models incorporate their previous outputs as inputs for future\\nThe model is simply trained to\\npredict the next   word\\nFigure 1.7\\nIn the next-word prediction pretraining task for GPT \\nmodels, the system learns to predict the upcoming word in a \\nsentence by looking at the words that have come before it. This \\napproach helps the model understand how words and phrases \\ntypically fit together in language, forming a foundation that can \\nbe applied to various other tasks.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 34}, page_content='13\\n1.6\\nA closer look at the GPT architecture\\npredictions. Consequently, in GPT, each new word is chosen based on the sequence\\nthat precedes it, which improves the coherence of the resulting text.\\n Architectures such as GPT-3 are also significantly larger than the original transformer\\nmodel. For instance, the original transformer repeated the encoder and decoder blocks\\nsix times. GPT-3 has 96 transformer layers and 175 billion parameters in total.\\nGPT-3 was introduced in 2020, which, by the standards of deep learning and large lan-\\nguage model development, is considered a long time ago. However, more recent archi-\\ntectures, such as Meta’s Llama models, are still based on the same underlying concepts,\\nintroducing only minor modifications. Hence, understanding GPT remains as relevant\\nas ever, so I focus on implementing the prominent architecture behind GPT while pro-\\nviding pointers to specific tweaks employed by alternative LLMs.\\n Although the original transformer model, consisting of encoder and decoder blocks,\\nwas explicitly designed for language translation, GPT models—despite their larger yet\\nOutput layers\\nDecoder\\nInput text\\nPreprocessing steps\\nOutput layers\\nDecoder\\nInput text\\nPreprocessing steps\\nOutput layers\\nDecoder\\nInput text\\nPreprocessing steps\\nIteration 1\\nIteration 2\\nIteration 3\\nCreates the next\\nword based on\\nthe input text\\nThe output of the\\nprevious round\\nserves as input to\\nthe next round.\\nFigure 1.8\\nThe GPT architecture employs only the decoder portion of the original transformer. It is designed for \\nunidirectional, left-to-right processing, making it well suited for text generation and next-word prediction tasks to \\ngenerate text in an iterative fashion, one word at a time.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 35}, page_content='14\\nCHAPTER 1\\nUnderstanding large language models\\nsimpler decoder-only architecture aimed at next-word prediction—are also capable of\\nperforming translation tasks. This capability was initially unexpected to researchers, as\\nit emerged from a model primarily trained on a next-word prediction task, which is a\\ntask that did not specifically target translation.\\n The ability to perform tasks that the model wasn’t explicitly trained to perform is\\ncalled an emergent behavior. This capability isn’t explicitly taught during training but\\nemerges as a natural consequence of the model’s exposure to vast quantities of multi-\\nlingual data in diverse contexts. The fact that GPT models can “learn” the translation\\npatterns between languages and perform translation tasks even though they weren’t\\nspecifically trained for it demonstrates the benefits and capabilities of these large-\\nscale, generative language models. We can perform diverse tasks without using diverse\\nmodels for each.\\n1.7\\nBuilding a large language model\\nNow that we’ve laid the groundwork for understanding LLMs, let’s code one from\\nscratch. We will take the fundamental idea behind GPT as a blueprint and tackle this\\nin three stages, as outlined in figure 1.9.\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2\\nSTAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\nImplements the data sampling and\\nunderstand the basic mechanism\\nPretrains the LLM on unlabeled\\ndata to obtain a foundation\\nmodel for further ﬁne-tuning\\nFine-tunes the pretrained\\nLLM to create a\\nclassiﬁcation model\\nFine-tunes the pretrained\\nLLM to create a personal\\nassistant or chat model\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights\\n8) Fine-tuning\\n3) LLM\\narchitecture\\nFigure 1.9\\nThe three main stages of coding an LLM are implementing the LLM architecture and data preparation \\nprocess (stage 1), pretraining an LLM to create a foundation model (stage 2), and fine-tuning the foundation \\nmodel to become a personal assistant or text classifier (stage 3).\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 36}, page_content='15\\nSummary\\nIn stage 1, we will learn about the fundamental data preprocessing steps and code the\\nattention mechanism at the heart of every LLM. Next, in stage 2, we will learn how to\\ncode and pretrain a GPT-like LLM capable of generating new texts. We will also go\\nover the fundamentals of evaluating LLMs, which is essential for developing capable\\nNLP systems. \\n Pretraining an LLM from scratch is a significant endeavor, demanding thousands\\nto millions of dollars in computing costs for GPT-like models. Therefore, the focus of\\nstage 2 is on implementing training for educational purposes using a small dataset. In\\naddition, I also provide code examples for loading openly available model weights.\\n Finally, in stage 3, we will take a pretrained LLM and fine-tune it to follow instruc-\\ntions such as answering queries or classifying texts—the most common tasks in many\\nreal-world applications and research.\\n I hope you are looking forward to embarking on this exciting journey!\\nSummary\\n\\uf0a1LLMs have transformed the field of natural language processing, which previ-\\nously mostly relied on explicit rule-based systems and simpler statistical meth-\\nods. The advent of LLMs introduced new deep learning-driven approaches\\nthat led to advancements in understanding, generating, and translating human\\nlanguage.\\n\\uf0a1Modern LLMs are trained in two main steps: \\n– First, they are pretrained on a large corpus of unlabeled text by using the\\nprediction of the next word in a sentence as a label.\\n– Then, they are fine-tuned on a smaller, labeled target dataset to follow\\ninstructions or perform classification tasks.\\n\\uf0a1LLMs are based on the transformer architecture. The key idea of the trans-\\nformer architecture is an attention mechanism that gives the LLM selective\\naccess to the whole input sequence when generating the output one word at\\na time.\\n\\uf0a1The original transformer architecture consists of an encoder for parsing text\\nand a decoder for generating text. \\n\\uf0a1LLMs for generating text and following instructions, such as GPT-3 and\\nChatGPT, only implement decoder modules, simplifying the architecture.\\n\\uf0a1Large datasets consisting of billions of words are essential for pretraining\\nLLMs.\\n\\uf0a1While the general pretraining task for GPT-like models is to predict the next\\nword in a sentence, these LLMs exhibit emergent properties, such as capabili-\\nties to classify, translate, or summarize texts.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 37}, page_content='16\\nCHAPTER 1\\nUnderstanding large language models\\n\\uf0a1Once an LLM is pretrained, the resulting foundation model can be fine-tuned\\nmore efficiently for various downstream tasks. \\n\\uf0a1LLMs fine-tuned on custom datasets can outperform general LLMs on specific\\ntasks.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 38}, page_content='17\\nWorking with text data\\nSo far, we’ve covered the general structure of large language models (LLMs) and\\nlearned that they are pretrained on vast amounts of text. Specifically, our focus was on\\ndecoder-only LLMs based on the transformer architecture, which underlies the mod-\\nels used in ChatGPT and other popular GPT-like LLMs.\\n During the pretraining stage, LLMs process text one word at a time. Training\\nLLMs with millions to billions of parameters using a next-word prediction task\\nyields models with impressive capabilities. These models can then be further fine-\\ntuned to follow general instructions or perform specific target tasks. But before we\\ncan implement and train LLMs, we need to prepare the training dataset, as illus-\\ntrated in figure 2.1.\\nThis chapter covers\\n\\uf0a1Preparing text for large language model training\\n\\uf0a1Splitting text into word and subword tokens\\n\\uf0a1Byte pair encoding as a more advanced way of \\ntokenizing text\\n\\uf0a1Sampling training examples with a sliding window \\napproach\\n\\uf0a1Converting tokens into vectors that feed into a \\nlarge language model\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 39}, page_content='18\\nCHAPTER 2\\nWorking with text data\\nYou’ll learn how to prepare input text for training LLMs. This involves splitting text\\ninto individual word and subword tokens, which can then be encoded into vector rep-\\nresentations for the LLM. You’ll also learn about advanced tokenization schemes like\\nbyte pair encoding, which is utilized in popular LLMs like GPT. Lastly, we’ll imple-\\nment a sampling and data-loading strategy to produce the input-output pairs neces-\\nsary for training LLMs.\\n2.1\\nUnderstanding word embeddings\\nDeep neural network models, including LLMs, cannot process raw text directly. Since\\ntext is categorical, it isn’t compatible with the mathematical operations used to imple-\\nment and train neural networks. Therefore, we need a way to represent words as\\ncontinuous-valued vectors. \\nNOTE\\nReaders unfamiliar with vectors and tensors in a computational con-\\ntext can learn more in appendix A, section A.2.2.\\nThe concept of converting data into a vector format is often referred to as embedding.\\nUsing a specific neural network layer or another pretrained neural network model, we\\ncan embed different data types—for example, video, audio, and text, as illustrated in\\nfigure 2.2. However, it’s important to note that different data formats require distinct\\nembedding models. For example, an embedding model designed for text would not\\nbe suitable for embedding audio or video data.\\nImplements the data\\nsampling pipeline\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2\\nSTAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\nImplements the data sampling and\\nunderstand the basic mechanism\\nPretrains the LLM on unlabeled\\ndata to obtain a foundation\\nmodel for further ﬁnetuning\\nFine-tunes the pretrained\\nLLM to create a\\nclassiﬁcation model\\nFine-tunes the pretrained\\nLLM to create a personal\\nassistant or chat model\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights\\n8) Fine-tuning\\n3) LLM\\narchitecture\\nFigure 2.1\\nThe three main stages of coding an LLM. This chapter focuses on step 1 of stage 1: implementing the \\ndata sample pipeline.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 40}, page_content='19\\n2.1\\nUnderstanding word embeddings\\nAt its core, an embedding is a mapping from discrete objects, such as words, images,\\nor even entire documents, to points in a continuous vector space—the primary pur-\\npose of embeddings is to convert nonnumeric data into a format that neural networks\\ncan process.\\n While word embeddings are the most common form of text embedding, there are\\nalso embeddings for sentences, paragraphs, or whole documents. Sentence or para-\\ngraph embeddings are popular choices for retrieval-augmented generation. Retrieval-\\naugmented generation combines generation (like producing text) with retrieval (like\\nsearching an external knowledge base) to pull relevant information when generating\\ntext, which is a technique that is beyond the scope of this book. Since our goal is to\\ntrain GPT-like LLMs, which learn to generate text one word at a time, we will focus on\\nword embeddings.\\n Several algorithms and frameworks have been developed to generate word embed-\\ndings. One of the earlier and most popular examples is the Word2Vec approach.\\nWord2Vec trained neural network architecture to generate word embeddings by pre-\\ndicting the context of a word given the target word or vice versa. The main idea\\nbehind Word2Vec is that words that appear in similar contexts tend to have similar\\nmeanings. Consequently, when projected into two-dimensional word embeddings for\\nvisualization purposes, similar terms are clustered together, as shown in figure 2.3.\\n Word embeddings can have varying dimensions, from one to thousands. A higher\\ndimensionality might capture more nuanced relationships but at the cost of computa-\\ntional efficiency.\\nVideo\\nsample\\nAudio\\nsample\\nText\\nsample\\nVideo embedding model\\nAudio embedding model\\nText embedding model\\nVideo embedding vector\\nAudio embedding vector\\nText embedding vector\\n1.23\\n-0.31\\n0.89\\n-0.15\\n0.45\\n2.11\\n1.78\\n0.18\\n-2.10\\nEmbedding model converts raw\\ninput into a vector representation\\nUnlabeled\\ninput data\\nVector representation\\nof the input\\nFigure 2.2\\nDeep learning models cannot process data formats like video, audio, and text in their raw \\nform. Thus, we use an embedding model to transform this raw data into a dense vector representation \\nthat deep learning architectures can easily understand and process. Specifically, this figure illustrates \\nthe process of converting raw data into a three-dimensional numerical vector.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 41}, page_content='20\\nCHAPTER 2\\nWorking with text data\\nWhile we can use pretrained models such as Word2Vec to generate embeddings for\\nmachine learning models, LLMs commonly produce their own embeddings that are\\npart of the input layer and are updated during training. The advantage of optimizing\\nthe embeddings as part of the LLM training instead of using Word2Vec is that the\\nembeddings are optimized to the specific task and data at hand. We will implement\\nsuch embedding layers later in this chapter. (LLMs can also create contextualized out-\\nput embeddings, as we discuss in chapter 3.)\\n Unfortunately, high-dimensional embeddings present a challenge for visualiza-\\ntion because our sensory perception and common graphical representations are\\ninherently limited to three dimensions or fewer, which is why figure 2.3 shows two-\\ndimensional embeddings in a two-dimensional scatterplot. However, when working\\nwith LLMs, we typically use embeddings with a much higher dimensionality. For\\nboth GPT-2 and GPT-3, the embedding size (often referred to as the dimensionality\\nof the model’s hidden states) varies based on the specific model variant and size. It\\nis a tradeoff between performance and efficiency. The smallest GPT-2 models (117M\\nand 125M parameters) use an embedding size of 768 dimensions to provide con-\\ncrete examples. The largest GPT-3 model (175B parameters) uses an embedding\\nsize of 12,288 dimensions. \\n Next, we will walk through the required steps for preparing the embeddings used\\nby an LLM, which include splitting text into words, converting words into tokens, and\\nturning tokens into embedding vectors.\\nFirst dimension\\nSecond\\ndimension\\neagle\\nduck\\ngoose\\nsquirrel\\nlong\\nlonger\\nlongest\\nGermany\\nBerlin\\nEngland\\nLondon\\nVector embedding of\\nthe word squirrel\\nVector embeddings of\\ndifferent types of birds\\nFigure 2.3\\nIf word embeddings are two-dimensional, we can plot them in a two-\\ndimensional scatterplot for visualization purposes as shown here. When using word \\nembedding techniques, such as Word2Vec, words corresponding to similar concepts \\noften appear close to each other in the embedding space. For instance, different types \\nof birds appear closer to each other in the embedding space than in countries and cities.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 42}, page_content='21\\n2.2\\nTokenizing text\\n2.2\\nTokenizing text\\nLet’s discuss how we split input text into individual tokens, a required preprocessing\\nstep for creating embeddings for an LLM. These tokens are either individual words or\\nspecial characters, including punctuation characters, as shown in figure 2.4.\\nThe text we will tokenize for LLM training is “The Verdict,” a short story by Edith\\nWharton, which has been released into the public domain and is thus permitted to be\\nused for LLM training tasks. The text is available on Wikisource at https://en.wikisource\\n.org/wiki/The_Verdict, and you can copy and paste it into a text file, which I copied\\ninto a text file \"the-verdict.txt\".\\n Alternatively, you can find this \"the-verdict.txt\" file in this book’s GitHub\\nrepository at https://mng.bz/Adng. You can download the file with the following\\nPython code:\\n \\n \\nGPT-like\\ndecoder-only\\ntransformer\\nInput text:\\nToken embeddings:\\nThis is an example.\\nTokenized text:\\nThis\\nis\\nan\\nexample\\nOutput text\\nPostprocessing steps\\nToken IDs:\\n40134\\n2052\\n133\\n389\\n.\\n12\\nThis section covers the\\nconcept of splitting\\ntext into tokens\\nFigure 2.4\\nA view of the text processing steps in the context of an LLM. Here, we split an \\ninput text into individual tokens, which are either words or special characters, such as \\npunctuation characters.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 43}, page_content='22\\nCHAPTER 2\\nWorking with text data\\nimport urllib.request\\nurl = (\"https://raw.githubusercontent.com/rasbt/\"\\n       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\\n       \"the-verdict.txt\")\\nfile_path = \"the-verdict.txt\"\\nurllib.request.urlretrieve(url, file_path)\\nNext, we can load the the-verdict.txt file using Python’s standard file reading utilities. \\nwith open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\\n    raw_text = f.read()\\nprint(\"Total number of character:\", len(raw_text))\\nprint(raw_text[:99])\\nThe print command prints the total number of characters followed by the first 100\\ncharacters of this file for illustration purposes:\\nTotal number of character: 20479\\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow \\nenough--so it was no \\nOur goal is to tokenize this 20,479-character short story into individual words and spe-\\ncial characters that we can then turn into embeddings for LLM training.\\nNOTE\\nIt’s common to process millions of articles and hundreds of thousands\\nof books—many gigabytes of text—when working with LLMs. However, for\\neducational purposes, it’s sufficient to work with smaller text samples like a\\nsingle book to illustrate the main ideas behind the text processing steps and\\nto make it possible to run it in a reasonable time on consumer hardware.\\nHow can we best split this text to obtain a list of tokens? For this, we go on a small\\nexcursion and use Python’s regular expression library re for illustration purposes.\\n(You don’t have to learn or memorize any regular expression syntax since we will later\\ntransition to a prebuilt tokenizer.)\\n Using some simple example text, we can use the re.split command with the fol-\\nlowing syntax to split a text on whitespace characters:\\nimport re\\ntext = \"Hello, world. This, is a test.\"\\nresult = re.split(r\\'(\\\\s)\\', text)\\nprint(result)\\nThe result is a list of individual words, whitespaces, and punctuation characters:\\n[\\'Hello,\\', \\' \\', \\'world.\\', \\' \\', \\'This,\\', \\' \\', \\'is\\', \\' \\', \\'a\\', \\' \\', \\'test.\\']\\nListing 2.1\\nReading in a short story as text sample into Python\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 44}, page_content='23\\n2.2\\nTokenizing text\\nThis simple tokenization scheme mostly works for separating the example text into\\nindividual words; however, some words are still connected to punctuation characters\\nthat we want to have as separate list entries. We also refrain from making all text lower-\\ncase because capitalization helps LLMs distinguish between proper nouns and com-\\nmon nouns, understand sentence structure, and learn to generate text with proper\\ncapitalization.\\n Let’s modify the regular expression splits on whitespaces (\\\\s), commas, and peri-\\nods ([,.]):\\nresult = re.split(r\\'([,.]|\\\\s)\\', text)\\nprint(result)\\nWe can see that the words and punctuation characters are now separate list entries just\\nas we wanted:\\n[\\'Hello\\', \\',\\', \\'\\', \\' \\', \\'world\\', \\'.\\', \\'\\', \\' \\', \\'This\\', \\',\\', \\'\\', \\' \\', \\'is\\',\\n\\' \\', \\'a\\', \\' \\', \\'test\\', \\'.\\', \\'\\']\\nA small remaining problem is that the list still includes whitespace characters. Option-\\nally, we can remove these redundant characters safely as follows:\\nresult = [item for item in result if item.strip()]\\nprint(result)\\nThe resulting whitespace-free output looks like as follows:\\n[\\'Hello\\', \\',\\', \\'world\\', \\'.\\', \\'This\\', \\',\\', \\'is\\', \\'a\\', \\'test\\', \\'.\\']\\nNOTE\\nWhen developing a simple tokenizer, whether we should encode\\nwhitespaces as separate characters or just remove them depends on our appli-\\ncation and its requirements. Removing whitespaces reduces the memory and\\ncomputing requirements. However, keeping whitespaces can be useful if we\\ntrain models that are sensitive to the exact structure of the text (for example,\\nPython code, which is sensitive to indentation and spacing). Here, we remove\\nwhitespaces for simplicity and brevity of the tokenized outputs. Later, we will\\nswitch to a tokenization scheme that includes whitespaces.\\nThe tokenization scheme we devised here works well on the simple sample text. Let’s\\nmodify it a bit further so that it can also handle other types of punctuation, such as ques-\\ntion marks, quotation marks, and the double-dashes we have seen earlier in the first 100\\ncharacters of Edith Wharton’s short story, along with additional special characters:\\ntext = \"Hello, world. Is this-- a test?\"\\nresult = re.split(r\\'([,.:;?_!\"()\\\\\\']|--|\\\\s)\\', text)\\nresult = [item.strip() for item in result if item.strip()]\\nprint(result)\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 45}, page_content='24\\nCHAPTER 2\\nWorking with text data\\nThe resulting output is:\\n[\\'Hello\\', \\',\\', \\'world\\', \\'.\\', \\'Is\\', \\'this\\', \\'--\\', \\'a\\', \\'test\\', \\'?\\']\\nAs we can see based on the results summarized in figure 2.5, our tokenization scheme\\ncan now handle the various special characters in the text successfully.\\nNow that we have a basic tokenizer working, let’s apply it to Edith Wharton’s entire\\nshort story:\\npreprocessed = re.split(r\\'([,.:;?_!\"()\\\\\\']|--|\\\\s)\\', raw_text)\\npreprocessed = [item.strip() for item in preprocessed if item.strip()]\\nprint(len(preprocessed))\\nThis print statement outputs 4690, which is the number of tokens in this text (without\\nwhitespaces). Let’s print the first 30 tokens for a quick visual check:\\nprint(preprocessed[:30])\\nThe resulting output shows that our tokenizer appears to be handling the text well\\nsince all words and special characters are neatly separated:\\n[\\'I\\', \\'HAD\\', \\'always\\', \\'thought\\', \\'Jack\\', \\'Gisburn\\', \\'rather\\', \\'a\\',\\n\\'cheap\\', \\'genius\\', \\'--\\', \\'though\\', \\'a\\', \\'good\\', \\'fellow\\', \\'enough\\',\\n\\'--\\', \\'so\\', \\'it\\', \\'was\\', \\'no\\', \\'great\\', \\'surprise\\', \\'to\\', \\'me\\', \\'to\\',\\n\\'hear\\', \\'that\\', \\',\\', \\'in\\']\\n2.3\\nConverting tokens into token IDs\\nNext, let’s convert these tokens from a Python string to an integer representation to\\nproduce the token IDs. This conversion is an intermediate step before converting the\\ntoken IDs into embedding vectors.\\n To map the previously generated tokens into token IDs, we have to build a vocabu-\\nlary first. This vocabulary defines how we map each unique word and special character\\nto a unique integer, as shown in figure 2.6.\\nInput text\\nHello, world. Is this-- a test?\\nHello\\n,\\nworld\\n.\\nIs\\nthis\\n--\\na\\ntest\\n?\\nTokenized text\\nFigure 2.5\\nThe tokenization scheme we implemented so far splits \\ntext into individual words and punctuation characters. In this specific \\nexample, the sample text gets split into 10 individual tokens.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 46}, page_content='25\\n2.3\\nConverting tokens into token IDs\\nNow that we have tokenized Edith Wharton’s short story and assigned it to a Python\\nvariable called preprocessed, let’s create a list of all unique tokens and sort them\\nalphabetically to determine the vocabulary size:\\nall_words = sorted(set(preprocessed))\\nvocab_size = len(all_words)\\nprint(vocab_size)\\nAfter determining that the vocabulary size is 1,130 via this code, we create the vocabu-\\nlary and print its first 51 entries for illustration purposes.\\nvocab = {token:integer for integer,token in enumerate(all_words)}\\nfor i, item in enumerate(vocab.items()):\\n    print(item)\\n    if i >= 50:\\n        break\\nListing 2.2\\nCreating a vocabulary\\nComplete training dataset\\nThe quick brown fox jumps\\nover the lazy dog\\nTokenized training dataset\\nThe\\nquick\\nbrown\\nVocabulary\\nbrown\\ndog\\nfox\\n0\\n1\\n2\\n1. Tokenization breaks\\ndown the input text\\ninto individual tokens.\\n2. Each unique token is\\nadded to the vocabulary\\nin alphabetical order.\\njumps\\nlazy\\nover\\n3\\n4\\n5\\nquick\\n6\\nthe\\n7\\nThe vocabulary\\ncontains all unique\\ntokens in the training\\nset and is usually\\nsorted alphabetically.\\nEach unique token is\\nmapped to a unique\\ninteger called token ID.\\nUnique tokens\\nToken IDs\\nThe training set consists\\nof only one sentence for\\nillustration purposes.\\nInput text\\nFigure 2.6\\nWe build a vocabulary by tokenizing the entire text in a training dataset into individual \\ntokens. These individual tokens are then sorted alphabetically, and duplicate tokens are removed. \\nThe unique tokens are then aggregated into a vocabulary that defines a mapping from each unique \\ntoken to a unique integer value. The depicted vocabulary is purposefully small and contains no \\npunctuation or special characters for simplicity.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 47}, page_content='26\\nCHAPTER 2\\nWorking with text data\\nThe output is\\n(\\'!\\', 0)\\n(\\'\"\\', 1)\\n(\"\\'\", 2)\\n...\\n(\\'Her\\', 49)\\n(\\'Hermia\\', 50)\\nAs we can see, the dictionary contains individual tokens associated with unique inte-\\nger labels. Our next goal is to apply this vocabulary to convert new text into token IDs\\n(figure 2.7).\\nWhen we want to convert the outputs of an LLM from numbers back into text, we need a\\nway to turn token IDs into text. For this, we can create an inverse version of the vocabu-\\nlary that maps token IDs back to the corresponding text tokens. \\nTokenization breaks down the\\ntraining set into individual tokens.\\nNew tokenized sample\\ntext is mapped to\\ntoken IDs using an\\nexisting vocabulary.\\nFigure 2.7\\nStarting with a new text sample, we tokenize the text and use the vocabulary to convert \\nthe text tokens into token IDs. The vocabulary is built from the entire training set and can be applied \\nto the training set itself and any new text samples. The depicted vocabulary contains no punctuation \\nor special characters for simplicity.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 48}, page_content='27\\n2.3\\nConverting tokens into token IDs\\n Let’s implement a complete tokenizer class in Python with an encode method that\\nsplits text into tokens and carries out the string-to-integer mapping to produce token\\nIDs via the vocabulary. In addition, we’ll implement a decode method that carries out\\nthe reverse integer-to-string mapping to convert the token IDs back into text. The fol-\\nlowing listing shows the code for this tokenizer implementation.\\nclass SimpleTokenizerV1:\\n    def __init__(self, vocab):\\n        self.str_to_int = vocab           \\n        self.int_to_str = {i:s for s,i in vocab.items()}       \\n    \\n    def encode(self, text):        \\n        preprocessed = re.split(r\\'([,.?_!\"()\\\\\\']|--|\\\\s)\\', text)\\n        preprocessed = [\\n            item.strip() for item in preprocessed if item.strip()\\n        ]\\n        ids = [self.str_to_int[s] for s in preprocessed]\\n        return ids\\n        \\n    def decode(self, ids):        \\n        text = \" \".join([self.int_to_str[i] for i in ids]) \\n        \\n        text = re.sub(r\\'\\\\s+([,.?!\"()\\\\\\'])\\', r\\'\\\\1\\', text)   \\n        return text\\nUsing the SimpleTokenizerV1 Python class, we can now instantiate new tokenizer\\nobjects via an existing vocabulary, which we can then use to encode and decode text,\\nas illustrated in figure 2.8.\\n Let’s instantiate a new tokenizer object from the SimpleTokenizerV1 class and\\ntokenize a passage from Edith Wharton’s short story to try it out in practice:\\ntokenizer = SimpleTokenizerV1(vocab)\\ntext = \"\"\"\"It\\'s the last he painted, you know,\" \\n       Mrs. Gisburn said with pardonable pride.\"\"\"\\nids = tokenizer.encode(text)\\nprint(ids)\\nThe preceding code prints the following token IDs:\\n[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, \\n754, 793, 7]\\nNext, let’s see whether we can turn these token IDs back into text using the decode\\nmethod:\\nprint(tokenizer.decode(ids))\\nListing 2.3\\nImplementing a simple text tokenizer\\nStores the vocabulary as a class attribute for\\naccess in the encode and decode methods\\nCreates an inverse\\nvocabulary that maps\\ntoken IDs back to the\\noriginal text tokens\\nProcesses \\ninput text \\ninto token \\nIDs\\nConverts token IDs \\nback into text\\nRemoves spaces \\nbefore the specified \\npunctuation\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 49}, page_content='28\\nCHAPTER 2\\nWorking with text data\\nThis outputs:\\n\\'\" It\\\\\\' s the last he painted, you know,\" Mrs. Gisburn said with \\npardonable pride.\\'\\nBased on this output, we can see that the decode method successfully converted the\\ntoken IDs back into the original text.\\n So far, so good. We implemented a tokenizer capable of tokenizing and detokeniz-\\ning text based on a snippet from the training set. Let’s now apply it to a new text sam-\\nple not contained in the training set:\\ntext = \"Hello, do you like tea?\"\\nprint(tokenizer.encode(text))\\nExecuting this code will result in the following error:\\nKeyError: \\'Hello\\'\\nThe problem is that the word “Hello” was not used in the “The Verdict” short story.\\nHence, it is not contained in the vocabulary. This highlights the need to consider\\nlarge and diverse training sets to extend the vocabulary when working on LLMs.\\nThe brown dog\\nplayfully chased\\nthe swift fox\\nSample text\\nTokenized sample text\\nThe\\nbrown\\ndog\\nToken IDs\\n7\\n0\\n1\\nThe brown dog\\nplayfully chased\\nthe swift fox\\nSample text\\nTokenized sample text\\nThe\\nbrown\\ndog\\nToken IDs\\n7\\n0\\n1\\nCalling tokenizer.encode(text) on sample text\\nCalling tokenizer.decode(ids) on token IDs\\nVocabulary\\nInverse\\nvocabulary\\nFigure 2.8\\nTokenizer implementations share two common methods: an encode method and a decode \\nmethod. The encode method takes in the sample text, splits it into individual tokens, and converts the \\ntokens into token IDs via the vocabulary. The decode method takes in token IDs, converts them back \\ninto text tokens, and concatenates the text tokens into natural text.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 50}, page_content='29\\n2.4\\nAdding special context tokens\\n Next, we will test the tokenizer further on text that contains unknown words and\\ndiscuss additional special tokens that can be used to provide further context for an\\nLLM during training.\\n2.4\\nAdding special context tokens\\nWe need to modify the tokenizer to handle unknown words. We also need to address\\nthe usage and addition of special context tokens that can enhance a model’s under-\\nstanding of context or other relevant information in the text. These special tokens\\ncan include markers for unknown words and document boundaries, for example. In\\nparticular, we will modify the vocabulary and tokenizer, SimpleTokenizerV2, to sup-\\nport two new tokens, <|unk|> and <|endoftext|>, as illustrated in figure 2.9.\\nWe can modify the tokenizer to use an <|unk|> token if it encounters a word that is\\nnot part of the vocabulary. Furthermore, we add a token between unrelated texts.\\nFor example, when training GPT-like LLMs on multiple independent documents or\\nbooks, it is common to insert a token before each document or book that follows a\\nprevious text source, as illustrated in figure 2.10. This helps the LLM understand\\nthat although these text sources are concatenated for training, they are, in fact,\\nunrelated.\\nSample text\\nTokenized sample text\\nThe\\nbrown\\ndog\\nToken IDs\\nplayfully\\n7\\n0\\n1\\n783\\nbrown\\ndog\\nfox\\n0\\n1\\n2\\n<|unk|>\\n<|endoftext|>\\n783\\n784\\nExisting vocabulary\\nExtend vocabulary\\nwith additional\\nspecial tokens\\nThe brown dog\\nplayfully chased\\nthe swift fox\\nFigure 2.9\\nWe add special tokens to a vocabulary to deal with certain contexts. For instance, \\nwe add an <|unk|> token to represent new and unknown words that were not part of the training \\ndata and thus not part of the existing vocabulary. Furthermore, we add an <|endoftext|> \\ntoken that we can use to separate two unrelated text sources. \\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 51}, page_content='30\\nCHAPTER 2\\nWorking with text data\\nLet’s now modify the vocabulary to include these two special tokens, <unk> and\\n<|endoftext|>, by adding them to our list of all unique words:\\nall_tokens = sorted(list(set(preprocessed)))\\nall_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\\nvocab = {token:integer for integer,token in enumerate(all_tokens)}\\nprint(len(vocab.items()))\\nBased on the output of this print statement, the new vocabulary size is 1,132 (the pre-\\nvious vocabulary size was 1,130).\\n As an additional quick check, let’s print the last five entries of the updated vocabulary:\\nfor i, item in enumerate(list(vocab.items())[-5:]):\\n    print(item)\\nThe code prints\\n(\\'younger\\', 1127)\\n(\\'your\\', 1128)\\n(\\'yourself\\', 1129)\\n(\\'<|endoftext|>\\', 1130)\\n(\\'<|unk|>\\', 1131)\\nThe <|endoftext|> tokens are\\nprepended to each subsequent text\\nsource.\\nIndependent text source\\nText concatenated from all\\nindependent sources\\n“… the underdog\\nteam ﬁnally clinched\\nthe championship in\\na thrilling overtime\\nvictory.”\\n“\\n…\\n<|endoftext|>\\nElara and Finn lived\\nwith kindness and\\nwisdom, enjoying\\ntheir days happily\\never after.”\\n“\\n…\\n<|endoftext|>\\nThe Dow Jones\\nIndustrial Average\\nclosed up 250 points\\ntoday, marking its\\nhighest gain in the\\npast three months.”\\n“\\n…\\n<|endoftext|>\\nAmelia smiled,\\nknowing her journey\\nhad forever changed\\nher heart.”\\n“… in a thrilling overtime victory.\\n… days happily ever after.\\n<|endoftext|>\\n<|endoftext|>\\n… marking its highest gain in the past three months.\\n… journey had forever\\n<|endoftext|>\\nchanged her heart.”\\nFigure 2.10\\nWhen working with multiple independent text source, we add <|endoftext|> \\ntokens between these texts. These <|endoftext|> tokens act as markers, signaling the \\nstart or end of a particular segment, allowing for more effective processing and understanding \\nby the LLM.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 52}, page_content='31\\n2.4\\nAdding special context tokens\\nBased on the code output, we can confirm that the two new special tokens were\\nindeed successfully incorporated into the vocabulary. Next, we adjust the tokenizer\\nfrom code listing 2.3 accordingly as shown in the following listing.\\nclass SimpleTokenizerV2:\\n    def __init__(self, vocab):\\n        self.str_to_int = vocab\\n        self.int_to_str = { i:s for s,i in vocab.items()}\\n    \\n    def encode(self, text):\\n        preprocessed = re.split(r\\'([,.:;?_!\"()\\\\\\']|--|\\\\s)\\', text)\\n        preprocessed = [\\n            item.strip() for item in preprocessed if item.strip()\\n        ]\\n        preprocessed = [item if item in self.str_to_int           \\n                        else \"<|unk|>\" for item in preprocessed]\\n        ids = [self.str_to_int[s] for s in preprocessed]\\n        return ids\\n        \\n    def decode(self, ids):\\n        text = \" \".join([self.int_to_str[i] for i in ids])\\n        text = re.sub(r\\'\\\\s+([,.:;?!\"()\\\\\\'])\\', r\\'\\\\1\\', text)   \\n        return text\\nCompared to the SimpleTokenizerV1 we implemented in listing 2.3, the new Simple-\\nTokenizerV2 replaces unknown words with <|unk|> tokens. \\n Let’s now try this new tokenizer out in practice. For this, we will use a simple text\\nsample that we concatenate from two independent and unrelated sentences:\\ntext1 = \"Hello, do you like tea?\"\\ntext2 = \"In the sunlit terraces of the palace.\"\\ntext = \" <|endoftext|> \".join((text1, text2))\\nprint(text)\\nThe output is\\nHello, do you like tea? <|endoftext|> In the sunlit terraces of \\nthe palace.\\nNext, let’s tokenize the sample text using the SimpleTokenizerV2 on the vocab we\\npreviously created in listing 2.2:\\ntokenizer = SimpleTokenizerV2(vocab)\\nprint(tokenizer.encode(text))\\nListing 2.4\\nA simple text tokenizer that handles unknown words\\nReplaces\\nunknown words\\nby <|unk|>\\ntokens\\nReplaces spaces \\nbefore the specified \\npunctuations\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 53}, page_content='32\\nCHAPTER 2\\nWorking with text data\\nThis prints the following token IDs:\\n[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\\nWe can see that the list of token IDs contains 1130 for the <|endoftext|> separator\\ntoken as well as two 1131 tokens, which are used for unknown words. \\n Let’s detokenize the text for a quick sanity check:\\nprint(tokenizer.decode(tokenizer.encode(text)))\\nThe output is\\n<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of \\nthe <|unk|>.\\nBased on comparing this detokenized text with the original input text, we know that\\nthe training dataset, Edith Wharton’s short story “The Verdict,” does not contain the\\nwords “Hello” and “palace.”\\n Depending on the LLM, some researchers also consider additional special tokens\\nsuch as the following:\\n\\uf0a1\\n[BOS] (beginning of sequence)—This token marks the start of a text. It signifies to\\nthe LLM where a piece of content begins.\\n\\uf0a1\\n[EOS] (end of sequence)—This token is positioned at the end of a text and\\nis especially useful when concatenating multiple unrelated texts, similar to\\n<|endoftext|>. For instance, when combining two different Wikipedia arti-\\ncles or books, the [EOS] token indicates where one ends and the next begins.\\n\\uf0a1\\n[PAD] (padding)—When training LLMs with batch sizes larger than one, the\\nbatch might contain texts of varying lengths. To ensure all texts have the same\\nlength, the shorter texts are extended or “padded” using the [PAD] token, up to\\nthe length of the longest text in the batch.\\nThe tokenizer used for GPT models does not need any of these tokens; it only uses an\\n<|endoftext|> token for simplicity. <|endoftext|> is analogous to the [EOS] token.\\n<|endoftext|> is also used for padding. However, as we’ll explore in subsequent\\nchapters, when training on batched inputs, we typically use a mask, meaning we don’t\\nattend to padded tokens. Thus, the specific token chosen for padding becomes incon-\\nsequential.\\n Moreover, the tokenizer used for GPT models also doesn’t use an <|unk|> token\\nfor out-of-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer,\\nwhich breaks words down into subword units, which we will discuss next.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 54}, page_content='33\\n2.5\\nByte pair encoding\\n2.5\\nByte pair encoding\\nLet’s look at a more sophisticated tokenization scheme based on a concept called byte\\npair encoding (BPE). The BPE tokenizer was used to train LLMs such as GPT-2, GPT-3,\\nand the original model used in ChatGPT.\\n Since implementing BPE can be relatively complicated, we will use an existing\\nPython open source library called tiktoken (https://github.com/openai/tiktoken), which\\nimplements the BPE algorithm very efficiently based on source code in Rust. Similar\\nto other Python libraries, we can install the tiktoken library via Python’s pip installer\\nfrom the terminal:\\npip install tiktoken\\nThe code we will use is based on tiktoken 0.7.0. You can use the following code to\\ncheck the version you currently have installed:\\nfrom importlib.metadata import version\\nimport tiktoken\\nprint(\"tiktoken version:\", version(\"tiktoken\"))\\nOnce installed, we can instantiate the BPE tokenizer from tiktoken as follows:\\ntokenizer = tiktoken.get_encoding(\"gpt2\")\\nThe usage of this tokenizer is similar to the SimpleTokenizerV2 we implemented pre-\\nviously via an encode method:\\ntext = (\\n    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\\n     \"of someunknownPlace.\"\\n)\\nintegers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\\nprint(integers)\\nThe code prints the following token IDs: \\n[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250,\\n 8812, 2114, 286, 617, 34680, 27271, 13]\\nWe can then convert the token IDs back into text using the decode method, similar to\\nour SimpleTokenizerV2:\\nstrings = tokenizer.decode(integers)\\nprint(strings)\\nThe code prints\\nHello, do you like tea? <|endoftext|> In the sunlit terraces of\\n someunknownPlace.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 55}, page_content='34\\nCHAPTER 2\\nWorking with text data\\nWe can make two noteworthy observations based on the token IDs and decoded text.\\nFirst, the <|endoftext|> token is assigned a relatively large token ID, namely, 50256.\\nIn fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and\\nthe original model used in ChatGPT, has a total vocabulary size of 50,257, with\\n<|endoftext|> being assigned the largest token ID.\\n Second, the BPE tokenizer encodes and decodes unknown words, such as\\nsomeunknownPlace, correctly. The BPE tokenizer can handle any unknown word. How\\ndoes it achieve this without using <|unk|> tokens?\\n The algorithm underlying BPE breaks down words that aren’t in its predefined\\nvocabulary into smaller subword units or even individual characters, enabling it to\\nhandle out-of-vocabulary words. So, thanks to the BPE algorithm, if the tokenizer\\nencounters an unfamiliar word during tokenization, it can represent it as a sequence\\nof subword tokens or characters, as illustrated in figure 2.11.\\nThe ability to break down unknown words into individual characters ensures that\\nthe tokenizer and, consequently, the LLM that is trained with it can process any text,\\neven if it contains words that were not present in its training data.\\nA detailed discussion and implementation of BPE is out of the scope of this book, but\\nin short, it builds its vocabulary by iteratively merging frequent characters into sub-\\nwords and frequent subwords into words. For example, BPE starts with adding all indi-\\nvidual single characters to its vocabulary (“a,” “b,” etc.). In the next stage, it merges\\ncharacter combinations that frequently occur together into subwords. For example,\\n“d” and “e” may be merged into the subword “de,” which is common in many English\\nExercise 2.1 Byte pair encoding of unknown words \\nTry the BPE tokenizer from the tiktoken library on the unknown words “Akwirw ier” and\\nprint the individual token IDs. Then, call the decode function on each of the resulting\\nintegers in this list to reproduce the mapping shown in figure 2.11. Lastly, call the\\ndecode method on the token IDs to check whether it can reconstruct the original\\ninput, “Akwirw ier.”\\n\"Akwirw\\nier \"\\n\"Ak\"\\n\"w\"\\n\"ir\"\\n\"w\"\\n\" \"\\n\"ier\"\\n33901\\n86\\n343\\n86\\n220\\n959\\nText sample with\\nunknown words\\nUnknown words are\\ntokenized into individual\\ncharacters or subwords.\\nTokens:\\nToken IDs:\\nFigure 2.11\\nBPE tokenizers \\nbreak down unknown words \\ninto subwords and individual \\ncharacters. This way, a BPE \\ntokenizer can parse any word \\nand doesn’t need to replace \\nunknown words with special \\ntokens, such as <|unk|>.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 56}, page_content='35\\n2.6\\nData sampling with a sliding window\\nwords like “define,” “depend,” “made,” and “hidden.” The merges are determined by\\na frequency cutoff.\\n2.6\\nData sampling with a sliding window\\nThe next step in creating the embeddings for the LLM is to generate the input–target\\npairs required for training an LLM. What do these input–target pairs look like? As we\\nalready learned, LLMs are pretrained by predicting the next word in a text, as depicted\\nin figure 2.12.\\nLet’s implement a data loader that fetches the input–target pairs in figure 2.12 from\\nthe training dataset using a sliding window approach. To get started, we will tokenize\\nthe whole “The Verdict” short story using the BPE tokenizer:\\nwith open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\\n    raw_text = f.read()\\nenc_text = tokenizer.encode(raw_text)\\nprint(len(enc_text))\\nExecuting this code will return 5145, the total number of tokens in the training set,\\nafter applying the BPE tokenizer.\\n Next, we remove the first 50 tokens from the dataset for demonstration purposes,\\nas it results in a slightly more interesting text passage in the next steps:\\nenc_sample = enc_text[50:]\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nInput the\\nLLM receives\\nTarget to\\npredict\\nText\\nsample:\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nLLMs   learn   to   predict   one   word   at   a   time\\nThe LLM can’t\\naccess words past\\nthe target.\\nFigure 2.12\\nGiven a text sample, extract input blocks as subsamples that serve as \\ninput to the LLM, and the LLM’s prediction task during training is to predict the next \\nword that follows the input block. During training, we mask out all words that are past \\nthe target. Note that the text shown in this figure must undergo tokenization before \\nthe LLM can process it; however, this figure omits the tokenization step for clarity.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 57}, page_content='36\\nCHAPTER 2\\nWorking with text data\\nOne of the easiest and most intuitive ways to create the input–target pairs for the next-\\nword prediction task is to create two variables, x and y, where x contains the input\\ntokens and y contains the targets, which are the inputs shifted by 1:\\ncontext_size = 4        \\nx = enc_sample[:context_size]\\ny = enc_sample[1:context_size+1]\\nprint(f\"x: {x}\")\\nprint(f\"y:      {y}\")\\nRunning the previous code prints the following output:\\nx: [290, 4920, 2241, 287]\\ny:      [4920, 2241, 287, 257]\\nBy processing the inputs along with the targets, which are the inputs shifted by one\\nposition, we can create the next-word prediction tasks (see figure 2.12), as follows:\\nfor i in range(1, context_size+1):\\n    context = enc_sample[:i]\\n    desired = enc_sample[i]\\n    print(context, \"---->\", desired)\\nThe code prints\\n[290] ----> 4920\\n[290, 4920] ----> 2241\\n[290, 4920, 2241] ----> 287\\n[290, 4920, 2241, 287] ----> 257\\nEverything left of the arrow (---->) refers to the input an LLM would receive, and\\nthe token ID on the right side of the arrow represents the target token ID that the\\nLLM is supposed to predict. Let’s repeat the previous code but convert the token IDs\\ninto text:\\nfor i in range(1, context_size+1):\\n    context = enc_sample[:i]\\n    desired = enc_sample[i]\\n    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))\\nThe following outputs show how the input and outputs look in text format:\\n and ---->  established\\n and established ---->  himself\\n and established himself ---->  in\\n and established himself in ---->  a\\nWe’ve now created the input–target pairs that we can use for LLM training.\\n There’s only one more task before we can turn the tokens into embeddings: imple-\\nmenting an efficient data loader that iterates over the input dataset and returns the\\nThe context size determines \\nhow many tokens are included \\nin the input.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 58}, page_content='37\\n2.6\\nData sampling with a sliding window\\ninputs and targets as PyTorch tensors, which can be thought of as multidimensional\\narrays. In particular, we are interested in returning two tensors: an input tensor con-\\ntaining the text that the LLM sees and a target tensor that includes the targets for the\\nLLM to predict, as depicted in figure 2.13. While the figure shows the tokens in string\\nformat for illustration purposes, the code implementation will operate on token IDs\\ndirectly since the encode method of the BPE tokenizer performs both tokenization\\nand conversion into token IDs as a single step.\\nNOTE\\nFor the efficient data loader implementation, we will use PyTorch’s\\nbuilt-in Dataset and DataLoader classes. For additional information and\\nguidance on installing PyTorch, please see section A.2.1.3 in appendix A.\\nThe code for the dataset class is shown in the following listing.\\nimport torch\\nfrom torch.utils.data import Dataset, DataLoader\\nclass GPTDatasetV1(Dataset):\\n    def __init__(self, txt, tokenizer, max_length, stride):\\n        self.input_ids = []\\n        self.target_ids = []\\n        token_ids = tokenizer.encode(txt)   \\nListing 2.5\\nA dataset for batched inputs and targets\\n\"In the heart of the city stood the old library, a relic from a bygone era. Its\\nstone walls bore the marks of time, and ivy clung tightly to its facade …\"\\nx = tensor([[  \"In\",      \"the\",     \"heart\",  \"of\"   ],\\n[  \"the\" ,    \"city\",    \"stood\",  \"the\"  ],\\n[  \"old\",     \"library\", \",\",      \"a\"    ],\\n[ …\\n]])\\ny = tensor([[  \"the\",     \"heart\",   \"of\",     \"the\"  ],\\n[  \"city\",    \"stood\",   \"the\",    \"old\"  ],\\n[  \"library\", “,\",\\n“a\",\\n“relic\"],\\n[ …\\n]])\\nSample text\\nTensor\\ncontaining\\nthe inputs\\nTensor\\ncontaining\\nthe targets\\nFigure 2.13\\nTo implement efficient data loaders, we collect the inputs in a tensor, x, where each row \\nrepresents one input context. A second tensor, y, contains the corresponding prediction targets (next \\nwords), which are created by shifting the input by one position.\\nTokenizes the \\nentire text\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 59}, page_content='38\\nCHAPTER 2\\nWorking with text data\\n        for i in range(0, len(token_ids) - max_length, stride):    \\n            input_chunk = token_ids[i:i + max_length]\\n            target_chunk = token_ids[i + 1: i + max_length + 1]\\n            self.input_ids.append(torch.tensor(input_chunk))\\n            self.target_ids.append(torch.tensor(target_chunk))\\n    def __len__(self):   \\n        return len(self.input_ids)\\n    def __getitem__(self, idx):        \\n        return self.input_ids[idx], self.target_ids[idx]\\nThe GPTDatasetV1 class is based on the PyTorch Dataset class and defines how indi-\\nvidual rows are fetched from the dataset, where each row consists of a number of\\ntoken IDs (based on a max_length) assigned to an input_chunk tensor. The target_\\nchunk tensor contains the corresponding targets. I recommend reading on to see what\\nthe data returned from this dataset looks like when we combine the dataset with a\\nPyTorch DataLoader—this will bring additional intuition and clarity.\\nNOTE\\nIf you are new to the structure of PyTorch Dataset classes, such as\\nshown in listing 2.5, refer to section A.6 in appendix A, which explains the\\ngeneral structure and usage of PyTorch Dataset and DataLoader classes.\\nThe following code uses the GPTDatasetV1 to load the inputs in batches via a PyTorch\\nDataLoader.\\ndef create_dataloader_v1(txt, batch_size=4, max_length=256,\\n                         stride=128, shuffle=True, drop_last=True,\\n                         num_workers=0):\\n    tokenizer = tiktoken.get_encoding(\"gpt2\")                        \\n    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)  \\n    dataloader = DataLoader(\\n        dataset,\\n        batch_size=batch_size,\\n        shuffle=shuffle,\\n        drop_last=drop_last,    \\n        num_workers=num_workers    \\n    )\\n    return dataloader\\nListing 2.6\\nA data loader to generate batches with input-with pairs\\nUses a sliding window to chunk\\nthe book into overlapping\\nsequences of max_length\\nReturns the total number \\nof rows in the dataset\\nReturns a single row \\nfrom the dataset\\nInitializes the \\ntokenizer\\nCreates \\ndataset\\ndrop_last=True drops the last \\nbatch if it is shorter than the \\nspecified batch_size to prevent \\nloss spikes during training.\\nThe number of CPU processes \\nto use for preprocessing\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 60}, page_content='39\\n2.6\\nData sampling with a sliding window\\nLet’s test the dataloader with a batch size of 1 for an LLM with a context size of 4 to\\ndevelop an intuition of how the GPTDatasetV1 class from listing 2.5 and the create_\\ndataloader_v1 function from listing 2.6 work together:\\nwith open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\\n    raw_text = f.read()\\ndataloader = create_dataloader_v1(\\n    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\\ndata_iter = iter(dataloader)     \\nfirst_batch = next(data_iter)\\nprint(first_batch)\\nExecuting the preceding code prints the following:\\n[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\\nThe first_batch variable contains two tensors: the first tensor stores the input token\\nIDs, and the second tensor stores the target token IDs. Since the max_length is set to\\n4, each of the two tensors contains four token IDs. Note that an input size of 4 is quite\\nsmall and only chosen for simplicity. It is common to train LLMs with input sizes of at\\nleast 256.\\n To understand the meaning of stride=1, let’s fetch another batch from this dataset:\\nsecond_batch = next(data_iter)\\nprint(second_batch)\\nThe second batch has the following contents:\\n[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\\nIf we compare the first and second batches, we can see that the second batch’s token\\nIDs are shifted by one position (for example, the second ID in the first batch’s input is\\n367, which is the first ID of the second batch’s input). The stride setting dictates the\\nnumber of positions the inputs shift across batches, emulating a sliding window\\napproach, as demonstrated in figure 2.14.\\nBatch sizes of 1, such as we have sampled from the data loader so far, are useful for\\nillustration purposes. If you have previous experience with deep learning, you may\\nknow that small batch sizes require less memory during training but lead to more\\nExercise 2.2 Data loaders with different strides and context sizes\\nTo develop more intuition for how the data loader works, try to run it with different\\nsettings such as max_length=2 and stride=2, and max_length=8 and stride=2.\\nConverts dataloader into a Python \\niterator to fetch the next entry via \\nPython’s built-in next() function\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 61}, page_content='40\\nCHAPTER 2\\nWorking with text data\\nnoisy model updates. Just like in regular deep learning, the batch size is a tradeoff and\\na hyperparameter to experiment with when training LLMs.\\n Let’s look briefly at how we can use the data loader to sample with a batch size\\ngreater than 1:\\ndataloader = create_dataloader_v1(\\n    raw_text, batch_size=8, max_length=4, stride=4,\\n    shuffle=False\\n)\\ndata_iter = iter(dataloader)\\ninputs, targets = next(data_iter)\\nprint(\"Inputs:\\\\n\", inputs)\\nprint(\"\\\\nTargets:\\\\n\", targets)\\nThis prints\\nInputs:\\n tensor([[   40,   367,  2885,  1464],\\n        [ 1807,  3619,   402,   271],\\n        [10899,  2138,   257,  7026],\\n        [15632,   438,  2016,   257],\\n        [  922,  5891,  1576,   438],\\n        [  568,   340,   373,   645],\\n\"In the heart of the city stood the old library, a relic from a bygone era. Its\\nstone walls bore the marks of time, and ivy clung tightly to its facade …\"\\nSample text\\nA stride of 1 moves the input ﬁeld by 1 position\\n\"In the heart of the city stood the old library, a relic from a bygone era. Its\\nstone walls bore the marks of time, and ivy clung tightly to its facade …\"\\nA stride of 4 moves the input ﬁeld by 4 positions\\n\"In the heart of\"\\n\"the heart of the\"\\nInputs of batch 1:\\nInputs of batch 2:\\n\"In the heart of\"\\n“the city stood the\"\\nInputs of batch 1:\\nInputs of batch 2:\\nFigure 2.14\\nWhen creating multiple batches from the input dataset, we slide an \\ninput window across the text. If the stride is set to 1, we shift the input window by \\none position when creating the next batch. If we set the stride equal to the input \\nwindow size, we can prevent overlaps between the batches.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 62}, page_content='41\\n2.7\\nCreating token embeddings\\n        [ 1049,  5975,   284,   502],\\n        [  284,  3285,   326,    11]])\\nTargets:\\n tensor([[  367,  2885,  1464,  1807],\\n        [ 3619,   402,   271, 10899],\\n        [ 2138,   257,  7026, 15632],\\n        [  438,  2016,   257,   922],\\n        [ 5891,  1576,   438,   568],\\n        [  340,   373,   645,  1049],\\n        [ 5975,   284,   502,   284],\\n        [ 3285,   326,    11,   287]])\\nNote that we increase the stride to 4 to utilize the data set fully (we don’t skip a single\\nword). This avoids any overlap between the batches since more overlap could lead to\\nincreased overfitting.\\n2.7\\nCreating token embeddings\\nThe last step in preparing the input text for LLM training is to convert the token IDs\\ninto embedding vectors, as shown in figure 2.15. As a preliminary step, we must initialize\\nGPT-like\\ndecoder-only\\ntransformer\\nInput text:\\nToken embeddings:\\nThis is an example.\\nTokenized text:\\nOutput text\\nPostprocessing steps\\nToken IDs:\\nThis\\nis\\nan\\nexample\\n.\\n40134\\n2052\\n133\\n389\\n12\\nCreating input\\ntoken embeddings\\nFigure 2.15\\nPreparation involves tokenizing text, converting text tokens to token IDs, and \\nconverting token IDs into embedding vectors. Here, we consider the previously created token \\nIDs to create the token embedding vectors.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 63}, page_content='42\\nCHAPTER 2\\nWorking with text data\\nthese embedding weights with random values. This initialization serves as the starting\\npoint for the LLM’s learning process. In chapter 5, we will optimize the embedding\\nweights as part of the LLM training.\\n A continuous vector representation, or embedding, is necessary since GPT-like\\nLLMs are deep neural networks trained with the backpropagation algorithm. \\nNOTE\\nIf you are unfamiliar with how neural networks are trained with back-\\npropagation, please read section B.4 in appendix A.\\nLet’s see how the token ID to embedding vector conversion works with a hands-on\\nexample. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:\\ninput_ids = torch.tensor([2, 3, 5, 1])\\nFor the sake of simplicity, suppose we have a small vocabulary of only 6 words (instead\\nof the 50,257 words in the BPE tokenizer vocabulary), and we want to create embed-\\ndings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):\\nvocab_size = 6\\noutput_dim = 3\\nUsing the vocab_size and output_dim, we can instantiate an embedding layer in\\nPyTorch, setting the random seed to 123 for reproducibility purposes:\\ntorch.manual_seed(123)\\nembedding_layer = torch.nn.Embedding(vocab_size, output_dim)\\nprint(embedding_layer.weight)\\nThe print statement prints the embedding layer’s underlying weight matrix:\\nParameter containing:\\ntensor([[ 0.3374, -0.1778, -0.1690],\\n        [ 0.9178,  1.5810,  1.3010],\\n        [ 1.2753, -0.2010, -0.1606],\\n        [-0.4015,  0.9666, -1.1481],\\n        [-1.1589,  0.3255, -0.6315],\\n        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\\nThe weight matrix of the embedding layer contains small, random values. These val-\\nues are optimized during LLM training as part of the LLM optimization itself. More-\\nover, we can see that the weight matrix has six rows and three columns. There is one row\\nfor each of the six possible tokens in the vocabulary, and there is one column for each of\\nthe three embedding dimensions.\\n Now, let’s apply it to a token ID to obtain the embedding vector: \\nprint(embedding_layer(torch.tensor([3])))\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 64}, page_content='43\\n2.8\\nEncoding word positions\\nThe returned embedding vector is\\ntensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\\nIf we compare the embedding vector for token ID 3 to the previous embedding\\nmatrix, we see that it is identical to the fourth row (Python starts with a zero index, so\\nit’s the row corresponding to index 3). In other words, the embedding layer is essen-\\ntially a lookup operation that retrieves rows from the embedding layer’s weight matrix\\nvia a token ID.\\nNOTE\\nFor those who are familiar with one-hot encoding, the embedding\\nlayer approach described here is essentially just a more efficient way of imple-\\nmenting one-hot encoding followed by matrix multiplication in a fully con-\\nnected layer, which is illustrated in the supplementary code on GitHub at\\nhttps://mng.bz/ZEB5. Because the embedding layer is just a more efficient\\nimplementation equivalent to the one-hot encoding and matrix-multiplica-\\ntion approach, it can be seen as a neural network layer that can be optimized\\nvia backpropagation.\\nWe’ve seen how to convert a single token ID into a three-dimensional embedding vec-\\ntor. Let’s now apply that to all four input IDs (torch.tensor([2, 3, 5, 1])):\\nprint(embedding_layer(input_ids))\\nThe print output reveals that this results in a 4 × 3 matrix:\\ntensor([[ 1.2753, -0.2010, -0.1606],\\n        [-0.4015,  0.9666, -1.1481],\\n        [-2.8400, -0.7849, -1.4096],\\n        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\\nEach row in this output matrix is obtained via a lookup operation from the embed-\\nding weight matrix, as illustrated in figure 2.16.\\n Having now created embedding vectors from token IDs, next we’ll add a small\\nmodification to these embedding vectors to encode positional information about a\\ntoken within a text.\\n2.8\\nEncoding word positions\\nIn principle, token embeddings are a suitable input for an LLM. However, a minor\\nshortcoming of LLMs is that their self-attention mechanism (see chapter 3) doesn’t\\nhave a notion of position or order for the tokens within a sequence. The way the pre-\\nviously introduced embedding layer works is that the same token ID always gets\\nmapped to the same vector representation, regardless of where the token ID is posi-\\ntioned in the input sequence, as shown in figure 2.17.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 65}, page_content='44\\nCHAPTER 2\\nWorking with text data\\n0.3374\\n−0.1778\\n−0.1690\\n0.9178\\n1.5810\\n1.3010\\n1.2753\\n−0.2010\\n−0.1606\\n−0.4015\\n0.9666\\n−1.1481\\n−1.1589\\n0.3255\\n−0.6315\\n−2.8400\\n−0.7849\\n−1.4096\\nWeight matrix of the\\nembedding layer\\n2\\n3\\n5\\n1\\n1.2753\\n−0.2010\\n−0.1606\\n−0.4015\\n0.9666\\n−1.1481\\n−2.8400\\n−0.7849\\n−1.4096\\n0.9178\\n1.5810\\n1.3010\\n2\\n1.2753\\n−0.2010\\n−0.1606\\n2\\n3\\n5\\n1\\n5\\nToken IDs to embed\\nEmbedded token IDs\\nEmbedding vector of\\nthe ﬁrst token ID\\nEmbedding vector of\\nthe third token ID\\n1.2753\\n−0.2010\\n−0.1606\\n−2.8400\\n−0.7849\\n−1.4096\\nfox\\njumps\\nover\\ndog\\nfox\\njumps\\nover\\ndog\\nInput text\\n−2.8400\\n−0.7849\\n−1.4096\\nFigure 2.16\\nEmbedding layers perform a lookup operation, retrieving the embedding \\nvector corresponding to the token ID from the embedding layer’s weight matrix. For \\ninstance, the embedding vector of the token ID 5 is the sixth row of the embedding \\nlayer weight matrix (it is the sixth instead of the fifth row because Python starts \\ncounting at 0). We assume that the token IDs were produced by the small vocabulary \\nfrom section 2.3.\\n0.3374\\n−0.1778 −0.1690\\n0.9178\\n1.5810\\n1.3010\\n1.2753\\n−0.2010 −0.1606\\n−0.4015\\n0.9666\\n−1.1481\\n−1.1589\\n0.3255\\n−0.6315\\n−2.8400 −0.7849 −1.4096\\n2\\n3\\n5\\n2\\n1.2753\\n−0.2010 −0.1606\\n−0.4015\\n0.9666\\n−1.1481\\n−2.8400 −0.7849 −1.4096\\n1.2753\\n−0.2010\\n−0.1606\\n2\\n1.2753\\n−0.2010 −0.1606\\n2\\n3\\n5\\n2\\nfox\\njumps\\nover\\nfox\\nfox\\njumps\\nover\\nfox\\n2\\n1.2753\\n−0.2010\\n−0.1606\\nWeight matrix of the\\nembedding layer\\nToken IDs to embed\\nThe same token IDs\\nresult in the same\\nembedding vectors\\n1.2753\\n−0.2010 −0.1606\\nFigure 2.17\\nThe embedding layer converts a token ID into the same vector \\nrepresentation regardless of where it is located in the input sequence. For \\nexample, the token ID 5, whether it’s in the first or fourth position in the \\ntoken ID input vector, will result in the same embedding vector.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 66}, page_content='45\\n2.8\\nEncoding word positions\\nIn principle, the deterministic, position-independent embedding of the token ID is\\ngood for reproducibility purposes. However, since the self-attention mechanism of\\nLLMs itself is also position-agnostic, it is helpful to inject additional position informa-\\ntion into the LLM.\\n To achieve this, we can use two broad categories of position-aware embeddings: rela-\\ntive positional embeddings and absolute positional embeddings. Absolute positional\\nembeddings are directly associated with specific positions in a sequence. For each posi-\\ntion in the input sequence, a unique embedding is added to the token’s embedding to\\nconvey its exact location. For instance, the first token will have a specific positional\\nembedding, the second token another distinct embedding, and so on, as illustrated in\\nfigure 2.18.\\nInstead of focusing on the absolute position of a token, the emphasis of relative posi-\\ntional embeddings is on the relative position or distance between tokens. This means\\nthe model learns the relationships in terms of “how far apart” rather than “at which\\nexact position.” The advantage here is that the model can generalize better to sequences\\nof varying lengths, even if it hasn’t seen such lengths during training.\\n Both types of positional embeddings aim to augment the capacity of LLMs to\\nunderstand the order and relationships between tokens, ensuring more accurate and\\ncontext-aware predictions. The choice between them often depends on the specific\\napplication and the nature of the data being processed.\\n OpenAI’s GPT models use absolute positional embeddings that are optimized\\nduring the training process rather than being fixed or predefined like the positional\\nencodings in the original transformer model. This optimization process is part of the\\nmodel training itself. For now, let’s create the initial positional embeddings to create the\\nLLM inputs.\\nInput embeddings:\\nPositional embeddings:\\nToken embeddings:\\n+\\n+\\n+\\n+\\n2.1\\n2.2\\n2.3\\n3.1\\n3.2\\n3.3\\n4.1\\n4.2\\n4.3\\n5.1\\n5.2\\n5.3\\n1.1\\n1.2\\n1.3\\n2.1\\n2.2\\n2.3\\n3.1\\n3.2\\n3.3\\n4.1\\n4.2\\n4.3\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\nEmbedding of the ﬁrst token\\nEmbedding of the third token\\nFigure 2.18\\nPositional embeddings are added to the token embedding vector to create the \\ninput embeddings for an LLM. The positional vectors have the same dimension as the original \\ntoken embeddings. The token embeddings are shown with value 1 for simplicity.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 67}, page_content='46\\nCHAPTER 2\\nWorking with text data\\n Previously, we focused on very small embedding sizes for simplicity. Now, let’s con-\\nsider more realistic and useful embedding sizes and encode the input tokens into a\\n256-dimensional vector representation, which is smaller than what the original GPT-3\\nmodel used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable\\nfor experimentation. Furthermore, we assume that the token IDs were created by the\\nBPE tokenizer we implemented earlier, which has a vocabulary size of 50,257:\\nvocab_size = 50257\\noutput_dim = 256\\ntoken_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\\nUsing the previous token_embedding_layer, if we sample data from the data loader,\\nwe embed each token in each batch into a 256-dimensional vector. If we have a batch\\nsize of 8 with four tokens each, the result will be an 8 × 4 × 256 tensor.\\n Let’s instantiate the data loader (see section 2.6) first:\\nmax_length = 4\\ndataloader = create_dataloader_v1(\\n    raw_text, batch_size=8, max_length=max_length,\\n   stride=max_length, shuffle=False\\n)\\ndata_iter = iter(dataloader)\\ninputs, targets = next(data_iter)\\nprint(\"Token IDs:\\\\n\", inputs)\\nprint(\"\\\\nInputs shape:\\\\n\", inputs.shape)\\nThis code prints\\nToken IDs:\\n tensor([[   40,   367,  2885,  1464],\\n        [ 1807,  3619,   402,   271],\\n        [10899,  2138,   257,  7026],\\n        [15632,   438,  2016,   257],\\n        [  922,  5891,  1576,   438],\\n        [  568,   340,   373,   645],\\n        [ 1049,  5975,   284,   502],\\n        [  284,  3285,   326,    11]])\\nInputs shape:\\n torch.Size([8, 4])\\nAs we can see, the token ID tensor is 8 × 4 dimensional, meaning that the data batch\\nconsists of eight text samples with four tokens each.\\n Let’s now use the embedding layer to embed these token IDs into 256-dimensional\\nvectors:\\ntoken_embeddings = token_embedding_layer(inputs)\\nprint(token_embeddings.shape)\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 68}, page_content='47\\n2.8\\nEncoding word positions\\nThe print function call returns\\ntorch.Size([8, 4, 256])\\nThe 8 × 4 × 256–dimensional tensor output shows that each token ID is now embed-\\nded as a 256-dimensional vector.\\n For a GPT model’s absolute embedding approach, we just need to create another\\nembedding layer that has the same embedding dimension as the token_embedding_\\nlayer:\\ncontext_length = max_length\\npos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\\npos_embeddings = pos_embedding_layer(torch.arange(context_length))\\nprint(pos_embeddings.shape)\\nThe input to the pos_embeddings is usually a placeholder vector torch.arange(con-\\ntext_length), which contains a sequence of numbers 0, 1, ..., up to the maximum\\ninput length –1. The context_length is a variable that represents the supported input\\nsize of the LLM. Here, we choose it similar to the maximum length of the input text.\\nIn practice, input text can be longer than the supported context length, in which case\\nwe have to truncate the text.\\n The output of the print statement is\\ntorch.Size([4, 256])\\nAs we can see, the positional embedding tensor consists of four 256-dimensional vec-\\ntors. We can now add these directly to the token embeddings, where PyTorch will add\\nthe 4 × 256–dimensional pos_embeddings tensor to each 4 × 256–dimensional token\\nembedding tensor in each of the eight batches:\\ninput_embeddings = token_embeddings + pos_embeddings\\nprint(input_embeddings.shape)\\nThe print output is\\ntorch.Size([8, 4, 256])\\nThe input_embeddings we created, as summarized in figure 2.19, are the embedded\\ninput examples that can now be processed by the main LLM modules, which we will\\nbegin implementing in the next chapter.\\n \\n \\n \\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 69}, page_content='48\\nCHAPTER 2\\nWorking with text data\\nSummary\\n\\uf0a1LLMs require textual data to be converted into numerical vectors, known as\\nembeddings, since they can’t process raw text. Embeddings transform discrete\\ndata (like words or images) into continuous vector spaces, making them com-\\npatible with neural network operations. \\n\\uf0a1As the first step, raw text is broken into tokens, which can be words or characters.\\nThen, the tokens are converted into integer representations, termed token IDs.\\n\\uf0a1Special tokens, such as <|unk|> and <|endoftext|>, can be added to enhance\\nthe model’s understanding and handle various contexts, such as unknown\\nwords or marking the boundary between unrelated texts.\\nGPT-like\\ndecoder-only\\ntransformer\\nOutput text\\nPostprocessing steps\\nInput text:\\nInput embeddings:\\nThis is an example.\\nTokenized text:\\nThis\\nis\\nan\\nexample\\nToken IDs:\\n40134\\n2052\\n133\\n389\\n.\\n12\\nToken embeddings:\\nPositional embeddings:\\n+\\nThe input embedding pipeline\\nFigure 2.19\\nAs part of the input processing pipeline, input text is first broken \\nup into individual tokens. These tokens are then converted into token IDs using a \\nvocabulary. The token IDs are converted into embedding vectors to which positional \\nembeddings of a similar size are added, resulting in input embeddings that are used \\nas input for the main LLM layers.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 70}, page_content='49\\nSummary\\n\\uf0a1The byte pair encoding (BPE) tokenizer used for LLMs like GPT-2 and GPT-3\\ncan efficiently handle unknown words by breaking them down into subword\\nunits or individual characters.\\n\\uf0a1We use a sliding window approach on tokenized data to generate input–target\\npairs for LLM training.\\n\\uf0a1Embedding layers in PyTorch function as a lookup operation, retrieving vectors\\ncorresponding to token IDs. The resulting embedding vectors provide continu-\\nous representations of tokens, which is crucial for training deep learning mod-\\nels like LLMs. \\n\\uf0a1While token embeddings provide consistent vector representations for each\\ntoken, they lack a sense of the token’s position in a sequence. To rectify this,\\ntwo main types of positional embeddings exist: absolute and relative. OpenAI’s\\nGPT models utilize absolute positional embeddings, which are added to the token\\nembedding vectors and are optimized during the model training.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 71}, page_content='50\\nCoding attention\\nmechanisms\\nAt this point, you know how to prepare the input text for training LLMs by splitting\\ntext into individual word and subword tokens, which can be encoded into vector rep-\\nresentations, embeddings, for the LLM. \\n Now, we will look at an integral part of the LLM architecture itself, attention\\nmechanisms, as illustrated in figure 3.1. We will largely look at attention mechanisms\\nin isolation and focus on them at a mechanistic level. Then we will code the remaining\\nThis chapter covers\\n\\uf0a1The reasons for using attention mechanisms in \\nneural networks\\n\\uf0a1A basic self-attention framework, progressing to \\nan enhanced self-attention mechanism \\n\\uf0a1A causal attention module that allows LLMs to \\ngenerate one token at a time\\n\\uf0a1Masking randomly selected attention weights with \\ndropout to reduce overfitting\\n\\uf0a1Stacking multiple causal attention modules into a \\nmulti-head attention module\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 72}, page_content='51\\nparts of the LLM surrounding the self-attention mechanism to see it in action and to\\ncreate a model to generate text.\\n We will implement four different variants of attention mechanisms, as illustrated in\\nfigure 3.2. These different attention variants build on each other, and the goal is to\\nThis chapter implements the\\nattention mechanism, an important\\nbuilding block of GPT-like LLMs\\n1) Data\\npreparation\\n& sampling\\n2) Attention\\nmechanism\\nBuilding an LLM\\nSTAGE 1\\nFoundation model\\nSTAGE 2\\nSTAGE 3\\nClassiﬁer\\nPersonal assistant\\nDataset with class labels\\nInstruction dataset\\n4) Pretraining\\n9) Fine-tuning\\n5) Training\\nloop\\n6) Model\\nevaluation\\n7) Load\\npretrained\\nweights\\n8) Fine-tuning\\n3) LLM\\narchitecture\\n2) Attention\\nmechanism\\nFigure 3.1\\nThe three main stages of coding an LLM. This chapter focuses on step 2 of stage 1: implementing \\nattention mechanisms, which are an integral part of the LLM architecture.\\n1) Simpliﬁed\\nself-attention\\n2) Self-attention\\n3) Causal attention\\n4) Multi-head\\nattention\\nA simpliﬁed self-attention\\ntechnique to introduce the\\nbroader idea\\nSelf-attention with trainable\\nweights that forms the basis of\\nthe mechanism used in LLMs\\nA type of self-attention used in LLMs\\nthat allows a model to consider only\\nprevious and current inputs in a\\nsequence, ensuring temporal order\\nduring the text generation\\nAn extension of self-attention and\\ncausal attention that enables the\\nmodel to simultaneously attend\\nto information from different\\nrepresentation subspaces\\nFigure 3.2\\nThe figure depicts different attention mechanisms we will code in this chapter, starting \\nwith a simplified version of self-attention before adding the trainable weights. The causal attention \\nmechanism adds a mask to self-attention that allows the LLM to generate one word at a time. Finally, \\nmulti-head attention organizes the attention mechanism into multiple heads, allowing the model to \\ncapture various aspects of the input data in parallel.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 73}, page_content='52\\nCHAPTER 3\\nCoding attention mechanisms\\narrive at a compact and efficient implementation of multi-head attention that we can\\nthen plug into the LLM architecture we will code in the next chapter.\\n3.1\\nThe problem with modeling long sequences\\nBefore we dive into the self-attention mechanism at the heart of LLMs, let’s consider\\nthe problem with pre-LLM architectures that do not include attention mechanisms.\\nSuppose we want to develop a language translation model that translates text from\\none language into another. As shown in figure 3.3, we can’t simply translate a text word\\nby word due to the grammatical structures in the source and target language.\\nTo address this problem, it is common to use a deep neural network with two submod-\\nules, an encoder and a decoder. The job of the encoder is to first read in and process the\\nentire text, and the decoder then produces the translated text.\\n Before the advent of transformers, recurrent neural networks (RNNs) were the most\\npopular encoder–decoder architecture for language translation. An RNN is a type of\\nneural network where outputs from previous steps are fed as inputs to the current\\ndu\\nKannst\\nhelfen\\nmir\\nSatz\\ndiesen\\nuebersetzen\\nzu\\nyou\\nCan\\nhelp\\nme\\nsentence\\nthis\\ntranslate\\nto\\ndu\\nKannst\\nhelfen\\nmir\\nSatz\\ndiesen\\nuebersetzen\\nzu\\nyou\\nCan\\nme\\nhelp\\ntranslate\\nto\\nsentence\\nthis\\nGerman input sentence to translate\\nThe word-by-word translation results\\nin a grammatically incorrect sentence\\nThe correct translation\\nCertain words in the generated translation\\nrequire access to words that appear earlier\\nor later in the original sentence.\\nFigure 3.3\\nWhen translating text from one language to another, such as German to English, it’s not \\npossible to merely translate word by word. Instead, the translation process requires contextual \\nunderstanding and grammatical alignment. \\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 74}, page_content='53\\n3.1\\nThe problem with modeling long sequences\\nstep, making them well-suited for sequential data like text. If you are unfamiliar with\\nRNNs, don’t worry—you don’t need to know the detailed workings of RNNs to fol-\\nlow this discussion; our focus here is more on the general concept of the encoder–\\ndecoder setup. \\n In an encoder–decoder RNN, the input text is fed into the encoder, which pro-\\ncesses it sequentially. The encoder updates its hidden state (the internal values at the\\nhidden layers) at each step, trying to capture the entire meaning of the input sen-\\ntence in the final hidden state, as illustrated in figure 3.4. The decoder then takes this\\nfinal hidden state to start generating the translated sentence, one word at a time. It\\nalso updates its hidden state at each step, which is supposed to carry the context nec-\\nessary for the next-word prediction.\\nWhile we don’t need to know the inner workings of these encoder–decoder RNNs,\\nthe key idea here is that the encoder part processes the entire input text into a hid-\\nden state (memory cell). The decoder then takes in this hidden state to produce the\\noutput. You can think of this hidden state as an embedding vector, a concept we dis-\\ncussed in chapter 2.\\n The big limitation of encoder–decoder RNNs is that the RNN can’t directly access\\nearlier hidden states from the encoder during the decoding phase. Consequently, it\\nrelies solely on the current hidden state, which encapsulates all relevant information.\\nThis can lead to a loss of context, especially in complex sentences where dependen-\\ncies might span long distances.\\ndu\\nKannst\\nmir\\nyou\\nCan\\nhelp\\nHidden states\\nOutputs\\nInputs\\nEncoder\\nDecoder\\nHidden states of a\\nneural network\\nA memory cell (hidden state)\\nmemorizing entire input\\nGerman input sentence to translate\\nThe translated English sentence\\nFigure 3.4\\nBefore the advent of transformer models, encoder–decoder RNNs were a popular choice \\nfor machine translation. The encoder takes a sequence of tokens from the source language as input, \\nwhere a hidden state (an intermediate neural network layer) of the encoder encodes a compressed \\nrepresentation of the entire input sequence. Then, the decoder uses its current hidden state to begin \\nthe translation, token by token.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 75}, page_content='54\\nCHAPTER 3\\nCoding attention mechanisms\\n Fortunately, it is not essential to understand RNNs to build an LLM. Just remem-\\nber that encoder–decoder RNNs had a shortcoming that motivated the design of\\nattention mechanisms.\\n3.2\\nCapturing data dependencies with attention \\nmechanisms\\nAlthough RNNs work fine for translating short sentences, they don’t work well for lon-\\nger texts as they don’t have direct access to previous words in the input. One major\\nshortcoming in this approach is that the RNN must remember the entire encoded\\ninput in a single hidden state before passing it to the decoder (figure 3.4).\\n Hence, researchers developed the Bahdanau attention mechanism for RNNs in\\n2014 (named after the first author of the respective paper; for more information, see\\nappendix B), which modifies the encoder–decoder RNN such that the decoder can\\nselectively access different parts of the input sequence at each decoding step as illus-\\ntrated in figure 3.5.\\nInterestingly, only three years later, researchers found that RNN architectures are\\nnot required for building deep neural networks for natural language processing and\\ndu\\nKannst\\nmir\\nyou\\nCan\\nhelp\\nHidden states\\nOutputs\\nInputs\\nWhen generating an output\\ntoken, the model has a way\\nto access to all input tokens.\\nThe dotted line width is proportional\\nto how important the input token is\\nfor the respective output token.\\nWe are focusing on\\ngenerating the second\\noutput token.\\nFigure 3.5\\nUsing an attention mechanism, the text-generating decoder part of the network can \\naccess all input tokens selectively. This means that some input tokens are more important than others \\nfor generating a given output token. The importance is determined by the attention weights, which we \\nwill compute later. Note that this figure shows the general idea behind attention and does not depict \\nthe exact implementation of the Bahdanau mechanism, which is an RNN method outside this book’s \\nscope.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 76}, page_content='55\\n3.3\\nAttending to different parts of the input with self-attention\\nproposed the original transformer architecture (discussed in chapter 1) including a\\nself-attention mechanism inspired by the Bahdanau attention mechanism. \\n Self-attention is a mechanism that allows each position in the input sequence to\\nconsider the relevancy of, or “attend to,” all other positions in the same sequence\\nwhen computing the representation of a sequence. Self-attention is a key component\\nof contemporary LLMs based on the transformer architecture, such as the GPT series. \\n This chapter focuses on coding and understanding this self-attention mechanism\\nused in GPT-like models, as illustrated in figure 3.6. In the next chapter, we will code\\nthe remaining parts of the LLM.\\n3.3\\nAttending to different parts of the input \\nwith self-attention\\nWe’ll now cover the inner workings of the self-attention mechanism and learn how to\\ncode it from the ground up. Self-attention serves as the cornerstone of every LLM\\nbased on the transformer architecture. This topic may require a lot of focus and atten-\\ntion (no pun intended), but once you grasp its fundamentals, you will have con-\\nquered one of the toughest aspects of this book and LLM implementation in general.\\nGPT-like\\ndecoder-only\\ntransformer\\nInput text\\nPreprocessing steps\\nOutput text\\nPostprocessing steps\\nTopic of the\\nprevious\\nchapter\\nSelf-attention module\\nTopic of the\\ncurrent\\nchapter\\nThe remaining parts of the\\nLLM architecture are the\\ntopic of the next chapter\\nFigure 3.6\\nSelf-attention is a mechanism in transformers used to compute \\nmore efficient input representations by allowing each position in a sequence to \\ninteract with and weigh the importance of all other positions within the same \\nsequence. In this chapter, we will code this self-attention mechanism from the \\nground up before we code the remaining parts of the GPT-like LLM in the \\nfollowing chapter.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 77}, page_content='56\\nCHAPTER 3\\nCoding attention mechanisms\\nSince self-attention can appear complex, especially if you are encountering it for the\\nfirst time, we will begin by examining a simplified version of it. Then we will imple-\\nment the self-attention mechanism with trainable weights used in LLMs.\\n3.3.1\\nA simple self-attention mechanism without trainable weights\\nLet’s begin by implementing a simplified variant of self-attention, free from any train-\\nable weights, as summarized in figure 3.7. The goal is to illustrate a few key concepts\\nin self-attention before adding trainable weights.\\nThe “self” in self-attention  \\nIn self-attention, the “self” refers to the mechanism’s ability to compute attention\\nweights by relating different positions within a single input sequence. It assesses and\\nlearns the relationships and dependencies between various parts of the input itself,\\nsuch as words in a sentence or pixels in an image. \\nThis is in contrast to traditional attention mechanisms, where the focus is on the rela-\\ntionships between elements of two different sequences, such as in sequence-to-\\nsequence models where the attention might be between an input sequence and an\\noutput sequence, such as the example depicted in figure 3.5.\\nThe context vector z(2) is\\ncomputed as a combination of\\nall input vectors weighted with\\nrespect to input element x(2)\\nAttention weight to\\nweigh the importance\\nof input x(1)\\nInput vector\\n(token embedding)\\ncorresponding to\\nthe ﬁrst token\\n0.4 0.1 0.8\\n0.5 0.8 0.6\\n0.5 0.8 0.6\\n0.0 0.8 0.5\\n0.4 0.6 0.5\\nFigure 3.7\\nThe goal of self-attention is to compute a context vector for each input \\nelement that combines information from all other input elements. In this example, \\nwe compute the context vector z(2). The importance or contribution of each input \\nelement for computing z(2) is determined by the attention weights \\uf06121 to \\uf0612T. When \\ncomputing z(2), the attention weights are calculated with respect to input element \\nx(2) and all other inputs.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 78}, page_content='57\\n3.3\\nAttending to different parts of the input with self-attention\\nFigure 3.7 shows an input sequence, denoted as x, consisting of T elements repre-\\nsented as x(1) to x(T). This sequence typically represents text, such as a sentence, that\\nhas already been transformed into token embeddings.\\n For example, consider an input text like “Your journey starts with one step.” In this\\ncase, each element of the sequence, such as x(1), corresponds to a d-dimensional\\nembedding vector representing a specific token, like “Your.” Figure 3.7 shows these\\ninput vectors as three-dimensional embeddings.\\n In self-attention, our goal is to calculate context vectors z(i) for each element x(i)\\nin the input sequence. A context vector can be interpreted as an enriched embedding\\nvector.\\n To illustrate this concept, let’s focus on the embedding vector of the second input\\nelement, x(2) (which corresponds to the token “journey”), and the corresponding con-\\ntext vector, z(2), shown at the bottom of figure 3.7. This enhanced context vector, z(2),\\nis an embedding that contains information about x(2) and all other input elements,\\nx(1) to x(T).\\n Context vectors play a crucial role in self-attention. Their purpose is to create\\nenriched representations of each element in an input sequence (like a sentence)\\nby incorporating information from all other elements in the sequence (figure 3.7).\\nThis is essential in LLMs, which need to understand the relationship and relevance\\nof words in a sentence to each other. Later, we will add trainable weights that help\\nan LLM learn to construct these context vectors so that they are relevant for the\\nLLM to generate the next token. But first, let’s implement a simplified self-atten-\\ntion mechanism to compute these weights and the resulting context vector one\\nstep at a time. \\n Consider the following input sentence, which has already been embedded into\\nthree-dimensional vectors (see chapter 2). I’ve chosen a small embedding dimension\\nto ensure it fits on the page without line breaks:\\nimport torch\\ninputs = torch.tensor(\\n  [[0.43, 0.15, 0.89], # Your     (x^1)\\n   [0.55, 0.87, 0.66], # journey  (x^2)\\n   [0.57, 0.85, 0.64], # starts   (x^3)\\n   [0.22, 0.58, 0.33], # with     (x^4)\\n   [0.77, 0.25, 0.10], # one      (x^5)\\n   [0.05, 0.80, 0.55]] # step     (x^6)\\n)\\nThe first step of implementing self-attention is to compute the intermediate values ω,\\nreferred to as attention scores, as illustrated in figure 3.8. Due to spatial constraints,\\nthe figure displays the values of the preceding inputs tensor in a truncated version;\\nfor example, 0.87 is truncated to 0.8. In this truncated version, the embeddings of the\\nwords “journey” and “starts” may appear similar by random chance.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 79}, page_content='58\\nCHAPTER 3\\nCoding attention mechanisms\\nFigure 3.8 illustrates how we calculate the intermediate attention scores between the\\nquery token and each input token. We determine these scores by computing the dot\\nproduct of the query, x(2), with every other input token:\\nquery = inputs[1]                           \\nattn_scores_2 = torch.empty(inputs.shape[0])\\nfor i, x_i in enumerate(inputs):\\n    attn_scores_2[i] = torch.dot(x_i, query)\\nprint(attn_scores_2)\\nThe computed attention scores are\\ntensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\\nUnderstanding dot products \\nA dot product is essentially a concise way of multiplying two vectors element-wise and\\nthen summing the products, which can be demonstrated as follows:\\nres = 0.\\nfor idx, element in enumerate(inputs[0]):\\n    res += inputs[0][idx] * query[idx]\\nprint(res)\\nprint(torch.dot(inputs[0], query))\\nThe output confirms that the sum of the element-wise multiplication gives the same\\nresults as the dot product:\\ntensor(0.9544)\\ntensor(0.9544)\\nEmbedded query token:\\nThe embedded query\\ntoken is one of the\\nembedded input tokens\\n(here, the query is the\\nsecond token).\\n0.4 0.1 0.8\\n0.5 0.8 0.6\\n0.5 0.8 0.6\\n0.0 0.8 0.5\\n0.5 0.8 0.6\\n0.9\\n1.4\\n1.4\\n1.0\\nInputs:\\nAttention score between\\ninput       and query\\nx(1)\\nx(2)\\nAttention score between\\ninput       and query\\nx(3)\\nx(2)\\nFigure 3.8\\nThe overall goal is to illustrate the computation of the context vector z(2) using the \\nsecond input element, x(2) as a query. This figure shows the first intermediate step, computing the \\nattention scores \\uf077 between the query x(2) and all other input elements as a dot product. (Note that \\nthe numbers are truncated to one digit after the decimal point to reduce visual clutter.)\\nThe second input \\ntoken serves as \\nthe query.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 80}, page_content='59\\n3.3\\nAttending to different parts of the input with self-attention\\nIn the next step, as shown in figure 3.9, we normalize each of the attention scores we\\ncomputed previously. The main goal behind the normalization is to obtain attention\\nweights that sum up to 1. This normalization is a convention that is useful for interpre-\\ntation and maintaining training stability in an LLM. Here’s a straightforward method\\nfor achieving this normalization step:\\nattn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\\nprint(\"Attention weights:\", attn_weights_2_tmp)\\nprint(\"Sum:\", attn_weights_2_tmp.sum())\\nAs the output shows, the attention weights now sum to 1: \\nAttention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\\nSum: tensor(1.0000)\\nIn practice, it’s more common and advisable to use the softmax function for normal-\\nization. This approach is better at managing extreme values and offers more favorable\\nBeyond viewing the dot product operation as a mathematical tool that combines\\ntwo vectors to yield a scalar value, the dot product is a measure of similarity\\nbecause it quantifies how closely two vectors are aligned: a higher dot product indi-\\ncates a greater degree of alignment or similarity between the vectors. In the con-\\ntext of self-attention mechanisms, the dot product determines the extent to which\\neach element in a sequence focuses on, or “attends to,” any other element: the\\nhigher the dot product, the higher the similarity and attention score between two\\nelements.\\nAttention weights:\\nWe computed these attention\\nscores in the previous step.\\nWe now normalize the\\nattention scores\\nto obtain\\nω\\nthe attention weights α\\n0.4 0.1 0.8\\n0.5 0.8 0.6\\n0.5 0.8 0.6\\n0.0 0.8 0.5\\n0.5 0.8 0.6\\n0.9\\n1.4\\n1.4\\n1.0\\n0.1\\n0.2\\n0.2\\n0.1\\nFigure 3.9\\nAfter computing the attention scores \\uf07721 to \\uf0772T with respect to the input query x(2), the next \\nstep is to obtain the attention weights \\uf06121 to \\uf0612T by normalizing the attention scores.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 20.0 (Windows); modified using iText® Core 8.0.2 (AGPL version) ©2000-2023 Apryse Group NV', 'creator': 'FrameMaker 16.0.1', 'creationdate': '2024-08-22T09:07:13+00:00', 'source': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'file_path': '/Users/carsten/Documents/Science/LLM/Build a Large Language Model.pdf', 'total_pages': 370, 'format': 'PDF 1.6', 'title': 'Build a Large Language Model (From Scratch)', 'author': 'Sebastian Raschka', 'subject': '', 'keywords': '', 'moddate': '2024-09-06T10:33:46-04:00', 'trapped': '', 'modDate': \"D:20240906103346-04'00'\", 'creationDate': 'D:20240822090713Z', 'page': 81}, page_content='60\\nCHAPTER 3\\nCoding attention mechanisms\\ngradient properties during training. The following is a basic implementation of the\\nsoftmax function for normalizing the attention scores:\\ndef softmax_naive(x):\\n    return torch.exp(x) / torch.exp(x).sum(dim=0)\\nattn_weights_2_naive = softmax_naive(attn_scores_2)\\nprint(\"Attention weights:\", attn_weights_2_naive)\\nprint(\"Sum:\", attn_weights_2_naive.sum())\\nAs the output shows, the softmax function also meets the objective and normalizes the\\nattention weights such that they sum to 1:\\nAttention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\\nSum: tensor(1.)\\nIn addition, the softmax function ensures that the attention weights are always posi-\\ntive. This makes the output interpretable as probabilities or relative importance,\\nwhere higher weights indicate greater importance.\\n Note that this naive softmax implementation (softmax_naive) may encounter\\nnumerical instability problems, such as overflow and underflow, when dealing with\\nlarge or small input values. Therefore, in practice, it’s advisable to use the PyTorch\\nimplementation of softmax, which has been extensively optimized for performance:\\nattn_weights_2 = torch.softmax(attn_scores_2, dim=0)\\nprint(\"Attention weights:\", attn_weights_2)\\nprint(\"Sum:\", attn_weights_2.sum())\\nIn this case, it yields the same results as our previous softmax_naive function:\\nAttention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\\nSum: tensor(1.)\\nNow that we have computed the normalized attention weights, we are ready for the\\nfinal step, as shown in figure 3.10: calculating the context vector z(2) by multiplying the\\nembedded input tokens, x(i), with the corresponding attention weights and then sum-\\nming the resulting vectors. Thus, context vector z(2) is the weighted sum of all input vec-\\ntors, obtained by multiplying each input vector by its corresponding attention weight:\\nquery = inputs[1]        \\ncontext_vec_2 = torch.zeros(query.shape)\\nfor i,x_i in enumerate(inputs):\\n    context_vec_2 += attn_weights_2[i]*x_i\\nprint(context_vec_2)\\nThe results of this computation are\\ntensor([0.4419, 0.6515, 0.5683])\\nThe second input \\ntoken is the query.\\nLicensed to Carsten Jorgensen <carstenj@gmail.com>'),\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PointStruct(id=1, vector={'text-sparse': SparseVector(indices=[19522071, 1497018987, 783977789, 842430915, 1172383616, 962346254, 1811985148, 1588272389], values=[1.6564705882352944, 1.6564705882352944, 1.6564705882352944, 1.6564705882352944, 1.6564705882352944, 1.6564705882352944, 1.6564705882352944, 1.6564705882352944])}, payload={'page': 2, 'source': '/Users/carsten/Documents/Science/LLM/The Hundred-page Language Models Book.pdf', 'text': '2 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThe Hundred-Page Language Models Book \\n \\nAndriy Burkov'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = ce.Document(page_content=docs[2].page_content, metadata=docs[2].metadata)\n",
    "\n",
    "points = ce.driver(foo)\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8000/document\"\n",
    "payload = {\n",
    "    \"page_content\": docs[2].page_content,\n",
    "    \"metadata\": docs[2].metadata,\n",
    "}\n",
    "response = requests.post(url, json=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': {'operation_id': 458, 'status': 'completed'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:8000/semantic_search\"\n",
    "payload = {\n",
    "    \"query\": \"MCMC\",\n",
    "    \"limit\": 3,\n",
    "}\n",
    "response = requests.post(url, json=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'id': 443118,\n",
       "   'page': 62,\n",
       "   'source': '/Users/carsten/Documents/Science/PML2024/Week3/PML_Pyro.pdf',\n",
       "   'text': 'MCMC with NUTS\\n# Get MCMC samples from Hamiltonian Monte carlo with NUTS\\nnuts_kernel = NUTS(linear_regression_model)  # Define the NUTS kernel\\nmcmc = MCMC(nuts_kernel, num_samples=500, warmup_steps=200, num_chains=2)  # Run MCMC with 2 chains\\nmcmc.run(x, y)  # Run the inference\\nposterior_samples = mcmc.get_samples()  # Retrieve samples from the posterior\\n# Extract NUTS posterior means\\nnuts_slope_mean = posterior_samples[\"slope\"].mean().item()',\n",
       "   'score': 3.4799998},\n",
       "  {'id': 442779,\n",
       "   'page': 13,\n",
       "   'source': '/Users/carsten/Documents/Science/PML2024/Week4/PML_HMC.pdf',\n",
       "   'text': 'MCMC estimators\\n●\\nMCMC Central Limit Theorem\\n●\\nThe MCMC estimates will be normally distributed, with mean equal to the true \\nexpectation and standard deviation equal to the MCMC standard error \\n(MCMC-SE)\\n○\\nFor the calculation of the MCMC-SE, we need to take into account that our N \\nsamples might be highly correlated.\\n14',\n",
       "   'score': 3.4703798},\n",
       "  {'id': 358519,\n",
       "   'page': 102,\n",
       "   'source': '/Users/carsten/Documents/Science/Bayes/Bayesian Methods for Hackers.pdf',\n",
       "   'text': '82\\nChapter 3\\nOpening the Black Box of MCMC\\n(Continued)\\n# Now we create a model class.\\nmodel = pm.Model([p, assignment, taus, centers])\\nPyMC has an MCMC class, MCMC in the main namespace of PyMC, that implements\\nthe MCMC exploring algorithm. We initialize it by passing in a Model instance:\\nmcmc = pm.MCMC(model)\\nThe method for asking the MCMC to explore the space is pm.sample(iterations),\\nwhere iterations is the number of steps you wish the algorithm to perform. The\\nfollowing code tries 50,000 steps:',\n",
       "   'score': 3.4653194}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512, chunk_overlap=0, add_start_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457477"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = text_splitter.split_documents(docs)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'iText® 5.5.13.2 ©2000-2020 iText Group NV (AGPL-version); modified using iText® 7.1.14 ©2000-2020 iText Group NV (AGPL-version)', 'creator': '', 'creationdate': '2023-03-21T08:13:09+05:30', 'source': '/Users/carsten/Documents/Science/MCMC from Scratch a Practical Introduction to Markov Chain Monte Carlo.pdf', 'file_path': '/Users/carsten/Documents/Science/MCMC from Scratch a Practical Introduction to Markov Chain Monte Carlo.pdf', 'total_pages': 198, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2023-03-21T08:28:46+05:30', 'trapped': '', 'modDate': \"D:20230321082846+05'30'\", 'creationDate': \"D:20230321081309+05'30'\", 'page': 0, 'start_index': 0}, page_content='Masanori\\xa0Hanada\\nSo\\xa0Matsuura\\nMCMC \\nfrom\\xa0Scratch\\nA\\xa0Practical Introduction to\\xa0Markov Chain \\nMonte Carlo')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_model_name = \"Qdrant/bm25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_model = SparseTextEmbedding(model_name=sparse_model_name, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = qdrant_client.QdrantClient(\"localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating collection\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"PDFs\"\n",
    "\n",
    "exists = qdrant_client.collection_exists(collection_name=collection_name)\n",
    "\n",
    "if not exists:\n",
    "    print(\"Creating collection\")\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name,\n",
    "        vectors_config={\n",
    "            \"text-dense\": VectorParams(\n",
    "                size=1024,\n",
    "                distance=Distance.COSINE,\n",
    "            )\n",
    "        },\n",
    "        sparse_vectors_config={\n",
    "            \"text-sparse\": SparseVectorParams(\n",
    "                index=SparseIndexParams(\n",
    "                    on_disk=False,\n",
    "                )\n",
    "            )\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sparse_embedding(texts: list[str]) -> list[SparseEmbedding]:\n",
    "    return list(sparse_model.embed(texts, batch_size=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    text = chunk.page_content\n",
    "    embeddings = make_sparse_embedding([text])\n",
    "    rows.append(\n",
    "        {\n",
    "            \"text\": text,\n",
    "            \"source\": chunk.metadata[\"source\"],\n",
    "            \"page_label\": chunk.metadata[\"page\"],\n",
    "            \"sparse_embedding_values\": embeddings[0].values,\n",
    "            \"sparse_embedding_indices\": embeddings[0].indices,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_points(df) -> list[PointStruct]:\n",
    "    points = []\n",
    "    idx = 1\n",
    "    for row in df.iter_rows(named=True):\n",
    "        sparse_vector = SparseVector(\n",
    "            indices=row[\"sparse_embedding_indices\"].tolist(),\n",
    "            values=row[\"sparse_embedding_values\"].tolist(),\n",
    "        )\n",
    "        point = PointStruct(\n",
    "            id=idx,\n",
    "            payload={\n",
    "                \"page\": row[\"page_label\"],\n",
    "                \"source\": row[\"source\"],\n",
    "                \"text\": row[\"text\"],\n",
    "            },\n",
    "            vector={\n",
    "                \"text-sparse\": sparse_vector,\n",
    "            },\n",
    "        )\n",
    "        points.append(point)\n",
    "        idx += 1\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "points: list[PointStruct] = make_points(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 458/458 [00:49<00:00,  9.31it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "for i in tqdm(range(0, len(points), batch_size)):\n",
    "    batch = points[i : i + batch_size]\n",
    "    qdrant_client.upsert(collection_name, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseHandlingException",
     "evalue": "timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:126\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    125\u001b[39m exc_map: ExceptionMapping = {socket.timeout: ReadTimeout, \u001b[38;5;167;01mOSError\u001b[39;00m: ReadError}\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/qdrant_client/http/api_client.py:116\u001b[39m, in \u001b[36mApiClient.send_inner\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    250\u001b[39m     resp = \u001b[38;5;28mself\u001b[39m._pool.handle_request(req)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mResponseHandlingException\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m snapshot_info = \u001b[43mqdrant_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_snapshot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m snapshot_url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/collections/test_collection/snapshots/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msnapshot_info.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/qdrant_client/qdrant_client.py:2696\u001b[39m, in \u001b[36mQdrantClient.create_snapshot\u001b[39m\u001b[34m(self, collection_name, wait, **kwargs)\u001b[39m\n\u001b[32m   2682\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create snapshot for a given collection.\u001b[39;00m\n\u001b[32m   2683\u001b[39m \n\u001b[32m   2684\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2692\u001b[39m \u001b[33;03m    Snapshot description\u001b[39;00m\n\u001b[32m   2693\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2694\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) == \u001b[32m0\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2696\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_snapshot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/qdrant_client/qdrant_remote.py:3144\u001b[39m, in \u001b[36mQdrantRemote.create_snapshot\u001b[39m\u001b[34m(self, collection_name, wait, **kwargs)\u001b[39m\n\u001b[32m   3139\u001b[39m     snapshot = \u001b[38;5;28mself\u001b[39m.grpc_snapshots.Create(\n\u001b[32m   3140\u001b[39m         grpc.CreateSnapshotRequest(collection_name=collection_name), timeout=\u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m   3141\u001b[39m     ).snapshot_description\n\u001b[32m   3142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GrpcToRest.convert_snapshot_description(snapshot)\n\u001b[32m-> \u001b[39m\u001b[32m3144\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopenapi_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msnapshots_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_snapshot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait\u001b[49m\n\u001b[32m   3146\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/qdrant_client/http/api/snapshots_api.py:746\u001b[39m, in \u001b[36mSyncSnapshotsApi.create_snapshot\u001b[39m\u001b[34m(self, collection_name, wait)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_snapshot\u001b[39m(\n\u001b[32m    739\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    740\u001b[39m     collection_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    741\u001b[39m     wait: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    742\u001b[39m ) -> m.InlineResponse20012:\n\u001b[32m    743\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    744\u001b[39m \u001b[33;03m    Create new snapshot for a collection\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_for_create_snapshot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/qdrant_client/http/api/snapshots_api.py:120\u001b[39m, in \u001b[36m_SnapshotsApi._build_for_create_snapshot\u001b[39m\u001b[34m(self, collection_name, wait)\u001b[39m\n\u001b[32m    117\u001b[39m     query_params[\u001b[33m\"\u001b[39m\u001b[33mwait\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(wait).lower()\n\u001b[32m    119\u001b[39m headers = {}\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mInlineResponse20012\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/collections/\u001b[39;49m\u001b[38;5;132;43;01m{collection_name}\u001b[39;49;00m\u001b[33;43m/snapshots\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/qdrant_client/http/api_client.py:89\u001b[39m, in \u001b[36mApiClient.request\u001b[39m\u001b[34m(self, type_, method, url, path_params, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mint\u001b[39m(kwargs[\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     88\u001b[39m request = \u001b[38;5;28mself\u001b[39m._client.build_request(method, url, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/qdrant_client/http/api_client.py:106\u001b[39m, in \u001b[36mApiClient.send\u001b[39m\u001b[34m(self, request, type_)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: Request, type_: Type[T]) -> T:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmiddleware\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend_inner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m200\u001b[39m, \u001b[32m201\u001b[39m, \u001b[32m202\u001b[39m]:\n\u001b[32m    108\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/qdrant_client/http/api_client.py:215\u001b[39m, in \u001b[36mBaseMiddleware.__call__\u001b[39m\u001b[34m(self, request, call_next)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: Request, call_next: Send) -> Response:\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/RAGapps/.venv/lib/python3.12/site-packages/qdrant_client/http/api_client.py:118\u001b[39m, in \u001b[36mApiClient.send_inner\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    116\u001b[39m     response = \u001b[38;5;28mself\u001b[39m._client.send(request)\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResponseHandlingException(e)\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[31mResponseHandlingException\u001b[39m: timed out"
     ]
    }
   ],
   "source": [
    "snapshot_info = qdrant_client.create_snapshot(\n",
    "    collection_name=collection_name, wait=True\n",
    ")\n",
    "\n",
    "snapshot_url = f\"/collections/test_collection/snapshots/{snapshot_info.name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query_text: str):\n",
    "    # # Compute sparse and dense vectors\n",
    "    query_sparse_vectors: list[SparseEmbedding] = make_sparse_embedding([query_text])\n",
    "    # query_dense_vector: list[np.ndarray] = make_dense_embedding([query_text])\n",
    "\n",
    "    search_results = qdrant_client.search_batch(\n",
    "        collection_name=collection_name,\n",
    "        requests=[\n",
    "            SearchRequest(\n",
    "                vector=NamedSparseVector(\n",
    "                    name=\"text-sparse\",\n",
    "                    vector=SparseVector(\n",
    "                        indices=query_sparse_vectors[0].indices.tolist(),\n",
    "                        values=query_sparse_vectors[0].values.tolist(),\n",
    "                    ),\n",
    "                ),\n",
    "                limit=10,\n",
    "                with_payload=True,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fy/zx0sb02j5rv66mcpx65ldmvc0000gn/T/ipykernel_38672/2870104145.py:6: DeprecationWarning: `search_batch` method is deprecated and will be removed in the future. Use `query_batch_points` instead.\n",
      "  search_results = qdrant_client.search_batch(\n"
     ]
    }
   ],
   "source": [
    "query_text = \"reinforcement learning\"\n",
    "search_results = search(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ScoredPoint(id=453329, version=453, score=6.888141, payload={'page': 382, 'source': '/Users/carsten/Documents/Science/ReinforcementLearning/Reinforcement Learning An introduction.pdf', 'text': 'several publications that propose theories of behavioral vigor based on reinforcement\\nlearning.\\nWe turn now to the subject of learning when reinforcing stimuli occur well after the\\nevents they reinforce. The mechanisms used by reinforcement learning algorithms to\\nenable learning with delayed reinforcement—eligibility traces and TD learning—closely\\ncorrespond to psychologists’ hypotheses about how animals can learn under these condi-\\ntions.\\n14.4\\nDelayed Reinforcement'}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=300346, version=300, score=6.8126993, payload={'page': 47, 'source': '/Users/carsten/Documents/Science/NotCategorized/Foundations of Deep Reinforcement Learning.pdf', 'text': '1.6 Reinforcement Learning and Supervised Learning\\n19\\n1.6\\nReinforcement Learning and\\nSupervised Learning\\nAt the core of deep reinforcement learning is function approximation. This is something\\nit shares with supervised learning (SL).6 However, reinforcement learning is unlike\\nsupervised learning in a number of ways. There are three main differences:\\n. Lack of an oracle7\\n. Sparsity of feedback\\n. Data generated during training\\n1.6.1\\nLack of an Oracle'}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=453178, version=453, score=6.803466, payload={'page': 362, 'source': '/Users/carsten/Documents/Science/ReinforcementLearning/Reinforcement Learning An introduction.pdf', 'text': 'at the core of reinforcement learning is contributing to our understanding of otherwise\\npuzzling features of animal learning and behavior.\\nSome of the correspondences between reinforcement learning and psychological theories\\nare not surprising because the development of reinforcement learning drew inspiration\\nfrom psychological learning theories. However, as developed in this book, reinforcement\\nlearning explores idealized situations from the perspective of an artiﬁcial intelligence'}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=266021, version=266, score=6.788436, payload={'page': 39, 'source': '/Users/carsten/Documents/Science/DeepLearning/Deep Learning Book.pdf', 'text': 'of reinforcement learning. In the context of reinforcement learning, an autonomous\\nagent must learn to perform a task by trial and error, without any guidance from\\nthe human operator. DeepMind demonstrated that a reinforcement learning system\\nbased on deep learning is capable of learning to play Atari video games, reaching\\nhuman-level performance on many tasks (\\n,\\n). Deep learning has\\nMnih et al. 2015\\nalso signiﬁcantly improved the performance of reinforcement learning for robotics\\n(\\n,\\n).\\nFinn et al. 2015'}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=189970, version=189, score=6.7852564, payload={'page': 431, 'source': '/Users/carsten/Documents/Science/Springer/ComputerScience/2018_Book_NeuralNetworksAndDeepLearning.pdf', 'text': '416\\nCHAPTER 9. DEEP REINFORCEMENT LEARNING\\nReinforcement learning can also improve deep learning models. This is achieved with the\\nnotion of attention [338, 540], in which reinforcement learning is used to focus on selective\\nparts of the data. The idea is that large parts of the data are often irrelevant for learning, and\\nlearning how to focus on selective portions of the data can signiﬁcantly improve results. The\\nselection of relevant portions of the data is achieved with reinforcement learning. Attention'}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=368927, version=368, score=6.7712884, payload={'page': 287, 'source': '/Users/carsten/Documents/Science/Bayes/Probabilistic Numerics.pdf', 'text': '276\\nV Global Optimisation\\n▶34.2\\nReinforcement Learning\\nBayesian optimisation has connections to, but distinctions from\\nreinforcement learning.6 Both reinforcement learning and Bayes-\\n6 Sutton and Barto (1998)\\nian optimisation address a (partially-observed) Markov decision\\nprocess. As a first point of distinction, however, reinforcement\\nlearning and Bayesian optimisation tackle slightly different prob-\\nlems. Reinforcement learning cares about the returned evalua-'}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=189649, version=189, score=6.7582893, payload={'page': 389, 'source': '/Users/carsten/Documents/Science/Springer/ComputerScience/2018_Book_NeuralNetworksAndDeepLearning.pdf', 'text': '374\\nCHAPTER 9. DEEP REINFORCEMENT LEARNING\\nA reward-driven trial-and-error process, in which a system learns to interact with a\\ncomplex environment to achieve rewarding outcomes, is referred to in machine learning\\nparlance as reinforcement learning. In reinforcement learning, the process of trial and error\\nis driven by the need to maximize the expected rewards over time. Reinforcement learning\\ncan be a gateway to the quest for creating truly intelligent agents such as game-playing'}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=279749, version=279, score=6.7539215, payload={'page': 358, 'source': '/Users/carsten/Documents/Science/Science/The Alignment Problem.pdf', 'text': 'Diuk, Cohen, and Littman, “An Object-Oriented Representation for Efficient Reinforcement\\nLearning,” which used the game Pitfall! as an environment for reinforcement learning.\\n5.\\n See Gendron-Bellemare, “Fast, Scalable Algorithms for Reinforcement Learning in High\\nDimensional Domains.”\\n6.\\n Mnih et al., “Playing Atari with Deep Reinforcement Learning.”\\n7.\\n Mnih et al., “Human-Level Control Through Deep Reinforcement Learning.”\\n8.\\n Robert Jaeger, interviewed by John Hardie,'}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=451170, version=451, score=6.7484503, payload={'page': 23, 'source': '/Users/carsten/Documents/Science/ReinforcementLearning/Reinforcement Learning An introduction.pdf', 'text': 'because it does not rely on examples of correct behavior, reinforcement learning is trying\\nto maximize a reward signal instead of trying to ﬁnd hidden structure. Uncovering\\nstructure in an agent’s experience can certainly be useful in reinforcement learning, but by\\nitself does not address the reinforcement learning problem of maximizing a reward signal.\\nWe therefore consider reinforcement learning to be a third machine learning paradigm,'}, vector=None, shard_key=None, order_value=None),\n",
       "  ScoredPoint(id=451306, version=451, score=6.73392, payload={'page': 40, 'source': '/Users/carsten/Documents/Science/ReinforcementLearning/Reinforcement Learning An introduction.pdf', 'text': 'reinforcement learning, and to our eventual focus on reinforcement learning. Much of\\nthe early work that we and colleagues accomplished was directed toward showing that\\nreinforcement learning and supervised learning were indeed di↵erent (Barto, Sutton, and\\nBrouwer, 1981; Barto and Sutton, 1981b; Barto and Anandan, 1985). Other studies\\nshowed how reinforcement learning could address important problems in artiﬁcial neural'}, vector=None, shard_key=None, order_value=None)]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScoredPoint(id=6863, version=0, score=3.4013352394104004, payload={'page': 198, 'source': '/Users/carsten/Documents/Science/LLM/Learn Generative AI with PyTorch.pdf', 'text': '# obtain average encoding for each group\\n_,_,women_g_encodings=vae.encoder(women_g_batch)\\nwomen_g_encoding=women_g_encodings.mean(dim=0)\\n_,_,men_ng_encodings=vae.encoder(men_ng_batch)\\nmen_ng_encoding=men_ng_encodings.mean(dim=0)\\n_,_,women_ng_encodings=vae.encoder(women_ng_batch)\\nwomen_ng_encoding=women_ng_encodings.mean(dim=0)                  #D\\n# decode for each group\\nwomen_g_recon=vae.decoder(women_g_encoding.unsqueeze(0))\\nmen_ng_recon=vae.decoder(men_ng_encoding.unsqueeze(0))'}, vector=None, shard_key=None, order_value=None)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/carsten/Documents/Science/LLM/Learn Generative AI with PyTorch.pdf'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results[0][0].payload[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///Users/carsten/Documents/Science/LLM/Learn%20Generative%20AI%20with%20PyTorch.pdf#page=194\n"
     ]
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "result = search_results[0][1]\n",
    "\n",
    "url = f\"file://{result.payload['source']}#page={result.payload['page']}\"\n",
    "url = url.replace(\" \", \"%20\")\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome()  # Optional argument, if not specified will search path.\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url2 = \"file:///Users/carsten/Documents/Science/LLM/Learn%20Generative%20AI%20with%20PyTorch.pdf#page=198\"\n",
    "webbrowser.open(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome()  # Optional argument, if not specified will search path.\n",
    "driver.get(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
